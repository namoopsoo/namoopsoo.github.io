

### quick outline
- Does the "k area" metric help?
- In an earlier notebook I calculated logloss w/ random data and got `4.29` , discussing  `random_logloss`  and `uniform_logloss` [here](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-04-aws.md) . In general, logloss maybe it has a theoretical worst case basically . 


#### the k area metric
In the notebook [here](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-11-local.md) I have been evaluating some of the results of a multi day hyper parameter tuning session that has been running in [here](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-10-aws.md). ( First mini tuning session also [here](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-09-aws.md) ).

I started discussing this [here](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-11-local.md#karea).

Also, first started calculating some of this data in [this notebook](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-04-aws.md)

Here is one excerpt from there
<img src="https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-04-aws_files/2020-07-04-aws_34_0.png">
