---
title: Back prop from scratch 2022-10-02
date: 2022-10-02
---
<!-- directives: [] -->
<div id="content">
  <ul>
    <li>my backprop SGD from scratch 2022-Aug
      <ul>
        <li>14:13 ok reviewing from last time ,
          <ul>
            <li>Yea so I had switched from relu to sigmoid on commit <code>b88ef76daf</code> , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid  therefore was only able to produce values greater than 0.5 anyway.</li>
            <li>So at this point one thought I have for sure is whether this network is just one layer more complicated than would be needed for a problem set this simple. The thought arose after seeing that weight output from last training, 
              <p>  
                <br />
  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-09-26T002513-weights_1664736071405_0.png" title="2022-09-26T002513-weights.png" />
                <br />
</p>
            </li>
            <li>But in any case, I think for now I am curious if I can find more bugs.</li>
          </ul>
        </li>
        <li>So, we are underfitting here. So the loss is just increasing steadily and I see the layer 1 and layer 2 weights are just increasing steadily as well. So makes me think this is related.
          <ul>
            <li>16:02 let me try to observe the updates , 
              <p>  
                <br />
</p>

              <pre><code data-lang="python" class="python">  import network as n
  import dataset
  import plot
  import runner
  import ipdb
  import matplotlib.pyplot as plt
  import pylab
  from collections import Counter
  from utils import utc_now, utc_ts
  
  data = dataset.build_dataset_inside_outside_circle(0.5)
  parameters = {&quot;learning_rate&quot;: 0.01,
                &quot;steps&quot;: 50,
                &quot;log_loss_every_k_steps&quot;: 10
  
               }
  
  runner.train_and_analysis(data, parameters)
  

</code></pre>
            </li>
            <li></li>
          </ul>
        </li>
      </ul>
    </li>
    <li>17:26 ah well spotted one silly bug in tracking the metrics, so I had the train and validation loss I was logging flipped, 
      <p>  
        <br />
</p>

      <pre><code data-lang="python" class="python">  if step % log_loss_every_k_steps == 0:
      _, total_loss = loss(model, data.X_validation, data.Y_validation)
      metrics[&quot;train&quot;][&quot;loss_vec&quot;].append(total_loss)
  
      _, total_loss = loss(model, data.X_train, data.Y_train)
      metrics[&quot;validation&quot;][&quot;loss_vec&quot;].append(total_loss)

</code></pre>

      <p>  fixed now so it is 
        <br />
</p>

      <pre><code data-lang="python" class="python">  if step % log_loss_every_k_steps == 0:
      _, total_loss = loss(model, data.X_validation, data.Y_validation)
      metrics[&quot;validation&quot;][&quot;loss_vec&quot;].append(total_loss)
  
      _, total_loss = loss(model, data.X_train, data.Y_train)
      metrics[&quot;train&quot;][&quot;loss_vec&quot;].append(total_loss)
  

</code></pre>

      <p>  A bug indeed, but would not affect the training itself .
        <br />
</p>
    </li>
    <li>Ok I think good thing to do next, continue my low level debugging such that as I calculate <code>g</code> , if gradient descent is working properly, then I should be able to write an assert that I think the loss at least for the single example should decrease after applying <code>g</code> update, otherwise something is wrong !</li>
    <li>19:27 ok to check this I then have to calculate the loss on the micro-batch I&apos;m using here,
      <ul>
        <li>ok, first here is how I would reshape a single example to obtain its loss, 
          <p>  
            <br />
</p>

          <pre><code data-lang="python" class="python">  i = 0 
  x, y = data.X_train[i], data.Y_train[i]
  x.shape, y.shape
  
  Y_actual, total_loss = n.loss(model, x.reshape((1, -1)), y.reshape((1, 1)))
  print(&quot;(x, y)&quot;, (x, y))
  print(&quot;Y_actual&quot;, Y_actual)
  print(&quot;loss&quot;, total_loss)

</code></pre>

          <pre><code data-lang="python" class="python">  (x, y) (array([ -7.55637702, -12.67353685]), 1)                                                                      
  Y_actual [0.93243955]
  loss 0.06995095896007311

</code></pre>
        </li>
        <li>And side not I realized technically I&apos;m not plotting the training loss, since the training set has <code>9,000</code> rows and I&apos;m only really using <code>500</code> or so of them so far. So I will adjust the training loss calculation for specifically that portion I use.</li>
        <li>20:27 ok cool, going to try out this new code where I also now am logging the before and after for each microbatch loss 
          <pre><code data-lang="python" class="python">  import network as n
  import dataset
  import plot
  import runner
  import ipdb
  import matplotlib.pyplot as plt
  import pylab
  from collections import Counter
  from utils import utc_now, utc_ts
  
  data = dataset.build_dataset_inside_outside_circle(0.5)
  parameters = {&quot;learning_rate&quot;: 0.01,
                &quot;steps&quot;: 500,
                &quot;log_loss_every_k_steps&quot;: 10
               }
  
  model, artifacts, metrics = runner.train_and_analysis(data, parameters)
   outer: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [00:12&lt;00:00, 40.35it/s]
                                                                                                                     saving to 2022-10-03T003158.png                                                                                      
  2022-10-03T003158.png
  2022-10-03T003159-weights.png
  2022-10-03T003200-hist.png
  saving to 2022-10-03T003201-scatter.png
  2022-10-03T003201-scatter.png

</code></pre>

          <p>  And let me look at those micro batch updates then 
            <br />
</p>

          <pre><code data-lang="python" class="python">  In [8]: metrics[&quot;micro_batch_updates&quot;][:5]
  Out[8]: 
  [{&apos;loss_before&apos;: 0.43903926069642474,
    &apos;y_actual_before&apos;: array([0.64465547]),
    &apos;x&apos;: array([-9.44442228,  1.4129736 ]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.43757904199626413,
    &apos;y_actual_after&apos;: array([0.6455975])},
   {&apos;loss_before&apos;: 1.0263273283159982,
    &apos;y_actual_before&apos;: array([0.64167946]),
    &apos;x&apos;: array([-3.4136343 , 17.13301918]),
    &apos;y&apos;: 0,
    &apos;loss_after&apos;: 1.0309406841349795,
    &apos;y_actual_after&apos;: array([0.64332871])},
   {&apos;loss_before&apos;: 0.4300753021013386,
    &apos;y_actual_before&apos;: array([0.65046011]),
    &apos;x&apos;: array([-2.26675345, -5.20582749]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.4285424015973017,
    &apos;y_actual_after&apos;: array([0.65145797])},
   {&apos;loss_before&apos;: 1.0544704530873739,
    &apos;y_actual_before&apos;: array([0.65162314]),
    &apos;x&apos;: array([ 14.74873303, -16.34664216]),
    &apos;y&apos;: 0,
    &apos;loss_after&apos;: 1.0598453040833464,
    &apos;y_actual_after&apos;: array([0.65349059])},
   {&apos;loss_before&apos;: 0.42370781274874675,
    &apos;y_actual_before&apos;: array([0.65461512]),
    &apos;x&apos;: array([  1.71615885, -11.0142264 ]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.42217509911520096,
    &apos;y_actual_after&apos;: array([0.65561923])}]
  
  import matplotlib.pyplot as plt
  from utils import utc_now, utc_ts
  import pylab
  
  deltas = [x[&quot;loss_after&quot;] - x[&quot;loss_before&quot;] for x in metrics[&quot;micro_batch_updates&quot;]]
  with plt.style.context(&quot;fivethirtyeight&quot;):
      plt.hist(deltas, bins=50)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()
  
  # saving to 2022-10-03T005623-micro-batch-loss-deltas.png

</code></pre>

          <p>  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-10-03T005623-micro-batch-loss-deltas_1664758696808_0.png" title="2022-10-03T005623-micro-batch-loss-deltas.png" />
            <br />
</p>
        </li>
        <li>Wow fascinating, so a lot of the loss is getting reduced, at least slightly more than not haha, 
          <pre><code data-lang="python" class="python">  In [17]: from collections import Counter
      ...: Counter([&quot;loss_reduction&quot; if x &lt; 0 else &quot;loss_increase&quot; for x in [y for y in deltas if y != 0]])
  Out[17]: Counter({&apos;loss_reduction&apos;: 260, &apos;loss_increase&apos;: 240})

</code></pre>

          <p>  And 
            <br />
</p>

          <pre><code data-lang="python" class="python">  with plt.style.context(&quot;fivethirtyeight&quot;):
      plt.plot(deltas)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()

</code></pre>

          <p>  But wow, this next plot is fascinating! 
            <br />
  
            <br />
</p>

          <pre><code data-lang="python" class="python">  with plt.style.context(&quot;fivethirtyeight&quot;):
      fig = plt.figure(figsize =(20, 9))
  
      plt.plot(deltas, linewidth=0.7)
      plt.title(&quot;Microbatch loss_after - loss_before&quot;)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()

</code></pre>

          <p>  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-10-03T010957-micro-batch-loss-deltas-over-steps_1664759458065_0.png" title="2022-10-03T010957-micro-batch-loss-deltas-over-steps.png" />
            <br />
</p>
        </li>
        <li>So according to the above, yes the microbatch delta loss is ping ponging back and forth and basically getting worse, for the different microbatch inputs . Wow. so glad I looked at this chronological kind of plot !</li>
        <li></li>
        <li></li>
      </ul>
    </li>
  </ul>
</div>


