---
title: Back prop from scratch 2022-10-12
date: 2022-10-12
---

<!-- directives: [] -->
<div id="content">
  <ul>
    <li>ok <a class="tag">[[my backprop SGD from scratch 2022-Aug]]</a>
      <ul>
        <li>looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. 
          <pre><code data-lang="python" class="python">		    deltas = [x[&quot;loss_after&quot;] - x[&quot;loss_before&quot;] for x in metrics[&quot;micro_batch_updates&quot;]]

</code></pre>

          <p>		  although initially the values were some negatives, as well.
            <br />
</p>

          <ul>
            <li>But I wonder does it indeed something is terribly wrong if this number ever goes up at all? I think maybe yes unless this indicates the learning rate is still too high ? I am using <code>0.01</code> , but maybe it is still too high when using a single example at a time.</li>
            <li>Ok let me try even smaller learning rate, 
              <pre><code data-lang="python" class="python">			  
import network as n
import dataset
import plot
import runner
import ipdb
import matplotlib.pyplot as plt
import pylab
from collections import Counter
from utils import utc_now, utc_ts

data = dataset.build_dataset_inside_outside_circle(0.5)
parameters = {&quot;learning_rate&quot;: 0.001,
              &quot;steps&quot;: 1000,
              &quot;log_loss_every_k_steps&quot;: 10
             }

model, artifacts, metrics = runner.train_and_analysis(data, parameters)

</code></pre>

              <pre><code>			  outer: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:11&lt;00:00, 83.82it/s]
			                                                                                                             saving to 2022-10-12T175402.png                                                                                
			  2022-10-12T175402.png
			  2022-10-12T175403-weights.png
			  2022-10-12T175404-hist.png
			  saving to 2022-10-12T175404-scatter.png
			  2022-10-12T175404-scatter.png
			  saving to 2022-10-12T175404-micro-batch-loss-deltas-over-steps.png
			  2022-10-12T175404-micro-batch-loss-deltas-over-steps.png

</code></pre>

              <p>			  
                <br />
			  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175402_1665597462150_0.png" title="2022-10-12T175402.png" />
                <br />
			  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175403-weights_1665597472123_0.png" title="2022-10-12T175403-weights.png" />
                <br />
			  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175404-hist_1665597478395_0.png" title="2022-10-12T175404-hist.png" />
                <br />
			  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175404-scatter_1665597484623_0.png" title="2022-10-12T175404-scatter.png" />
                <br />
			  <img src="https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175404-micro-batch-loss-deltas-over-steps_1665597490756_0.png" title="2022-10-12T175404-micro-batch-loss-deltas-over-steps.png" />
                <br />
</p>
            </li>
            <li>14:01 well the worsening of the loss trend is still there and  only thing that seems to have changed is the scale difference in the loss is now proportionally smaller, following the reduction of the learning rate from <code>0.01</code> to <code>0.001</code> I suppose</li>
            <li>So yea wondering if I ought to next just look for more bugs or consider increasing the batch size from one to more.</li>
            <li>Oh yea and in any case the fact that the train loss is slightly worse than the validation loss is another red flag. And of course loss in both cases is going up so yea still fundamental problems.</li>
          </ul>
        </li>
        <li>Matt Mazur article. will look again.</li>
        <li>14:46 going to do a super simple test of the feed forward now. Unclear what the problem is maybe there is some fundamental matrix multiplication bug?
          <ul>
            <li>ok , starting with a blank network, with random weights, going to follow one or two inputs to the end, 
              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
import network as n
parameters = {&quot;learning_rate&quot;: 0.01}
model = n.initialize_model(parameters)

def feed_forward_manually(model, x):
    x1, x2 = x[0], x[1]
    w1, w2, w3 = model.layers[0].weights[0]
    w4, w5, w6 = model.layers[0].weights[1]
    h1 = n.logit_to_prob(x1*w1 + x2*w4 + 1)
    h2 = n.logit_to_prob(x1*w2 + x2*w5 + 1)
    h3 = n.logit_to_prob(x1*w3 + x2*w6 + 1)

    
    w7, w8 = model.layers[1].weights[0]
    w9, w10 = model.layers[1].weights[1]
    w11, w12 = model.layers[1].weights[2]
    h4 = n.logit_to_prob(h1*w7 + h2*w9 + h3*w11 + 1)
    h5 = n.logit_to_prob(h1*w8 + h1*w10 + h3*w12 + 1)
    
    w13 = model.layers[2].weights[0][0]
    w14 = model.layers[2].weights[1][0]
    
    y_prob = n.logit_to_prob(h4*w13 + h5*w14 + 1)
    return y_prob
    
x = [1, 2]
y_prob_manually = feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)

# y_prob_manually 0.7103884357305136 y_prob_mat_mul 0.47438553403530753
			  

</code></pre>
            </li>
            <li>15:57 ok well is this a bug? not sure why this is different. But maybe this is a good test then?!</li>
            <li>And in any case sort of perhaps I should be randomizing and also doing updates on the bias term as well. But yea first should make sure this feed forward works as expected.</li>
            <li>And in addition I&apos;m seeing, kind of weird but for some hand selected inputs basically the outputs seem to be kind of tightly constrained. Not much movement here , that&apos;s not ideal , 
              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
In [32]: x = [-1, 20]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.711842713079452 y_prob_mat_mul 0.4761151735416374

In [33]: x = [10, 20]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.7105989141221917 y_prob_mat_mul 0.47474962375739577

In [34]: x = [10, 200]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.711961055211584 y_prob_mat_mul 0.4762497657849592

In [35]: x = [0, 0]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.7105745880018329 y_prob_mat_mul 0.4745660467842152

In [36]: x = [-5, -5]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.7111368385474176 y_prob_mat_mul 0.4751433263136077
			  

</code></pre>
            </li>
            <li>maybe I can pinpoint which layer has the bug? 
              <pre><code data-lang="python" class="python">			  
from pprint import pprint
import test_feed_forward
x = [-5, -5]
frozen = test_feed_forward.feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
pprint([
  [&quot;--&quot;, &quot;manually&quot;, &quot;matmul&quot;],
  [&quot;h1&quot;, frozen[&quot;h1&quot;], model.layers[0].nodes[&quot;h1&quot;]],
  [&quot;h2&quot;, frozen[&quot;h2&quot;], model.layers[0].nodes[&quot;h2&quot;]],
  [&quot;h3&quot;, frozen[&quot;h3&quot;], model.layers[0].nodes[&quot;h3&quot;]],
  [&quot;h4&quot;, frozen[&quot;h4&quot;], model.layers[1].nodes[&quot;h4&quot;]],
  [&quot;h5&quot;, frozen[&quot;h5&quot;], model.layers[1].nodes[&quot;h5&quot;]],
  [&quot;y_prob&quot;, frozen[&quot;y_prob&quot;], y_prob_mat_mul],
])

</code></pre>

              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
[[&apos;--&apos;, &apos;manually&apos;, &apos;matmul&apos;],
 [&apos;h1&apos;, 0.44128214463701015, 0.44128214463701015],
 [&apos;h2&apos;, 0.9894985068325902, 0.9894985068325902],
 [&apos;h3&apos;, 0.7973686345809591, 0.7973686345809591],
 [&apos;h4&apos;, 0.7643256444514099, 0.7643256444514099],
 [&apos;h5&apos;, 0.7752070858908596, 0.7604738710471179],
 [&apos;y_prob&apos;, 0.7111368385474176, 0.4751433263136077]]

</code></pre>

              <p>			  ok So h1, h2, h3, h4 are matching and then h5, is where the problem starts hmm .
                <br />
</p>
            </li>
            <li>16:24 ok think I found a small bug , 
              <pre><code data-lang="python" class="python">			  
from pprint import pprint
import test_feed_forward
x = [-5, -5]
frozen = test_feed_forward.feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
pprint([
  [&quot;--&quot;, &quot;manually&quot;, &quot;matmul&quot;],
  [&quot;h1&quot;, frozen[&quot;h1&quot;], model.layers[0].nodes[&quot;h1&quot;]],
  [&quot;h2&quot;, frozen[&quot;h2&quot;], model.layers[0].nodes[&quot;h2&quot;]],
  [&quot;h3&quot;, frozen[&quot;h3&quot;], model.layers[0].nodes[&quot;h3&quot;]],
  [&quot;h4&quot;, frozen[&quot;h4&quot;], model.layers[1].nodes[&quot;h4&quot;]],
  [&quot;h5&quot;, frozen[&quot;h5&quot;], model.layers[1].nodes[&quot;h5&quot;]],
  [&quot;y_prob&quot;, frozen[&quot;y_prob&quot;], y_prob_mat_mul],
])

</code></pre>

              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
[[&apos;--&apos;, &apos;manually&apos;, &apos;matmul&apos;],
 [&apos;h1&apos;, 0.44128214463701015, 0.44128214463701015],
 [&apos;h2&apos;, 0.9894985068325902, 0.9894985068325902],
 [&apos;h3&apos;, 0.7973686345809591, 0.7973686345809591],
 [&apos;h4&apos;, 0.7643256444514099, 0.7643256444514099],
 [&apos;h5&apos;, 0.7604738710471179, 0.7604738710471179],
 [&apos;y_prob&apos;, 0.7110504493887152, 0.4751433263136077]]

</code></pre>

              <p>			  So weird. ok now h5 matches. just not y_prob. that bug was in my test func.
                <br />
</p>
            </li>
            <li>hmm ok reran one more time, so I had in my test func , an extra bias term I was adding , in the final logit, but not in the main matmul feed forward func.</li>
          </ul>
        </li>
        <li>16:44 ok well then theres no bug in the feed forward func, but it is weird how tight the outputs are . Something tells me actually this is related to the hard coded bias values of 1 ?
          <ul>
            <li>Let me loosen up the bias, maybe that helps. 
              <p>			  Ok so before, 
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]]
pprint([[x, n.feed_forward(x, model.layers)] for x in data])
[[[0, 0], 0.4745660467842152],
 [[4, 5], 0.4738876007756717],
 [[-4, 5], 0.4762211475351283],
 [[-5, -5], 0.4751433263136077],
 [[5, -5], 0.4737412140572382],
 [[-20, -20], 0.4754772076192018],
 [[100, 100], 0.4748013350557756]]

</code></pre>

              <p>			  And , 
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
import numpy as np
model.layers[0] = model.layers[0]._replace(bias=np.array([0.1]))
model.layers[1] = model.layers[1]._replace(bias=np.array([0.1]))

data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]]
pprint([[x, n.feed_forward(x, model.layers)] for x in data])

</code></pre>

              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  [[[0, 0], 0.4813065143876082],
			   [[4, 5], 0.4804879738767632],
			   [[-4, 5], 0.48321160815195635],
			   [[-5, -5], 0.4823729185086716],
			   [[5, -5], 0.4793507289434747],
			   [[-20, -20], 0.4822354979795412],
			   [[100, 100], 0.4810180811163541]]

</code></pre>

              <p>			  hmm doesn&apos;t seem to have helped. Let me go lower, 
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
model.layers[0] = model.layers[0]._replace(bias=np.array([0.01]))
model.layers[1] = model.layers[1]._replace(bias=np.array([0.01]))
print(&quot;biases, &quot;, model.layers[0].bias, model.layers[1].bias, model.layers[2].bias)
# biases,  [0.01] [0.01] [0]

data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]]
pprint([[x, n.feed_forward(x, model.layers)] for x in data])

</code></pre>

              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  [[[0, 0], 0.4820947777588216],
			   [[4, 5], 0.48128620914361286],
			   [[-4, 5], 0.4839510956396274],
			   [[-5, -5], 0.4831921147269756],
			   [[5, -5], 0.4800346864013735],
			   [[-20, -20], 0.48300320046986295],
			   [[100, 100], 0.48173375233047927]]

</code></pre>
            </li>
            <li>17:03 ok so weird not even adjustments to bias helped . Just double check , with the manual feedforward too, 
              <p>			  
                <br />
</p>

              <pre><code data-lang="python" class="python">			  
from pprint import pprint
import test_feed_forward
x = [-5, -5]
frozen = test_feed_forward.feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
pprint([
  [&quot;--&quot;, &quot;manually&quot;, &quot;matmul&quot;],
  [&quot;h1&quot;, frozen[&quot;h1&quot;], model.layers[0].nodes[&quot;h1&quot;]],
  [&quot;h2&quot;, frozen[&quot;h2&quot;], model.layers[0].nodes[&quot;h2&quot;]],
  [&quot;h3&quot;, frozen[&quot;h3&quot;], model.layers[0].nodes[&quot;h3&quot;]],
  [&quot;h4&quot;, frozen[&quot;h4&quot;], model.layers[1].nodes[&quot;h4&quot;]],
  [&quot;h5&quot;, frozen[&quot;h5&quot;], model.layers[1].nodes[&quot;h5&quot;]],
  [&quot;y_prob&quot;, frozen[&quot;y_prob&quot;], y_prob_mat_mul],
])
			  

</code></pre>

              <pre><code data-lang="python" class="python">			  [[&apos;--&apos;, &apos;manually&apos;, &apos;matmul&apos;],
			   [&apos;h1&apos;, 0.22688927424734276, 0.22688927424734276],
			   [&apos;h2&apos;, 0.9722312068731663, 0.9722312068731663],
			   [&apos;h3&apos;, 0.5938559073859204, 0.5938559073859204],
			   [&apos;h4&apos;, 0.51632734891332, 0.51632734891332],
			   [&apos;h5&apos;, 0.5124838141374718, 0.5124838141374718],
			   [&apos;y_prob&apos;, 0.4831921147269756, 0.4831921147269756]]
			  

</code></pre>

              <p>			  Ok yea, so looks like no bug and reducing the bias has not diminished how frozen the outputs seem to be.
                <br />
</p>
            </li>
            <li>17:11 so yea for now , feels like it is good I verified the feed forward func does what it is supposed to, but it is super weird that the network is really tightly configured. Super weird.</li>
            <li>Maybe I should not have activation functions on the inner layers? Nah I don&apos;t think that&apos;s the problem.</li>
            <li>Makes me wonder what about something about this particular multi-layer network architecture that is being weird? Maybe I should try different architectures? Do they have similar properties?</li>
            <li></li>
            <li></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</div>


