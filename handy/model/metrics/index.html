<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>michal.piekarczyk.xyz</title>
<meta name=keywords content><meta name=description content="TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob >= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=['actual'], colnames=['predictions']) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/handy/model/metrics/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content><meta property="og:description" content="TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob >= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=['actual'], colnames=['predictions']) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/handy/model/metrics/"><meta property="article:section" content="handy"><meta property="article:modified_time" content="2023-02-26T18:33:54-05:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob >= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=['actual'], colnames=['predictions']) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Handies","item":"https://michal.piekarczyk.xyz/handy/"},{"@type":"ListItem","position":2,"name":"","item":"https://michal.piekarczyk.xyz/handy/model/metrics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob \u0026gt;= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=[\u0026#39;actual\u0026#39;], colnames=[\u0026#39;predictions\u0026#39;]) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook.","keywords":[],"articleBody":"TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob \u003e= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=['actual'], colnames=['predictions']) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook.com/ .\nSo first, when using ConfusionMatrixDisplay out of the box, I get\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix with plt.style.context('dark_background'): cm = confusion_matrix(y_valid, y_preds, normalize=\"true\") disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot() plt.show() And I can see why that was not used in the book haha because the modified version, below, looks much better indeed,\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix def plot_confusion_matrix(y_preds, y_true, labels): with plt.style.context(\"dark_background\"): cm = confusion_matrix(y_true, y_preds, normalize=\"true\") fix, ax = plt.subplots(figsize=(6, 6)) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False) plt.title(\"Normalized confusion matrix\") plt.show() y_preds = lr_clf.predict(X_valid) plot_confusion_matrix(y_preds, y_valid, labels) f1 def calc_f1(confusion): TN = confusion.loc[0, 0] FP = confusion.loc[0, 1] FN = confusion.loc[1, 0] TP = confusion.loc[1, 1] precision = TP/(FP + TP) recall = TP/(FN + TP) return 2*(precision**2)/(precision + recall) predictions = [] # list of probabilities , e.g. array([0.05567192, 0.03781519, 0.05437384, 0.01572161, ...]) cutoffs = np.arange(0.01, 0.5, 0.01) f1_vec = [] for c in cutoffs: confusion = pd.crosstab(index=testdf.iloc[:, 0], columns= (predictions \u003e c), rownames=['actual'], colnames=['predictions']) try: f1 = calc_f1(confusion) except TypeError: f1 = np.nan f1_vec.append(f1) # fig = plt.figure() plt.plot(cutoffs, np.array(f1_vec)) plt.xlabel('cutoff') plt.ylabel('f1') plt.show() ks for a cutoff def get_flargs(confusion): cols = confusion.columns.tolist() if False not in cols: TN = 0 FN = 0 else: TN = confusion.loc[0, False] # loc[0, 0] this works in newer pandas , not 0.18 FN = confusion.loc[1, False] if True not in cols: FP = 0 TP = 0 else: FP = confusion.loc[0, True] TP = confusion.loc[1, True] return (TP, FP, TN, FN) def calc_f1(TP, FP, TN, FN): if (FP + TP) == 0 or (FN + TP) == 0: return np.nan precision = 1.0*TP/(FP + TP) recall = 1.0*TP/(FN + TP) return {2*(precision*recall)/(precision + recall)} def ks_for_cutoff(TP, FP, TN, FN): # It is the maximum difference between TPR (aka recall) and FPR () tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) return tpr - fpr def thisthings(y_true, y_prob): cutoffs = np.arange(0.01, 1.0, 0.01) f1_vec = [] ks_vec = [] tpr_vec = [] fpr_vec = [] tnr_vec = [] for c in cutoffs: y_pred = (y_prob \u003e c) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=['actual'], colnames=['predictions']) # print (c, confusion.shape, confusion.columns.tolist()) (TP, FP, TN, FN) = get_flargs(confusion) try: tpr = 1.0*TP/(FN + TP) # aka recall except: tpr = np.nan try: fpr = 1.0*FP/(FP + TN) except: fpr = np.nan tpr_vec.append(tpr) fpr_vec.append(fpr) # f1 = calc_f1(confusion) f1 = sklearn.metrics.f1_score(y_true, y_pred) f1_vec.append(f1) ks = ks_for_cutoff(TP, FP, TN, FN) ks_vec.append(ks) return [cutoffs, f1_vec, ks_vec, tpr_vec, fpr_vec, tnr_vec] [cutoffs, f1_vec, ks_vec, tpr_vec, fpr_vec, tnr_vec] = thisthings(y_true, y_prob) plt.plot(cutoffs[:20], np.array(ks_vec)[:20], label='ks') plt.plot(cutoffs[:20], np.array(fpr_vec)[:20], label='fpr') plt.plot(cutoffs[:20], np.array(tpr_vec)[:20], label='tpr') plt.xlabel('cutoff') plt.legend() plt.show() Weighted Precision Had to reverse engineer this from the source code here , https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html … But for a 2 class classification problem weighted precision is basically weighing correctness of positive predictions and correctness of negative predictions, proportional to the number of positive and negative labels in the data. weighted_precision = [(TN/(FN + TN)) * ((size (label = N))/( size (total)))] + [(TP/(FP + TP)) * ((size(label = P))/(size (total)))] References https://www.datavedas.com/model-evaluation-in-python/ ","wordCount":"582","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"2023-02-26T18:33:54-05:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/handy/model/metrics/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/handy/>Handies</a></div><h1 class=post-title></h1><div class=post-meta>3 min&amp;nbsp;·&amp;nbsp;582 words&amp;nbsp;·&amp;nbsp;Michal Piekarczyk</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#tpr-fpr>TPR, FPR</a></li><li><a href=#confusion-matrix>Confusion matrix</a></li><li><a href=#f1>f1</a></li><li><a href=#weighted-precision>Weighted Precision</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=tpr-fpr>TPR, FPR<a hidden class=anchor aria-hidden=true href=#tpr-fpr>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tpr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>TP<span style=color:#f92672>/</span>(FN <span style=color:#f92672>+</span> TP) <span style=color:#75715e># aka recall</span>
</span></span><span style=display:flex><span>fpr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>FP<span style=color:#f92672>/</span>(FP <span style=color:#f92672>+</span> TN) <span style=color:#75715e># </span>
</span></span></code></pre></div><h3 id=confusion-matrix>Confusion matrix<a hidden class=anchor aria-hidden=true href=#confusion-matrix>#</a></h3><ul><li>Given a <code>testdf</code> where first column contains actual labels, <code>0</code>, <code>1</code>, and <code>predictions</code> is a list of probabilities,</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> (y_prob <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0.08</span>)
</span></span><span style=display:flex><span>confusion <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>crosstab(index<span style=color:#f92672>=</span>y_true, 
</span></span><span style=display:flex><span>            columns<span style=color:#f92672>=</span>y_pred, 
</span></span><span style=display:flex><span>            rownames<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;actual&#39;</span>], colnames<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;predictions&#39;</span>])
</span></span></code></pre></div><table><thead><tr><th>predictions</th><th>False</th><th>True</th></tr></thead><tbody><tr><td>actual</td><td></td><td></td></tr><tr><td>0</td><td>509</td><td>132</td></tr><tr><td>1</td><td>32</td><td>22</td></tr></tbody></table><h4 id=also-there-is-a-super-nice-helper-in-scikitlearn>Also there is a super nice helper in scikitlearn<a hidden class=anchor aria-hidden=true href=#also-there-is-a-super-nice-helper-in-scikitlearn>#</a></h4><p>Below, using some pre-baked results from running some of the chapter 2 code from <a href=https://transformersbook.com/>https://transformersbook.com/</a> .</p><p>So first, when using <code>ConfusionMatrixDisplay</code> out of the box, I get</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> ConfusionMatrixDisplay, confusion_matrix
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> plt<span style=color:#f92672>.</span>style<span style=color:#f92672>.</span>context(<span style=color:#e6db74>&#39;dark_background&#39;</span>):
</span></span><span style=display:flex><span>    cm <span style=color:#f92672>=</span> confusion_matrix(y_valid, y_preds, normalize<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;true&#34;</span>)
</span></span><span style=display:flex><span>    disp <span style=color:#f92672>=</span> ConfusionMatrixDisplay(confusion_matrix<span style=color:#f92672>=</span>cm, display_labels<span style=color:#f92672>=</span>labels)
</span></span><span style=display:flex><span>    disp<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()    
</span></span></code></pre></div><p>And I can see why that was not used in the book haha because the modified version, below, looks much better indeed,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> ConfusionMatrixDisplay, confusion_matrix 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_confusion_matrix</span>(y_preds, y_true, labels):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> plt<span style=color:#f92672>.</span>style<span style=color:#f92672>.</span>context(<span style=color:#e6db74>&#34;dark_background&#34;</span>):
</span></span><span style=display:flex><span>        cm <span style=color:#f92672>=</span> confusion_matrix(y_true, y_preds, normalize<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;true&#34;</span>)
</span></span><span style=display:flex><span>        fix, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>        disp <span style=color:#f92672>=</span> ConfusionMatrixDisplay(confusion_matrix<span style=color:#f92672>=</span>cm, display_labels<span style=color:#f92672>=</span>labels)
</span></span><span style=display:flex><span>        disp<span style=color:#f92672>.</span>plot(cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Blues&#34;</span>, values_format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.2f&#34;</span>, ax<span style=color:#f92672>=</span>ax, colorbar<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Normalized confusion matrix&#34;</span>)
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_preds <span style=color:#f92672>=</span> lr_clf<span style=color:#f92672>.</span>predict(X_valid)     
</span></span><span style=display:flex><span>plot_confusion_matrix(y_preds, y_valid, labels)
</span></span></code></pre></div><h3 id=f1>f1<a hidden class=anchor aria-hidden=true href=#f1>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calc_f1</span>(confusion):
</span></span><span style=display:flex><span>    TN <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    FP <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    FN <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    TP <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    precision <span style=color:#f92672>=</span> TP<span style=color:#f92672>/</span>(FP <span style=color:#f92672>+</span> TP)
</span></span><span style=display:flex><span>    recall <span style=color:#f92672>=</span> TP<span style=color:#f92672>/</span>(FN <span style=color:#f92672>+</span> TP)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>(precision<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>/</span>(precision <span style=color:#f92672>+</span> recall)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> [] <span style=color:#75715e># list of probabilities , e.g. array([0.05567192, 0.03781519, 0.05437384, 0.01572161, ...])</span>
</span></span><span style=display:flex><span>cutoffs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0.01</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>f1_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> cutoffs:
</span></span><span style=display:flex><span>    confusion <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>crosstab(index<span style=color:#f92672>=</span>testdf<span style=color:#f92672>.</span>iloc[:, <span style=color:#ae81ff>0</span>], 
</span></span><span style=display:flex><span>                columns<span style=color:#f92672>=</span> (predictions <span style=color:#f92672>&gt;</span> c), 
</span></span><span style=display:flex><span>                rownames<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;actual&#39;</span>], colnames<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;predictions&#39;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        f1 <span style=color:#f92672>=</span> calc_f1(confusion)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>TypeError</span>:
</span></span><span style=display:flex><span>        f1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>nan
</span></span><span style=display:flex><span>    f1_vec<span style=color:#f92672>.</span>append(f1)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># fig = plt.figure()</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(cutoffs, np<span style=color:#f92672>.</span>array(f1_vec))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;cutoff&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;f1&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h4 id=ks-for-a-cutoff>ks for a cutoff<a hidden class=anchor aria-hidden=true href=#ks-for-a-cutoff>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_flargs</span>(confusion):
</span></span><span style=display:flex><span>    cols <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#66d9ef>False</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> cols:
</span></span><span style=display:flex><span>        TN <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        FN <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        TN <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>False</span>] <span style=color:#75715e># loc[0, 0] this works in newer pandas , not 0.18</span>
</span></span><span style=display:flex><span>        FN <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>1</span>, <span style=color:#66d9ef>False</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#66d9ef>True</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> cols:
</span></span><span style=display:flex><span>        FP <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        TP <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        FP <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>True</span>]
</span></span><span style=display:flex><span>        TP <span style=color:#f92672>=</span> confusion<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>1</span>, <span style=color:#66d9ef>True</span>]
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (TP, FP, TN, FN)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>calc_f1</span>(TP, FP, TN, FN):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (FP <span style=color:#f92672>+</span> TP) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>or</span> (FN <span style=color:#f92672>+</span> TP) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>nan
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    precision <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>TP<span style=color:#f92672>/</span>(FP <span style=color:#f92672>+</span> TP)
</span></span><span style=display:flex><span>    recall <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>TP<span style=color:#f92672>/</span>(FN <span style=color:#f92672>+</span> TP)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>(precision<span style=color:#f92672>*</span>recall)<span style=color:#f92672>/</span>(precision <span style=color:#f92672>+</span> recall)}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ks_for_cutoff</span>(TP, FP, TN, FN):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>#  It is the maximum difference between TPR (aka recall) and FPR ()</span>
</span></span><span style=display:flex><span>    tpr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>TP<span style=color:#f92672>/</span>(FN <span style=color:#f92672>+</span> TP) <span style=color:#75715e># aka recall</span>
</span></span><span style=display:flex><span>    fpr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>FP<span style=color:#f92672>/</span>(FP <span style=color:#f92672>+</span> TN)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tpr <span style=color:#f92672>-</span> fpr
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>thisthings</span>(y_true, y_prob):
</span></span><span style=display:flex><span>    cutoffs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0.01</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>    f1_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    ks_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    tpr_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    fpr_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    tnr_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> cutoffs:
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> (y_prob <span style=color:#f92672>&gt;</span> c)
</span></span><span style=display:flex><span>        confusion <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>crosstab(index<span style=color:#f92672>=</span>y_true, 
</span></span><span style=display:flex><span>                    columns<span style=color:#f92672>=</span>y_pred, 
</span></span><span style=display:flex><span>                    rownames<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;actual&#39;</span>], colnames<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;predictions&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># print (c, confusion.shape, confusion.columns.tolist())</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        (TP, FP, TN, FN) <span style=color:#f92672>=</span> get_flargs(confusion)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            tpr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>TP<span style=color:#f92672>/</span>(FN <span style=color:#f92672>+</span> TP) <span style=color:#75715e># aka recall</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>            tpr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>nan
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            fpr <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>*</span>FP<span style=color:#f92672>/</span>(FP <span style=color:#f92672>+</span> TN)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>            fpr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>nan
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        tpr_vec<span style=color:#f92672>.</span>append(tpr)
</span></span><span style=display:flex><span>        fpr_vec<span style=color:#f92672>.</span>append(fpr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># f1 = calc_f1(confusion)</span>
</span></span><span style=display:flex><span>        f1 <span style=color:#f92672>=</span> sklearn<span style=color:#f92672>.</span>metrics<span style=color:#f92672>.</span>f1_score(y_true, y_pred)
</span></span><span style=display:flex><span>        f1_vec<span style=color:#f92672>.</span>append(f1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ks <span style=color:#f92672>=</span> ks_for_cutoff(TP, FP, TN, FN)
</span></span><span style=display:flex><span>        ks_vec<span style=color:#f92672>.</span>append(ks)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [cutoffs,
</span></span><span style=display:flex><span>            f1_vec,
</span></span><span style=display:flex><span>            ks_vec,
</span></span><span style=display:flex><span>            tpr_vec,
</span></span><span style=display:flex><span>            fpr_vec,
</span></span><span style=display:flex><span>            tnr_vec]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[cutoffs,
</span></span><span style=display:flex><span>            f1_vec,
</span></span><span style=display:flex><span>            ks_vec,
</span></span><span style=display:flex><span>            tpr_vec,
</span></span><span style=display:flex><span>            fpr_vec,
</span></span><span style=display:flex><span>            tnr_vec] <span style=color:#f92672>=</span> thisthings(y_true, y_prob)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(cutoffs[:<span style=color:#ae81ff>20</span>], np<span style=color:#f92672>.</span>array(ks_vec)[:<span style=color:#ae81ff>20</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ks&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(cutoffs[:<span style=color:#ae81ff>20</span>], np<span style=color:#f92672>.</span>array(fpr_vec)[:<span style=color:#ae81ff>20</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;fpr&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(cutoffs[:<span style=color:#ae81ff>20</span>], np<span style=color:#f92672>.</span>array(tpr_vec)[:<span style=color:#ae81ff>20</span>], label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tpr&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;cutoff&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h3 id=weighted-precision>Weighted Precision<a hidden class=anchor aria-hidden=true href=#weighted-precision>#</a></h3><ul><li>Had to reverse engineer this from the source code here , <a href=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html</a> &mldr;</li><li>But for a 2 class classification problem weighted precision is basically weighing correctness of positive predictions and correctness of negative predictions, proportional to the number of positive and negative labels in the data.</li></ul><pre tabindex=0><code>weighted_precision = [(TN/(FN + TN)) * ((size (label = N))/( size (total)))]  + [(TP/(FP + TP)) * ((size(label = P))/(size (total)))]
</code></pre><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ul><li><a href=https://www.datavedas.com/model-evaluation-in-python/>https://www.datavedas.com/model-evaluation-in-python/</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>