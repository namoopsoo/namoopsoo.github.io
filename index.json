[{"content":"Inspiration Haha so per the joke of a friend about \u0026ldquo;Reynaud\u0026rsquo;s\u0026rdquo; sounds like \u0026ldquo;Raymond\u0026rdquo; as in \u0026ldquo;Everyone loves Raymond\u0026rdquo; , I was wondering, can sentence transformers tell the difference? But haha spoiler alert sort is is that if a model finds this funny then that might also not really understand the medical condition at play here , haha nervous laughter, ðŸ˜…\nQuick look let\u0026rsquo;s compare this first embedding model\nimport os from sentence_transformers.util import semantic_search, cos_sim from sentence_transformers import SentenceTransformer, util import torch hf_token = os.getenv(\u0026#34;HF_TOKEN\u0026#34;) embedder = SentenceTransformer( \u0026#39;msmarco-MiniLM-L-6-v3\u0026#39;, use_auth_token=hf_token, ) corpus = [\u0026#34;Everybody Loves Raymond\u0026#34;, \u0026#34; blood vessels go into a temporary spasm\u0026#34;, \u0026#34;blocked flow of blood in fingers\u0026#34;, \u0026#34;Everybody Loves Numbness\u0026#34;, \u0026#34;Everybody Loves cold fingers\u0026#34;, \u0026#34;Everybody Loves Reindeer\u0026#34;, \u0026#34;Everybody Loves Riynudno\u0026#34;] corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) query = \u0026#34;Everybody Loves Raynaud\u0026#34; query_embedding = embedder.encode(query, convert_to_tensor=True) cos_scores = cos_sim(query_embedding, corpus_embeddings)[0] top_results = torch.topk(cos_scores, k=7) cos_scores, top_results print(\u0026#34;Query:\\n\u0026#34;, query, \u0026#34;\\n\u0026#34;) for score, idx in zip(top_results[0], top_results[1]): print(corpus[idx].strip(), \u0026#34;(Score: {:.4f})\u0026#34;.format(score)) output\nQuery: Everybody Loves Raynaud Everybody Loves Raymond (Score: 0.5585) Everybody Loves Reindeer (Score: 0.4803) Everybody Loves cold fingers (Score: 0.4661) Everybody Loves Riynudno (Score: 0.4394) Everybody Loves Numbness (Score: 0.3434) blocked flow of blood in fingers (Score: 0.0891) blood vessels go into a temporary spasm (Score: -0.0003) I don\u0026rsquo;t think that embedding model is using a semantic embedding space, but rather just a more simplistic text similarity embedding. Or maybe the vocabulary is too small and simply does not include medical terminology?\nHmm okay lets look at more embedding models Ok, lets use a bunch from https://www.sbert.net/docs/pretrained_models.html , Let\u0026rsquo;s try a different maybe larger embedding model\nfrom tqdm import tqdm def compare_raymond(model_name, query, corpus): hf_token = os.getenv(\u0026#34;HF_TOKEN\u0026#34;) embedder = SentenceTransformer( model_name, use_auth_token=hf_token, ) corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) query_embedding = embedder.encode(query, convert_to_tensor=True) cos_scores = cos_sim(query_embedding, corpus_embeddings)[0] top_results = torch.topk(cos_scores, k=len(corpus)) cos_scores, top_results # print(\u0026#34;Query:\\n\u0026#34;, query, \u0026#34;\\n\u0026#34;) results = [] for score, idx in zip(top_results[0], top_results[1]): results.append( {\u0026#34;sentence\u0026#34;: corpus[idx].strip(), \u0026#34;score\u0026#34;: round(float(score), 4), \u0026#34;model_name\u0026#34;: model_name}) # print(corpus[idx].strip(), \u0026#34;(Score: {:.4f})\u0026#34;.format(score)) return results corpus = [\u0026#34;Everybody Loves Raymond\u0026#34;, \u0026#34; blood vessels go into a temporary spasm\u0026#34;, \u0026#34;blocked flow of blood in fingers\u0026#34;, \u0026#34;Everybody Loves Numbness\u0026#34;, \u0026#34;Everybody Loves cold fingers\u0026#34;, \u0026#34;Everybody Loves Reindeer\u0026#34;, \u0026#34;Everybody Loves Riynudno\u0026#34;] query = \u0026#34;Everybody Loves Raynaud\u0026#34; model_names = [\u0026#34;all-MiniLM-L12-v2\u0026#34;, \u0026#34;all-mpnet-base-v2\u0026#34;, \u0026#34;all-distilroberta-v1\u0026#34;, \u0026#34;multi-qa-distilbert-cos-v1\u0026#34;, \u0026#34;msmarco-MiniLM-L-6-v3\u0026#34;] results = [] for model_name in tqdm(model_names): print(\u0026#34;\\n=====================\u0026#34;) print(\u0026#34;Lets try\u0026#34;, model_name) results.extend(compare_raymond(model_name, query, corpus)) df = pd.DataFrame.from_records(results) Interesting Some results, so looking here, some models appear to score the semantic medical concepts higher, such as \u0026ldquo;all-MiniLM-L12-v2\u0026rdquo; , and most of the others appear to be somewhat confused haha.\nIn [63]: df.sort_values(by=[\u0026#34;model_name\u0026#34;, \u0026#34;score\u0026#34;], ascending=False) Out[63]: sentence score model_name 21 Everybody Loves Raymond 0.5439 multi-qa-distilbert-cos-v1 22 Everybody Loves Riynudno 0.4822 multi-qa-distilbert-cos-v1 23 Everybody Loves Reindeer 0.3610 multi-qa-distilbert-cos-v1 24 Everybody Loves cold fingers 0.3152 multi-qa-distilbert-cos-v1 25 Everybody Loves Numbness 0.3014 multi-qa-distilbert-cos-v1 26 blocked flow of blood in fingers 0.0435 multi-qa-distilbert-cos-v1 27 blood vessels go into a temporary spasm 0.0012 multi-qa-distilbert-cos-v1 28 Everybody Loves Raymond 0.5585 msmarco-MiniLM-L-6-v3 29 Everybody Loves Reindeer 0.4803 msmarco-MiniLM-L-6-v3 30 Everybody Loves cold fingers 0.4661 msmarco-MiniLM-L-6-v3 31 Everybody Loves Riynudno 0.4394 msmarco-MiniLM-L-6-v3 32 Everybody Loves Numbness 0.3434 msmarco-MiniLM-L-6-v3 33 blocked flow of blood in fingers 0.0891 msmarco-MiniLM-L-6-v3 34 blood vessels go into a temporary spasm -0.0003 msmarco-MiniLM-L-6-v3 7 Everybody Loves Riynudno 0.4585 all-mpnet-base-v2 8 Everybody Loves Numbness 0.4368 all-mpnet-base-v2 9 Everybody Loves cold fingers 0.4192 all-mpnet-base-v2 10 Everybody Loves Reindeer 0.3416 all-mpnet-base-v2 11 Everybody Loves Raymond 0.2641 all-mpnet-base-v2 12 blocked flow of blood in fingers 0.2256 all-mpnet-base-v2 13 blood vessels go into a temporary spasm 0.1393 all-mpnet-base-v2 14 Everybody Loves Riynudno 0.7082 all-distilroberta-v1 15 Everybody Loves Raymond 0.5761 all-distilroberta-v1 16 Everybody Loves Reindeer 0.4612 all-distilroberta-v1 17 Everybody Loves Numbness 0.4181 all-distilroberta-v1 18 Everybody Loves cold fingers 0.3880 all-distilroberta-v1 19 blocked flow of blood in fingers 0.0297 all-distilroberta-v1 20 blood vessels go into a temporary spasm -0.0023 all-distilroberta-v1 0 Everybody Loves cold fingers 0.4929 all-MiniLM-L12-v2 1 Everybody Loves Numbness 0.4379 all-MiniLM-L12-v2 2 blocked flow of blood in fingers 0.4075 all-MiniLM-L12-v2 3 Everybody Loves Riynudno 0.3814 all-MiniLM-L12-v2 4 Everybody Loves Raymond 0.3706 all-MiniLM-L12-v2 5 blood vessels go into a temporary spasm 0.3055 all-MiniLM-L12-v2 6 Everybody Loves Reindeer 0.2809 all-MiniLM-L12-v2 Okay so then Actually wow this is kind of inspiring, with respect to another problem I was facing in that if semantic similarity was not working for me earlier maybe it is because the embedding model I was using just was not trained on the subject matter I was trying to use it on.\nAnd a different one is needed or I need to fine tune one instead if that is easier.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-06-25-everybody-loves-reynauds/","summary":"Inspiration Haha so per the joke of a friend about \u0026ldquo;Reynaud\u0026rsquo;s\u0026rdquo; sounds like \u0026ldquo;Raymond\u0026rdquo; as in \u0026ldquo;Everyone loves Raymond\u0026rdquo; , I was wondering, can sentence transformers tell the difference? But haha spoiler alert sort is is that if a model finds this funny then that might also not really understand the medical condition at play here , haha nervous laughter, ðŸ˜…\nQuick look let\u0026rsquo;s compare this first embedding model\nimport os from sentence_transformers.","title":"everybody loves reynauds"},{"content":"public:: true blog-date:: 2023-06-13 Ok continuing from last time, where I ran sentence_transformers model 'msmarco-MiniLM-L-6-v3' against a code search problem of comparing query lines against a source code corpus actually from the sentence_transformers github\nThis time, I wanted to write some code around measuring the successful hits from a model run using cosine similarity from sentence_transformers\nQuery set choice I expanded the set of queries slightly, but not that much yet so I could focus on the evaluation code.\nInitially building a dataset, had a snag with reproducibility So take earlier queries, adding some more manually, and building dataset from them,\nqueries = [ \u0026#34;snapshot_download\u0026#34;, \u0026#34;SentenceEvaluator\u0026#34;, \u0026#34;get_torch_home\u0026#34;, \u0026#34;os.path.join\u0026#34;, \u0026#34;flax_model.msgpack\u0026#34;, \u0026#34;torch.nn.functional.normalize\u0026#34;, \u0026#34;BATCH_HARD_TRIPLET_LOSS\u0026#34;, \u0026#34;SentenceTransformer(\u0026#39;model-name\u0026#39;)\u0026#34;, \u0026#34;DEPRECATED: This class is no longer used\u0026#34;, \u0026#34;module_config[\u0026#39;path\u0026#39;]\u0026#34;, \u0026#34;config_sentence_transformers.json\u0026#34;, \u0026#34;os.path.join(checkpoint_path, subdir)\u0026#34; \u0026#34;torch.nn.utils.clip_grad_norm\u0026#34;, \u0026#34;getEffectiveLevel\u0026#34;, \u0026#34;torch.cuda\u0026#34;, ] corpus = [x[\u0026#34;line\u0026#34;] for x in dataset if \u0026#34;Sentence\u0026#34; in x[\u0026#34;path\u0026#34;]] import json import os import pandas as pd import code_search as cs from pathlib import Path from date_utils import utc_ts repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) target_dir = Path(repos_dir) / \u0026#34;sentence-transformers\u0026#34; dataset = cs.build_texts_from_repository(target_dir) workdir = Path(repos_dir) / \u0026#34;code_search\u0026#34; # Let me save the dataset actually, and build from there, df = pd.DataFrame.from_records(dataset) path = (workdir / \u0026#34;datasets\u0026#34; / f\u0026#34;{utc_ts()}-dataset.csv\u0026#34;) print(\u0026#34;saving\u0026#34;, path.relative_to(repos_dir)) # saving code_search/datasets/2023-06-13T164234Z-dataset.csv # Only the non-blank lines, df[df.line != \u0026#34;\u0026#34;].shape, df.shape # ((16593, 3), (21953, 3)) df.to_csv(path, index=False) df2 = pd.read_csv(path) print(df.equals(df2)) # False, oops was getting False , because of blank lines,\nIn [20]: df.iloc[:2] Out[20]: line_number line path 0 0 from setuptools import setup, find_packages setup.py 1 1 setup.py In [21]: df2.iloc[:2] Out[21]: line_number line path 0 0 from setuptools import setup, find_packages setup.py 1 1 NaN setup.py But yea that\u0026rsquo;s okay, I updated the build_texts_from_repository func to ignore blanks def build_texts_from_repository(repo_dir): \u0026#34;\u0026#34;\u0026#34;Return a dataset of the non-blank lines of code \u0026#34;\u0026#34;\u0026#34; dataset = [] for path in chain( Path(repo_dir).glob(\u0026#34;**/*.py\u0026#34;), Path(repo_dir).glob(\u0026#34;**/*.md\u0026#34;), ): assert path.is_file() and path.suffix lines = path.read_text().splitlines() dataset.extend( [ { \u0026#34;line_number\u0026#34;: i, \u0026#34;line\u0026#34;: line, \u0026#34;path\u0026#34;: str(path.relative_to(repo_dir))} for i, line in enumerate(lines) if line.strip() != \u0026#34;\u0026#34; ] ) return dataset 12:52 slightly updated my func, \u0026quot;build_texts_from_repository\u0026quot; to not include the blank lines or lines with just whitespace also !\nok trying again,\nimport json import os import pandas as pd import code_search as cs from pathlib import Path from date_utils import utc_ts repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) target_dir = Path(repos_dir) / \u0026#34;sentence-transformers\u0026#34; dataset = cs.build_texts_from_repository(target_dir) workdir = Path(repos_dir) / \u0026#34;code_search\u0026#34; df = pd.DataFrame.from_records(dataset) # Now should be only the non-blank lines, print(\u0026#34;blanks\u0026#34;, df[df.line == \u0026#34;\u0026#34;].shape[0]) # blanks 0 path = (workdir / \u0026#34;datasets\u0026#34; / f\u0026#34;{utc_ts()}-dataset.csv\u0026#34;) print(\u0026#34;saving\u0026#34;, path.relative_to(repos_dir)) # saving code_search/datasets/2023-06-13T170451Z-dataset.csv df.to_csv(path, index=False) df2 = pd.read_csv(path) print(df.equals(df2)) # True # And vanilla python equals? df2.to_dict(orient=\u0026#34;records\u0026#34;) == dataset # Out[62]: True 13:05 ok cool, worked this time ! 13:48 save the subset, too,\n# Only the subset with sentence filenames, path = (workdir / \u0026#34;datasets\u0026#34; / f\u0026#34;{utc_ts()}-dataset-sentence-filenames.csv\u0026#34;) print(\u0026#34;saving\u0026#34;, path.relative_to(repos_dir)) # saving code_search/datasets/2023-06-13T174927Z-dataset-sentence-filenames.csv df = pd.DataFrame.from_records( [x for x in dataset if \u0026#34;Sentence\u0026#34; in x[\u0026#34;path\u0026#34;]] ) df.to_csv(path, index=False) print(\u0026#34;read equals\u0026#34;, df.equals(pd.read_csv(path))) # read equals True First let\u0026rsquo;s build a query set 13:37 ok lets run the simplistic search first,\ndef build_query_dataset(queries, dataset): \u0026#34;\u0026#34;\u0026#34; Args: queries: plain list of strings dataset: list of dictionaries with [\u0026#34;line_number\u0026#34;, \u0026#34;line\u0026#34;, \u0026#34;path\u0026#34;] Example [{\u0026#39;line_number\u0026#39;: 35, \u0026#39;line\u0026#39;: \u0026#39; name = \u0026#34;_\u0026#34;+name\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;sentence_transformers/evaluation/MSEEvaluatorFromDataFrame.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 110, \u0026#39;line\u0026#39;: \u0026#39;if not os.path.exists(queries_filepath):\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;examples/training/ms_marco/train_bi-encoder_mnrl.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 52, \u0026#39;line\u0026#39;: \u0026#34;tracer = logging.getLogger(\u0026#39;elasticsearch\u0026#39;) \u0026#34;, \u0026#39;path\u0026#39;: \u0026#39;examples/training/data_augmentation/train_sts_indomain_bm25.py\u0026#39;}] \u0026#34;\u0026#34;\u0026#34; search_results = [] for query in tqdm(queries): findings = [ {\u0026#34;query\u0026#34;: query, **x } for x in dataset if query in x[\u0026#34;line\u0026#34;] ] search_results.extend(findings) return search_results import os from pathlib import Path import code_search as cs import pandas as pd repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) path = (Path(repos_dir) / \u0026#34;code_search/datasets/2023-06-13T174927Z-dataset-sentence-filenames.csv\u0026#34; ) dataset = pd.read_csv(path).to_dict(orient=\u0026#34;records\u0026#34;) queries = [ \u0026#34;snapshot_download\u0026#34;, \u0026#34;SentenceEvaluator\u0026#34;, \u0026#34;get_torch_home\u0026#34;, \u0026#34;os.path.join\u0026#34;, \u0026#34;flax_model.msgpack\u0026#34;, \u0026#34;torch.nn.functional.normalize\u0026#34;, \u0026#34;BATCH_HARD_TRIPLET_LOSS\u0026#34;, \u0026#34;SentenceTransformer(\u0026#39;model-name\u0026#39;)\u0026#34;, \u0026#34;DEPRECATED: This class is no longer used\u0026#34;, \u0026#34;module_config[\u0026#39;path\u0026#39;]\u0026#34;, \u0026#34;config_sentence_transformers.json\u0026#34;, \u0026#34;os.path.join(checkpoint_path, subdir)\u0026#34; \u0026#34;torch.nn.utils.clip_grad_norm\u0026#34;, \u0026#34;getEffectiveLevel\u0026#34;, \u0026#34;torch.cuda\u0026#34;, ] search_results = cs.build_query_dataset(queries, dataset) 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00\u0026lt;00:00, 6913.96it/s] In [5]: len(search_results) Out[5]: 43 In [8]: len(queries) Out[8]: 14 searchdf = pd.DataFrame.from_records(search_results) searchdf[\u0026#34;query\u0026#34;].value_counts() # os.path.join 20 torch.cuda 5 SentenceEvaluator 4 config_sentence_transformers.json 3 snapshot_download 2 get_torch_home 2 flax_model.msgpack 1 torch.nn.functional.normalize 1 BATCH_HARD_TRIPLET_LOSS 1 SentenceTransformer(\u0026#39;model-name\u0026#39;) 1 DEPRECATED: This class is no longer used 1 module_config[\u0026#39;path\u0026#39;] 1 getEffectiveLevel 1 Name: query, dtype: int64 13:59 ok haha that was instantaneous. Ok so lets evaluate the sentence_transformer accuracy then, Maybe this is a bit unbalanced, but this is good enough for now.\nAnd running the query semantic search And again using the use of semantic_search as nicely described in https://www.sbert.net/examples/applications/semantic-search/README.html#python ,\n14:55 For now let\u0026rsquo;s use the number 20 from above, as the number of results to retrieve for each query, since that is the max number of results we get for any query.\nimport torch import os import pandas as pd from pathlib import Path from sentence_transformers import SentenceTransformer, util from sentence_transformers.util import semantic_search from importlib import reload import code_search as cs hf_token = os.getenv(\u0026#34;HF_TOKEN\u0026#34;) embedder = SentenceTransformer( \u0026#39;msmarco-MiniLM-L-6-v3\u0026#39;, use_auth_token=hf_token, ) repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) path = (Path(repos_dir) / \u0026#34;code_search/datasets/2023-06-13T174927Z-dataset-sentence-filenames.csv\u0026#34; ) dataset = pd.read_csv(path).to_dict(orient=\u0026#34;records\u0026#34;) corpus = [x[\u0026#34;line\u0026#34;] for x in dataset] corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) queries = [ \u0026#34;snapshot_download\u0026#34;, \u0026#34;SentenceEvaluator\u0026#34;, \u0026#34;get_torch_home\u0026#34;, \u0026#34;os.path.join\u0026#34;, \u0026#34;flax_model.msgpack\u0026#34;, \u0026#34;torch.nn.functional.normalize\u0026#34;, \u0026#34;BATCH_HARD_TRIPLET_LOSS\u0026#34;, \u0026#34;SentenceTransformer(\u0026#39;model-name\u0026#39;)\u0026#34;, \u0026#34;DEPRECATED: This class is no longer used\u0026#34;, \u0026#34;module_config[\u0026#39;path\u0026#39;]\u0026#34;, \u0026#34;config_sentence_transformers.json\u0026#34;, \u0026#34;os.path.join(checkpoint_path, subdir)\u0026#34; \u0026#34;torch.nn.utils.clip_grad_norm\u0026#34;, \u0026#34;getEffectiveLevel\u0026#34;, \u0026#34;torch.cuda\u0026#34;, ] top_k = 20 # hat is the max number of results we get for any query so far. resultsdf = cs.run_semantic_search(embedder, dataset, queries, top_k) Merge results 16:46 merging results and initial dataset, the results are pytorch tensors which looks like they are treated as objects in pandas land ,\nipdb\u0026gt; p resultsdf.head() score idx query 0 tensor(0.6807) tensor(73) snapshot_download 1 tensor(0.5153) tensor(24) snapshot_download 2 tensor(0.4511) tensor(72) snapshot_download 3 tensor(0.3790) tensor(584) snapshot_download 4 tensor(0.3619) tensor(55) snapshot_download ipdb\u0026gt; p resultsdf.head().dtypes score object idx object query object dtype: object ipdb\u0026gt; so let me cast that,\nipdb\u0026gt; p resultsdf.astype({\u0026#34;idx\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;score\u0026#34;: \u0026#34;float\u0026#34;}).merge(truthdf, left_on=\u0026#34;idx\u0026#34;, right_index=True, how=\u0026#34;left\u0026#34;).head() score idx query line_number line path 0 0.680669 73 snapshot_download 86 snapshot_download(model_na... sentence_transformers/SentenceTransformer.py 1 0.515324 24 snapshot_download 25 from .util import import_from_string, batch_to... sentence_transformers/SentenceTransformer.py 2 0.451125 72 snapshot_download 85 # Download from hub with c... sentence_transformers/SentenceTransformer.py 3 0.379038 584 snapshot_download 725 optimizer.step() sentence_transformers/SentenceTransformer.py 4 0.361899 55 snapshot_download 62 cache_folder = os.path.join(to... sentence_transformers/SentenceTransformer.py ipdb\u0026gt; In [25]: with ipdb.launch_ipdb_on_exception(): ...: resultsdf = cs.run_semantic_search(embedder, dataset, queries, top_k) ...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00\u0026lt;00:00, 100.97it/s] In [26]: resultsdf.head() Out[26]: score query line_number line path 0 0.680669 snapshot_download 86 snapshot_download(model_na... sentence_transformers/SentenceTransformer.py 1 0.515324 snapshot_download 25 from .util import import_from_string, batch_to... sentence_transformers/SentenceTransformer.py 2 0.451125 snapshot_download 85 # Download from hub with c... sentence_transformers/SentenceTransformer.py 3 0.379038 snapshot_download 725 optimizer.step() sentence_transformers/SentenceTransformer.py 4 0.361899 snapshot_download 62 cache_folder = os.path.join(to... sentence_transformers/SentenceTransformer.py Evaluate 17:11 ok so how to evaluate truth against the results now\nMaybe, given a threshold, like 0.5 , how many hits and misses. Perhaps one way to evaluate , would be to left join truthdf with resultsdf in order to count the hits at least. And misses are the left anti-join then.\nimport pandas as pd from date_utils import utc_ts repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) search_results = cs.build_query_dataset(queries, dataset) truthdf = pd.DataFrame.from_records(search_results) workdir = Path(repos_dir) / \u0026#34;code_search\u0026#34; In [37]: set(truthdf.columns.tolist()) \u0026amp; set(resultsdf.columns.tolist()) Out[37]: {\u0026#39;line\u0026#39;, \u0026#39;line_number\u0026#39;, \u0026#39;path\u0026#39;, \u0026#39;query\u0026#39;} eval_df = pd.merge( truthdf, resultsdf.drop([\u0026#34;line\u0026#34;], axis=1), on=[\u0026#34;query\u0026#34;, \u0026#34;path\u0026#34;, \u0026#34;line_number\u0026#34;], how=\u0026#34;left\u0026#34;) path = (workdir / f\u0026#34;{utc_ts()}-evaldf.csv\u0026#34;) print(\u0026#34;saving\u0026#34;, path.relative_to(repos_dir)) # saving code_search/2023-06-13T213516Z-evaldf.csv eval_df.to_csv(path, index=False) In [46]: eval_df Out[46]: query line_number line path score 0 snapshot_download 25 from .util import import_from_string, batch_to... sentence_transformers/SentenceTransformer.py 0.515324 1 snapshot_download 86 snapshot_download(model_na... sentence_transformers/SentenceTransformer.py 0.680669 2 SentenceEvaluator 24 from .evaluation import SentenceEvaluator sentence_transformers/SentenceTransformer.py 0.700384 3 SentenceEvaluator 576 evaluator: SentenceEvaluator = None, sentence_transformers/SentenceTransformer.py 0.749687 4 SentenceEvaluator 756 def evaluate(self, evaluator: SentenceEval... sentence_transformers/SentenceTransformer.py 0.403172 5 SentenceEvaluator 0 class SentenceEvaluator: sentence_transformers/evaluation/SentenceEvalu... 0.818287 6 get_torch_home 56 from torch.hub import _get... sentence_transformers/SentenceTransformer.py 0.865429 7 get_torch_home 58 torch_cache_home = _get_to... sentence_transformers/SentenceTransformer.py 0.859171 8 os.path.join 60 torch_cache_home = os.path... sentence_transformers/SentenceTransformer.py NaN 9 os.path.join 62 cache_folder = os.path.join(to... sentence_transformers/SentenceTransformer.py NaN 10 os.path.join 82 model_path = os.path.join(cach... sentence_transformers/SentenceTransformer.py 0.662760 11 os.path.join 84 if not os.path.exists(os.path.... sentence_transformers/SentenceTransformer.py 0.737865 12 os.path.join 93 if os.path.exists(os.path.join(mod... sentence_transformers/SentenceTransformer.py 0.704441 13 os.path.join 362 with open(os.path.join(path, \u0026#39;config_s... sentence_transformers/SentenceTransformer.py NaN 14 os.path.join 371 model_path = os.path.join(path... sentence_transformers/SentenceTransformer.py 0.666143 15 os.path.join 377 with open(os.path.join(path, \u0026#39;modules.... sentence_transformers/SentenceTransformer.py 0.623291 16 os.path.join 428 with open(os.path.join(path, \u0026#34;README.m... sentence_transformers/SentenceTransformer.py 0.619979 17 os.path.join 487 create_model_card = replace_mo... sentence_transformers/SentenceTransformer.py NaN 18 os.path.join 494 file_path = os.path.join(r... sentence_transformers/SentenceTransformer.py 0.800986 19 os.path.join 520 shutil.rmtree(os.path.join... sentence_transformers/SentenceTransformer.py NaN 20 os.path.join 774 eval_path = os.path.join(output_pa... sentence_transformers/SentenceTransformer.py 0.621379 21 os.path.join 788 self.save(os.path.join(checkpoint_path... sentence_transformers/SentenceTransformer.py 0.598843 22 os.path.join 795 old_checkpoints.append({\u0026#39;s... sentence_transformers/SentenceTransformer.py NaN 23 os.path.join 816 config_sentence_transformers_json_path... sentence_transformers/SentenceTransformer.py NaN 24 os.path.join 825 model_card_path = os.path.join(model_p... sentence_transformers/SentenceTransformer.py 0.637539 25 os.path.join 834 modules_json_path = os.path.join(model... sentence_transformers/SentenceTransformer.py 0.627189 26 os.path.join 841 module = module_class.load(os.path... sentence_transformers/SentenceTransformer.py NaN 27 os.path.join 20 for line in open(os.path.join(self.fol... sentence_transformers/readers/LabelSentenceRea... 0.568704 28 flax_model.msgpack 90 ignore... sentence_transformers/SentenceTransformer.py 0.427975 29 torch.nn.functional.normalize 183 embeddings = torch.nn.... sentence_transformers/SentenceTransformer.py 0.624402 30 BATCH_HARD_TRIPLET_LOSS 13 This dataset can be used for some specific... sentence_transformers/datasets/SentenceLabelDa... 0.738400 31 SentenceTransformer(\u0026#39;model-name\u0026#39;) 5 model = SentenceTransformer(\u0026#39;model-name\u0026#39;) docs/package_reference/SentenceTransformer.md 0.909370 32 DEPRECATED: This class is no longer used 8 DEPRECATED: This class is no longer used. ... sentence_transformers/datasets/SentencesDatase... 0.766369 33 module_config[\u0026#39;path\u0026#39;] 841 module = module_class.load(os.path... sentence_transformers/SentenceTransformer.py 0.627141 34 config_sentence_transformers.json 362 with open(os.path.join(path, \u0026#39;config_s... sentence_transformers/SentenceTransformer.py 0.548365 35 config_sentence_transformers.json 815 # Check if the config_sentence_transfo... sentence_transformers/SentenceTransformer.py 0.662415 36 config_sentence_transformers.json 816 config_sentence_transformers_json_path... sentence_transformers/SentenceTransformer.py 0.760809 37 getEffectiveLevel 135 show_progress_bar = (logger.getEff... sentence_transformers/SentenceTransformer.py 0.428602 38 torch.cuda 103 device = \u0026#34;cuda\u0026#34; if torch.cuda.is_a... sentence_transformers/SentenceTransformer.py 0.779811 39 torch.cuda 215 if torch.cuda.is_available(): sentence_transformers/SentenceTransformer.py 0.835975 40 torch.cuda 216 target_devices = [\u0026#39;cuda:{}\u0026#39;.fo... sentence_transformers/SentenceTransformer.py 0.616090 41 torch.cuda 637 from torch.cuda.amp import autocast sentence_transformers/SentenceTransformer.py 0.695091 42 torch.cuda 638 scaler = torch.cuda.amp.GradScaler() sentence_transformers/SentenceTransformer.py 0.645202 see the results slightly better perhaps,\nIn [58]: eval_df[\u0026#34;line\u0026#34;] = eval_df[\u0026#34;line\u0026#34;].map(lambda x:x.strip()) In [63]: eval_df[[\u0026#34;query\u0026#34;, \u0026#34;line\u0026#34;, \u0026#34;score\u0026#34;]] Out[63]: query line score 0 snapshot_download from .util import import_from_string, batch_to_device, fullname, snapshot_download 0.515324 1 snapshot_download snapshot_download(model_name_or_path, 0.680669 2 SentenceEvaluator from .evaluation import SentenceEvaluator 0.700384 3 SentenceEvaluator evaluator: SentenceEvaluator = None, 0.749687 4 SentenceEvaluator def evaluate(self, evaluator: SentenceEvaluator, output_path: str = None): 0.403172 5 SentenceEvaluator class SentenceEvaluator: 0.818287 6 get_torch_home from torch.hub import _get_torch_home 0.865429 7 get_torch_home torch_cache_home = _get_torch_home() 0.859171 8 os.path.join torch_cache_home = os.path.expanduser(os.getenv(\u0026#39;TORCH_HOME\u0026#39;, os.path.join(os.getenv(\u0026#39;XDG_CACHE_... NaN 9 os.path.join cache_folder = os.path.join(torch_cache_home, \u0026#39;sentence_transformers\u0026#39;) NaN 10 os.path.join model_path = os.path.join(cache_folder, model_name_or_path.replace(\u0026#34;/\u0026#34;, \u0026#34;_\u0026#34;)) 0.662760 11 os.path.join if not os.path.exists(os.path.join(model_path, \u0026#39;modules.json\u0026#39;)): 0.737865 12 os.path.join if os.path.exists(os.path.join(model_path, \u0026#39;modules.json\u0026#39;)): #Load as SentenceTransformer model 0.704441 13 os.path.join with open(os.path.join(path, \u0026#39;config_sentence_transformers.json\u0026#39;), \u0026#39;w\u0026#39;) as fOut: NaN 14 os.path.join model_path = os.path.join(path, str(idx)+\u0026#34;_\u0026#34;+type(module).__name__) 0.666143 15 os.path.join with open(os.path.join(path, \u0026#39;modules.json\u0026#39;), \u0026#39;w\u0026#39;) as fOut: 0.623291 16 os.path.join with open(os.path.join(path, \u0026#34;README.md\u0026#34;), \u0026#34;w\u0026#34;, encoding=\u0026#39;utf8\u0026#39;) as fOut: 0.619979 17 os.path.join create_model_card = replace_model_card or not os.path.exists(os.path.join(tmp_dir, \u0026#39;README.md\u0026#39;)) NaN 18 os.path.join file_path = os.path.join(root, filename) 0.800986 19 os.path.join shutil.rmtree(os.path.join(tmp_dir, f), onerror=on_rm_error) NaN 20 os.path.join eval_path = os.path.join(output_path, \u0026#34;eval\u0026#34;) 0.621379 21 os.path.join self.save(os.path.join(checkpoint_path, str(step))) 0.598843 22 os.path.join old_checkpoints.append({\u0026#39;step\u0026#39;: int(subdir), \u0026#39;path\u0026#39;: os.path.join(checkpoint_path, subdir)}) NaN 23 os.path.join config_sentence_transformers_json_path = os.path.join(model_path, \u0026#39;config_sentence_transformers.... NaN 24 os.path.join model_card_path = os.path.join(model_path, \u0026#39;README.md\u0026#39;) 0.637539 25 os.path.join modules_json_path = os.path.join(model_path, \u0026#39;modules.json\u0026#39;) 0.627189 26 os.path.join module = module_class.load(os.path.join(model_path, module_config[\u0026#39;path\u0026#39;])) NaN 27 os.path.join for line in open(os.path.join(self.folder, filename), encoding=\u0026#34;utf-8\u0026#34;): 0.568704 28 flax_model.msgpack ignore_files=[\u0026#39;flax_model.msgpack\u0026#39;, \u0026#39;rust_model.ot\u0026#39;, \u0026#39;tf_model.h5\u0026#39;], 0.427975 29 torch.nn.functional.normalize embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1) 0.624402 30 BATCH_HARD_TRIPLET_LOSS This dataset can be used for some specific Triplet Losses like BATCH_HARD_TRIPLET_LOSS which req... 0.738400 31 SentenceTransformer(\u0026#39;model-name\u0026#39;) model = SentenceTransformer(\u0026#39;model-name\u0026#39;) 0.909370 32 DEPRECATED: This class is no longer used DEPRECATED: This class is no longer used. Instead of wrapping your List of InputExamples in a Se... 0.766369 33 module_config[\u0026#39;path\u0026#39;] module = module_class.load(os.path.join(model_path, module_config[\u0026#39;path\u0026#39;])) 0.627141 34 config_sentence_transformers.json with open(os.path.join(path, \u0026#39;config_sentence_transformers.json\u0026#39;), \u0026#39;w\u0026#39;) as fOut: 0.548365 35 config_sentence_transformers.json # Check if the config_sentence_transformers.json file exists (exists since v2 of the framework) 0.662415 36 config_sentence_transformers.json config_sentence_transformers_json_path = os.path.join(model_path, \u0026#39;config_sentence_transformers.... 0.760809 37 getEffectiveLevel show_progress_bar = (logger.getEffectiveLevel()==logging.INFO or logger.getEffectiveLevel()==log... 0.428602 38 torch.cuda device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; 0.779811 39 torch.cuda if torch.cuda.is_available(): 0.835975 40 torch.cuda target_devices = [\u0026#39;cuda:{}\u0026#39;.format(i) for i in range(torch.cuda.device_count())] 0.616090 41 torch.cuda from torch.cuda.amp import autocast 0.695091 42 torch.cuda scaler = torch.cuda.amp.GradScaler() 0.645202 17:40 ok cool , although wonder how not all the \u0026ldquo;os.path.join\u0026rdquo; lines were matched. Maybe some debugging to do. Also should still built the [[anti join]] to get the misses aka [[false-positive]] too.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-06-13-semantic-code-search-part-2/","summary":"public:: true blog-date:: 2023-06-13 Ok continuing from last time, where I ran sentence_transformers model 'msmarco-MiniLM-L-6-v3' against a code search problem of comparing query lines against a source code corpus actually from the sentence_transformers github\nThis time, I wanted to write some code around measuring the successful hits from a model run using cosine similarity from sentence_transformers\nQuery set choice I expanded the set of queries slightly, but not that much yet so I could focus on the evaluation code.","title":"semantic code search part 2"},{"content":"public:: true blog-date:: 2023-06-12\nbriefly Inspired by the super popular #[[schrodinger logseq plugin]], I wrote up something in #python today, to publish to #Hugo like https://github.com/sawhney17/logseq-schrodinger but also with support for block embeds.\nIn particular, I was also really inspired by [[Bas Grolleman]]\u0026rsquo;s concept here around how to be able to use #interstitial-journaling and be able to nicely coalesce selected sources through their block embeds, into a target logseq concept they all refer to.\nFull code from this blog post By the way, this code is still just at the \u0026ldquo;first stab\u0026rdquo; / #proof-of-concept stage, but it is here, https://github.com/namoopsoo/logseq_utils\nAnd usage is just below Say you have a page \u0026ldquo;blogpost/2023-06-12-name-of-your-logseq-page\u0026rdquo; , where you happen to use embeds like,\n{{embed ((64864e08-de92-4127-9162-8b5b946b021b))}} then to create a markdown file like \u0026ldquo;content/post/2023-06-12-name-of-your-logseq-page.md\u0026rdquo; , locating it in \u0026ldquo;content/post/\u0026rdquo; say if that is your Hugo post location, then use,\nfrom pathlib import Path import logseq_utils as lu page = \u0026#34;blogpost/2023-06-12-name-of-your-logseq-page\u0026#34; filename = page.split(\u0026#34;/\u0026#34;)[1] + \u0026#34;.md\u0026#34; target_dir = \u0026#34;content/post/\u0026#34; target_loc = str(Path(target_dir) / filename) print(\u0026#34;target_loc\u0026#34;, target_loc) # target_loc content/post/2023-06-12-logseq-publish-hugo-with-python.md lu.build_markdown(page, target_loc) The logseq REST API Would have loved to help w/ logseq-schrodinger but Ideally I would love to attempt a pull request on https://github.com/sawhney17/logseq-schrodinger , and I have left the block embed as a feature idea here, but my knowledge of the logseq dev setup , including Clojure Script and react is smaller than my desire to first get something working to help solve my immediate problem hah ðŸ˜….\nBut the logseq REST API looks great! According to https://docs.logseq.com/#/page/local%20http%20server , and https://plugins-doc.logseq.com , looks like one just needs to add a local API token and then you are basically ready to interact with your logseq using \u0026ldquo;127.0.0.1:12315\u0026rdquo;\nIterating Using getPageBlocksTree gave the blocks on a page Like this id:: 6487a8b5-ac96-4a45-aa2a-28bbc7002aa4\ndef get_page_blocks_tree(name, include_children=False): token = os.getenv(\u0026#34;LOGSEQ_TOKEN\u0026#34;) url = \u0026#34;http://127.0.0.1:12315/api\u0026#34; headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {token}\u0026#34;} payload = { \u0026#34;method\u0026#34;: \u0026#34;logseq.Editor.getPageBlocksTree\u0026#34;, \u0026#34;args\u0026#34;: [name, {\u0026#34;includeChildren\u0026#34;: include_children}] } response = requests.post(url, json=payload, headers=headers) return response A stab at a recursive call here 13:51 ok some simple stab at using this API then, to build #[[log-seq markdown hugo integration]] [[Hugo]] id:: 64875626-e55d-4613-9351-e6a5806e8912 14:27 ok, wrote some code in log_utils.py , and running,\nimport requests import time def build_markdown_from_page_blocks(blocks): print(\u0026#34;DEBUG\u0026#34;, [x[\u0026#34;level\u0026#34;] for x in blocks]) time.sleep(1) stuff = [] for block in blocks: stuff.append({\u0026#34;level\u0026#34;: block[\u0026#34;level\u0026#34;], \u0026#34;content\u0026#34;: block[\u0026#34;content\u0026#34;]}) if block[\u0026#34;children\u0026#34;]: stuff.extend(build_markdown_from_page_blocks(block[\u0026#34;children\u0026#34;])) return stuff page = \u0026#34;blogpost/2023-06-11-semantic-code-search-first-stab\u0026#34; response = lu.get_page_blocks_tree(page) response.status_code blocks = response.json() len(blocks) # 4 stuff = lu.build_markdown_from_page_blocks(blocks) ok nice haha worked on first try as intended #moment/satisfaction ðŸ˜€\nIn [57]: stuff = lu.build_markdown_from_page_blocks(blocks) DEBUG [1, 1, 1, 1] DEBUG [2, 2] DEBUG [3, 3, 3] DEBUG [2, 2, 2, 2, 2, 2, 2] DEBUG [3, 3] [{\u0026#39;level\u0026#39;: 1, \u0026#39;content\u0026#39;: \u0026#39;public:: true\u0026#39;}, {\u0026#39;level\u0026#39;: 1, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}, {\u0026#39;level\u0026#39;: 1, \u0026#39;content\u0026#39;: \u0026#39;# Initial Learnings\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;## Learned about [[symmetric vs asymmetric semantic search]]\u0026#39;}, {\u0026#39;level\u0026#39;: 3, \u0026#39;content\u0026#39;: \u0026#39;This distinction refers to #[[sentence similarity task]] where the query is of the same size or asymmetrically, smaller size, such as a one or two word query.\u0026#39;}, {\u0026#39;level\u0026#39;: 3, \u0026#39;content\u0026#39;: \u0026#39;And wow that is exactly what. Iwas looking for !\u0026#39;}, {\u0026#39;level\u0026#39;: 3, \u0026#39;content\u0026#39;: \u0026#39;So apparently this includes the \u0026#34;msmarco\u0026#34; models.\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}, {\u0026#39;level\u0026#39;: 1, \u0026#39;content\u0026#39;: \u0026#39;# Final test run\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;Idea is to build a corpus with, hey why not, the code from the #sentence-transformers repo.\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;Had a few test runs today, iterating on the approach, using queries from files other than the ones I built a corpus for , #moment/doh haha . Also weirdly the msmarco model documented as the v3 that should be used is MIA somehow, but the v2 seems fine. And \u0026#34;msmarco-MiniLM-L-6-v3\u0026#34; is fine too. \\n\\nBut here is the last run for today.\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;{{embed ((64864e08-de92-4127-9162-8b5b946b021b))}}\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;## And code for building that corpus\u0026#39;}, {\u0026#39;level\u0026#39;: 3, \u0026#39;content\u0026#39;: \u0026#39;{{embed ((648728d8-5958-4df6-95d4-b81de6665974))}}\u0026#39;}, {\u0026#39;level\u0026#39;: 3, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}, {\u0026#39;level\u0026#39;: 2, \u0026#39;content\u0026#39;: \u0026#39;\u0026#39;}] And then filling out the embeds 14:47 ok just need to fill the embeds with the block embed outputs then, id:: 648764a3-6ede-4550-82cf-5ce07e765ac0 14:56,\nimport re block = {\u0026#34;content\u0026#34;: \u0026#34;{{embed ((648728d8-5958-4df6-95d4-b81de6665974))}}\u0026#34;} if match := re.match( r\u0026#34;^{{embed \\(\\(([a-zA-Z0-9-]+)\\)\\)}}$\u0026#34;, block[\u0026#34;content\u0026#34;] ): print(\u0026#34;yes\u0026#34;, match.groups()[0]) # yes 648728d8-5958-4df6-95d4-b81de6665974 hmm so for instance if we have the block below, which is at level 3,\n{\u0026#39;properties\u0026#39;: {}, \u0026#39;unordered\u0026#39;: True, \u0026#39;parent\u0026#39;: {\u0026#39;id\u0026#39;: 28919}, \u0026#39;children\u0026#39;: [], \u0026#39;id\u0026#39;: 29214, \u0026#39;pathRefs\u0026#39;: [{\u0026#39;id\u0026#39;: 28906}, {\u0026#39;id\u0026#39;: 29113}, {\u0026#39;id\u0026#39;: 29220}], \u0026#39;level\u0026#39;: 3, \u0026#39;uuid\u0026#39;: \u0026#39;64875997-b4fd-4079-9a06-19b1606d5f33\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;{{embed ((648728d8-5958-4df6-95d4-b81de6665974))}}\u0026#39;, \u0026#39;journal?\u0026#39;: False, \u0026#39;macros\u0026#39;: [{\u0026#39;id\u0026#39;: 29221}], \u0026#39;page\u0026#39;: {\u0026#39;id\u0026#39;: 28906}, \u0026#39;left\u0026#39;: {\u0026#39;id\u0026#39;: 28919}, \u0026#39;format\u0026#39;: \u0026#39;markdown\u0026#39;, \u0026#39;refs\u0026#39;: [{\u0026#39;id\u0026#39;: 29113}, {\u0026#39;id\u0026#39;: 29220}]} And we fetch the block \u0026ldquo;648728d8-5958-4df6-95d4-b81de6665974\u0026rdquo; , collapsed:: true\nIn [59]: block_uuid = \u0026#34;648728d8-5958-4df6-95d4-b81de6665974\u0026#34; In [60]: response = lu.get_block(block_uuid) In [61]: response.json() Out[61]: {\u0026#39;properties\u0026#39;: {\u0026#39;id\u0026#39;: \u0026#39;648728d8-5958-4df6-95d4-b81de6665974\u0026#39;}, \u0026#39;parent\u0026#39;: {\u0026#39;id\u0026#39;: 28846}, \u0026#39;children\u0026#39;: [{\u0026#39;properties\u0026#39;: {}, \u0026#39;parent\u0026#39;: {\u0026#39;id\u0026#39;: 29113}, \u0026#39;children\u0026#39;: [], \u0026#39;id\u0026#39;: 29219, \u0026#39;pathRefs\u0026#39;: [{\u0026#39;id\u0026#39;: 28}, {\u0026#39;id\u0026#39;: 1967}, {\u0026#39;id\u0026#39;: 5812}, {\u0026#39;id\u0026#39;: 27790}, {\u0026#39;id\u0026#39;: 27795}, {\u0026#39;id\u0026#39;: 28383}, {\u0026#39;id\u0026#39;: 28504}, {\u0026#39;id\u0026#39;: 28556}, {\u0026#39;id\u0026#39;: 28906}, {\u0026#39;id\u0026#39;: 28907}], \u0026#39;level\u0026#39;: 1, \u0026#39;uuid\u0026#39;: \u0026#39;64875a31-5142-4f50-a542-5d5c00021cb4\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Put the following into a file `code_search.py`\\n```python\\nfrom pathlib import Path\\nfrom itertools import chain\\n\\n\\ndef build_texts_from_repository(repo_dir):\\n \u0026#34;\u0026#34;\u0026#34;Return a dataset of the code\\n \u0026#34;\u0026#34;\u0026#34;\\n dataset = []\\n file_types = []\\n for path in chain(\\n Path(repo_dir).glob(\u0026#34;**/*.py\u0026#34;),\\n Path(repo_dir).glob(\u0026#34;**/*.md\u0026#34;),\\n ):\\n assert path.is_file() and path.suffix\\n lines = path.read_text().splitlines()\\n \\n dataset.extend(\\n [{\u0026#34;line_number\u0026#34;: i,\\n \u0026#34;line\u0026#34;: line,\\n \u0026#34;path\u0026#34;: str(path.relative_to(repo_dir))}\\n for i, line in enumerate(lines)\\n ]\\n )\\n return dataset\\n\\n```\u0026#39;, \u0026#39;page\u0026#39;: {\u0026#39;journalDay\u0026#39;: 20230611, \u0026#39;name\u0026#39;: \u0026#39;jun 11th, 2023\u0026#39;, \u0026#39;originalName\u0026#39;: \u0026#39;Jun 11th, 2023\u0026#39;, \u0026#39;id\u0026#39;: 28504}, \u0026#39;left\u0026#39;: {\u0026#39;id\u0026#39;: 29113}, \u0026#39;format\u0026#39;: \u0026#39;markdown\u0026#39;}, {\u0026#39;properties\u0026#39;: {}, \u0026#39;parent\u0026#39;: {\u0026#39;id\u0026#39;: 29113}, \u0026#39;children\u0026#39;: [], \u0026#39;id\u0026#39;: 29218, \u0026#39;pathRefs\u0026#39;: [{\u0026#39;id\u0026#39;: 28}, {\u0026#39;id\u0026#39;: 1967}, {\u0026#39;id\u0026#39;: 5812}, {\u0026#39;id\u0026#39;: 27790}, {\u0026#39;id\u0026#39;: 27795}, {\u0026#39;id\u0026#39;: 28383}, {\u0026#39;id\u0026#39;: 28504}, {\u0026#39;id\u0026#39;: 28556}, {\u0026#39;id\u0026#39;: 28906}, {\u0026#39;id\u0026#39;: 28907}], \u0026#39;level\u0026#39;: 1, \u0026#39;uuid\u0026#39;: \u0026#39;64875a2e-7b2a-47fb-891b-419cd3347643\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;```python\\nimport os\\nimport code_search as cs\\nfrom pathlib import Path\\nrepos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;)\\ntarget_dir = Path(repos_dir) / \u0026#34;sentence-transformers\u0026#34;\\ndataset = cs.build_texts_from_repository(target_dir)\\n\\n```\\ndouble checking , \\n```python\\n\\nIn [12]: dataset[:10]\\nOut[12]: \\n[{\\\u0026#39;line_number\\\u0026#39;: 0,\\n \\\u0026#39;line\\\u0026#39;: \\\u0026#39;from setuptools import setup, find_packages\\\u0026#39;,\\n \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 1, \\\u0026#39;line\\\u0026#39;: \\\u0026#39;\\\u0026#39;, \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 2,\\n \\\u0026#39;line\\\u0026#39;: \\\u0026#39;with open(\u0026#34;README.md\u0026#34;, mode=\u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as readme_file:\\\u0026#39;,\\n \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 3,\\n \\\u0026#39;line\\\u0026#39;: \\\u0026#39; readme = readme_file.read()\\\u0026#39;,\\n \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 4, \\\u0026#39;line\\\u0026#39;: \\\u0026#39;\\\u0026#39;, \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 5, \\\u0026#39;line\\\u0026#39;: \\\u0026#39;\\\u0026#39;, \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 6, \\\u0026#39;line\\\u0026#39;: \\\u0026#39;\\\u0026#39;, \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 7, \\\u0026#39;line\\\u0026#39;: \\\u0026#39;setup(\\\u0026#39;, \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 8,\\n \\\u0026#39;line\\\u0026#39;: \\\u0026#39; name=\u0026#34;sentence-transformers\u0026#34;,\\\u0026#39;,\\n \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;},\\n {\\\u0026#39;line_number\\\u0026#39;: 9, \\\u0026#39;line\\\u0026#39;: \\\u0026#39; version=\u0026#34;2.2.2\u0026#34;,\\\u0026#39;, \\\u0026#39;path\\\u0026#39;: \\\u0026#39;setup.py\\\u0026#39;}]\\n\\nIn [13]: set([Path(x[\u0026#34;path\u0026#34;]).suffix for x in dataset])\\nOut[13]: {\\\u0026#39;.md\\\u0026#39;, \\\u0026#39;.py\\\u0026#39;}\\n```\u0026#39;, \u0026#39;page\u0026#39;: {\u0026#39;journalDay\u0026#39;: 20230611, \u0026#39;name\u0026#39;: \u0026#39;jun 11th, 2023\u0026#39;, \u0026#39;originalName\u0026#39;: \u0026#39;Jun 11th, 2023\u0026#39;, \u0026#39;id\u0026#39;: 28504}, \u0026#39;left\u0026#39;: {\u0026#39;id\u0026#39;: 29219}, \u0026#39;format\u0026#39;: \u0026#39;markdown\u0026#39;}], \u0026#39;id\u0026#39;: 29113, \u0026#39;pathRefs\u0026#39;: [{\u0026#39;id\u0026#39;: 28}, {\u0026#39;id\u0026#39;: 1967}, {\u0026#39;id\u0026#39;: 5812}, {\u0026#39;id\u0026#39;: 27790}, {\u0026#39;id\u0026#39;: 27795}, {\u0026#39;id\u0026#39;: 28383}, {\u0026#39;id\u0026#39;: 28504}, {\u0026#39;id\u0026#39;: 28556}, {\u0026#39;id\u0026#39;: 28906}, {\u0026#39;id\u0026#39;: 28907}], \u0026#39;propertiesTextValues\u0026#39;: {\u0026#39;id\u0026#39;: \u0026#39;648728d8-5958-4df6-95d4-b81de6665974\u0026#39;}, \u0026#39;uuid\u0026#39;: \u0026#39;648728d8-5958-4df6-95d4-b81de6665974\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;ok cool, let me just focus on markdown and python\\nid:: 648728d8-5958-4df6-95d4-b81de6665974\u0026#39;, \u0026#39;page\u0026#39;: {\u0026#39;journalDay\u0026#39;: 20230611, \u0026#39;name\u0026#39;: \u0026#39;jun 11th, 2023\u0026#39;, \u0026#39;originalName\u0026#39;: \u0026#39;Jun 11th, 2023\u0026#39;, \u0026#39;id\u0026#39;: 28504}, \u0026#39;left\u0026#39;: {\u0026#39;id\u0026#39;: 29215}, \u0026#39;format\u0026#39;: \u0026#39;markdown\u0026#39;, \u0026#39;refs\u0026#39;: [{\u0026#39;id\u0026#39;: 29214}]} 15:12 hmm strangely it does not have its own level and its children start from level 1, so maybe this is meant to be incremental sort of. 15:26 ok added some kind of offset then\ndef build_markdown_from_page_blocks(blocks, level_offset=0): print(\u0026#34;DEBUG\u0026#34;, [x[\u0026#34;level\u0026#34;] for x in blocks]) stuff = [] for block in blocks: # Replace embed if match := re.match( r\u0026#34;^{{embed \\(\\(([a-zA-Z0-9-]+)\\)\\)}}$\u0026#34;, block[\u0026#34;content\u0026#34;] ): block_uuid = match.groups()[0] print(\u0026#34;yes\u0026#34;, block_uuid) response = get_block(block_uuid) assert response.status_code == 200 new_block = response.json() new_block[\u0026#34;level\u0026#34;] = block[\u0026#34;level\u0026#34;] stuff.append({\u0026#34;level\u0026#34;: new_block[\u0026#34;level\u0026#34;], \u0026#34;content\u0026#34;: new_block[\u0026#34;content\u0026#34;]}) if new_block[\u0026#34;children\u0026#34;]: stuff.extend( build_markdown_from_page_blocks( new_block[\u0026#34;children\u0026#34;], level_offset=(level_offset + new_block[\u0026#34;level\u0026#34;]) )) else: stuff.append( {\u0026#34;level\u0026#34;: block[\u0026#34;level\u0026#34;] + level_offset, \u0026#34;content\u0026#34;: block[\u0026#34;content\u0026#34;]}) if block[\u0026#34;children\u0026#34;]: stuff.extend( build_markdown_from_page_blocks( block[\u0026#34;children\u0026#34;], level_offset=level_offset) ) return stuff And the markdown output 15:26 ok then now , to generate the markdown, id:: 64877179-6d5e-4a1a-88ef-7826edf78ee5 Did this in a super simple way,\ndef build_markdown(page_name, target_loc): response = get_page(page_name) assert response.status_code == 200 and response.json() response = get_page_blocks_tree(page_name) assert response.status_code == 200 and response.json() blocks = response.json() blog_date = blocks[0][\u0026#34;properties\u0026#34;][\u0026#34;blogDate\u0026#34;] stuff = build_markdown_from_page_blocks(blocks) page_title = page_name.split(\u0026#34;/\u0026#34;)[1] # if match := re.match(r\u0026#34;(\\d{4}-\\d{2}-\\d{2})-(.*)\u0026#34;, page_title): date_from_title, page_title = match.groups() print(\u0026#34;page_title\u0026#34;, page_title) page_title = page_title.replace(\u0026#34;-\u0026#34;, \u0026#34; \u0026#34;) print(\u0026#34;page_title\u0026#34;, page_title) text = [ \u0026#34;---\u0026#34;, f\u0026#34;date: {date_from_title}\u0026#34;, f\u0026#34;title: {page_title}\u0026#34;, \u0026#34;---\u0026#34;, ] + [x[\u0026#34;content\u0026#34;] for x in stuff] path = Path(target_loc) assert path.parent.is_dir() path.write_text(\u0026#34;\\n\u0026#34;.join(text)) ... 15:50 ok lets try this,\ntarget_dir = \u0026#34;content/post/\u0026#34; # 2023-06-12-spinoza-vs-descartes.md page = \u0026#34;blogpost/2023-06-11-semantic-code-search-first-stab\u0026#34; filename = page.split(\u0026#34;/\u0026#34;)[1] + \u0026#34;.md\u0026#34; target_loc = str(Path(target_dir) / filename) print(\u0026#34;target_loc\u0026#34;, target_loc) # \u0026#39;content/post/2023-06-11-semantic-code-search-first-stab.md\u0026#39; lu.build_markdown(page, target_loc) So per above, the first page built ended up being this page And the second page was actually this one\nSome tips no hyphens in your API token Oddly enough I was getting\nOut[12]: {\u0026#39;statusCode\u0026#39;: 401, \u0026#39;error\u0026#39;: \u0026#39;Unauthorized\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;Access Denied!\u0026#39;} and then switched to a token without hyphens and it was fine.\nThe right REST endpoint Initially I was getting a 404 'Route POST:/ not found' when hitting\nurl = \u0026#34;http://127.0.0.1:12315\u0026#34; as opposed to\nurl = \u0026#34;http://127.0.0.1:12315/api\u0026#34; getPage returned no children Oddly enough, hitting https://plugins-doc.logseq.com/logseq/Editor/getPage on a valid page, even with setting includeChildren to true, I was getting a 200 but an empty response payload. However, I found using https://plugins-doc.logseq.com/logseq/Editor/getPageBlocksTree gave me all the child blocks for a page.\nMaybe that is the intended behavior, but I\u0026rsquo;m not sure.\nRetrieving a page with a literal slash worked but not with the \u0026ldquo;%2F\u0026rdquo; 12:52 let me try getting a page too, id:: 64874d2d-6999-48bb-b19a-a6cba44c4f96\ndef get_page(name, include_children=False): token = os.getenv(\u0026#34;LOGSEQ_TOKEN\u0026#34;) url = \u0026#34;http://127.0.0.1:12315/api\u0026#34; headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {token}\u0026#34;} payload = { \u0026#34;method\u0026#34;: \u0026#34;logseq.Editor.getPage\u0026#34;, \u0026#34;args\u0026#34;: [name, {\u0026#34;includeChildren\u0026#34;: include_children}] } response = requests.post(url, json=payload, headers=headers) return response response = get_page( \u0026#34;blogpost%2F2023-06-11-semantic-code-search-first-stab\u0026#34; ) response.status_code hmm weird saying it got a page but response.json() is null. Ok, trying by id now, and below, this is working now,\nresponse = get_page(28504) In [12]: response.json() Out[12]: {\u0026#39;updatedAt\u0026#39;: 1686580509936, \u0026#39;journalDay\u0026#39;: 20230611, \u0026#39;createdAt\u0026#39;: 1686463272305, \u0026#39;id\u0026#39;: 28504, \u0026#39;name\u0026#39;: \u0026#39;jun 11th, 2023\u0026#39;, \u0026#39;uuid\u0026#39;: \u0026#39;648728d8-8c86-4afd-8bb2-4cd519ec4c76\u0026#39;, \u0026#39;journal?\u0026#39;: True, \u0026#39;originalName\u0026#39;: \u0026#39;Jun 11th, 2023\u0026#39;, \u0026#39;file\u0026#39;: {\u0026#39;id\u0026#39;: 28527}, \u0026#39;format\u0026#39;: \u0026#39;markdown\u0026#39;} ok but then how to get a page by name then? 13:06 ok interesting, so I tried again but this time instead of the \u0026ldquo;%2F\u0026rdquo; where the \u0026ldquo;/\u0026rdquo; forward slash is, using the literal, and now worked ! collapsed:: true\nIn [13]: response = get_page( ...: \u0026#34;blogpost/2023-06-11-semantic-code-search-first-stab\u0026#34; ...: ) ...: response.status_code Out[13]: 200 In [14]: response.json() Out[14]: {\u0026#39;properties\u0026#39;: {\u0026#39;public\u0026#39;: True}, \u0026#39;updatedAt\u0026#39;: 1686526563885, \u0026#39;createdAt\u0026#39;: 1686526225155, \u0026#39;id\u0026#39;: 28906, \u0026#39;propertiesTextValues\u0026#39;: {\u0026#39;public\u0026#39;: \u0026#39;true\u0026#39;}, \u0026#39;name\u0026#39;: \u0026#39;blogpost/2023-06-11-semantic-code-search-first-stab\u0026#39;, \u0026#39;uuid\u0026#39;: \u0026#39;648728d8-d545-42ce-b487-986d6cac3d55\u0026#39;, \u0026#39;journal?\u0026#39;: False, \u0026#39;originalName\u0026#39;: \u0026#39;blogpost/2023-06-11-semantic-code-search-first-stab\u0026#39;, \u0026#39;file\u0026#39;: {\u0026#39;id\u0026#39;: 28909}, \u0026#39;namespace\u0026#39;: {\u0026#39;id\u0026#39;: 28907}, \u0026#39;format\u0026#39;: \u0026#39;markdown\u0026#39;} ok\n","permalink":"https://michal.piekarczyk.xyz/post/2023-06-12-logseq-publish-hugo-with-python/","summary":"public:: true blog-date:: 2023-06-12\nbriefly Inspired by the super popular #[[schrodinger logseq plugin]], I wrote up something in #python today, to publish to #Hugo like https://github.com/sawhney17/logseq-schrodinger but also with support for block embeds.\nIn particular, I was also really inspired by [[Bas Grolleman]]\u0026rsquo;s concept here around how to be able to use #interstitial-journaling and be able to nicely coalesce selected sources through their block embeds, into a target logseq concept they all refer to.","title":"logseq publish hugo with python"},{"content":"public:: true blog_date:: 2023-06-11 The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.\nThe documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first.\nAlso, the test results here look pretty decent, but the next step will be to create a reference set of the line numbers that were expected to be found, to get a good precision/recall score for this.\nInitial Learnings Learned about [[symmetric vs asymmetric semantic search]] Here is the explanation, https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search . This distinction refers to #[[sentence similarity task]] where the query is of the same size or asymmetrically, smaller size, such as a one or two word query. And wow that is exactly what I was looking for ! So apparently this includes the \u0026ldquo;msmarco\u0026rdquo; models.\nFinal test run Had a few test runs today, iterating on the approach, using queries from files other than the ones I built a corpus for , #moment/doh haha . Also weirdly the msmarco model documented as the v3 that should be used is MIA somehow, but the v2 seems fine. And \u0026ldquo;msmarco-MiniLM-L-6-v3\u0026rdquo; is fine too.\nBut here is the last run for today. 18:43 lets try just a few files, so I had initially passed queries I wrote out for the CrossEncode.py file , #moment/doh haha , so that is why below, I am pulling 'msmarco-MiniLM-L-6-v3' and re-encoding because I was puzzled w/ the initial lackluster results and was trying a different model. But yea I realized and just trying this model with the good set of queries instead, id:: 64864e08-de92-4127-9162-8b5b946b021b id:: 6486561e-d050-4bca-bc74-64fa5b475ab4\nimport torch from sentence_transformers import SentenceTransformer, util from sentence_transformers.util import semantic_search corpus = [x[\u0026#34;line\u0026#34;] for x in dataset if \u0026#34;Sentence\u0026#34; in x[\u0026#34;path\u0026#34;]] embedder = SentenceTransformer( \u0026#39;msmarco-MiniLM-L-6-v3\u0026#39;, use_auth_token=hf_token, ) corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) In [27]: set([x[\u0026#34;path\u0026#34;] for x in dataset if \u0026#34;Sentence\u0026#34; in x[\u0026#34;path\u0026#34;]]) Out[27]: {\u0026#39;docs/package_reference/SentenceTransformer.md\u0026#39;, \u0026#39;sentence_transformers/SentenceTransformer.py\u0026#39;, \u0026#39;sentence_transformers/datasets/ParallelSentencesDataset.py\u0026#39;, \u0026#39;sentence_transformers/datasets/SentenceLabelDataset.py\u0026#39;, \u0026#39;sentence_transformers/datasets/SentencesDataset.py\u0026#39;, \u0026#39;sentence_transformers/evaluation/SentenceEvaluator.py\u0026#39;, \u0026#39;sentence_transformers/readers/LabelSentenceReader.py\u0026#39;} In [28]: len(corpus) Out[28]: 1266 In [34]: %%time ...: corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) ...: ...: # CPU times: user 22.3 s, sys: 4.25 s, total: 26.5 s # Wall time: 14.8 s queries = [ \u0026#34;snapshot_download\u0026#34;, \u0026#34;SentenceEvaluator\u0026#34;, \u0026#34;get_torch_home\u0026#34;, \u0026#34;os.path.join\u0026#34;, \u0026#34;flax_model.msgpack\u0026#34;, \u0026#34;torch.nn.functional.normalize\u0026#34;, ] top_k = min(5, len(corpus)) for query in queries: query_embedding = embedder.encode(query, convert_to_tensor=True) # We use cosine-similarity and torch.topk to find the highest 5 scores cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0] top_results = torch.topk(cos_scores, k=top_k) print(\u0026#34;\\n\\n======================\\n\\n\u0026#34;) print(\u0026#34;Query:\u0026#34;, query) print(\u0026#34;\\nTop 5 most similar sentences in corpus:\u0026#34;) for score, idx in zip(top_results[0], top_results[1]): print(corpus[idx].strip(), \u0026#34;(Score: {:.4f})\u0026#34;.format(score)) \u0026#34;\u0026#34;\u0026#34; # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5) hits = hits[0] #Get the hits for the first query for hit in hits: print(corpus[hit[\u0026#39;corpus_id\u0026#39;]], \u0026#34;(Score: {:.4f})\u0026#34;.format(hit[\u0026#39;score\u0026#39;])) \u0026#34;\u0026#34;\u0026#34; 19:11 ok nice, this looks really good ! id:: 64864f3c-17ce-41b1-a279-39e89b1952e8\n====================== Query: snapshot_download Top 5 most similar sentences in corpus: snapshot_download(model_name_or_path, (Score: 0.6807) from .util import import_from_string, batch_to_device, fullname, snapshot_download (Score: 0.5153) # Download from hub with caching (Score: 0.4511) optimizer.step() (Score: 0.3790) cache_folder = os.path.join(torch_cache_home, \u0026#39;sentence_transformers\u0026#39;) (Score: 0.3619) ====================== Query: SentenceEvaluator Top 5 most similar sentences in corpus: class SentenceEvaluator: (Score: 0.8183) evaluator: SentenceEvaluator = None, (Score: 0.7497) from .evaluation import SentenceEvaluator (Score: 0.7004) the evaluator (Score: 0.5470) # SentenceTransformer (Score: 0.5083) ====================== Query: get_torch_home Top 5 most similar sentences in corpus: from torch.hub import _get_torch_home (Score: 0.8654) torch_cache_home = _get_torch_home() (Score: 0.8592) torch_cache_home = os.path.expanduser(os.getenv(\u0026#39;TORCH_HOME\u0026#39;, os.path.join(os.getenv(\u0026#39;XDG_CACHE_HOME\u0026#39;, \u0026#39;~/.cache\u0026#39;), \u0026#39;torch\u0026#39;))) (Score: 0.6630) import torch (Score: 0.6166) import torch (Score: 0.6166) ====================== Query: os.path.join Top 5 most similar sentences in corpus: file_path = os.path.join(root, filename) (Score: 0.8010) if not os.path.exists(os.path.join(model_path, \u0026#39;modules.json\u0026#39;)): (Score: 0.7379) if os.path.exists(os.path.join(model_path, \u0026#39;modules.json\u0026#39;)): #Load as SentenceTransformer model (Score: 0.7044) if os.path.exists(model_card_path): (Score: 0.6783) if os.path.exists(model_name_or_path): (Score: 0.6707) ====================== Query: flax_model.msgpack Top 5 most similar sentences in corpus: model = SentenceTransformer(\u0026#39;model-name\u0026#39;) (Score: 0.4369) ignore_files=[\u0026#39;flax_model.msgpack\u0026#39;, \u0026#39;rust_model.ot\u0026#39;, \u0026#39;tf_model.h5\u0026#39;], (Score: 0.4280) model: SentenceTransformer (Score: 0.4155) the model to evaluate (Score: 0.4112) Evaluate the model (Score: 0.4007) ====================== Query: torch.nn.functional.normalize Top 5 most similar sentences in corpus: torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm) (Score: 0.7218) torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm) (Score: 0.7218) embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1) (Score: 0.6244) scaler = torch.cuda.amp.GradScaler() (Score: 0.5749) from torch import nn, Tensor, device (Score: 0.5742) And code for building that corpus ok cool, let me just focus on markdown and python id:: 648728d8-5958-4df6-95d4-b81de6665974 Put the following into a file code_search.py\nfrom pathlib import Path from itertools import chain def build_texts_from_repository(repo_dir): \u0026#34;\u0026#34;\u0026#34;Return a dataset of the code \u0026#34;\u0026#34;\u0026#34; dataset = [] file_types = [] for path in chain( Path(repo_dir).glob(\u0026#34;**/*.py\u0026#34;), Path(repo_dir).glob(\u0026#34;**/*.md\u0026#34;), ): assert path.is_file() and path.suffix lines = path.read_text().splitlines() dataset.extend( [{\u0026#34;line_number\u0026#34;: i, \u0026#34;line\u0026#34;: line, \u0026#34;path\u0026#34;: str(path.relative_to(repo_dir))} for i, line in enumerate(lines) ] ) return dataset import os import code_search as cs from pathlib import Path repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) target_dir = Path(repos_dir) / \u0026#34;sentence-transformers\u0026#34; dataset = cs.build_texts_from_repository(target_dir) double checking ,\nIn [12]: dataset[:10] Out[12]: [{\u0026#39;line_number\u0026#39;: 0, \u0026#39;line\u0026#39;: \u0026#39;from setuptools import setup, find_packages\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 1, \u0026#39;line\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 2, \u0026#39;line\u0026#39;: \u0026#39;with open(\u0026#34;README.md\u0026#34;, mode=\u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as readme_file:\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 3, \u0026#39;line\u0026#39;: \u0026#39; readme = readme_file.read()\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 4, \u0026#39;line\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 5, \u0026#39;line\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 6, \u0026#39;line\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 7, \u0026#39;line\u0026#39;: \u0026#39;setup(\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 8, \u0026#39;line\u0026#39;: \u0026#39; name=\u0026#34;sentence-transformers\u0026#34;,\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}, {\u0026#39;line_number\u0026#39;: 9, \u0026#39;line\u0026#39;: \u0026#39; version=\u0026#34;2.2.2\u0026#34;,\u0026#39;, \u0026#39;path\u0026#39;: \u0026#39;setup.py\u0026#39;}] In [13]: set([Path(x[\u0026#34;path\u0026#34;]).suffix for x in dataset]) Out[13]: {\u0026#39;.md\u0026#39;, \u0026#39;.py\u0026#39;} ","permalink":"https://michal.piekarczyk.xyz/post/2023-06-11-semantic-code-search-first-stab/","summary":"public:: true blog_date:: 2023-06-11 The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.\nThe documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first.","title":"semantic code search first stab"},{"content":"This is a school English class project on philosophy from around 2004 where I recruited friends and family to do the voice acting of Spinoza and Descartes ðŸ˜‚. Glad this did not get lost.\nMy brother and uncle are voicing here Scene 1 Scene 2 Scene 3 Scene 4 George and Eugene Here is the second set of voice acting.\nScene 5 Scene 6 Scene 7 ","permalink":"https://michal.piekarczyk.xyz/post/2023-06-10-spinoza-vs-descartes/","summary":"This is a school English class project on philosophy from around 2004 where I recruited friends and family to do the voice acting of Spinoza and Descartes ðŸ˜‚. Glad this did not get lost.\nMy brother and uncle are voicing here Scene 1 Scene 2 Scene 3 Scene 4 George and Eugene Here is the second set of voice acting.\nScene 5 Scene 6 Scene 7 ","title":"Decartes vs Spinoza"},{"content":"Briefly I want to have a curated photo gallery, but I take way too many photos for notes, so I need weed them out.\nThe main idea is in this setup, both the Apple icloud photo thing and the Dropbox photo thing are both used. And python scripts are run to move what gets saved into the initial Dropbox \u0026ldquo;~/Dropbox/Camera Uploads/\u0026rdquo; area, to a \u0026ldquo;~/Dropbox/myphotos/\u0026rdquo; area, separated by month-dates like \u0026ldquo;2020/2020-05\u0026rdquo;, \u0026ldquo;2020/2020-06\u0026rdquo; where python scripts next can be run to clean things up.\nThe python scripts live on github.\nThere are a few logical components.\nThe script that moves photos to their year/month directories. The script that deduplicates (aka dedupes) photos. The food-not-food Docker server to identify photos of food. The flask server that helps to curate the rest. Dig deeper What\u0026rsquo;s with the food photos? There are some earlier notes here about how the food photos get split out. The purpose here is that at one point I was inspired by a Peter Attia statement about how you might have various fluctuations in your metabolic health, but that you can typically look back at what you were eating when you were doing more or less well. Looking at the macro nutrient data like in this post is great too, but it is also nice to have the parallel photographic information to help inform you what it is that you are eating.\nMaking curating the photos slightly less tedious Earlier here, I described building a local flask app that can be accessed by ipad say, to help other photos that cannot yet be automatically sorted by a model such as the food-not-food model.\nFuture though But later on the food-not-food model can be extended to also sort out receipts and any other photos I don\u0026rsquo;t exactly want in my pristine gallery.\n","permalink":"https://michal.piekarczyk.xyz/project/2023-photo-unchaos/","summary":"an interlude to having better grasp of an unwieldly photo situation","title":"How to unchaos your photos?"},{"content":"TLDR Diabetes is what happens when high blood sugar wreaks havoc on the body because either with type I diabetes, the pancreas cannot produce insulin for various reasons or with type II diabetes, no matter how much insulin the pancreas produces, it is not enough insulin to meet the high levels of blood sugar because of insulin resistance (insulin is ignored by cells and blood sugar is not taken up). But fortunately dietary carbohydrate is the main lever here.\nMy summary is that at this point we have enough long term studies (at least a year) comparing low carb, very low carb, mediterranean diets, low fat diets etc, that the American Diebetes Association chose to cite them in 2019, (https://diabetesjournals.org/care/article/42/5/731/40480/Nutrition-Therapy-for-Adults-With-Diabetes-or), weighing in that hands down the very low carb diets will help you manage your glycemic outcomes on type I or II diabetes, and the A1C outcomes will be equal to or better than what available medications can do. But they strongly underline that registered dietician nutritionists are the key factor because although very low carb and even mediterranean styles have strong outcomes, adherence is the biggest barrier. But as they write, medical nutrition therapy is a covered medicare benefit, so that is good news.\nSome quoting from that ADA Report My go-to for links to the latest nih studies and just well written articles on this topic is dietdoctor.com, and I\u0026rsquo;m glad that here they pointed out the ADA report, \u0026ldquo;Nutrition Therapy for Adults With Diabetes or Prediabetes: A Consensus Report\u0026rdquo;. It is amazing that it took the ADA until 2019 to write this, but they finally did it!\nThis report starts out with a strong statement about the efficacy of nutrition as a therapy of diabetes And that it saves money!\nâ€œThis Consensus Report is intended to provide clinical professionals with evidence-based guidance about individualizing nutrition therapy for adults with diabetes or prediabetes. Strong evidence supports the efficacy and cost-effectiveness of nutrition therapy as a component of quality diabetes care, including its integration into the medical management of diabetes; therefore, it is important that all members of the health care team know and champion the benefits of nutrition therapy and key nutrition messages. Nutrition counseling that works toward improving or maintaining glycemic targets, achieving weight management goals, and improving cardiovascular risk factors (e.g., blood pressure, lipids, etc.) within individualized treatment goals is recommended for all adults with diabetes and prediabetes.â€\nBut they underline this should be managed care.\nâ€œThe American Diabetes Association (ADA) emphasizes that medical nutrition therapy (MNT) is fundamental in the overall diabetes management plan, and the need for MNT should be reassessed frequently by health care providers in collaboration with people with diabetes across the life span, with special attention during times of changing health status and life stages â€œ\nThey recommend use of medical nutritional therapy throughout and defer to #registered-dietician-nutritionist but they avoid â€œone size fits allâ€ recommendations\nIn the section â€œIs MNT effective in improving outcomes?â€ This statement is blunt about nutrition being more effective than currently available medication.\nâ€œReported hemoglobin A1cÂ (A1C) reductions from MNT can be similar to or greater than what would be expected with treatment using currently available medication for type 2 diabetes (). Strong evidence supports the effectiveness of MNT interventions provided by RDNs for improving A1C, with absolute decreases up to 2.0% (in type 2 diabetes) and up to 1.9% (in type 1 diabetes) at 3â€“6 months. Ongoing MNT support is helpful in maintaining glycemic improvements ().â€\nAnd further along the language is more precise about carbohydrates \u0026ldquo;Reducing overall carbohydrate intake for individuals with diabetes has demonstrated the most evidence for improving glycemia and may be applied in a variety of eating patterns that meet individual needs and preferences.\u0026rdquo;\n\u0026ldquo;For select adults with type 2 diabetes not meeting glycemic targets or where reducing antiglycemic medications is a priority, reducing overall carbohydrate intake with low- or very low-carbohydrate eating plans is a viable approach.\u0026rdquo;\nThey point to two significat randomized controlled trials Dietary Intervention Randomized Controlled Trial (DIRECT) , lasting 2 years showing best #a1c results for the #[[low carb]] group,\nThe #PREDIMED trial, 4 years, wow, This one was #mediterranean-style-pattern vs #low-fat-pattern , so after 4 years, \u0026ldquo;glycemic management improved and the need for glucose-lowering medications was lower in the Mediterranean eating pattern group\u0026rdquo;\nMore references from that DietDoctor article ( from here )\n\u0026ldquo;Effect of a very low-carbohydrate ketogenic diet vs recommended diets in patients with type 2 diabetes: a meta-analysis\u0026rdquo; From here, this is a good #meta-analysis review from #2022 of 8 #randomized-controlled-trial.\nThe #ketogenic-diet showed much greater reduction in #a1c after 6 months.\nAnd â€œIt was superior in decreasing triglyceride levels, increasing high-density lipoprotein cholesterol levels, and reducing the use of antidiabetic medications for up to 12 months.â€\nBut it seems they say the benefit disappears after 12 years and they attribute to reduction in diet adherence #adherence to #restriction #[[low carb]]\nReversal and remission Going back to the ADA report, here is their synthesis about remission From the section, \u0026ldquo;What is the role of weight loss on potential for type 2 diabetes remission?\u0026rdquo;\nBasically \u0026ldquo;remission\u0026rdquo;, divided into \u0026ldquo;complete remission\u0026rdquo; and \u0026ldquo;partial remission\u0026rdquo;, is defined by these two trials as the maintenance of earlier glycemic levels without diabetic medication for at least a year.\nThe Look AHEAD trial (177) and the Diabetes Remission Clinical Trial (DiRECT) (138) highlight the potential for type 2 diabetes remissionâ€”defined as the maintenance of euglycemia (complete remission) or prediabetes level of glycemia (partial remission) with no diabetes medication for at least 1 year (177,178)â€”in people undergoing weight loss treatment. In the Look AHEAD trial, when compared with the control group, the intensive lifestyle arm resulted in at least partial diabetes remission in 11.5% of participants as compared with 2% in the control group (177). The DiRECT trial showed that at 1 year, weight loss associated with the lifestyle intervention resulted in diabetes remission in 46% of participants (138). Remission rates were related to magnitude of weight loss, rising progressively from 7% to 86% as weight loss at 1 year increased from \u0026lt;5% to â‰¥15% (138). Diet composition may also play a role; in an RCT by Esposito et al. (179), despite only a 2-kg difference in weight loss, the group following a low-carbohydrate Mediterranean-style eating pattern (see Table 3) experienced greater rates of at least partial diabetes remission, with rates of 14.7% at year 1 and 5% at year 6 compared with 4.7% and 0%, respectively, in the group following a low-fat eating plan.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-05-20-diabetes/","summary":"TLDR Diabetes is what happens when high blood sugar wreaks havoc on the body because either with type I diabetes, the pancreas cannot produce insulin for various reasons or with type II diabetes, no matter how much insulin the pancreas produces, it is not enough insulin to meet the high levels of blood sugar because of insulin resistance (insulin is ignored by cells and blood sugar is not taken up). But fortunately dietary carbohydrate is the main lever here.","title":"A diabetes summary (draft)"},{"content":"Context Per an earlier on-going interlude, I\u0026rsquo;ve been trying to more easily curate the photos I care to reference in passing in a conversation. This started out as deduping photos and splitting out the photos of food, but at some point I realized I have so many photos I take as notes, taking those out by hand was getting tedious. I used this as an opportunity to test chat GPT\u0026rsquo;s code-generating skills.\nBasically, I asked for a web interface around a flask app to help me out. Here was my prompt\ncan you please write a minimal simple python flask web app called \u0026ldquo;app.py\u0026rdquo; that will read a folder on my laptop that contains image files and has a simple REST API interface where GET will return the filename of a single image as json, say \u0026lsquo;{\u0026ldquo;filename\u0026rdquo;: \u0026ldquo;some_file.jpg\u0026rdquo;}\u0026rsquo; and a POST will take a json input with a filename key and choice key for example like \u0026lsquo;{\u0026ldquo;filename\u0026rdquo;: \u0026ldquo;some_file.jpg\u0026rdquo;, \u0026ldquo;choice\u0026rdquo;: \u0026ldquo;some_choice\u0026rdquo;}\u0026rsquo; and the \u0026ldquo;app.py\u0026rdquo; will move \u0026ldquo;some_file.jpg\u0026rdquo; into another folder named \u0026ldquo;some_choice\u0026rdquo;. And then please create a minimal single page html5 app called \u0026ldquo;choose_front_end.html\u0026rdquo; that can interact with the REST API of the flask app \u0026ldquo;app.py\u0026rdquo; previously described using the REST API GET and POST interface. I want to be able to open the \u0026ldquo;choose_front_end.html\u0026rdquo; in my web browser and have the GET API be called on \u0026ldquo;app.py\u0026rdquo; and I want to see the next image to get displayed and I want to be able to write into a text box a choice and I want then that choice that I write to be submitted as part of a POST to \u0026ldquo;app.py\u0026rdquo; . I want to be able to run both the flask \u0026ldquo;app.py\u0026rdquo; and the \u0026ldquo;choose_front_end.html\u0026rdquo; locally on my laptop on localhost.\nAnd the response was pretty cool, with an app.py and choose_front_end.html like I asked,\nalong with some helpful instructions on how to use this more or less.\nHowever, when I started test driving this, I realized some initial issues. First I noticed actually perhaps because of the output size limitations of the free version, the choose_front_end.html file was actually cut off in the middle, so I got the displayNextImage func but the submitChoice func was without a body.\nI also started testing out just the GET part since I did have the full app.py of the flask app, however this did not run because yea of course CORS was not setup here. And CORS is one of those notoriously difficult pieces of boilerplate to setup in any client server application communication even if you are just debugging something locally on your laptop.\nAccess to fetch at \u0026#39;http://127.0.0.1:5000/\u0026#39; from origin \u0026#39;null\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. If an opaque response serves your needs, set the request\u0026#39;s mode to \u0026#39;no-cors\u0026#39; to fetch the resource with CORS disabled. I had setup CORS within many web frameworks in the past but I have trouble with this every time. But after a bit of troubleshooting, I stumbled upon the fact that flask actually has some built-in CORS functionality syntactic sugar and I took advantage of that nicely!\nIf you pip install, pip install flask-cors, you can basically do the CORS part with the few additional lines at the top here, from flask_cors import CORS app = Flask(\u0026#34;blah\u0026#34;) CORS(app) @app.route(\u0026#34;/\u0026#34;, methods=[\u0026#34;GET\u0026#34;]) def get_image(): return make_response({\u0026#34;blah\u0026#34;: \u0026#34;blah\u0026#34;}) def make_response(data): str_payload = json.dumps(data) mimetype = \u0026#34;application/json\u0026#34; response = flask.Response(response=str_payload, status=200, mimetype=mimetype) # response.headers[\u0026#34;Access-Control-Allow-Origin\u0026#34;] = \u0026#34;*\u0026#34; # CORS_ORIGIN_ALLOW_ALL = True? # response.headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34; return response Ultimately after building out a good bit of functionality additional I did not realize I also wanted, I ended up with a solution I was able to run from my laptop and access from my ipad, for convenience, Definitely skipped a lot of details here! But will update more here later.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-05-14-chat-gpt-flask-kickstart/","summary":"Context Per an earlier on-going interlude, I\u0026rsquo;ve been trying to more easily curate the photos I care to reference in passing in a conversation. This started out as deduping photos and splitting out the photos of food, but at some point I realized I have so many photos I take as notes, taking those out by hand was getting tedious. I used this as an opportunity to test chat GPT\u0026rsquo;s code-generating skills.","title":"Using ChatGPT to kickstart a flask app?"},{"content":"Github Pages build queued for a day \u0026#34;Waiting for a runner to pick up this job\u0026#34; But I was excited to release a date fix The dates on hugo posts were funky since switching to PaperMod, like \u0026ldquo;121212-34-7878\u0026rdquo;\nBut I was reading newer hugo uses date_format instead of dateFormat.\nAnyway I had a typo because I had DateFormat ï˜‚ï˜‚, would not have worked anyway.\nBut I could not publish this!\nOh ubuntu-18- no longer supported So per stacko, I switched from\nruns-on: ubuntu-18.04 to\nruns-on: ubuntu-latest because https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources\nDates displaying normally now haha Nice\n","permalink":"https://michal.piekarczyk.xyz/post/2023-04-13-gh-pages-and-dates/","summary":"Github Pages build queued for a day \u0026#34;Waiting for a runner to pick up this job\u0026#34; But I was excited to release a date fix The dates on hugo posts were funky since switching to PaperMod, like \u0026ldquo;121212-34-7878\u0026rdquo;\nBut I was reading newer hugo uses date_format instead of dateFormat.\nAnyway I had a typo because I had DateFormat ï˜‚ï˜‚, would not have worked anyway.\nBut I could not publish this!","title":"Run forever!"},{"content":"This is what happens to me When editing with https://logseq.com , on my phone, laptop and ipad, I end up seeing this\n$ fd \u0026#39; \u0026#39; journals/ journals/2021_03_13 2.md journals/2021_05_30 2.md journals/2021_10_17 2.md journals/2022_01_23 2.md journals/2022_05_15 2.md journals/2022_06_11 2.md journals/2022_07_16 2.md journals/2022_08_21 2.md journals/2022_09_01 2.md journals/2022_09_07 2.md journals/2022_09_07 3.md journals/2022_09_10 2.md journals/2022_09_26 2.md journals/2022_10_22 2.md journals/2022_10_22 3.md journals/2022_12_14 2.md journals/2023_01_12 2.md journals/2023_02_04 2.md journals/2023_03_10 2.md journals/2023_03_21 2.md I think this is mostly happening when you start editing a file before a icloud sync has had a chance to take place.\nLook at the forked files Since this is rare and my daily journal files are not too intense, I just wanted to look at everything, so I did it like this,\nfd \u0026#39; \u0026#39; journals/ |xargs -I {} sh -c \u0026#39;echo \u0026#34;file: {}\u0026#34; ; arr=({}); first=${arr[0]};echo \u0026#34;first thing $first\u0026#34; ; for file in ${first}*; do echo \u0026#34;=============\u0026#34;; echo \u0026#34;$file\u0026#34;;echo \u0026#34;=============\u0026#34;; cat \u0026#34;$file\u0026#34;;echo ; done\u0026#39; Actually the above one-liner is hard to read So I put it into https://github.com/namoopsoo/handy/blob/master/bash/logseq-fork-display-helper.sh ,\n# (helper script for quickly displaying logseq journal files that have forked off) # Takes as a parameter a filename that has spaces in it, # applies a glob to the first substring when splitting by spaces, # and cat all the files that are available when using that glob. file=$1 echo \u0026#34;file: $file\u0026#34; ; arr=($file); first=${arr[0]}; echo \u0026#34;first thing $first\u0026#34; ; for file in ${first}*; do echo \u0026#34;=============\u0026#34;; echo \u0026#34;$file\u0026#34;;echo \u0026#34;=============\u0026#34;; cat \u0026#34;$file\u0026#34;;echo ; done And just using that like, fd \u0026#39; \u0026#39; journals/ |xargs -I {} /blah/path/to/my/handy/bash/logseq-fork-display-helper.sh {} which shows me,\nfile: journals/2021_03_13 2.md first thing journals/2021_03_13 ============= journals/2021_03_13 2.md ============= something here oh wow ============= journals/2021_03_13.md ============= oops something else though I realized, sometimes the above forks are completely different, but also the changes are just a few lines, so I just used vimdiff \u0026quot;journals/2021_03_13 2.md\u0026quot; journals/2021_03_13.md to help me resolve the diffs.\nActually a bunch of them ended up being somehow identical So checking hash first\nmd5 \u0026#34;journals/2021_03_13 2.md\u0026#34; journals/2021_03_13.md MD5 (journals/2021_03_13 2.md) = 3d25f70f50d699a9401695a8cb1fe38a MD5 (journals/2021_03_13.md) = 3d25f70f50d699a9401695a8cb1fe38a since then I can just rm the culprit fork.\nOtherwise, md5 journals/2023_01_12* MD5 (journals/2023_01_12 2.md) = f98be369e775312a0c6357562f150ea9 MD5 (journals/2023_01_12.md) = 1a0217cbaa534810a8175f62ad3e8148 I can jump straight to the vimdiff, or macvim diff, mvim -d journals/2023_01_12*\nWow stumbled on one with 3 forks (base) $ md5 journals/2022_10_22* MD5 (journals/2022_10_22 2.md) = 293736c508df2f05e07516e2b4f6e4c9 MD5 (journals/2022_10_22 3.md) = ea02d222564aadd005be662e131b7914 MD5 (journals/2022_10_22.md) = be9ba3be408d55dcd24cc89421a66e36 ","permalink":"https://michal.piekarczyk.xyz/post/2023-04-01--merge-log-seq-journal-forks/","summary":"This is what happens to me When editing with https://logseq.com , on my phone, laptop and ipad, I end up seeing this\n$ fd \u0026#39; \u0026#39; journals/ journals/2021_03_13 2.md journals/2021_05_30 2.md journals/2021_10_17 2.md journals/2022_01_23 2.md journals/2022_05_15 2.md journals/2022_06_11 2.md journals/2022_07_16 2.md journals/2022_08_21 2.md journals/2022_09_01 2.md journals/2022_09_07 2.md journals/2022_09_07 3.md journals/2022_09_10 2.md journals/2022_09_26 2.md journals/2022_10_22 2.md journals/2022_10_22 3.md journals/2022_12_14 2.md journals/2023_01_12 2.md journals/2023_02_04 2.md journals/2023_03_10 2.md journals/2023_03_21 2.md I think this is mostly happening when you start editing a file before a icloud sync has had a chance to take place.","title":"Merge log-seq journal forks"},{"content":"Draft status Note haha, this is highly in draft mode, but I like to publish my early drafts because iterating is my preferred psychological happy place. But yea this is definitely not organized writing yet.\nBook in three sentences We escape what we intend to do when it makes us uncomfortable, but we can learn to notice the impulse to escape and stay with ( lean in ) the discomfort a little bit at a time You control your input and not your output, so align the input with your values and forgive yourself (be kind )for crappy outcomes. (corollary throw away the to do list and keep the calendar ) Make time for strategic thinking, (aka 50,000 foot thinking ), to \u0026ldquo;reflect and refine\u0026rdquo;, are you really really really doing what you intend? Really though? Are your actions congruent with your intentions? Or are you letting some other random human plan your day or worse yet, is it just pure chaos? Think critically once a week say. The environment has always had distractions, and it may seem like itâ€™s always getting harder to resist. (Do Young minds have it worse today? What does author believe? ) The chapter 7 on going deep too makes it interesting, really vibes with me Impact (TODO: rearrange this section, since lot of impact is still scattered below)\nAlso chapter 8, how finite willpower is a made up thing , super interesting , have not heard it before. Forest app Thoughts, questions [[you are not your khakis]] Thinking haha just like [[Fight club]] [[You are not your thoughts]]\nWhen it comes to values, I still think the [[closed-door-open-door]] is relevant also [[teamwork is dream work]] , [[motivated-by-helping]] , [[Creativity]] , [[spark-joy]] , these are at conflict with [[traction]] , and maybe [[explore-exploit-balance]] can help\n[[Apr 1st, 2023]] , recently I\u0026rsquo;ve been thinking about the [[gamification vs going deep]] angle. In particular, I think [[stress]] from [[performing under stress and duress]] and from [[Andrew Huberman]] episode on [[The Power of Play]], so to me this is not just that you can have [[fun-factor]] by [[deep-work]] , also probably the opposite [[shallow-vs-going-deep]] [[shallow work]] and maybe any kinds of [[shallow]] anything is even painful to some degree because it is a [[wasting-time]] and not in the [[low stakes]] kind with [[play]]. collapsed:: true\nI relate how [[Andrew Huberman]] was saying that [[play]] is [[Homeostatic regulation]], per here , this is what [[The Sims]] exhibit with their six core needs bars, and other than \u0026ldquo;social\u0026rdquo;, \u0026ldquo;bladder\u0026rdquo;, \u0026ldquo;hunger\u0026rdquo;, \u0026ldquo;sleep\u0026rdquo; , \u0026ldquo;cleanliness\u0026rdquo;, the sixth the had was \u0026ldquo;fun\u0026rdquo;. Indeed I wonder although [[Andrew Huberman]] doesn\u0026rsquo;t discuss it directly, I wonder if just like [[adenosine-sleep-pressure]] is a thing, (and [[hunger]] [[signaling]] like [[leptin]] is associated with [[energy homeostasis]] ) is there some similar chemical signaling with [[play]] and [[having fun]]? So my main thought is are we doing work all wrong ? I was recently part of a [[hackathon]] at my company where in that [[low stakes]] environment, yes my colleagues and I produced some really cool work, and we were [[having fun]] and of course [[teamwork is dream work]], but later on company wide we were all celebrating the amazing results all the teams produced, but ultimately as a organization, no one really knows how to harness fun and play so that conversation is going nowhere. My overall industry is currently in its \u0026ldquo;lets explore agile\u0026rdquo; [[agile]] phase , which is the exact opposite of fun.\nSome talk about real agile vs [[fake-agile]] or how agile causes [[MVP syndrome]] but no matter how you slice it, haha pun intended ðŸ˜€ I think agile is a [[fun killer]], a [[downer]], because ultimately it tries to [[bottle excitement]] and [[defer]] it.\nThat is like a [[opiate of the masses]] maybe like in the [[movie-type]] [[Network 1976 movie]] [[Stephen Wolfram]] and his take on [[energy-draining-vs-energy-adding-activity]]. And the whole [[996]], and [[Meritocray Trap book]]\n[[explore-exploit-balance]] is the topic that comes to my mind whenever I feel perhaps I\u0026rsquo;m going down a [[rabbit hole]] or that I risk not creating something [[live in production]] , [[deliver working software frequently]] [[continuous delivery]] [[iterative]] style\nThat [[strategic-thinking]], section, was from \u0026ldquo;chapter 9\u0026rdquo;, and [[implementation-intention]] \u0026ldquo;Traction draws you toward what you want in life, while distraction pulls you away\u0026rdquo; (chapter 9)\nSo well put, however I realize often times what floods my mind can also be other things that I want that are simply unrelated to the thing that I want that I am doing at the present moment ðŸ˜€. For this reason I really appreciate having [[log-seq]] in my personal life and [[org-mode]] and [[Org Roam]] on [[Doom Emacs]] at my work laptop. Because this lets me effortlessly write a quick note into my [[second-brain]] [[cultivating a digital knowledge garden]] , without really getting too off track with my task at hand. If we internally feel we are [[ignoring something important]] that is nagging at us, that will prevent us from being [[being present]] He was saying this requires [[CalendarTechnique]] [[calendar as your lash]] split your time across your life domains [[core values]] in a way that feels [[congruent]] with your [[intention]]. [[reflect]], [[reflect and refine]] specifically he was saying to book time on your calendar specifically to ask yourself, how did the week go? Did I get distracted? Maybe I wrote in my [[distraction tracker]]?\n\u0026ldquo;Approach the exercise of making a schedule as a curious scientist, rather than a drill sergeant, gives us the freedom to get better with each iteration\u0026rdquo; (page 58, chapter 9)\nthe [[emptying your mind]] like [[Naval Ravikant]] with [[inbox-zero-for-your-mind]]. Always feels great. And you can even burn it after if you want ðŸ˜…, part of the point is to reduce stress. the relationship between [[Clarity of purpose]] [[meaning of life]], [[meaningful-life]] [[finding my meaning]] [[my values]] . think relates to [[Oliver Burkeman]] and his [[Four Thousand Weeks book]] because he had the notion that people use [[productivity]] to [[Work hard play hard]] [[I Do Coke song]] by [[Feed Me]] and trying to [[time management]] but that haha [[Time Management for Mortals]] , time will do what it wants, My critiques I think the Marshmellow Experiment gives the wrong direction The author recommends Surfing the Urge for ten minutes say before giving in to some desire. I like that he\u0026rsquo;s discussing this and yea I agree it is good to practice impulse control. However, I think surfing the urge can just end up training yourself for \u0026ldquo;delayed gratification\u0026rdquo; as opposed to what you really want which is not to delay but to just allow the urge to pass all together. And in the other section I think with Dr Brickman, he does talk about just placing the urges aka internal triggers, on leaves on a stream. And leaning in to them, actually focusing on them rather than ignoring them. I have been trying to incorporate this and I like it. I agree \u0026ldquo;surfing the urge\u0026rdquo; is most likely not meant to be synonymous with \u0026ldquo;delayed gratification\u0026rdquo; though. The idea is to really intend to watch the urge subside, go away. But if you do not succeed, then don\u0026rsquo;t beat yourself up over it. This requires practice! I like the Joel Goldstein metaphore of this being like lifting weights at the gym and each time you challenge an internal trigger you are doing a rep ! I feel like there is a lot about [[collaborate with multiple people at work]], that can be said, which is maybe not the direct subject of the book, but is really important . I think at this point the dilema is [[closed-door-open-door]] as described by [[richard hamming]] in his [[Richard Hamming turing award lecture]] or at least one of his lectures I recall learning about this from. We need to guard our time and we need the opportunity to think things through carefully, and we can also make room for [[collaboration-opportunity]]. I think one answer I like is [[person Cal Newport]] [[Time block planner]] and more or less be open to [[collaboration-opportunity]], checking the [[Slack]] in between [[focused]] blocks of time like [[Forest pomodoro app]]. And I want to keep practicing getting better at the [[NoEstimates]] approach, given a section of time, cut down the scope to [[if it fits it ships box]] [[ship working software]] basically.\nQuotes \u0026ldquo;If we chronically neglect our values, we become something we\u0026rsquo;re not proud of our lives feel out of balance and diminished. Ironically, this ugly feeling makes us more likely to seek distractions to escape our dissatisfaction with. out actually solving the problem.\u0026rdquo; (chapter 9)\nif I know how you spend your time, then I know what might become of you. (Wolfgang von Goethe , chapter 9)\n\u0026ldquo;Traction draws you toward what you want in life, while distraction pulls you away\u0026rdquo; (chapter 9)\n\u0026ldquo;It\u0026rsquo;s a curious truth that when you gently pay attention to negative emotions, they tend to dissipate but positive ones expand.\u0026rdquo; (Oliver Burke-man wrote in the Guardian, chapter 6)\nActionable items [[choose your own adventure]] as a way of life Identify internal triggers, possibly using a Distraction Journal. And planning, very much like Cal Newport\u0026rsquo;s Time Block Planning ! At least once daily and hopefully once weekly, do strategic thinking, the 50,000 foot view, reflecting and adjusting. The various breathing like [[Physiological sigh]] I think are highly related to this too , because you can get [[burnout]] stuck in the [[get-lost-in-the-weeds]], lost the [[big-picture]]. And especially [[stress eating]] if [[working long hours]] I like the idea that in order to even have room for some strategic thinking though, you need to be in tune with the parts of your life that you care about. So you need to pay attention In a very stoic way, lean in to your worst internal triggers and practice watching them disappear. ","permalink":"https://michal.piekarczyk.xyz/post/2023-04-01--indistractable-book-summary/","summary":"Draft status Note haha, this is highly in draft mode, but I like to publish my early drafts because iterating is my preferred psychological happy place. But yea this is definitely not organized writing yet.\nBook in three sentences We escape what we intend to do when it makes us uncomfortable, but we can learn to notice the impulse to escape and stay with ( lean in ) the discomfort a little bit at a time You control your input and not your output, so align the input with your values and forgive yourself (be kind )for crappy outcomes.","title":"Indistractable book summary"},{"content":"First Stab at Alternate Day Feasting Wait Don\u0026rsquo;t you mean Alternate Day Fasting? Haha yes but I am still working up to the level of stoicism required to do proper Alternate Day Fasting and a low stress life also helps especially if you rely on snacks to counter the stress of the daily Work-From-Home-Wake-Up-Surprise-You-Are-At-Work-And-Work-Throughout-The-Entire-Day-Life haha!\nBut having already been doing Time Restricted Feeding for several years now, I think I have the stoicism requirements to at least give Alternate Day Feasting a go, which is basically aiming at calorie restriction every other day.\nI have pulled my daily calorie data from Carb Manager and electronic weigh-in data from a Withings scale I have. The first plot I did looks like\nbut maybe it is a bit noisy since it has all the data for each day, so trying also a 7 day average too,\nAlso tried , 7 day sum too, So What happened? Well the whole reason I started looking at all this was that I started a Work From Home job in December 2021 and although prior to that, I used to bike-commute to work every day, now bike rides in 2022 I admit have been way shorter, say, 3 to 6 miles a day as opposed to 12+ miles a day.\nAnd alarmingly I saw my weight go up in 2022, perhaps similar to what I recall IÃ±igo San-MillÃ¡n described in this interview, https://peterattiamd.com/inigosanmillan/ with Peter Attia , about cyclists who reduce their cycling without reducing their calorie intake.\nOf course I was nowhere near a professional cyclist and I\u0026rsquo;m sure the weight gain is related to a lack of stress management to a large extent, but nevertheless, basically starting 2023, every other day, I aimed for a lower amount of calories.\nThe first plot above does not show this very well, but the second chart, which takes 7-day averages, hopefully does a better job.\nThere are also other behavior changes I\u0026rsquo;m trying too that are not being plotted here, including the replacement of highly processed food with slightly less processed food and also improving the overall protein to energy ratio.\nHow to generate these plots? There was a bit of trial and error I can describe later, but basically,\nimport os import numpy as np import matplotlib.pyplot as plt import pandas as pd import pylab from pathlib import Path from datetime import datetime, date, timedelta from core.date_utils import utc_ts def make_xtick_labels(x, step=5): \u0026#39;\u0026#39;\u0026#39;Given x, step the labels every \u0026lt;step\u0026gt; Aka, take every \u0026lt;step\u0026gt;th x label \u0026#39;\u0026#39;\u0026#39; x_ticks = [i for i in range(len(x)) if i % step == 0] x_labels = [x[i] for i in x_ticks] return x_ticks, x_labels root_dir = Path(os.getenv(\u0026#34;ROOT_DIR\u0026#34;)) def get_data(): # Prepare food df food_logs_loc = (root_dir / \u0026#34;CarbManager/daily_logs_20210312-20230318_fe628b90-9859-4259-ab49-1ba3ce95b90c.csv\u0026#34;) food_df = pd.read_csv(food_logs_loc) cols = [\u0026#34;RawDate\u0026#34;, \u0026#34;Meal\u0026#34;, \u0026#34;Calories\u0026#34;] food_df[\u0026#34;RawDate\u0026#34;] = food_df[\u0026#34;Date\u0026#34;].map(lambda x: x.split(\u0026#34; \u0026#34;)[0]) food_df[food_df.Meal.isin([\u0026#34;Snack\u0026#34;, \u0026#34;Dinner\u0026#34;, \u0026#34;Lunch\u0026#34;, \u0026#34;Breakfast\u0026#34;])][cols].iloc[-10:] food_agg_df = food_df[[\u0026#34;RawDate\u0026#34;, \u0026#34;Calories\u0026#34;]].groupby(by=[\u0026#34;RawDate\u0026#34;]).sum().reset_index() weight_loc = root_dir / (\u0026#34;withings/2023-03-18-data_MIC_1679186118/weight.csv\u0026#34;) weight_df = pd.read_csv(weight_loc) cols = [\u0026#34;Date\u0026#34;, \u0026#34;Weight (lb)\u0026#34;] weight_df[cols].sort_values(by=\u0026#34;Date\u0026#34;).iloc[-10:] weight_df[\u0026#34;RawDate\u0026#34;] = weight_df[\u0026#34;Date\u0026#34;].map(lambda x: x.split(\u0026#34; \u0026#34;)[0]) cols = [\u0026#34;RawDate\u0026#34;, \u0026#34;Weight (lb)\u0026#34;] weight_agg_df = weight_df[cols].groupby(by=\u0026#34;RawDate\u0026#34;).min().reset_index() return food_agg_df, weight_agg_df def find_closest_date(dates, date): \u0026#34;\u0026#34;\u0026#34; Examples In [13]: from datetime import datetime, timedelta ...: ...: start_date = datetime(2022, 1, 1) ...: dates = [start_date + timedelta(days=x) for x in range(1000)] In [14]: find_closest_date(dates, datetime(2023, 1, 1)) Out[14]: datetime.datetime(2023, 1, 1, 0, 0) In [15]: start_date = datetime(2022, 1, 1) ...: dates = [start_date + timedelta(days=7) for x in range(1000)] In [16]: find_closest_date(dates, datetime(2023, 1, 1)) Out[16]: datetime.datetime(2022, 1, 8, 0, 0) \u0026#34;\u0026#34;\u0026#34; assert dates smallest = np.inf closest_date = None for x in sorted(dates): if delta := (x - date).total_seconds() \u0026lt; smallest: closest_date = x smallest = delta else: break assert closest_date return closest_date def plot_data(food_agg_df, weight_agg_df, start_date, end_date, labels=None): fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(111) color = \u0026#34;tab:blue\u0026#34; weight_label = labels.get(\u0026#34;weight_label\u0026#34;, \u0026#34;Weight (lb)\u0026#34;) food_label = labels.get(\u0026#34;food_label\u0026#34;, \u0026#34;Calories\u0026#34;) ax1.set_ylabel(weight_label, color=color) narrowed_df1 = weight_agg_df[ weight_agg_df[\u0026#34;RawDate\u0026#34;] \u0026gt;= start_date.strftime(\u0026#34;%Y-%m-%d\u0026#34;) ] data1 = narrowed_df1.to_dict(orient=\u0026#34;list\u0026#34;) x1, y1 = data1[\u0026#34;RawDate\u0026#34;], data1[\u0026#34;Weight (lb)\u0026#34;] X = narrowed_df1[\u0026#34;RawDate\u0026#34;].tolist() x_ticks, x_labels = make_xtick_labels(X, step=3) ax1.set_xticks(x_ticks) ax1.set_xticklabels(x_labels, rotation=-45) ax1.plot(x1, y1, color=color) # vertical labels if labels: if v_labels := labels.get(\u0026#34;v_labels\u0026#34;): for (k, v) in v_labels.items(): # Find closest value to k, in x_labels, closeset = find_closest_date( [datetime.strptime(x, \u0026#34;%Y-%m-%d\u0026#34;) for x in x_labels], datetime.strptime(k, \u0026#34;%Y-%m-%d\u0026#34;) ) ax1.axvline(closeset.strftime(\u0026#34;%Y-%m-%d\u0026#34;), label=v) # Flip, ax2 = ax1.twinx() data2 = food_agg_df[food_agg_df[\u0026#34;RawDate\u0026#34;] \u0026gt;= start_date.strftime(\u0026#34;%Y-%m-%d\u0026#34;)].to_dict( orient=\u0026#34;list\u0026#34; ) color = \u0026#34;tab:red\u0026#34; x2, y2 = data2[\u0026#34;RawDate\u0026#34;], data2[\u0026#34;Calories\u0026#34;] ax2.plot(x2, y2, color=color) ax2.set_xlabel(\u0026#34;Date\u0026#34;) ax2.set_ylabel(food_label, color=color) out_loc = f\u0026#34;output-data/{utc_ts()}-figure.png\u0026#34; ax1.legend() ax1.grid(True) print(\u0026#34;out_loc\u0026#34;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026#34;tight\u0026#34;) def bucket_n_days(df, days, aggregate_type): \u0026#34;\u0026#34;\u0026#34; Aggregate days with mean or sum. \u0026#34;\u0026#34;\u0026#34; df[\u0026#34;timestamp\u0026#34;] = pd.to_datetime(df[\u0026#34;RawDate\u0026#34;]) if aggregate_type == \u0026#34;mean\u0026#34;: agg_df = df.groupby(pd.Grouper(key=\u0026#34;timestamp\u0026#34;, freq=f\u0026#34;{days}D\u0026#34;)).mean().reset_index() elif aggregate_type == \u0026#34;sum\u0026#34;: agg_df = df.groupby(pd.Grouper(key=\u0026#34;timestamp\u0026#34;, freq=f\u0026#34;{days}D\u0026#34;)).sum().reset_index() agg_df[\u0026#34;RawDate\u0026#34;] = agg_df[\u0026#34;timestamp\u0026#34;].map(lambda x: x.strftime(\u0026#34;%Y-%m-%d\u0026#34;)) return agg_df # Plot day by day data, food_agg_df, weight_agg_df = get_data() start_date = date(2022, 1, 1) # \u0026#34;2023-01-01\u0026#34; end_date = date(2023, 3, 17) plot_data( food_agg_df, weight_agg_df, start_date, end_date, labels={ \u0026#34;vlabels\u0026#34;: {\u0026#34;2023-01-01\u0026#34;: \u0026#34;ADF starts\u0026#34;}, \u0026#34;food_label\u0026#34;: \u0026#34;Calories\u0026#34;, \u0026#34;weight_label\u0026#34;: \u0026#34;Weight (lb)\u0026#34;, }, ) # Try 3 day average of weight, and 3 day total, for calories, # weight_agg_3day_df = bucket_n_days(weight_agg_df, 3) # food_agg_3day_df = bucket_n_days(food_agg_df, 3) # plot_data(food_agg_3day_df, weight_agg_3day_df, start_date, end_date) # Try 7 days weight_agg_7day_df = bucket_n_days(weight_agg_df, 7, aggregate_type=\u0026#34;mean\u0026#34;) food_agg_7day_df = bucket_n_days(food_agg_df, 7, aggregate_type=\u0026#34;mean\u0026#34;) plot_data( food_agg_7day_df, weight_agg_7day_df, start_date, end_date, labels={ \u0026#34;v_labels\u0026#34;: {\u0026#34;2023-01-01\u0026#34;: \u0026#34;ADF starts\u0026#34;}, \u0026#34;food_label\u0026#34;: \u0026#34;Calories 7 day mean\u0026#34;, \u0026#34;weight_label\u0026#34;: \u0026#34;Weight (lb) 7 day mean\u0026#34;, }, ) # Sum agg instead food_agg_7day_df = bucket_n_days(food_agg_df, 7, aggregate_type=\u0026#34;sum\u0026#34;) plot_data( food_agg_7day_df, weight_agg_7day_df, start_date, end_date, labels={ \u0026#34;v_labels\u0026#34;: {\u0026#34;2023-01-01\u0026#34;: \u0026#34;ADF starts\u0026#34;}, \u0026#34;food_label\u0026#34;: \u0026#34;Calories 7 day sum\u0026#34;, \u0026#34;weight_label\u0026#34;: \u0026#34;Weight (lb) 7 day mean\u0026#34;, }, ) I am doing the version control on github, too.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-03-25--alternate-day-fasting/","summary":"First Stab at Alternate Day Feasting Wait Don\u0026rsquo;t you mean Alternate Day Fasting? Haha yes but I am still working up to the level of stoicism required to do proper Alternate Day Fasting and a low stress life also helps especially if you rely on snacks to counter the stress of the daily Work-From-Home-Wake-Up-Surprise-You-Are-At-Work-And-Work-Throughout-The-Entire-Day-Life haha!\nBut having already been doing Time Restricted Feeding for several years now, I think I have the stoicism requirements to at least give Alternate Day Feasting a go, which is basically aiming at calorie restriction every other day.","title":"Alternate Day Feasting"},{"content":"Quick note, I had moved to the PaperMod theme last month and have been slowly fixing a few things that borked. Images disappeared? So first, my images disappeared all together , from this post I had them like\n\u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.39.42.png\u0026#34; width=\u0026#34;30%\u0026#34;/\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.39.49.png\u0026#34; width=\u0026#34;30%\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.44.39.png\u0026#34; width=\u0026#34;30%\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; and I had to change them to be using the hugo language, below, then I saw the images again.\n\u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; {{\u0026lt; figure src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.39.42.png\u0026#34; width=\u0026#34;30%\u0026#34; \u0026gt;}} \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; {{\u0026lt; figure src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.39.49.png\u0026#34; width=\u0026#34;30%\u0026#34; \u0026gt;}} \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; {{\u0026lt; figure src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.44.39.png\u0026#34; width=\u0026#34;30%\u0026#34; \u0026gt;}} \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; However, they were stacked on top of each other as opposed to three images like three columns in a table.\nImages in column layout Prior to PaperMod, I had \u0026lt;table\u0026gt;\u0026lt;row\u0026gt;\u0026lt;td\u0026gt;blah\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;blah\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;blah\u0026lt;/td\u0026gt;\u0026lt;/row\u0026gt;\u0026lt;/table\u0026gt; to lay out those three images in this post, but afterwards that broke. I read that style is not data on mozilla that as a history lesson haha, using tables for layout is a thing of the past. Okay maybe that is why it broke?\nI read here about adding\nlayouts/shortcodes/column.html \u0026lt;div class=\u0026#34;col-md\u0026#34;\u0026gt;{{ .Inner }}\u0026lt;/div\u0026gt; layouts/shortcodes/row.html \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;{{ .Inner }}\u0026lt;/div\u0026gt; and\n{{\u0026lt; row \u0026gt;}} {{\u0026lt; column \u0026gt;}} something1 {{\u0026lt; /column \u0026gt;}} {{\u0026lt; column \u0026gt;}} something1 {{\u0026lt; /column \u0026gt;}} {{\u0026lt; column \u0026gt;}} something1 {{\u0026lt; /column \u0026gt;}} {{\u0026lt; /row \u0026gt;}} to do this job, but that did not work. And I suspected it was because I did not have row and col-md css classes defined anywhere. So I copied some interesting looking css from here ,\n.row { display: -webkit-box; display: flex; flex-wrap: wrap; } .col-md { flex-basis: 0; -webkit-box-flex: 1; flex-grow: 1; max-width: 100%; } I tested this locally and it worked nicely.\nBut how to persist the style? So Hugo uses git submodules for themes. Before, I was using the ananke theme, but I did not make any customizations to the css.\nAnd now I git clone -d from here and according to\nthe faq on PaperMod, to update the css, one can do that with\n.(site root) â”œâ”€â”€ config.yml â”œâ”€â”€ content/ â”œâ”€â”€ theme/hugo-PaperMod/ â””â”€â”€ assets/ â””â”€â”€ css/ â””â”€â”€ extended/ \u0026lt;--- â”œâ”€â”€ custom_css1.css \u0026lt;--- â””â”€â”€ any_name.css \u0026lt;--- but for some reason the style on my custom branch was not taking when I published to github.io .\nI think that is because somewhere in there only the master branch was getting freshly pulled through Github Actions, so I forked https://github.com/adityatelange/hugo-PaperMod, made the custom css updates in my fork, and that did the trick for me.\nCode wrapping! I did not mention this above yet but actually my first custom css that I needed in hugo-PaperMod was the use of\ncode { white-space: pre-wrap !important; } which I had read about on stackoverflow, earlier as the solution with the most votes.\nThe lack of code wrapping had been bugging me for over a year and now with this simple custom css it was a reality!! Finally you don\u0026rsquo;t need to endlessly scroll horizontally to actually read the code in a blog post.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-03-18-hugo-columns-papermod-migrate-theme/","summary":"Quick note, I had moved to the PaperMod theme last month and have been slowly fixing a few things that borked. Images disappeared? So first, my images disappeared all together , from this post I had them like\n\u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.39.42.png\u0026#34; width=\u0026#34;30%\u0026#34;/\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.39.49.png\u0026#34; width=\u0026#34;30%\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;img src=\u0026#34;https://my-blog-content.s3.amazonaws.com/2020/06/06/wahoo/2020-06-06+11.44.39.png\u0026#34; width=\u0026#34;30%\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; and I had to change them to be using the hugo language, below, then I saw the images again.","title":"adjustments when migrating to hugo paper mod theme"},{"content":"So my friend and I were discussing the general topic of what happens to our employability as people once we reach the age of retirement. My feelings on this are I am hopeful and I think people can retain their skills and probably express different kinds of skills too. (some call this wisdom =D ).\nAt this point my friend introduced some dark humor, citing this link with an actuarial table, saying that,\n\u0026ldquo;Most people over 75 are unemployable simply for the fact that theyâ€™re dead now\u0026rdquo;\nSo I like jokes but I figured it was important to fact check his joke here, so in looking at this table, a subset of it,\nI stitched together this table, and modified the joke as follows\nâ€œMost people over 80.5 are unemployable simply for the fact that theyâ€™re dead now â€œ\n","permalink":"https://michal.piekarczyk.xyz/post/2023-02-18-fact-checking-dark-humor/","summary":"So my friend and I were discussing the general topic of what happens to our employability as people once we reach the age of retirement. My feelings on this are I am hopeful and I think people can retain their skills and probably express different kinds of skills too. (some call this wisdom =D ).\nAt this point my friend introduced some dark humor, citing this link with an actuarial table, saying that,","title":"fact checking dark humor"},{"content":"type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18\nNote This is not a blog post but kind of a landing page I\u0026rsquo;m using to aggregate on-going project notes here\nVision Broadly would like to do here something like the following\ncompare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since thatâ€™s a moving target . And you can interview yourself too haha .\nI can use the [[my projects/personal/langchain-interview-me-2023-feb]] stuff concepts to see , what roles online do I align with and am I progressing towards them at #Humana or stagnating?\nMaking updating your #brag-document like a #fun-factor #[[having fun]] experience ðŸ˜€ And original intent was a UI to actually ask questions Also better #TellMeAboutYourself , #[[tell a story]] . Since the #brag-document has lots of cool stories, and also #chronological-story , this could be a cool way to weave together the personal story. collapsed:: true And for [[my projects/personal/langchain-interview-me-2023-feb]] thing, so I was in this [[May 28th, 2023]] too. Would be cool to make it easier for an individual to construct their [[TellMeAboutYourself]] since this is so important and at least to myself cannot rely on my memory haha\nGetting feedback about your text corpus of your experience Maybe the documents out there can help inform you, of other relevant terms that you forgot to discuss. Also maybe there is low-information density in your corpus. Take out the stop words haha.\nmy blog posts initial post with the #question-answer-task 20:55 So I have the #blog-post from [[Feb 18th, 2023]] here, where I put together my technical background , create embeddings from them and run a #question-answer-task #langchain , with one of the chains called \u0026ldquo;load_qa_with_sources_chain\u0026rdquo; that gives intermediate source text results too.\nAlso this one collapsed:: true [[blogpost/2023-06-25-everybody-loves-reynauds]] https://michal.piekarczyk.xyz/post/2023-06-25-everybody-loves-reynauds with a comparison across a few embedding models, to suss out which of them do or do not have medical vocabulary\nAlso applying sentence transformers to code search part one and part two\nresearch went through that [[article/Getting Started With Embeddings]] , which was useful to start learning about #sentence-transformers library collapsed:: true And more recently, I went through the #[[hugging face]] example around #Medicare and with the #article-type , [[article/Getting Started With Embeddings]] , link,\nAnd used the \u0026ldquo;langchainz\u0026rdquo; virtual env I have, and I used the https://api-inference.huggingface.co REST API specifying to use the \u0026ldquo;sentence-transformers/all-MiniLM-L6-v2\u0026rdquo; model to produce embeddings , and then the #sentence-transformers library, semantic_search , ( from sentence_transformers.util import semantic_search ) , to a question to a set of frequently asked questions\nI have this question, is the #sentence-transformers #average-pooling noisy? Can I use better #NER [[Named Entity Recognition NER]] ? Maybe help from https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da ?\nattempts on [[May 28th, 2023]], I started defining the #job-description comparison concept, and I ran a comparison of my blurb \u0026ldquo;2023-02-19T011846-the-story-blurb.txt\u0026rdquo; against \u0026ldquo;2023-05-28-enigma-mle.txt\u0026rdquo; . The results were maybe somewhat not easy to read. Perhaps a lot of text. Maybe I need shorter sentences? collapsed:: true\nMotivation / plan So, now , let me create a quick tool, to cross the sentences of a brag document, against like 10 job description embeddings, and help match them, to understand say, two kinds of problems,\n(1) Which job descriptions match the best, (2) but then also, for a specific job description, which sentences are matched and which are not matched. (3) So help you know, say even if you are not necessarily looking for something right now, can you know do you align with recent postings in your field?\nOutcome 21:50 okay here\u0026rsquo;s a quick example,\nimport torch import json from pathlib import Path import os import requests from sentence_transformers.util import semantic_search model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; api_url = f\u0026#34;https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\u0026#34; headers = {\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {hf_token}\u0026#34;} def query(texts): response = requests.post(api_url, headers=headers, json={\u0026#34;inputs\u0026#34;: texts, \u0026#34;options\u0026#34;:{\u0026#34;wait_for_model\u0026#34;:True}}) return response.json() repos_dir = os.getenv(\u0026#34;REPOS_DIR\u0026#34;) workdir = str(Path(repos_dir) / \u0026#34;2023-interview-me\u0026#34; ) loc = str(Path(repos_dir) / \u0026#34;2023-interview-me/2023-02-19T011846-the-story-blurb.txt\u0026#34;) my_story_vec = Path(loc).read_text().split(\u0026#34;\\n\u0026#34;) folder = \u0026#34;job_descriptions\u0026#34; jd1 = Path(folder) / \u0026#34;2023-05-28-enigma-mle.txt\u0026#34; texts = jd1.read_text().split(\u0026#34;\\n\u0026#34;) output = query(my_story_vec) my_story_embeddings = torch.FloatTensor(output) output = query(texts) jd_embeddings = torch.FloatTensor(output) hits = semantic_search(my_story_embeddings, jd_embeddings, top_k=10) 22:24 Ah interesting, so since unlike the #Medicare #faq tutorial, where one question was given, I am passing an array now so my output is now also multi-dimensional\n# [[texts[x[\u0026#34;corpus_id\u0026#34;]], x[\u0026#34;corpus_id\u0026#34;]] for x in hits[0] ] for i, row in enumerate(hits): print(f\u0026#34;({i})\u0026#34;, \u0026#34;matching,\u0026#34;, my_story_vec[i], \u0026#34;:\u0026#34;) hmm = [[texts[x[\u0026#34;corpus_id\u0026#34;]], x[\u0026#34;corpus_id\u0026#34;], x[\u0026#34;score\u0026#34;]] for x in row[:3] ] print(hmm, \u0026#34;\\n\\n\u0026#34;) (1) matching, : [[\u0026#39;\u0026#39;, 34, 1.0], [\u0026#39;\u0026#39;, 1, 0.9999997615814209], [\u0026#39;\u0026#39;, 9, 0.9999997615814209]] (2) matching, When I worked at zibby1, there was a project in 2015, various earlier projects.Created a Vagrant virtual machine based staging environment that developers can quickly use to stage code, to help us transition from personalized AWS staging environments which can potentially help us save several hundreds of dollars a month.. : [[\u0026#39;â€¢ Has experience working with distributed computing and building CI/CD tools.\u0026#39;, 26, 0.34121203422546387], [\u0026#39;â€¢ Engineers best-in-class solutions that enables data scientists to develop, test, explain, deploy and monitor statistical models to production environments (we use PySpark)\u0026#39;, 14, 0.3387572765350342], [\u0026#39;As a member of Machine Learning team, you will build the ML systems and infrastructure at the core of our small business data product. Your impact will be measured by the performance, testability and reliability of our ML systems.\u0026#39;, 10, 0.28739088773727417]] (3) matching, Implemented the retailer lead list reporting, so that big data heavy retailers like Sears could finally be more involved in following up with customers who were not originating their preapprovals.. : [[\u0026#39;â€¢ Is driven to work with customers to have an impact on the real world\u0026#39;, 29, 0.3841177821159363], [\u0026#39;â€¢ Impact: your work product will have a direct impact on hundreds of millions of significant decisions within the massive small business economy\u0026#39;, 21, 0.28213953971862793], [\u0026#39;This is a critical and exciting time at Enigma. We are hearing from repeated customers that our product is creating tremendous value for them and is aligned perfectly with their needs. This creates an urgent need to accelerate the build out of our machine learning capabilities\u0026#39;, 4, 0.2662915289402008]] Okay there is some beginnings of something here. Got to do some more preprocessing on this data though, do get way more cleaner comparisons .\nand on [[Jun 18th, 2023]] , how about #spacy and #[[Named Entity Recognition NER]] , collapsed:: true Think because yea I saw that #sentence-transformers #[[cosine similarity]] between my #brag-document sentences and #job-description was super low, so thinking hey how about extract entities and then attempt matches using that instead, Initially I saw that the first extraction was pulling only very few entities for this job description for instance, 19:33 hmm ok but , this is not capturing all the entities, hmm weird,\nimport spacy nlp = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) In [3]: jd = \u0026#34;\u0026#34;\u0026#34; ...: Google\u0026#39;s software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our pro ...: ducts need to handle information at massive scale, and extend well beyond web search. We\u0026#39;re looking for engineers who bring fresh ideas from all areas, including information r ...: etrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; ...: the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Googleâ€™s needs with opportunities to switch teams and projec ...: ts as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across t ...: he full-stack as we continue to push technology forward. ...: ...: With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions ...: . ...: ...: The web is what you make of it and our team is helping the world make more of the web. From open-source pros to user-experience extraordinaires, we develop products that help ...: users connect, communicate and collaborate with others. Our consumer products and cloud platforms are giving millions of users at homes, businesses, universities and nonprofit ...: s around the world the tools that shape their web experience -- and changing the way they think about computing. ...: \u0026#34;\u0026#34;\u0026#34; In [4]: doc = nlp(jd) In [5]: for ent in doc.ents: ...: print(ent.text, ent.start_char, ent.end_char, ent.label_) ...: Google 1 7 ORG billions 86 94 CARDINAL UI 504 506 GPE Googleâ€™s 641 649 ORG millions 1396 1404 CARDINAL and on [[Jun 25th, 2023]] the [[blogpost/2023-06-25-everybody-loves-reynauds]] So in that mini blogpost, I tried out multiple #[[embedding space]] using different embedding models. And it looked like only â€œall-MiniLM-L12-v2â€ appeared to have some kind of [[medical-condition]] knowledge .\n[[Jul 6th, 2023]] , can I do a #[[supervised fine-tuning]] #[[my first]] , collapsed:: true yea so just starting , going through , between https://www.sbert.net/docs/training/overview.html and [[article/Train and Fine-Tune Sentence Transformers Models]]\n08:35 so yea if a particular out of the box model uses [[average-pooling]] then for sure that yells at me that [[stop-words]] should be removed hmm 08:39 ACtually looking at https://www.kaggle.com/datasets?search=job and hmm I do see job related datasets. Maybe there are some relevant ones !? How about, say, https://www.kaggle.com/datasets/niyamatalmass/google-job-skills , obtained by way of #selenium . Ok cool so this gives me hope that maybe in the future I can pull some more posts in the future, hopefully [[web-scrape]] is still possible later. 08:55 ok that is actually pretty decent, looking at the \u0026ldquo;job_skills.csv\u0026rdquo; . Some nice jargon in there ! 09:04 ok so of the 4 dataset cases in https://huggingface.co/blog/how-to-train-sentence-transformers , I think makes most sense to use Case 2, where instead of assigning a number from 0 to 1 for similarity, I can just choose sentences that. I feel are similar to beuild a dataset. These are \u0026ldquo;positive pairs\u0026rdquo; [[positive pair]] So https://huggingface.co/datasets/embedding-data/sentence-compression here is a reference example that uses this. #[[Lossy compression]] perhaps . Kind of cool since yea #summarization-task is kind of this. Some details are missed yes but get the main idea #TLDR . I see pretty simple, each row is a json looking pair. Ah ok [[json lines]] right. Learned about this from [[Michael Light]]. https://jsonlines.org/examples/ nice. 09:13 ok so I can write some dataset building code like this,\nfrom sentence_transformers import InputExample from torch.utils.data import DataLoader train_examples = [] dataset = read_json_lines(...) for i, x in enumerate(dataset): s1, s2 = x train_examples.append( InputExample(texts=[s1, s2])) train_dataloader = DataLoader( train_examples, shuffle=True, batch_size=16) feels like I should use my [[my projects/personal/manage-my-photos]] labeler to help me kind of somewhat quickly build some labels. Ok and for case 2 of [[positive pair]] looks like https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss [[Multiple negatives ranking loss]] #[[loss function]] should be used\nfrom sentence_transformers import losses from sentence_transformers import SentenceTransformer model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; model = SentenceTransformer(model_id) train_loss = losses.MultipleNegativesRankingLoss(model=model) # fine tune model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10) just try for a handful then?\nimport pandas as pd import os from pathlib import Path loc = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/job_skills.csv\u0026#34;) df = pd.read_csv(loc) 09:30 ok stopping here. next can continue to try out the first example of this fine tuning.\n[[Jul 7th, 2023]] some [[paraphrase-mining]] hmm how can I build my dataset 08:40 [[my projects/personal/langchain-interview-me-2023-feb]]\ncollapsed:: true so yea next was going to write that csv data, see can I do a fine tune , first try haha, Wonder if I can dump out all the sentences , use out of the box similarity to see what looks like might be related, and maybe I can use the labeling annotation system I have to refine? Ok, what are the closest right now?\nimport pandas as pd import os from pathlib import Path from functools import reduce from collections import Counter loc = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/job_skills.csv\u0026#34;) df = pd.read_csv(loc) raw_sentences = reduce(lambda x, y: x + y, [re.split(r\u0026#34;[\\n\\.]\u0026#34;, df.iloc[i][col]) for i in range(df.shape[0]) for col in [\u0026#34;Responsibilities\u0026#34;, \u0026#39;Minimum Qualifications\u0026#39;, \u0026#39;Preferred Qualifications\u0026#39;] if not pd.isnull(df.iloc[i][col]) ] ) sentences = list(set(raw_sentences)) hmm ok\nIn [101]: dict(Counter(raw_sentences).most_common(5)) Out[101]: {\u0026#39;\u0026#39;: 14038, \u0026#39;BA/BS degree or equivalent practical experience\u0026#39;: 521, \u0026#39;g\u0026#39;: 261, \u0026#34;Bachelor\u0026#39;s degree or equivalent practical experience\u0026#34;: 71, \u0026#39; Specific responsibilities are assigned to interns at the start of the program\u0026#39;: 69} In [102]: len(raw_sentences), len(sentences) Out[102]: (31424, 9421) 09:13 ok and similarities , [[paraphrase-mining]]\n%%time from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) paraphrases = util.paraphrase_mining(model, sentences) for paraphrase in paraphrases[0:10]: score, i, j = paraphrase print(\u0026#34;{} \\t\\t {} \\t\\t Score: {:.4f}\u0026#34;.format(sentences[i], sentences[j], score)) 09:21 wow that was pretty fast .\nWork with Google Cloud Platform Partners to develop campaigns Work with Google Cloud Platform partners to develop campaigns Score: 1.0000 Collect customer support data from partners and derive insights for cross-functional teams Collect customer support data from partners and derive insights for cross-functional teams Score: 1.0000 10 years of partner programs experience at an enterprise software (or Cloud) company and experience with competitive partner programs 10 years of partner programs experience at an Enterprise Software (or Cloud) company and experience with competitive partner programs Score: 1.0000 9 years of experience serving in the capacity of a technical sales engineer in a cloud computing environment or equivalent experience in a customer facing role (including working as a member of a professional services or systems engineering team) 9 years of experience serving in the capacity of a Technical Sales Engineer in a cloud computing environment or equivalent experience in a customer facing role (including working as a member of a professional services or systems engineering team) Score: 1.0000 Identify, engage, and advise Google-caliber talent with a focus on creating a great experience for each candidate Identify, engage, and advise Google-caliber talent with a focus on creating a great experience for each candidate Score: 1.0000 Manage a team of software engineers, including task planning and code reviews Manage a team of Software Engineers, including task planning and code reviews Score: 1.0000 Perform an array of administrative tasks (Manage calendars, book travel, and schedule facilities and equipment) Perform an array of administrative tasks (manage calendars, book travel, and schedule facilities and equipment) Score: 1.0000 Understanding of solution architecture within web and mobile environments and technical experience of web/internet related technologies, architecture across SAAS, PAAS and IAAS and competitive cloud productivity suites Understanding of solution architecture within web and mobile environments and technical experience of web/internet related technologies, architecture across SaaS, PaaS and IaaS and competitive cloud productivity suites Score: 1.0000 Extensive knowledge of UNIX/Linux environments Extensive knowledge of Unix/Linux environments Score: 1.0000 Experience working towards strategic business goals Experience working towards strategic business goals Score: 1.0000 CPU times: user 1min 41s, sys: 3.35 s, total: 1min 44s Wall time: 57.3 s Ok seeing even though I used \u0026ldquo;set\u0026rdquo; I still have dupes . Ok seeing , should also use strip and lower case too 09:25 ok\nstripped_raw_sentences = [x.strip().lower() for x in raw_sentences] sentences = list(set(stripped_raw_sentences)) print(len(raw_sentences), len(set(raw_sentences)), len(sentences)) # 31424 9421 9325 %%time from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) paraphrases = util.paraphrase_mining(model, sentences) for paraphrase in paraphrases[0:10]: score, i, j = paraphrase print(\u0026#34;{} \\t\\t {} \\t\\t Score: {:.4f}\u0026#34;.format(sentences[i], sentences[j], score)) cpa / ca or other professional accounting accreditation cpa/ca or other professional accounting accreditation Score: 1.0000 10 years of partner programs experience at a enterprise software (or cloud) company and experience with competitive partner programs 10 years of partner programs experience at an enterprise software (or cloud) company and experience with competitive partner programs Score: 1.0000 5 years of partner programs experience at a enterprise software (or cloud) company 5 years of partner programs experience at an enterprise software (or cloud) company Score: 1.0000 technically minded, with an understanding of the technology and cloud computing market, and a passion for google cloud products (g-suite, google cloud platform) technically minded, with a understanding of the technology and cloud computing market, and a passion for google cloud products (g-suite, google cloud platform) Score: 0.9999 shape googleâ€™s approach to partnership strategy with stakeholders in partner programs, product management, engineering, sales, and marketing; support regional engagement with strategic global and regional partners shape googleâ€™s approach to partnership strategy with stakeholders in partner programs, product management, engineering, sales and marketing; support regional engagement with strategic global and regional partners Score: 0.9998 a combination of hr experience in the following areas: organizational design, succession planning, performance management, diversity and inclusion, business consulting, coaching and development, talent management, data analysis and employee relations a combination of hr experience in the following areas: organizational design, succession planning, performance management, diversity and inclusion, business consulting, coaching and development, talent management, data analysis, and employee relations Score: 0.9998 assist clients in the adoption of new products via upgrades and migrations to develop their long term success and improve product offerings by providing client feedback on features to product management and engineering assist clients in the adoption of new products via upgrades and migrations to develop their long-term success and improve product offerings by providing client feedback on features to product management and engineering Score: 0.9995 build strong relationships and operating rhythms with leaders inside and outside their core product team to efficiently implement user experiences that are cohesive, inclusive and well-informed build strong relationships and operating rhythms with leaders inside and outside their core product team to efficiently implement user experiences that are cohesive, inclusive, and well-informed Score: 0.9995 take responsibility for technical aspects of solutions to include such activities as supporting bid responses, product and solution briefings, proof-of-concept work and the coordination of supporting technical resources take responsibility for technical aspects of solutions to include such activities as supporting bid responses, product and solution briefings, proof-of-concept work, and the coordination of supporting technical resources Score: 0.9994 experience serving in the capacity of a technical sales engineer in a cloud computing environment or equivalent experience in a customer facing role (including working as a member of a professional services or systems engineering team) experience serving in the capacity of a technical sales engineer in a cloud computing environment or equivalent experience in a customer-facing role (including working as a member of a professional services or systems engineering team) Score: 0.9994 CPU times: user 1min 45s, sys: 2.84 s, total: 1min 48s Wall time: 1min 09:41 ok haha I can see there are some arbitrary internal space differences as well haha 08:49 ok lets find 10 [[positive pair]] , and use that ,\npairs = [ [] ] [[Jul 10th, 2023]] looked at the vocabulary misses and job titles collapsed:: true\nnoticed that the paraphrase mining output is not full looks like more or less we get the better matches first\nthe model I\u0026rsquo;m testing with does have technical data sources 08:56 haha this is not simple, so many sentences, is there any way of getting around hand labeling?\nMaybe I can look for technical terms which I suspect are not part of the #vocabulary , hmm So https://huggingface.co/datasets/code_search_net and [[stack exchange]] duplicate questions and actually many other technical datasets are used per https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 ,\nhmm oh the AutoTokenizer is a way to get tokens and vocabulary in the model collapsed:: true 09:14 tokenizer?\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:00\u0026lt;00:00, 36.5kB/s] Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00\u0026lt;00:00, 7.02MB/s] Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00\u0026lt;00:00, 8.44MB/s] Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00\u0026lt;00:00, 28.4kB/s] In [129]: tokenizer.vocab_files_names Out[129]: {\u0026#39;vocab_file\u0026#39;: \u0026#39;vocab.txt\u0026#39;, \u0026#39;tokenizer_file\u0026#39;: \u0026#39;tokenizer.json\u0026#39;} well that looks good ! Like a nice way perhaps to see the vocabulary,\nIn [131]: vocabulary = tokenizer.get_vocab() In [133]: len(vocabulary) Out[133]: 30522 In [135]: print(list(vocabulary.keys())[:30]) [\u0026#39;##iq\u0026#39;, \u0026#34;##\u0026#39;\u0026#34;, \u0026#39;1723\u0026#39;, \u0026#39;italians\u0026#39;, \u0026#39;caretaker\u0026#39;, \u0026#39;debbie\u0026#39;, \u0026#39;bloomberg\u0026#39;, \u0026#39;enforcing\u0026#39;, \u0026#39;sex\u0026#39;, \u0026#39;flicking\u0026#39;, \u0026#39;likes\u0026#39;, \u0026#39;glimpse\u0026#39;, \u0026#39;relax\u0026#39;, \u0026#39;coward\u0026#39;, \u0026#39;eyelids\u0026#39;, \u0026#39;worth\u0026#39;, \u0026#39;dynamics\u0026#39;, \u0026#39;##Â¹\u0026#39;, \u0026#39;recognizes\u0026#39;, \u0026#39;arcadia\u0026#39;, \u0026#39;deportivo\u0026#39;, \u0026#39;pointedly\u0026#39;, \u0026#39;iowa\u0026#39;, \u0026#39;##rio\u0026#39;, \u0026#39;moved\u0026#39;, \u0026#39;Ñ\u0026#39;, \u0026#39;news\u0026#39;, \u0026#39;whoever\u0026#39;, \u0026#39;blossom\u0026#39;, \u0026#39;preserved\u0026#39;] 09:21 Okay let me look for like if a few vocabulary terms in job descriptions are there,\njob_terms = [ \u0026#34;html\u0026#34;, \u0026#34;databricks\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;css\u0026#34;, \u0026#34;api\u0026#34;, \u0026#34;postgresql\u0026#34;, \u0026#34;database\u0026#34;, \u0026#34;mysql\u0026#34;, \u0026#34;clojure\u0026#34;, \u0026#34;java\u0026#34;, \u0026#34;javascript\u0026#34;, \u0026#34;angular\u0026#34;, \u0026#34;idempotent\u0026#34;, \u0026#34;azure\u0026#34;, \u0026#34;github\u0026#34;, \u0026#34;git\u0026#34;, \u0026#34;concurrency\u0026#34;, \u0026#34;asyncio\u0026#34;, \u0026#34;dbutils\u0026#34;, \u0026#34;ipython\u0026#34;, \u0026#34;docker\u0026#34;, \u0026#34;pipeline\u0026#34;, \u0026#34;sklearn\u0026#34;, \u0026#34;tensorflow\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;numpy\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;ec2\u0026#34;, \u0026#34;ecs\u0026#34;, \u0026#34;aws\u0026#34;, \u0026#34;sagemaker\u0026#34;, \u0026#34;nginx\u0026#34;, \u0026#34;redis\u0026#34;, \u0026#34;cli\u0026#34;, \u0026#34;auc\u0026#34;, \u0026#34;xgboost\u0026#34;, \u0026#34;repository\u0026#34;] from tqdm import tqdm hits = [] no_hits = [] for term in job_terms: for token in tqdm(vocabulary.keys()): if term in token: hits.append([term, token]) no_hits = list(set(job_terms) - set([x[0] for x in hits])) In [137]: len(hits) Out[137]: 117 In [138]: hits Out[138]: [[\u0026#39;api\u0026#39;, \u0026#39;rapidly\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;shapiro\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;shaping\u0026#39;], [\u0026#39;database\u0026#39;, \u0026#39;database\u0026#39;], [\u0026#39;ecs\u0026#39;, \u0026#39;ecstasy\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;illegitimate\u0026#39;], [\u0026#39;angular\u0026#39;, \u0026#39;triangular\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;digits\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clicks\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;inclination\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;apical\u0026#39;], [\u0026#39;java\u0026#39;, \u0026#39;java\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;cycling\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clip\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clit\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;capitals\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clicked\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;cliff\u0026#39;], [\u0026#39;concurrency\u0026#39;, \u0026#39;concurrency\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;caucus\u0026#39;], [\u0026#39;java\u0026#39;, \u0026#39;javanese\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clifton\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;client\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;legitimacy\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;capita\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clicking\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;digit\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;capitalist\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;flaws\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;incline\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climbs\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;inclined\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;digitally\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;legitimate\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;decline\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinical\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;longitude\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;declining\u0026#39;], [\u0026#39;pipeline\u0026#39;, \u0026#39;pipeline\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climax\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinics\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;capitol\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;laws\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;claws\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;rapid\u0026#39;], [\u0026#39;azure\u0026#39;, \u0026#39;azure\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;dilapidated\u0026#39;], [\u0026#39;angular\u0026#39;, \u0026#39;rectangular\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;api\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;gaping\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;caucasus\u0026#39;], [\u0026#39;redis\u0026#39;, \u0026#39;rediscovered\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;declines\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;eclipse\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;agitated\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;bureaucracy\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;scraping\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clive\u0026#39;], [\u0026#39;database\u0026#39;, \u0026#39;databases\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;therapist\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;longitudinal\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;cyclist\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climates\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinging\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;caucasian\u0026#39;], [\u0026#39;angular\u0026#39;, \u0026#39;angular\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;radcliffe\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinched\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;agitation\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;capitalism\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;recycling\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;lawson\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;fugitive\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;cyclists\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;capital\u0026#39;], [\u0026#39;python\u0026#39;, \u0026#39;python\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;paws\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clint\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clifford\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;##cliff\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;auction\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;circling\u0026#39;], [\u0026#39;repository\u0026#39;, \u0026#39;repository\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;sauce\u0026#39;], [\u0026#39;html\u0026#39;, \u0026#39;html\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clips\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;outlaws\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;##cliffe\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;escaping\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;lawsuit\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinch\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;leaping\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;rapids\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clients\u0026#39;], [\u0026#39;auc\u0026#39;, \u0026#39;auckland\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climb\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climate\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;cyclic\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;dawson\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;declined\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;click\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climatic\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinic\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;lawsuits\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climbing\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;napier\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;draws\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;landscaping\u0026#39;], [\u0026#39;aws\u0026#39;, \u0026#39;jaws\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;digital\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clinton\u0026#39;], [\u0026#39;redis\u0026#39;, \u0026#39;redistribution\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;##git\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;climbed\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;clipped\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;cliffs\u0026#39;], [\u0026#39;cli\u0026#39;, \u0026#39;euclidean\u0026#39;]] ok this is interesting then\nIn [140]: no_hits Out[140]: [\u0026#39;github\u0026#39;, \u0026#39;databricks\u0026#39;, \u0026#39;mysql\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;sklearn\u0026#39;, \u0026#39;postgresql\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;nginx\u0026#39;, \u0026#39;idempotent\u0026#39;, \u0026#39;sagemaker\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;css\u0026#39;, \u0026#39;clojure\u0026#39;, \u0026#39;dbutils\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;asyncio\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;numpy\u0026#39;, \u0026#39;ec2\u0026#39;, \u0026#39;ipython\u0026#39;, \u0026#39;javascript\u0026#39;] and job titles, maybe I could group along that first collapsed:: true 09:36 also another thing for trying next is I should also cut up and do paraphrase mining perhaps within the particular job title, (printing just a sample below )\nIn [144]: print(df[\u0026#34;Title\u0026#34;].unique().tolist()[:20]) [\u0026#39;Google Cloud Program Manager\u0026#39;, \u0026#39;Supplier Development Engineer (SDE), Cable/Connector\u0026#39;, \u0026#39;Data Analyst, Product and Tools Operations, Google Technical Services\u0026#39;, \u0026#39;Developer Advocate, Partner Engineering\u0026#39;, \u0026#39;Program Manager, Audio Visual (AV) Deployments\u0026#39;, \u0026#39;Associate Account Strategist (Czech/Slovak), Global Customer Experience\u0026#39;, \u0026#39;Supplier Development Engineer, Camera, Consumer Hardware\u0026#39;, \u0026#39;Strategic Technology Partner Manager, Healthcare and Life Sciences\u0026#39;, \u0026#39;Manufacturing Business Manager, Google Hardware\u0026#39;, \u0026#39;Solutions Architect, Healthcare and Life Sciences, Google Cloud\u0026#39;, \u0026#39;Data Analyst, Consumer Hardware\u0026#39;, \u0026#39;Partner Onboarding Manager (Americas)\u0026#39;, \u0026#39;Associate Account Strategist (Ukrainian), GMS Sales\u0026#39;, \u0026#39;Survey Lead, Google Cloud Support\u0026#39;, \u0026#39;Solution Architect, Google Cloud Platform (Apigee)\u0026#39;, \u0026#39;Manufacturing Test Engineer\u0026#39;, \u0026#39;Machine Learning Product Specialist, Google Cloud (EMEA)\u0026#39;, \u0026#39;Software Engineering Manager, Cloud Storage, Site Reliability Engineering\u0026#39;, \u0026#39;Global Supply Chain Manager, Display/Touch, Consumer Hardware\u0026#39;, \u0026#39;Technical Program Manager, ASIC Development\u0026#39;] In [145]: len(df[\u0026#34;Title\u0026#34;].unique().tolist()) Out[145]: 794 this list of titles is pretty extensive and might have duplicates also\nthoughts for later use vocabulary misses maybe to figure out what to fine tune with\n[[Jul 11th, 2023]] refined the nohits per the vocabulary of the model and used it to tokenize to verify they are unknown collapsed:: true\nyea no hits first let me use more precise way of looking for hits,\njob_terms = [ \u0026#34;html\u0026#34;, \u0026#34;databricks\u0026#34;, \u0026#34;python\u0026#34;, \u0026#34;css\u0026#34;, \u0026#34;api\u0026#34;, \u0026#34;postgresql\u0026#34;, \u0026#34;database\u0026#34;, \u0026#34;mysql\u0026#34;, \u0026#34;clojure\u0026#34;, \u0026#34;java\u0026#34;, \u0026#34;javascript\u0026#34;, \u0026#34;angular\u0026#34;, \u0026#34;idempotent\u0026#34;, \u0026#34;azure\u0026#34;, \u0026#34;github\u0026#34;, \u0026#34;git\u0026#34;, \u0026#34;concurrency\u0026#34;, \u0026#34;asyncio\u0026#34;, \u0026#34;dbutils\u0026#34;, \u0026#34;ipython\u0026#34;, \u0026#34;docker\u0026#34;, \u0026#34;pipeline\u0026#34;, \u0026#34;sklearn\u0026#34;, \u0026#34;tensorflow\u0026#34;, \u0026#34;pytorch\u0026#34;, \u0026#34;numpy\u0026#34;, \u0026#34;pandas\u0026#34;, \u0026#34;ec2\u0026#34;, \u0026#34;ecs\u0026#34;, \u0026#34;aws\u0026#34;, \u0026#34;sagemaker\u0026#34;, \u0026#34;nginx\u0026#34;, \u0026#34;redis\u0026#34;, \u0026#34;cli\u0026#34;, \u0026#34;auc\u0026#34;, \u0026#34;xgboost\u0026#34;, \u0026#34;repository\u0026#34;] from tqdm import tqdm hits = [] no_hits = [] for term in job_terms: for token in tqdm(vocabulary.keys()): if term == token.strip(\u0026#34;#\u0026#34;): hits.append([term, token]) no_hits = list(set(job_terms) - set([x[0] for x in hits])) 08:58 yea thats a lot of nohits,\nIn [151]: hits Out[151]: [[\u0026#39;html\u0026#39;, \u0026#39;html\u0026#39;], [\u0026#39;python\u0026#39;, \u0026#39;python\u0026#39;], [\u0026#39;api\u0026#39;, \u0026#39;api\u0026#39;], [\u0026#39;database\u0026#39;, \u0026#39;database\u0026#39;], [\u0026#39;java\u0026#39;, \u0026#39;java\u0026#39;], [\u0026#39;angular\u0026#39;, \u0026#39;angular\u0026#39;], [\u0026#39;azure\u0026#39;, \u0026#39;azure\u0026#39;], [\u0026#39;git\u0026#39;, \u0026#39;##git\u0026#39;], [\u0026#39;concurrency\u0026#39;, \u0026#39;concurrency\u0026#39;], [\u0026#39;pipeline\u0026#39;, \u0026#39;pipeline\u0026#39;], [\u0026#39;repository\u0026#39;, \u0026#39;repository\u0026#39;]] In [152]: no_hits Out[152]: [\u0026#39;github\u0026#39;, \u0026#39;databricks\u0026#39;, \u0026#39;ecs\u0026#39;, \u0026#39;mysql\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;sklearn\u0026#39;, \u0026#39;auc\u0026#39;, \u0026#39;aws\u0026#39;, \u0026#39;postgresql\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;nginx\u0026#39;, \u0026#39;idempotent\u0026#39;, \u0026#39;sagemaker\u0026#39;, \u0026#39;cli\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;css\u0026#39;, \u0026#39;clojure\u0026#39;, \u0026#39;dbutils\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;asyncio\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;redis\u0026#39;, \u0026#39;numpy\u0026#39;, \u0026#39;ec2\u0026#39;, \u0026#39;ipython\u0026#39;, \u0026#39;javascript\u0026#39;] and drafting looking for the nohits in the dataset, So do I see job descriptions that have those job terms I did not find vocabulary hits for?\nimport pandas as pd import os from pathlib import Path loc = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/job_skills.csv\u0026#34;) jobsdf = pd.read_csv(loc) import utils as ut columns = [\u0026#34;Responsibilities\u0026#34;, \u0026#39;Minimum Qualifications\u0026#39;, \u0026#39;Preferred Qualifications\u0026#39;] raw_sentences = ut.extract_raw_sentences(jobsdf, columns) sentences_with_oov_tokens = [] for sentence in raw_sentences: words = re.split(r\u0026#34;[^a-zA-Z0-9]\u0026#34;, sentence) words = [x for x in words if x] # for token in no_hits: realized how to tokenize an arbitrary sentence and therefore I can see this model indeed does not know about that vocabulary ! 09:23 let me just try to tokenize a fabricated sentence that has the no hit tokens,\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) sentence = \u0026#34;familiar with xgboost pandas and tensorflow including docker and other technologies\u0026#34; sentences = [sentence] encoded_input = tokenizer( sentences, padding=True, truncation=True, return_tensors=\u0026#39;pt\u0026#39;) {\u0026#39;input_ids\u0026#39;: tensor([[ 101, 5220, 2007, 1060, 18259, 9541, 3367, 25462, 2015, 1998, 23435, 12314, 2164, 8946, 2121, 1998, 2060, 6786, 102]]), \u0026#39;token_type_ids\u0026#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \u0026#39;attention_mask\u0026#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} hmm not really clear since these are numeric encodings, how to get the vocabulary debugging part here. 09:38 ah ok never mind found it in the docs here https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization 09:38 ok so here is how to #debugging #tokenizer #tokenization\nsentence = \u0026#34;familiar with xgboost pandas and tensorflow including docker and other technologies\u0026#34; tokenizer.tokenize(sentence) In [178]: print(tokenizer.tokenize(sentence)) [\u0026#39;familiar\u0026#39;, \u0026#39;with\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;##gb\u0026#39;, \u0026#39;##oo\u0026#39;, \u0026#39;##st\u0026#39;, \u0026#39;panda\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;tensor\u0026#39;, \u0026#39;##flow\u0026#39;, \u0026#39;including\u0026#39;, \u0026#39;dock\u0026#39;, \u0026#39;##er\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;other\u0026#39;, \u0026#39;technologies\u0026#39;] so yea super interesting ! if a particular word is not recognized in the vocabulary, it just gets split up into stuff that is known or the ## is used perhaps to create some kinds of smaller #subword-tokenization . I was reading the documentation of that tokenizer and found this section, #+BEGIN_QUOTE\nis_split_into_words (`bool`, *optional*, defaults to `False`): Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize. This is useful for NER or token classification. #+END_QUOTE which I think is pretty cool, referring to #[[Named Entity Recognition NER]] , used with this, 09:41 next ok yea thinking would love to inform this model of the entities, vocabulary t hat is missing.\n[[Jul 12th, 2023]] ok started building up code to capture a mini corpus, of the sentences, which have words that are not part of the vocabulary, collapsed:: true 08:17 [[my projects/personal/langchain-interview-me-2023-feb]]\n09:05 ok wow organized earlier notes a bit ! So should I therefore, collect the sentences that have the no hits and at least see what happens if I fine tune with those, if the new #sentence-transformers model has the new vocabulary ? ok, so to build a corpus, thinking for each sentence in this dataset I am working with right now, if I tokenize it using the AutoTokenizer from 'sentence-transformers/all-MiniLM-L6-v2' , and the output does not include my desired tokens or tokens prefixed with ## but the sentence does have the words in question visible in plain text, then that sentence is a candidate for the fine tuning set I think ! 09:16 Also I just glanced through what this out put, looks like,\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) vocabulary = tokenizer.get_vocab() print(vocabulary.keys()) And I don\u0026rsquo;t see anything upper case so pretty sure I can stick to lower case ! So first the slower way, and maybe I can find a faster #PyTorch way later, 09:34 ok drafting this on the side still. but high level concept yea, find sentences that have the one or more of the desired terms in plain text, that actually might be good enough, as long as I have checked indeed the words are no hits against the model vocabulary but can also tokenize such sentences and verify that the expected tokens do not exist 09:36 have a high level #question though, per #subword-tokenization how do you contain #[[Named Entity Recognition NER]] concepts if they can end up being broken up? #card Like even in the example in https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization , somehow \u0026ldquo;transformer\u0026rdquo; is not in\n\u0026#34;bert-base-cased\u0026#34; isn\u0026rsquo;t that kind of silly?\nand [[Jul 13th, 2023]] , got a bunch of the no hit sentences, at least for some definition, 08:41 [[my projects/personal/langchain-interview-me-2023-feb]]\ncollapsed:: true hm ok,\nimport utils as ut nohit_list = ut.current_nohit_list(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) raw_sentences = ut.extract_raw_sentences(jobsdf, columns) 09:02 ok will filter oov words like this\nIn [191]: for x in raw_sentences[:4]: ...: print(\u0026#34;=============\u0026#34;) ...: print(x, \u0026#34;\\n\u0026#34;, ut.sequence_from_sentence(x), \u0026#34;\\n\u0026#34;) ...: ============= lead projects from start to finish and manage all issues that impact design [\u0026#39;lead\u0026#39;, \u0026#39;projects\u0026#39;, \u0026#39;from\u0026#39;, \u0026#39;start\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;finish\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;manage\u0026#39;, \u0026#39;all\u0026#39;, \u0026#39;issues\u0026#39;, \u0026#39;that\u0026#39;, \u0026#39;impact\u0026#39;, \u0026#39;design\u0026#39;] ============= break the mold, and bring creativity and innovation in strategy and tactics [\u0026#39;break\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;mold\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;bring\u0026#39;, \u0026#39;creativity\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;innovation\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;strategy\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;tactics\u0026#39;] ============= become a brand advocate; engage and influence internal and external relationships; build, customize and deliver solutions through forums to achieve outcomes in support of the brand advertising annual plan [\u0026#39;become\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;brand\u0026#39;, \u0026#39;advocate\u0026#39;, \u0026#39;engage\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;influence\u0026#39;, \u0026#39;internal\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;external\u0026#39;, \u0026#39;relationships\u0026#39;, \u0026#39;build\u0026#39;, \u0026#39;customize\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;deliver\u0026#39;, \u0026#39;solutions\u0026#39;, \u0026#39;through\u0026#39;, \u0026#39;forums\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;achieve\u0026#39;, \u0026#39;outcomes\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;support\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;brand\u0026#39;, \u0026#39;advertising\u0026#39;, \u0026#39;annual\u0026#39;, \u0026#39;plan\u0026#39;] ============= experience in java and/or python development [\u0026#39;experience\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;java\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;development\u0026#39;] 09:08 only search some of the technical roles maybe, to try to get faster results,\nraw_titles = ut.extract_raw_sentences(jobsdf, [\u0026#34;Title\u0026#34;]) title_vocab = reduce(lambda x, y: x + y, [ut.sequence_from_sentence(x) for x in raw_titles] ) print(Counter(title_vocab).most_common(25)) [(\u0026#39;manager\u0026#39;, 300), (\u0026#39;google\u0026#39;, 237), (\u0026#39;cloud\u0026#39;, 167), (\u0026#39;and\u0026#39;, 127), (\u0026#39;sales\u0026#39;, 89), (\u0026#39;marketing\u0026#39;, 87), (\u0026#39;engineer\u0026#39;, 79), (\u0026#39;technical\u0026#39;, 71), (\u0026#39;account\u0026#39;, 64), (\u0026#39;lead\u0026#39;, 64), (\u0026#39;business\u0026#39;, 63), (\u0026#39;partner\u0026#39;, 62), (\u0026#39;solutions\u0026#39;, 61), (\u0026#39;operations\u0026#39;, 59), (\u0026#39;product\u0026#39;, 57), (\u0026#39;specialist\u0026#39;, 57), (\u0026#39;services\u0026#39;, 53), (\u0026#39;english\u0026#39;, 52), (\u0026#39;analyst\u0026#39;, 52), (\u0026#39;hardware\u0026#39;, 51), (\u0026#39;associate\u0026#39;, 48), (\u0026#39;global\u0026#39;, 46), (\u0026#39;program\u0026#39;, 44), (\u0026#39;customer\u0026#39;, 43), (\u0026#39;development\u0026#39;, 42)] ok based off of that, \u0026ldquo;engineer\u0026rdquo; feels like a safe assumption here, 09:17 ok so just the engineer sentences then,\nIn [203]: jobsdf[jobsdf[\u0026#34;Title\u0026#34;].str.contains(\u0026#34;engineer\u0026#34;)].shape Out[203]: (0, 7) In [204]: jobsdf[jobsdf[\u0026#34;Title\u0026#34;].str.contains(\u0026#34;Engineer\u0026#34;)].shape Out[204]: (140, 7) In [205]: jobsdf.shape Out[205]: (1250, 7) columns = [\u0026#34;Responsibilities\u0026#34;, \u0026#39;Minimum Qualifications\u0026#39;, \u0026#39;Preferred Qualifications\u0026#39;] engineer_df = jobsdf[jobsdf[\u0026#34;Title\u0026#34;].str.contains(\u0026#34;Engineer\u0026#34;)].copy() raw_sentences = ut.extract_raw_sentences(engineer_df, columns) nohit_sentences = ut.find_nohit_sentences(raw_sentences, nohit_list) In [211]: len(raw_sentences), len(nohit_sentences) Out[211]: (1217, 38) In [212]: nohit_sentences Out[212]: [[\u0026#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with java for android, objective-c for ios, html, javascript\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)\u0026#39;, [\u0026#39;tensorflow\u0026#39;]], [\u0026#39;software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;familiarity in one or more common web or mobile development language such as java, python, go, php, javascript, etc\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with front-end web technologies (html5, css3, and javascript)\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar\u0026#39;, [\u0026#39;mysql\u0026#39;]], [\u0026#39;familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)\u0026#39;, [\u0026#39;tensorflow\u0026#39;]], [\u0026#39;html5, css3, and javascript development experience\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;java, c/c++, c#, python, javascript, or go)\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with web technologies (object-oriented javascript, html, css), and experience with the latest web standards including html5 and css3\u0026#39;, [\u0026#39;javascript\u0026#39;, \u0026#39;css\u0026#39;]], [\u0026#39;experience programming in one of the following: java, javascript and/or c++\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;4 years of relevant work experience, including web application experience or skills using ajax, html, css or javascript\u0026#39;, [\u0026#39;javascript\u0026#39;, \u0026#39;css\u0026#39;]], [\u0026#39;, sql, mysql, mapreduce, hadoop)\u0026#39;, [\u0026#39;mysql\u0026#39;]], [\u0026#39;experience working with deployment and orchestration technologies (such as pxe, docker, kubernetes, puppet, chef, salt, ansible, jenkins)\u0026#39;, [\u0026#39;docker\u0026#39;]], [\u0026#39;development experience in c, c++ or java and experience designing modular, object-oriented javascript\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;expert html and css skills\u0026#39;, [\u0026#39;css\u0026#39;]], [\u0026#39;, unit, functional, integration, stress testing) for your code, using one or more of the following: c, c++, c#, java, javascript, go, or python\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience in writing software in one or more languages such as java, c++, python, go, javascript\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with one or more general purpose programming languages including but not limited to: c/c++, c#, python, javascript, go, objective-c, swift\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;fluency in one or more of the following: python, javascript, java, php, perl, or c++\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;previous tech internships or relevant work experience programming in c, c++, c#, java, javascript, go or python\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;, object-oriented javascript, html, css)\u0026#39;, [\u0026#39;javascript\u0026#39;, \u0026#39;css\u0026#39;]], [\u0026#39;restful, soap, etc), and javascript\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience in backend development and using one or more cloud platform services (aws, azure, gcp)\u0026#39;, [\u0026#39;aws\u0026#39;]], [\u0026#39;1 year of experience in software engineering and coding, working with two or more of the following languages: java, c/c++, c#, objective-c, python, javascript, php, ruby and/or go\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;4 years of experience working with front end languages such as html5, css, javascript (angularjs)\u0026#39;, [\u0026#39;javascript\u0026#39;, \u0026#39;css\u0026#39;]], [\u0026#39;experience with web technologies such as html, css, javascript, and http\u0026#39;, [\u0026#39;javascript\u0026#39;, \u0026#39;css\u0026#39;]], [\u0026#39;software development platforms and solutions to include j2ee, java servlets, javascript, python, go, php, asp, cgi, ajax\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;, r, python, matlab, pandas) and database languages (e\u0026#39;, [\u0026#39;pandas\u0026#39;]], [\u0026#39;experience with modern javascript frameworks (such as backbone, angular, or ember) and css pre-processing frameworks (such as sass or less)\u0026#39;, [\u0026#39;javascript\u0026#39;, \u0026#39;css\u0026#39;]], [\u0026#39;experience in writing code fixes and tools to solve problems in c, c++, c#, java, javascript, go or python (e\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;net, python, shell, perl, javascript)\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;programming experience in one or more of the following languages/platforms: android, java, kotlin, ios, javascript\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with one or more general purpose programming languages including but not limited to: java, c/c++, c#, objective c, python, javascript, or go\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience in writing software in one or more languages such as java, python, go, javascript, c++, or similar\u0026#39;, [\u0026#39;javascript\u0026#39;]], [\u0026#39;experience with java for android, and objective-c for ios, html and javascript\u0026#39;, [\u0026#39;javascript\u0026#39;]]] 09:25 hmm also actually seeing sometimes splitting on a \u0026quot;.\u0026quot; is not quite accurate. okay so next, since this is not looking terribly like a whole lot of sentences, can manually assign the ones that are similar, say, and try a fit.\n[[Jul 15th, 2023]] finally tried the [[supervised fine-tuning]] but didn\u0026rsquo;t seem to add to the vocabulary collapsed:: true\ncreated clusters manually, by looking at my no hit list from earlier, of sentences containing words that were not in the vocabulary, collapsed:: true 20:07 going to just manually create some groups,\n# web stuff, front end leaning group1 = [ \u0026#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby\u0026#39;, \u0026#39;experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver\u0026#39;, \u0026#39;software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)\u0026#39;, \u0026#39;experience with front-end web technologies (html5, css3, and javascript)\u0026#39;, \u0026#39;html5, css3, and javascript development experience\u0026#39;, \u0026#39;experience with web technologies (object-oriented javascript, html, css), and experience with the latest web standards including html5 and css3\u0026#39;, \u0026#39;4 years of relevant work experience, including web application experience or skills using ajax, html, css or javascript\u0026#39;, \u0026#39;expert html and css skills\u0026#39;, \u0026#39;restful, soap, etc), and javascript\u0026#39;, \u0026#39;4 years of experience working with front end languages such as html5, css, javascript (angularjs)\u0026#39;, \u0026#39;experience with web technologies such as html, css, javascript, and http\u0026#39;, \u0026#39;software development platforms and solutions to include j2ee, java servlets, javascript, python, go, php, asp, cgi, ajax\u0026#39;, \u0026#39;experience with modern javascript frameworks (such as backbone, angular, or ember) and css pre-processing frameworks (such as sass or less)\u0026#39;, ] # feeling more back end mle ish, group3 = [ \u0026#39;experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)\u0026#39;, \u0026#39;technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar\u0026#39;, \u0026#39;familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)\u0026#39;, \u0026#39;, sql, mysql, mapreduce, hadoop)\u0026#39;, \u0026#39;experience working with deployment and orchestration technologies (such as pxe, docker, kubernetes, puppet, chef, salt, ansible, jenkins)\u0026#39;, \u0026#39;experience in backend development and using one or more cloud platform services (aws, azure, gcp)\u0026#39;, \u0026#39;, r, python, matlab, pandas) and database languages \u0026#39;, ] # mobile dev group2 = [ \u0026#39;experience with java for android, objective-c for ios, html, javascript\u0026#39;, \u0026#39;familiarity in one or more common web or mobile development language such as java, python, go, php, javascript, etc\u0026#39;, \u0026#39;java, c/c++, c#, python, javascript, or go)\u0026#39;, \u0026#39;experience programming in one of the following: java, javascript and/or c++\u0026#39;, \u0026#39;development experience in c, c++ or java and experience designing modular, object-oriented javascript\u0026#39;, \u0026#39;, unit, functional, integration, stress testing) for your code, using one or more of the following: c, c++, c#, java, javascript, go, or python\u0026#39;, \u0026#39;experience in writing software in one or more languages such as java, c++, python, go, javascript\u0026#39;, \u0026#39;experience with one or more general purpose programming languages including but not limited to: c/c++, c#, python, javascript, go, objective-c, swift\u0026#39;, \u0026#39;fluency in one or more of the following: python, javascript, java, php, perl, or c++\u0026#39;, \u0026#39;1 year of experience in software engineering and coding, working with two or more of the following languages: java, c/c++, c#, objective-c, python, javascript, php, ruby and/or go\u0026#39;, \u0026#39;experience in writing code fixes and tools to solve problems in c, c++, c#, java, javascript, go or python \u0026#39;, \u0026#39;programming experience in one or more of the following languages/platforms: android, java, kotlin, ios, javascript\u0026#39;, \u0026#39;experience with one or more general purpose programming languages including but not limited to: java, c/c++, c#, objective c, python, javascript, or go\u0026#39;, \u0026#39;experience in writing software in one or more languages such as java, python, go, javascript, c++, or similar\u0026#39;, \u0026#39;experience with java for android, and objective-c for ios, html and javascript\u0026#39;, ] Then created a dataset from that, and ran fit with the out of the box \u0026lsquo;all-MiniLM-L6-v2\u0026rsquo; sentence transformer model collapsed:: true 20:36 since https://huggingface.co/datasets/embedding-data/sentence-compression/tree/main is given as the example and since I see those [[json lines]] , but it is with [[git-lfs]] , let me try pull it as appropriate,\nok file was \u0026ldquo;sentence-compression_compressed.jsonl.gz\u0026rdquo;, internally looks like this\n$ head data/kaggle-google-job-skills/sentence-compression_compressed.jsonl {\u0026#34;set\u0026#34;: [\u0026#34;The USHL completed an expansion draft on Monday as 10 players who were on the rosters of USHL teams during the 2009-10 season were selected by the League\u0026#39;s two newest entries, the Muskegon Lumberjacks and Dubuque Fighting Saints.\u0026#34;, \u0026#34;USHL completes expansion draft\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;Major League Baseball Commissioner Bud Selig will be speaking at St. Norbert College next month.\u0026#34;, \u0026#34;Bud Selig to speak at St. Norbert College\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;It\u0026#39;s fresh cherry time in Michigan and the best time to enjoy this delicious and nutritious fruit.\u0026#34;, \u0026#34;It\u0026#39;s cherry time\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;An Evesham man is facing charges in Pennsylvania after he allegedly dragged his girlfriend from the side of his pickup truck on the campus of Kutztown University in the early morning hours of Dec. 5, police said.\u0026#34;, \u0026#34;Evesham man faces charges for Pa.\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;NRT LLC, one of the nation\u0026#39;s largest residential real estate brokerage companies, announced several executive appointments within its Coldwell Banker Residential Brokerage operations in Southern California.\u0026#34;, \u0026#34;NRT announces executive appointments at its Coldwell Banker operations in Southern California\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;THE JSE kept toying with an all time high by midday today as resources continued to fuel the bourse.\u0026#34;, \u0026#34;JSE keeps toying with all time high\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;The government is defending the latest police crime statistics despite a worrying rise in the recorded amount of violent offending.\u0026#34;, \u0026#34;Government defends crime statistics\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;The renovated Marappalam bridge, which had been opened for two-wheelers last week, was opened for other vehicles also on Friday.\u0026#34;, \u0026#34;Marappalam bridge opened\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;A new survey shows 30 percent of Californians use Twitter, and more and more of us are using our smart phones to go online.\u0026#34;, \u0026#34;Survey: 30 percent of Californians use Twitter\u0026#34;]} {\u0026#34;set\u0026#34;: [\u0026#34;Brightpoint ,a provider of logistic services to the mobile industry, has started operations in the Turkish market.\u0026#34;, \u0026#34;Brightpoint starts operations on Turkish market\u0026#34;]} 20:50 ah ok the literal word \u0026ldquo;set\u0026rdquo; really is in there okay !\nimport os import utils as u from pathlib import Path path = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl\u0026#34;) dataset = u.make_positive_pairs_from_groups(group1, group2, group3) path.write_text(\u0026#34;\\n\u0026#34;.join([json.dumps(x) for x in dataset])) from sentence_transformers import InputExample from torch.utils.data import DataLoader train_examples = [] for i, x in enumerate(dataset): train_examples.append( InputExample(texts=[x[\u0026#34;set\u0026#34;][0], x[\u0026#34;set\u0026#34;][1]]) ) train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) # MultipleNegativesRankingLoss from sentence_transformers import losses model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) train_loss = losses.MultipleNegativesRankingLoss(model=model) model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10) 21:31 ok started that . Actually going pretty fast as I expected since yea my dataset is small for a proof of concept ,\nIteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:16\u0026lt;00:00, 1.26s/it] Epoch: 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1/10 [00:16\u0026lt;02:27, 16.36s/it] Iteration: 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 12/13 [00:19\u0026lt;00:01, 1.64s/it] ... ... CPU times: user 4min 8s, sys: 38.8 s, total: 4min 47s Wall time: 2min 43s hmm but new vocabulary does not seem to reflect new terms somehow And yea curious if I can see the vocabulary now as different,\npath = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/2023-07-15-fine-tuned-on-pairs.foo\u0026#34;) model.save(path) 21:42 oh nice, I see the vocab.txt got saved, in that folder,\n# 2023-07-15-fine-tuned-on-pairs.foo/vocab.txt\u0026#34; path = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/2023-07-15-fine-tuned-on-pairs.foo\u0026#34; / \u0026#34;vocab.txt\u0026#34;) vocab = path.read_text() In [250]: set(nohit_list) \u0026amp; set(vocab) Out[250]: set() In [251]: set([f\u0026#34;##{x}\u0026#34; for x in nohit_list]) \u0026amp; set(vocab) Out[251]: set() 21:48 hmm not seeing any words from the nohit list in the dumped out vocab though. hmm ok back to the drawing board then? haha\n[[Jul 16th, 2023]] yea tried a different take on adding tokens to a tokenizer and that seemed to do it. collapsed:: true\nyea it was not \u0026ldquo;tokenizer.json\u0026rdquo; 11:17 ok so next though,\n11:23 hmm interesting, I also looked at the \u0026ldquo;tokenizer.json\u0026rdquo; file that got created when doing model.save(), next to the \u0026ldquo;vocab.txt\u0026rdquo;. They have the same tokens looks like except \u0026ldquo;tokenizer.json\u0026rdquo; also refers to the input ids [[tokenized-input-ids]] , 11:30 hmm but maybe fine tuning simply does not update the vocabulary?\nbut \u0026ldquo;add_tokens\u0026rdquo; collapsed:: true 12:01 ok super interesting, reading here on medium someone kind of confirming that to expand the vocabulary [[add to transformer vocabulary]], and prevent [[out-of-vocabulary-words-OOV]], you need another approach,\n12:32 lets try their recommendation, just took out the for-loop since yea looking at current documentation for add_tokens function, you can add a list instead. Incorporating w/ a check of what is my hit list and no hit list ,\nfrom transformers import AutoTokenizer, AutoModel model = SentenceTransformer(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) nohit_list = ut.current_nohit_list(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) # Before vocabulary_before = list(tokenizer.get_vocab().keys()) tokenizer.add_tokens(nohit_list) # add new embeddings to the embedding matrix of the transformer model model.resize_token_embeddings(len(tokenizer)) # After vocabulary_after = list(tokenizer.get_vocab().keys()) # Did it work? 14:09 hmm got an error,\nAttributeError: \u0026#39;SentenceTransformer\u0026#39; object has no attribute \u0026#39;resize_token_embeddings\u0026#39; when trying to resize . Maybe need to go one level down, to the lower layer.\nfor child in model.children(): print(child, hasattr(child, \u0026#34;resize_token_embeddings\u0026#34;), \u0026#34;\\n\u0026#34;) Transformer({\u0026#39;max_seq_length\u0026#39;: 256, \u0026#39;do_lower_case\u0026#39;: False}) with Transformer model: BertModel False Pooling({\u0026#39;word_embedding_dimension\u0026#39;: 384, \u0026#39;pooling_mode_cls_token\u0026#39;: False, \u0026#39;pooling_mode_mean_tokens\u0026#39;: True, \u0026#39;pooling_mode_max_tokens\u0026#39;: False, \u0026#39;pooling_mode_mean_sqrt_len_tokens\u0026#39;: False}) False Normalize() False 14:13 hmm nope, weird. But ok looks like this part worked,\nIn [263]: print(set(vocabulary_after) - set(vocabulary_before)) {\u0026#39;github\u0026#39;, \u0026#39;databricks\u0026#39;, \u0026#39;nlp\u0026#39;, \u0026#39;ecs\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;pyspark\u0026#39;, \u0026#39;sklearn\u0026#39;, \u0026#39;auc\u0026#39;, \u0026#39;aws\u0026#39;, \u0026#39;postgresql\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;nginx\u0026#39;, \u0026#39;idempotent\u0026#39;, \u0026#39;sagemaker\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;cli\u0026#39;, \u0026#39;css\u0026#39;, \u0026#39;clojure\u0026#39;, \u0026#39;spacy\u0026#39;, \u0026#39;ipython\u0026#39;, \u0026#39;dbutils\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;asyncio\u0026#39;, \u0026#39;redis\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;numpy\u0026#39;, \u0026#39;ec2\u0026#39;, \u0026#39;mysql\u0026#39;, \u0026#39;javascript\u0026#39;} How about the tokenize command?\nsentence = \u0026#34;familiar with xgboost pandas and tensorflow including docker and other technologies\u0026#34; print(tokenizer.tokenize(sentence)) [\u0026#39;familiar\u0026#39;, \u0026#39;with\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;including\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;other\u0026#39;, \u0026#39;technologies\u0026#39;] 14:18 ok nice #moment/satisfaction well that does seem to work. so next, question is then, I should attempt to do some #[[cosine similarity]] , before and after, to understand did this really help ðŸ˜€\n[[Jul 17th, 2023]] Reading more, I learn you do likely need to train a new tokenizer and you can\u0026rsquo;t just simply update its vocabulary collapsed:: true\nQuick side question I had about this last tokenizer and its case awareness, out of curiosity, does tokenize now show this for upper case too now? Should be yes right since this is a uncased model\nsentence = \u0026#34;familiar with XGBoost pandas and TensorFlow including Docker and other technologies\u0026#34; print(tokenizer.tokenize(sentence)) [\u0026#39;familiar\u0026#39;, \u0026#39;with\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;including\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;other\u0026#39;, \u0026#39;technologies\u0026#39;] 08:47 nice . answer is yes.\nyea hugging face docs, collapsed:: true So I suppose that now okay this is how you add tokens to this tokenizer, but two problems still.\nWell one obvious problem is the tokenizer now needs to be thrown back into the model, But also, so what if the tokenizer now has this vocabulary, I think now the [[supervised fine-tuning]] step next can help tell this model what is the association of these new tokens in the [[embedding space]] right? Otherwise, without that, I\u0026rsquo;m curious what would the output vector , embedding, even look like for sentences with those new words? Like a undefined error? or like a equivalent of a zero vector ? 09:03 ok wow so the answer is in their nice course here, chapter 6 , on training [[tokenizer]] [[train new tokenizer from an old one]] So funny enough, the example being used here is [[code understanding]] , [[source code embedding]] and so this dataset is used, to update the tokenizer of gpt-2,\nraw_datasets = load_dataset(\u0026#34;code_search_net\u0026#34;, \u0026#34;python\u0026#34;) from transformers import AutoTokenizer old_tokenizer = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) fastinating side note mentioned here is that there are tokenizers that can be written in python, which are slow and also can be written in #Rust-lang and also #cuda . hmm so ok then you can save that tokenizer,\ntokenizer.save_pretrained(\u0026#34;code-search-net-tokenizer\u0026#34;) but how about updating the original model then ? 09:21 ok well conceptually, skipping ahead in the [[hugging face]] course there, I see here in chapter 7, that you can use a Trainer from\nfrom transformers import Trainer in order to fine tune a model and pass a tokenizer as an input, So per above I suspect that is the answer to my question!\nSo thinking about next steps Ok so a conceptual update here, I think maybe I need to hunt down some datasets or build a dataset which has additional technical language, and then use that to fine tune a tokenizer, and not just add vocabulary to it with tokenizer.add_tokens haha that was not a full answer. Yea and then I would need to use some of the tips in chapter 6 and 7 of the [[hugging face]] course to fine tune a model but a sentence transformer model say, with the tokenizer that I updated.\n[[Jul 18th, 2023]] reading more about subword tokenization and purpose of tokenizer tuning collapsed:: true not sure if my use case where technical terms are lacking from a tokenizer warrants this but maybe 09:07 So then I\u0026rsquo;m not sure why the https://huggingface.co/blog/how-to-train-sentence-transformers method I used earlier for fine tuning, earlier here the other day , did not update the vocabulary. ,\n09:13 Hmm ok maybe the explanation they give up front here helps to confirm that fine tuning a transformer model will not update the tokenizer. My impression is that fine tuning a transformer model is really just going to update the weights and since we know that many transformer models use [[subword-tokenization]], although you have new words, the fine tuning weight adjustments are made off of the subword tokens that are less meaningful than if they had concepts mapped out for those new words Although I am also slightly getting the impression that training a tokenizer is less about vocabulary and more about like #grammar because in their phrasing they refer not to the differences in the vocabulary between #English and #Japanese but to the differences in punctuation . And they are fine tuning a tokenizer on [[source code]] which definitely has different #grammar . 09:26 yea and additionally here they spell out that [[train new tokenizer from an old one]] is really about deciding what are good [[subword-tokenization]] to use, actually what sub-words. hmm But a tokenizer does indeed have a vocabulary so hmm, is my problem a tokenizer vocabulary problem or is it really I should be looking at this as a [[Named Entity Recognition NER]] problem ?\nbut yea next should try, 09:32 anyway I should still just try this,\nso next let me apply the mini corpus I had from last time, to this, and lets see what the new tokenizer does\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) [[Jul 19th, 2023]] Ran the tokenizer fine tuning with a small dataset The vocabulary of the output of train_new_from_iterator had only the new data. So \u0026ldquo;mission accomplished\u0026rdquo; haha I got the new vocabulary in there but at the cost of missing the original vocabulary ðŸ˜…\nTrying this out collapsed:: true so let\u0026rsquo;s follow along per hugging face nlp chapter 6 ,\n09:23 what does the data they are passing in for their use case look like?\nfrom datasets import load_dataset # This can take a few minutes to load, so grab a coffee or tea while you wait! raw_datasets = load_dataset(\u0026#34;code_search_net\u0026#34;, \u0026#34;python\u0026#34;) training_corpus = ( raw_datasets[\u0026#34;train\u0026#34;][i : i + 1000][\u0026#34;whole_func_string\u0026#34;] for i in range(0, len(raw_datasets[\u0026#34;train\u0026#34;]), 1000) ) ok haha I don\u0026rsquo;t want to download the whole thing because I am using a laptop tethered to my phone [[phone-tether]] #moment/haha . But I can just look at it online, https://huggingface.co/datasets/code_search_net , per the above, each row is a record and \u0026ldquo;whole_func_string\u0026rdquo; is a string of the function definition. So their training_corpus data would look like a list of strings basically. Ok let me repurpose my mini [[positive pair]] dataset from earlier for this then,\nimport os import json import utils as u from functools import reduce from pathlib import Path path = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl\u0026#34;) \u0026#34;2023-07-15-positive-pairs.jsonl\u0026#34; training_corpus = list(set( reduce( lambda x, y: x + y, [json.loads(x)[\u0026#34;set\u0026#34;] for x in Path(path).read_text().splitlines()] ) )) 09:40 ok so the second argument to\ntrain_new_from_iterator is the vocab_size . And currently it is,\nfrom transformers import AutoTokenizer old_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) old_tokenizer.vocab_size # Out[275]: 30522 ok the 52,000 in that doc is a lot more. Curious what happens ,\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) [00:00:00] Pre-processing sequences â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0 / 0 [00:00:00] Tokenize words â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 182 / 182 [00:00:00] Count pairs â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 182 / 182 [00:00:00] Compute merges â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 527 / 527 In [278]: tokenizer.vocab_size Out[278]: 601 09:44 hmm interesting, so perhaps for this to work properly, I would have needed all the original data also, concatenated with the new data? Let me just last thing, look at the vocab and hit or no hit with my terms at least,\nold_vocabulary = old_tokenizer.get_vocab() vocabulary = tokenizer.get_vocab() len(old_vocabulary), len(vocabulary) # Out[281]: (30522, 601) import utils as ut job_terms = ut.get_nohit_job_terms() print(job_terms) # [\u0026#39;html\u0026#39;, \u0026#39;databricks\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;css\u0026#39;, \u0026#39;api\u0026#39;, \u0026#39;postgresql\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;mysql\u0026#39;, \u0026#39;clojure\u0026#39;, \u0026#39;java\u0026#39;, \u0026#39;javascript\u0026#39;, \u0026#39;angular\u0026#39;, \u0026#39;idempotent\u0026#39;, \u0026#39;azure\u0026#39;, \u0026#39;github\u0026#39;, \u0026#39;git\u0026#39;, \u0026#39;concurrency\u0026#39;, \u0026#39;asyncio\u0026#39;, \u0026#39;dbutils\u0026#39;, \u0026#39;ipython\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;pipeline\u0026#39;, \u0026#39;sklearn\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;numpy\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;ec2\u0026#39;, \u0026#39;ecs\u0026#39;, \u0026#39;aws\u0026#39;, \u0026#39;sagemaker\u0026#39;, \u0026#39;nginx\u0026#39;, \u0026#39;redis\u0026#39;, \u0026#39;cli\u0026#39;, \u0026#39;auc\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;repository\u0026#39;, \u0026#39;pyspark\u0026#39;, \u0026#39;nlp\u0026#39;, \u0026#39;spacy\u0026#39;] In [285]: set(job_terms) \u0026amp; set(vocabulary) Out[285]: {\u0026#39;angular\u0026#39;, \u0026#39;aws\u0026#39;, \u0026#39;azure\u0026#39;, \u0026#39;css\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;html\u0026#39;, \u0026#39;java\u0026#39;, \u0026#39;javascript\u0026#39;, \u0026#39;mysql\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;tensorflow\u0026#39;} ok haha well good to know at least this does indeed update the vocabulary for at least the new stuff. But since vocab is super small, makes me think yea this needs to be built using original and new data , from scratch perhaps.\nThoughts summary thinkings here\nI think I should go back to the original task of evaluating how good is a model at matching a brag document to job description texts, perhaps also building my own dataset that can be used for this evaluation on a out of the box model, comparing then to something new\n[[Jul 20th, 2023]] did bit of reading learning , research mode Think I am now convinced that yes, having new terminology is a good reason for a new tokenizer, because otherwise a tokenizer that does not have the new words, will do excessive splitting and am embedding model will be less likely to get useful signal from them,\nmain benefit of train_new_from_iterator , is lets you quickly use the same class as an earlier tokenizer, but yea this is not a fine tuning step like I thought before Looking at notes from yesterday, and that new tokenizer, yea it has some of the new vocabulary,\nSome of it is not sub-worded, remaining intact\nprint(set(job_terms) \u0026amp; set(vocabulary)) {\u0026#39;aws\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;angular\u0026#39;, \u0026#39;docker\u0026#39;, \u0026#39;pandas\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;java\u0026#39;, \u0026#39;mysql\u0026#39;, \u0026#39;azure\u0026#39;, \u0026#39;css\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;html\u0026#39;, \u0026#39;javascript\u0026#39;} And yea some of it underwent [[subword-tokenization]]\nprint(set(job_terms) - set(vocabulary)) {\u0026#39;databricks\u0026#39;, \u0026#39;api\u0026#39;, \u0026#39;nlp\u0026#39;, \u0026#39;ecs\u0026#39;, \u0026#39;concurrency\u0026#39;, \u0026#39;pyspark\u0026#39;, \u0026#39;pytorch\u0026#39;, \u0026#39;sklearn\u0026#39;, \u0026#39;auc\u0026#39;, \u0026#39;pipeline\u0026#39;, \u0026#39;postgresql\u0026#39;, \u0026#39;nginx\u0026#39;, \u0026#39;idempotent\u0026#39;, \u0026#39;sagemaker\u0026#39;, \u0026#39;cli\u0026#39;, \u0026#39;xgboost\u0026#39;, \u0026#39;git\u0026#39;, \u0026#39;repository\u0026#39;, \u0026#39;clojure\u0026#39;, \u0026#39;spacy\u0026#39;, \u0026#39;dbutils\u0026#39;, \u0026#39;asyncio\u0026#39;, \u0026#39;redis\u0026#39;, \u0026#39;numpy\u0026#39;, \u0026#39;ec2\u0026#39;, \u0026#39;ipython\u0026#39;, \u0026#39;github\u0026#39;} 09:11 So this sentence become,\nprint(tokenizer.tokenize( \u0026#34;Within databricks, you can use pyspark or scala, but to use tensorflow or pytorch in databricks, you need to stick to pyspark.\u0026#34;)) [\u0026#39;with\u0026#39;, \u0026#39;##i\u0026#39;, \u0026#39;##n\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;##b\u0026#39;, \u0026#39;##ri\u0026#39;, \u0026#39;##ck\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;yo\u0026#39;, \u0026#39;##u\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;##a\u0026#39;, \u0026#39;##n\u0026#39;, \u0026#39;us\u0026#39;, \u0026#39;##e\u0026#39;, \u0026#39;py\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;##p\u0026#39;, \u0026#39;##ark\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;sca\u0026#39;, \u0026#39;##la\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;but\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;us\u0026#39;, \u0026#39;##e\u0026#39;, \u0026#39;tensorflow\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;py\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ch\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;##b\u0026#39;, \u0026#39;##ri\u0026#39;, \u0026#39;##ck\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;yo\u0026#39;, \u0026#39;##u\u0026#39;, \u0026#39;ne\u0026#39;, \u0026#39;##e\u0026#39;, \u0026#39;##d\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;##ic\u0026#39;, \u0026#39;##k\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;py\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;##p\u0026#39;, \u0026#39;##ark\u0026#39;, \u0026#39;[UNK]\u0026#39;] Ok so I suppose the advantage of the form,\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) is you are using the specific class of that earlier tokenizer,\nprint(old_tokenizer.__class__.__name__, tokenizer.__class__.__name__) BertTokenizerFast BertTokenizerFast and training a tokenizer from scratch, I can find,\nfrom tokenizers import BertWordPieceTokenizer but I don\u0026rsquo;t see BertTokenizerFast in that same namespace\nand learned more precisely why , [[why a custom tokenizer]] , and also actually how a typical [[byte-pair encoding]] [[tokenizer]] algorithm relies on sub word frequencies when building a vocabulary 09:56 ok so then I am missing the primary reason maybe also of the custom tokenizer? [[why a custom tokenizer]]\nPerson with this medium post, is also under the impression that a custom tokenizer [[tokenizer]] is useful as a way of focusing on your unique vocabulary,\nour domain is very specific, words and concepts about clothes, shapes, colors, â€¦ Therefore, we are interested in defining our own tokenizer created from our specific vocabulary, avoiding including more common words from other domains or use cases that are irrelevant for our final purpose. 10:13 ok reading, here for some more detail, So the focus around tokenizing\n\u0026#34;Don\u0026#39;t you love ðŸ¤— Transformers? We sure do.\u0026#34; w.r.t. [\u0026quot;Don't\u0026quot;, \u0026quot;you\u0026quot;,] versus [\u0026quot;Don\u0026quot;, \u0026quot;'\u0026quot;, \u0026quot;t\u0026quot;] versus [\u0026quot;Do\u0026quot;, \u0026quot;n't\u0026quot;] does hint that yes you do want to help extract units of meaning, And they say that yes you ideally want a smaller vocabulary size to help constrain computation, but using say [[character tokenization]] although ends up w/ a small vocabulary, will be less expressive and capturing the meaning of words will be more difficult. Feels like in the case of jargon words, like #acronym or just brand new words like \u0026ldquo;Tensorflow\u0026rdquo; or \u0026ldquo;pytorch\u0026rdquo; , I can see sometimes subword tokenization will be helpful, since say it would be great if the word \u0026ldquo;tensor\u0026rdquo; was tokenized as meaningfully related to \u0026ldquo;tensorflow\u0026rdquo;, similarly between \u0026ldquo;pytorch\u0026rdquo; and \u0026ldquo;python\u0026rdquo;. And I get that if you are using a subword tokenizer with a good bit of language as a input, then you should have enough to prevent the [[out-of-vocabulary-words-OOV]] misses , 10:51 Also they describe the example of BertTokenizer tokenizing an acronym it has not seen,\nfrom transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\u0026#34;bert-base-uncased\u0026#34;) tokenizer.tokenize(\u0026#34;I have a new GPU!\u0026#34;) [\u0026#34;i\u0026#34;, \u0026#34;have\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;new\u0026#34;, \u0026#34;gp\u0026#34;, \u0026#34;##u\u0026#34;, \u0026#34;!\u0026#34;] And they only mention the benefit, of not having the vocabulary miss, but no mention of the meaning of the concept of #GPU getting missed . Oh and they explain that the #double-hashtag allows tokenization to be reversible since now you know to re-attach the subwords. But not every tokenizer is reversible I think. Or at least not every tokenizer has the same syntax, since they point out , the use of the underscore _ instead, in XLNetTokenizer\nfrom transformers import XLNetTokenizer tokenizer = XLNetTokenizer.from_pretrained(\u0026#34;xlnet-base-cased\u0026#34;) tokenizer.tokenize(\u0026#34;Don\u0026#39;t you love ðŸ¤— Transformers? We sure do.\u0026#34;) [\u0026#34;â–Don\u0026#34;, \u0026#34;\u0026#39;\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;â–you\u0026#34;, \u0026#34;â–love\u0026#34;, \u0026#34;â–\u0026#34;, \u0026#34;ðŸ¤—\u0026#34;, \u0026#34;â–\u0026#34;, \u0026#34;Transform\u0026#34;, \u0026#34;ers\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;â–We\u0026#34;, \u0026#34;â–sure\u0026#34;, \u0026#34;â–do\u0026#34;, \u0026#34;.\u0026#34;] 11:10 going back to chapter 6 here, their statement helps with [[why a custom tokenizer]] , They highlight extreme reasons like your language is different than the original languages used in a model, or that your corpus is \u0026ldquo;very different\u0026rdquo; ðŸ˜€. So I am leaning more that this is about the statistics of your text data as it relates to allowing language models to extract meaning , but without being a performance burden. 12:36 continue reading there, see if I missed something, collapsed:: true Ah interesting so per here, the [[vocabulary size]] is a hyper parameter, that is like a ceiling for splitting words, so if when just say space-splitting, we have too many words, then we split until the vocabulary size is under the input there. collapsed:: true But I know per my earlier attempt, yesterday, where I provided ,\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) I ended up with python In [278]: tokenizer.vocab_size Out[278]: 601 and still I had quite a lot of [[subword-tokenization]] going on, so there are other details looks like. So at least the [[byte-pair encoding]] #algorithm-type , will start with a base vocabulary which consists of just the characters. collapsed:: true 12:51 ok actually glad I kept reading this is interesting, so this algo, will iteratively, find the most frequent symbol pair, adding the merger to its vocabulary, then finding the next most frequent symbol pair after that. So symbols start out as just letters, but after one iteration, a symbol would consist of two characters together. And larger clumps can form after that. So their example, started out with space split words with frequencies,\n(\u0026#34;hug\u0026#34;, 10), (\u0026#34;pug\u0026#34;, 5), (\u0026#34;pun\u0026#34;, 12), (\u0026#34;bun\u0026#34;, 4), (\u0026#34;hugs\u0026#34;, 5) and starting with a base vocabulary of\n[\u0026#34;b\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;n\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;u\u0026#34;] and after performing three merges, having a vocabulary of\n[\u0026#34;b\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;n\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;u\u0026#34;, \u0026#34;ug\u0026#34;, \u0026#34;un\u0026#34;, \u0026#34;hug\u0026#34;] and at that point they showed that the original set of words and their frequencies would be represented as\n(\u0026#34;hug\u0026#34;, 10), (\u0026#34;p\u0026#34;, \u0026#34;ug\u0026#34;, 5), (\u0026#34;p\u0026#34;, \u0026#34;un\u0026#34;, 12), (\u0026#34;b\u0026#34;, \u0026#34;un\u0026#34;, 4), (\u0026#34;hug\u0026#34;, \u0026#34;s\u0026#34;, 5) and that a new test word like \u0026quot;mug\u0026quot; say would end up getting represented as [\u0026quot;\u0026lt;unk\u0026gt;\u0026quot;, \u0026quot;ug\u0026quot;], since for this case \u0026quot;m\u0026quot; just was not part of the initial vocabulary. 13:10 and ultimately the size of the vocabulary for a tokenizer, [[vocabulary size]] will be the size of the base vocabulary plus the number of merges before it was decided to stop . And clearly, haha we can have some absurd algorithm implementation that never stops and we end up with a vocabulary that includes all the full words that were encountered, and then therefore we would get fewer computational benefits . 13:21 ok so [[why a custom tokenizer]], in the video link #video-type, embedded in chapter 6 link, the lean is now to yes train a tokenizer from scratch if there is new jargon yes as in a new \u0026ldquo;domain\u0026rdquo; , #card So high level four good reasons, for [[why a custom tokenizer]] [[derive-from-scratch]] #card new language, new characters ( with accents) new domain (medical, technical, legal) new style (haha like [[Old English]] or [[Old French]] ) #take-away ohhh and #moment/aha a really good example is explained that a tokenizer unfamiliar with a #corpus will excessively split and that is not good because #[[input sequence]] is limited [[context-window]] [[maximum-context-size]] ! And so you will risk not capturing the full sentence you want to pass to a #LLM . Nice. Excessive tokenizer splitting, can impact model performance, too, #question #card , why though? Maybe the argument is similar to like \u0026ldquo;\u0026rdquo; those unknowns, in that there is less information being captured. My intuitive reasoning is that tiny subwords embedding representations will be likely meaningless . The #attention will get thrown off by basically letter chunks that will end up being as common as the word \u0026ldquo;the\u0026rdquo; , so perhaps you will have just #stop-words at that point with low information. Example of this particular model tokenizer missing a lot of #unicode characters from [[Bangla]] #language-type . And yea [[out-of-vocabulary-words-OOV]] \u0026ldquo;\u0026rdquo; , has no useful information for the model to use there. And [[excessive splitting by tokenizer]] , for say #[[biomedical Glossary]] And for the other example given, of using code-search-net python dataset to train a tokenizer, I like the question that gets asked is [[performance-lift]] at least eye-balling. And in below example, she does show it is desirable to capture a concept as one token, but I think this will ultimately only happen if that example is more frequent , relative to other patterns when doing merges per [[byte-pair encoding]] [[Jul 21st, 2023]] mainly just brief thoughts about building a new dataset Thinking next I should look through various datasets out there , and choose which would have good english language breadth but also depth into the technical jargon world.\nSo for sure a dataset from job descriptions from that nice #Kaggle data. , https://www.kaggle.com/datasets/niyamatalmass/google-job-skills?resource=download maybe there are others like that , but for sure I think this is something that would need to be updated at least once or twice a year because technical terms change frequently 09:29 side note this very much feels like a [[data-drift]] problem but feels like [[nlp-drift]]\n[[Jul 22nd, 2023]] one brief look model card for, 'sentence-transformers/all-MiniLM-L6-v2' Think I am seeing that this model did not update the tokenizer of the pretrained model that it fine tuned. So what are the dataset sources in 'sentence-transformers/all-MiniLM-L6-v2' which has been my goto model recently.\nSo per https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 here, haha there are a billion sentence tuples in there haha. Not sure how easy it will be to perform the same fine tuning steps, but maybe the tokenizer step requires way less data. 13:31 So next want to answer yea what data was used for the tokenizer there. Ok so looking at https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/blob/main/train_script.py the train_script.py in there, it doesn\u0026rsquo;t look like they modified the tokenizer that was from the pre-trained model, given as an input at the bottom,\n#python train_many_data_files_v2.py --steps 1000000 --batch_size 128 \\ --model nreimers/MiniLM-L6-H384-uncased train_data_configs/all_datasets_v4.json output/all_datasets_v4_MiniLM-L6-H384-uncased-batch128 ok so then perhaps I should look up the card for that then.\n[[Jul 23rd, 2023]] More practical dive today Refreshed my story blurb texts after several months. Pulled another kaggle job description dataset for analysis. And another run of cosine similarity for the corpori corpi (what is plural of corpus haha )\nwrapping up thoughts on tokenizers from other day 09:53 ok two thoughts, so\ncollapsed:: true I can look at nreimers/MiniLM-L6-H384-uncased and if a tokenizer is described there, And I can also look at that chapter 7 fine tuning link that takes a tokenizer passed in . 10:07 I also do want to think more high level, about the overall goal , task and reevaluate approaches. But ok, quick look, of possible, at https://huggingface.co/nreimers/MiniLM-L6-H384-uncased , 10:13 only points to https://huggingface.co/microsoft/MiniLM-L12-H384-uncased but not much else to go on 10:18 and that points to this paper, https://arxiv.org/abs/2002.10957 , hmm [[LLM distillation]] with a [[LLM distillation/teacher model and teacher assistant]] 10:28 well looking at their train code in github , it looks like for each teacher assistant model (where the MiniLM on hugging face is one of them) , they appear to use these tokenizers directly,\nfrom transformers import ( BertTokenizer, DistilBertTokenizer, XLMTokenizer, XLMRobertaTokenizer, ) without modification.\nDatasets 10:31 ok hmm think I should define dataset and problem bit more now\ncollapsed:: true ok going back to a core early example, from here , I ran [[cosine similarity]] per [[sentence-transformers]] , let me try it again , ( [[May 28th, 2023]] ) This time with the new dataset I have and maybe I will add another one too. So look at that dataset job titles again,\nfrom sentence_transformers.util import semantic_search import pandas as pd import os from functools import reduce from collections import Counter from pathlib import Path import utils as ut loc = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/job_skills.csv\u0026#34;) jobsdf = pd.read_csv(loc) import utils as ut columns = [\u0026#34;Responsibilities\u0026#34;, \u0026#39;Minimum Qualifications\u0026#39;, \u0026#39;Preferred Qualifications\u0026#39;] raw_titles = ut.extract_raw_sentences(jobsdf, [\u0026#34;Title\u0026#34;]) title_vocab = reduce(lambda x, y: x + y, [ut.sequence_from_sentence(x) for x in raw_titles] ) print(Counter(title_vocab).most_common(25)) print(\u0026#34;total\u0026#34;, len(set(title_vocab))) [(\u0026#39;manager\u0026#39;, 300), (\u0026#39;google\u0026#39;, 237), (\u0026#39;cloud\u0026#39;, 167), (\u0026#39;and\u0026#39;, 127), (\u0026#39;sales\u0026#39;, 89), (\u0026#39;marketing\u0026#39;, 87), (\u0026#39;engineer\u0026#39;, 79), (\u0026#39;technical\u0026#39;, 71), (\u0026#39;account\u0026#39;, 64), (\u0026#39;lead\u0026#39;, 64), (\u0026#39;business\u0026#39;, 63), (\u0026#39;partner\u0026#39;, 62), (\u0026#39;solutions\u0026#39;, 61), (\u0026#39;operations\u0026#39;, 59), (\u0026#39;product\u0026#39;, 57), (\u0026#39;specialist\u0026#39;, 57), (\u0026#39;services\u0026#39;, 53), (\u0026#39;english\u0026#39;, 52), (\u0026#39;analyst\u0026#39;, 52), (\u0026#39;hardware\u0026#39;, 51), (\u0026#39;associate\u0026#39;, 48), (\u0026#39;global\u0026#39;, 46), (\u0026#39;program\u0026#39;, 44), (\u0026#39;customer\u0026#39;, 43), (\u0026#39;development\u0026#39;, 42)] total 624 Picking a few more that stand out,\ntechnical_job_title_terms = [ \u0026#34;engineer\u0026#34;, \u0026#34;developer\u0026#34;, \u0026#34;research\u0026#34;, \u0026#34;technical\u0026#34;, \u0026#34;analyst\u0026#34;, \u0026#34;engineering\u0026#34;, \u0026#34;data\u0026#34;, \u0026#34;sciences\u0026#34;, \u0026#34;ux\u0026#34;, \u0026#34;analytics\u0026#34;, \u0026#34;systems\u0026#34;, \u0026#34;architect\u0026#34;, \u0026#34;researcher\u0026#34;, \u0026#34;web\u0026#34;, \u0026#34;infrastructure\u0026#34;, \u0026#34;intelligence\u0026#34;, \u0026#34;quantitative\u0026#34;, \u0026#34;learning\u0026#34;, \u0026#34;software\u0026#34;, \u0026#34;scientist\u0026#34;, ] 11:48 Maybe can see if more #MLE jobs in this kaggle dataset of #Amazon job descriptions. 12:47 ok so lets run cosine similarity, ranked, between my corpus and these, descriptions, 14:36 my last blurb was a few months ago, here , so refreshing slightly,\nimport yaml import tempfile from pathlib import Path from datetime import datetime import pytz import os import utils as ut def utc_now(): return datetime.utcnow().replace(tzinfo=pytz.UTC) def utc_ts(dt): return dt.strftime(\u0026#34;%Y-%m-%dT%H%M%S\u0026#34;) def read_yaml(loc): with open(loc) as fd: return yaml.safe_load(fd) repos_dir = Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) assert repos_dir.is_dir() experience_loc = repos_dir / \u0026#34;my-challenges-and-accomplishments/experience.yaml\u0026#34; experiences_dict = read_yaml(experience_loc)[\u0026#34;Descriptions\u0026#34;] my_sentences = ut.build_my_blurb(experiences_dict) 15:44 ok and compare with those datasets, Mini filter example,\nimport pandas as pd vec = [ {\u0026#34;title\u0026#34;: \u0026#34;Software Engineer yea\u0026#34;}, {\u0026#34;title\u0026#34;: \u0026#34;Some Scientist\u0026#34;}, {\u0026#34;title\u0026#34;: \u0026#34;Product Manager\u0026#34;}, {\u0026#34;title\u0026#34;: \u0026#34;Industrial Designer\u0026#34;} ] df = pd.DataFrame.from_records(vec) In [31]: df Out[31]: title 0 Software Engineer yea 1 Some Scientist 2 Product Manager 3 Industrial Designer In [32]: df.query(\u0026#34;title.str.contains(\u0026#39;software\u0026#39;, case=False) or title.str.contains(\u0026#39;scientist\u0026#39;, case=False)\u0026#34;) Out[32]: title 0 Software Engineer yea 1 Some Scientist 16:16 ok put that into a func, ( putting it here )\ndef filter_pandas_multiple_contains(df, column, vec, case=False): \u0026#34;\u0026#34;\u0026#34;filter dataframe for column containing any string from list vec given. Example \u0026gt;\u0026gt;\u0026gt; vec = [ ... {\u0026#34;title\u0026#34;: \u0026#34;Software Engineer yea\u0026#34;}, ... {\u0026#34;title\u0026#34;: \u0026#34;Some Scientist\u0026#34;}, ... {\u0026#34;title\u0026#34;: \u0026#34;Product Manager\u0026#34;}, ... {\u0026#34;title\u0026#34;: \u0026#34;Industrial Designer\u0026#34;} ] \u0026gt;\u0026gt;\u0026gt; df = pd.DataFrame.from_records(vec) \u0026gt;\u0026gt;\u0026gt; df title 0 Software Engineer yea 1 Some Scientist 2 Product Manager 3 Industrial Designer \u0026gt;\u0026gt;\u0026gt; import utils as ut \u0026gt;\u0026gt;\u0026gt; ut.filter_pandas_multiple_contains(df, \u0026#34;title\u0026#34;, [\u0026#34;engineer\u0026#34;, \u0026#34;scientist\u0026#34;]) title 0 Software Engineer yea 1 Some Scientist \u0026#34;\u0026#34;\u0026#34; query = \u0026#34; or \u0026#34;.join( [f\u0026#34;{column}.str.contains(\u0026#39;{x}\u0026#39;, case={case})\u0026#34; for x in vec]) return df.query(query) In [39]: ut.filter_pandas_multiple_contains(df, \u0026#34;title\u0026#34;, [\u0026#34;engineer\u0026#34;, \u0026#34;scientist\u0026#34;]) Out[39]: title 0 Software Engineer yea 1 Some Scientist import pandas as pd import os from pathlib import Path import utils as ut raw_sentences = [] technical_job_title_terms = [ \u0026#34;engineer\u0026#34;, \u0026#34;developer\u0026#34;, \u0026#34;research\u0026#34;, \u0026#34;technical\u0026#34;, \u0026#34;analyst\u0026#34;, \u0026#34;engineering\u0026#34;, \u0026#34;data\u0026#34;, \u0026#34;sciences\u0026#34;, \u0026#34;ux\u0026#34;, \u0026#34;analytics\u0026#34;, \u0026#34;systems\u0026#34;, \u0026#34;architect\u0026#34;, \u0026#34;researcher\u0026#34;, \u0026#34;web\u0026#34;, \u0026#34;infrastructure\u0026#34;, \u0026#34;intelligence\u0026#34;, \u0026#34;quantitative\u0026#34;, \u0026#34;learning\u0026#34;, \u0026#34;software\u0026#34;, \u0026#34;scientist\u0026#34;, ] loc = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/job_skills.csv\u0026#34;) jobsdf = ut.filter_pandas_multiple_contains( pd.read_csv(loc), \u0026#34;Title\u0026#34;, technical_job_title_terms) columns = [\u0026#34;Responsibilities\u0026#34;, \u0026#39;Minimum Qualifications\u0026#39;, \u0026#39;Preferred Qualifications\u0026#39;] raw_sentences.extend(ut.extract_raw_sentences( jobsdf, columns)) print(\u0026#34;len raw_sentences after ingestion\u0026#34;, len(raw_sentences)) loc = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-amazon-job-skills/amazon_jobs_dataset.csv\u0026#34;) jobsdf = ut.filter_pandas_multiple_contains( pd.read_csv(loc), \u0026#34;Title\u0026#34;, technical_job_title_terms) columns = [ \u0026#39;DESCRIPTION\u0026#39;, \u0026#39;BASIC QUALIFICATIONS\u0026#39;, \u0026#39;PREFERRED QUALIFICATIONS\u0026#39;] raw_sentences.extend(ut.extract_raw_sentences( jobsdf, columns)) print(\u0026#34;len raw_sentences after ingestion\u0026#34;, len(raw_sentences)) len raw_sentences after ingestion 3259 len raw_sentences after ingestion 33081 17:44 ok yea and doing that top k cosine similarity then\nimport json from pathlib import Path import os import requests from sentence_transformers.util import semantic_search model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; hf_token = os.getenv(\u0026#34;HF_TOKEN\u0026#34;) my_story_embeddings = ut.vec_to_embeddings(model_id, hf_token, my_sentences) jd_embeddings = ut.vec_to_embeddings(model_id, hf_token, raw_sentences[:1000]) # this line took # Wall time: 7.05 s hits = semantic_search(my_story_embeddings, jd_embeddings, top_k=100) for i, row in enumerate(hits[:5]): print(f\u0026#34;({i})\u0026#34;, \u0026#34;matching,\u0026#34;, my_sentences[i], \u0026#34;:\u0026#34;) hmm = [[raw_sentences[x[\u0026#34;corpus_id\u0026#34;]], x[\u0026#34;corpus_id\u0026#34;], x[\u0026#34;score\u0026#34;]] for x in row[:3] ] print(hmm, \u0026#34;\\n\\n\u0026#34;) 18:07\nIn [20]: for i, row in enumerate(hits[:5]): ...: print(f\u0026#34;({i})\u0026#34;, \u0026#34;matching,\u0026#34;, my_sentences[i], \u0026#34;:\u0026#34;) ...: hmm = [[raw_sentences[x[\u0026#34;corpus_id\u0026#34;]], x[\u0026#34;corpus_id\u0026#34;], x[\u0026#34;score\u0026#34;]] for x in row[:3] ] ...: print(hmm, \u0026#34;\\n\\n\u0026#34;) ...: (0) matching, Built our initial method for model hosting, transitioning from a purely business rule based flow : [[\u0026#39;experience in practical business modeling or financial modeling\\ndistinctive problem solving and analytical skills, combined with impeccable business judgment\u0026#39;, 740, 0.45160090923309326], [\u0026#39;review and fulfill managed software requests to ensure products meet business needs, while overseeing programmatic compliance with associated software licenses/other agreements, contractual terms, and policies\u0026#39;, 394, 0.433631956577301], [\u0026#39;experience in business planning and/or financial modeling\u0026#39;, 425, 0.4310281574726105]] (1) matching, Created a Vagrant virtual machine based staging environment that developers can quickly use to stage code, to help us transition from personalized AWS staging environments which can potentially help us save several hundreds of dollars a month. : [[\u0026#39;experience architecting, developing software, or internet scale production-grade big data solutions in virtualized environments such as google cloud platform\u0026#39;, 753, 0.4532877206802368], [\u0026#39;establish and drive planning and execution steps towards production deployments\u0026#39;, 959, 0.4100092053413391], [\u0026#39;experience in designing and implementing build automation, and configuration management for operating system platforms.\u0026#39;, 859, 0.3592562973499298]] (2) matching, Implemented the retailer lead list reporting, so that big data heavy retailers like Sears could finally be more involved in following up with customers who were not originating their preapprovals. : [[\u0026#39;create effective, scalable, and easy to understand reporting solutions (e.g\u0026#39;, 262, 0.45110809803009033], [\u0026#39;lead global analysis of in-store demo device analytics\u0026#39;, 833, 0.4380985200405121], [\u0026#39;demonstrated understanding of customer support verticals\u0026#39;, 961, 0.42506536841392517]] (3) matching, Troubleshooted and fixed rare and difficult to detect buyout bugs. When customers had multiple payments being taken on a day when they also did a buyout, for example, there was a bug where we were incorrectly discounting the additional payments that they made that day. : [[\u0026#39;serve as a central coordination point for customer bugs and issues escalated by internal sales teams\u0026#39;, 941, 0.4794199466705322], [\u0026#39;author test plans with the goal of catching issues and fixing them at early design stage to improve the overall product quality and meet aggressive schedule\u0026#39;, 313, 0.39714837074279785], [\u0026#39;support product implementation and help partners in their day to day challenges by delivering innovative and scalable solutions to their problems and troubleshooting their issues\u0026#39;, 796, 0.3908129930496216]] (4) matching, Refactored payment processing to reflect a better interpretation of the law around customer suspense dollars. Previously, customers would pay down their next months payment. In the change, any payments that are made outside of the due date count towards the suspense account. This change required splitting out the plan shifting, away from the payment processing, into its own separate task, to simplify the new implementation of the payment law. : [[\u0026#39;work with partner teams to re-engineer process workflows around demand planning, supply planning, ordering, and fulfillment\u0026#39;, 219, 0.32384926080703735], [\u0026#39;experience in devising and implementing strategies and business improvements.\u0026#39;, 863, 0.31515663862228394], [\u0026#39;consult with internal account management teams and customers to track the progress and impact\u0026#39;, 621, 0.30947157740592957]] Ok so definitely still not impressed with the hits I\u0026rsquo;m getting here.\nThoughts A few follow up items coming to mind,\ncollapsed:: true I think per above experiment I ran, I want to find a few false negative matches , look at the scores they are producing, and then probably take a closer dive into the [[average-pooling]] . I want to really answer the question, do I need to do pre-processing, removing [[stop-words]] so the [[sentence-transformers]] [[cosine similarity]] after average pooling does not suffer? And then after doing the preprocessing if necessary , excluding it as an issue or acting to remove stop words or fluff words, then lets run cosine similarity like that. And then maybe a refined , more granular approach would be to think about using [[Named Entity Recognition NER]] maybe to better remove stop words , especially if I do not perhaps have the luxury of fine tuning. but yea side note I think fine tuning would be really helpful to help with embedding these interesting jargon words close to each other if they are indeed related And as a visual debugging I really should plot out or at least someone must have some nice tool to visualize embeddings\n[[Jul 24th, 2023]] started a nice debug session today , false negative analysis here, Ok per my notes from [[Jul 23rd, 2023]], let me hunt down one good [[false negative]],\nFor sure I am realizing yes a lot of my sentences in my personal corpus are lacking a succinctness and there is a lot of filler in there ideally I should cut out. Actually it would be cool to have that kind of feedback actually as a tool, ranking sentences by fluff haha that should be improved. And even, how many sentences are used in describing each individual project/story, can be helpful to see analyzed too.\nimport random # Using \u0026#34;my_sentences\u0026#34; defined last time for i, x in enumerate(random.choices(my_sentences, k=4)): print(i, x) 0 So the behavior was changed and I had this now, erroneously haha, so I needed to now remove it 1 Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions 2 Improved our sklearn model training that was crashing on a laptop, by cutting up the data into chunks and using python multiprocessing 3 (7) \u0026#34;humana hackathon exploration of langchain against health plan documents\u0026#34; ( I have described this in more detail earlier above ) Ok in any case, so let me search the corpus I have, for terms, say, spark, cluster pyspark, databricks . Of course I am realizing Google and Amazon have their own options for clustering and these job descriptions might not mention pyspark, but let\u0026rsquo;s see,\nimport pandas as pd import utils as ut # Using \u0026#34;raw_sentences\u0026#34; defined last time df = pd.DataFrame({\u0026#34;description\u0026#34;: raw_sentences}) In [29]: df.iloc[:5] Out[29]: description 0 app scripts, spreadsheet software, etc) 1 leadership, problem solving and analysis exper... 2 hands-on experience using and/or managing data... 3 background in solving complex challenges and d... 4 travel frequently around emea for meetings, te... terms = [\u0026#34;pyspark\u0026#34;, \u0026#34;spark\u0026#34;, \u0026#34;databricks\u0026#34;, \u0026#34;multiprocessing\u0026#34;, \u0026#34;cluster\u0026#34;] resultsdf = ut.filter_pandas_multiple_contains(df, \u0026#34;description\u0026#34;, terms) In [34]: resultsdf.shape Out[34]: (154, 1) In [36]: resultsdf.iloc[:10][\u0026#34;description\u0026#34;].tolist() Out[36]: [\u0026#39;hands-on experience using and/or managing databases, or cloud technologies such as sql, nosql, hadoop or spark\u0026#39;, \u0026#39;familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)\\ntechnical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar\u0026#39;, \u0026#39;regression, classification, clustering, etc)\u0026#39;, \u0026#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, \u0026#39;experience using and/or managing databases, and with one or all of the following: mapreduce, hadoop, spark, flume, hive, impala, spark sql and/or bigquery\u0026#39;, \u0026#39;experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)\u0026#39;, \u0026#39;experience with data processing software (such as hadoop, spark, pig, hive) and data processing algorithms (mapreduce, flume)\u0026#39;, \u0026#39;technologies you will employ to solve these complex real-world business problems include natural language processing, machine learning, image recognition, elastic computing, spark, and a host of other state of the art aws technologies\u0026#39;, \u0026#39;in order to drive expansion of the amazon catalog, we use cluster-computing technologies like mapreduce, spark and hive to process billions of products and algorithmically find products not already sold on amazon\u0026#39;, \u0026#39;experience working with scala/python/java on spark to build and deploy ml models in production\u0026#39;] 09:19 Ok cool, so there are a few meaty sentences here that would be good to compare more directly with debugging eyes . for example,\nfrom sentence_transformers.util import semantic_search, cos_sim from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; hf_token = os.getenv(\u0026#34;HF_TOKEN\u0026#34;) s1 = \u0026#34;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\u0026#34; s2 = \u0026#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39; s3 = \u0026#39;experience working with scala/python/java on spark to build and deploy ml models in production\u0026#39; for s in [s1, s2, s3]: print(s, tokenizer.tokenize(s), \u0026#34;\\n\\n\u0026#34;) embeddings = ut.vec_to_embeddings(model_id, hf_token, [s1, s2, s3]) In [45]: cos_sim(embeddings[0,:], embeddings[1, :]) Out[45]: tensor([[0.2323]]) In [46]: cos_sim(embeddings[0,:], embeddings[2, :]) Out[46]: tensor([[0.2320]]) also try the other way too ,\nIn [43]: sentences = [s1, s2, s3] ...: hits = semantic_search(embeddings, embeddings, top_k=3) ...: for i, row in enumerate(hits[:5]): ...: print(f\u0026#34;({i})\u0026#34;, \u0026#34;matching,\u0026#34;, sentences[i], \u0026#34;:\u0026#34;) ...: hmm = [[sentences[x[\u0026#34;corpus_id\u0026#34;]], x[\u0026#34;corpus_id\u0026#34;], x[\u0026#34;score\u0026#34;]] for x in row[:3] ] ...: print(hmm, \u0026#34;\\n\\n\u0026#34;) ...: (0) matching, Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions : [[\u0026#39;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\u0026#39;, 0, 1.0], [\u0026#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, 1, 0.23231448233127594], [\u0026#39;experience working with scala/python/java on spark to build and deploy ml models in production\u0026#39;, 2, 0.2320040762424469]] (1) matching, know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery : [[\u0026#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, 1, 0.9999999403953552], [\u0026#39;experience working with scala/python/java on spark to build and deploy ml models in production\u0026#39;, 2, 0.4644332528114319], [\u0026#39;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\u0026#39;, 0, 0.23231443762779236]] (2) matching, experience working with scala/python/java on spark to build and deploy ml models in production : [[\u0026#39;experience working with scala/python/java on spark to build and deploy ml models in production\u0026#39;, 2, 1.000000238418579], [\u0026#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, 1, 0.4644332826137543], [\u0026#39;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\u0026#39;, 0, 0.2320040762424469]] Ok so that gave around same result. Ok cool, no matter which func was used, Ok cool, so next want to keep diving deeper, probably ultimately looking at [[average-pooling]] here.\n[[Jul 25th, 2023]] Stop word removal experiment So for the question yesterday, why was it that two cosine similarity comparisons had basically same score, 0.2323 and 0.2320, maybe that is a clue.\nI think I have seen that completely unrelated sentences can have a zero score comparison right?\ns1 = \u0026#34;If a tree falls in the forest and there is no one there to hear it, then does it make a sound?\u0026#34; s2 = \u0026#34;A bagel with cream cheese with some lox and some capers would go great with coffee.\u0026#34; for s in [s1, s2]: print(s, tokenizer.tokenize(s), \u0026#34;\\n\\n\u0026#34;) set(tokenizer.tokenize(s1)) \u0026amp; set(tokenizer.tokenize(s2)) # {\u0026#39;a\u0026#39;, \u0026#39;and\u0026#39;} embeddings = ut.vec_to_embeddings(model_id, hf_token, [s1, s2]) cos_sim(embeddings[0,:], embeddings[1, :]) # tensor([[0.0240]]) 09:03 ok cool So for example from yesterday, lets see what happens if we remove [[stop-words]] . ( Side note stacko link someone suggests also trying to mask stop words ) .\nimport nltk from sentence_transformers.util import semantic_search, cos_sim from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; hf_token = os.getenv(\u0026#34;HF_TOKEN\u0026#34;) from nltk.corpus import stopwords s1 = \u0026#34;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\u0026#34; s2 = \u0026#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39; s3 = \u0026#39;experience working with scala/python/java on spark to build and deploy ml models in production\u0026#39; def dont_stop(s): stop_words = stopwords.words(\u0026#39;english\u0026#39;) return \u0026#34; \u0026#34;.join([x for x in s.split() if x.lower() not in stop_words]) for s in [s1, s2, s3]: print(s, \u0026#34;\\n\u0026#34;, dont_stop(s), \u0026#34;\\n\\n\u0026#34;) Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery experience working with scala/python/java on spark to build and deploy ml models in production experience working scala/python/java spark build deploy ml models production 09:23 hmm not a whole lot of stop words there but lets try anyway,\nsentences = [s1, s2, s3] stopped = [dont_stop(x) for x in sentences] embeddings = ut.vec_to_embeddings(model_id, hf_token, stopped) hits = semantic_search(embeddings, embeddings, top_k=3) for i, row in enumerate(hits[:5]): print(f\u0026#34;({i})\u0026#34;, \u0026#34;matching,\u0026#34;, stopped[i], \u0026#34;:\u0026#34;) hmm = [[stopped[x[\u0026#34;corpus_id\u0026#34;]], x[\u0026#34;corpus_id\u0026#34;], x[\u0026#34;score\u0026#34;]] for x in row[:3] ] print(hmm, \u0026#34;\\n\\n\u0026#34;) (0) matching, Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions : [[\u0026#39;Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions\u0026#39;, 0, 1.0], [\u0026#39;know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, 1, 0.3553411066532135], [\u0026#39;experience working scala/python/java spark build deploy ml models production\u0026#39;, 2, 0.25089341402053833]] (1) matching, know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery : [[\u0026#39;know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, 1, 1.0000001192092896], [\u0026#39;experience working scala/python/java spark build deploy ml models production\u0026#39;, 2, 0.37388813495635986], [\u0026#39;Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions\u0026#39;, 0, 0.3553411364555359]] (2) matching, experience working scala/python/java spark build deploy ml models production : [[\u0026#39;experience working scala/python/java spark build deploy ml models production\u0026#39;, 2, 1.0000001192092896], [\u0026#39;know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery\u0026#39;, 1, 0.3738881051540375], [\u0026#39;Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions\u0026#39;, 0, 0.25089341402053833]] 09:33 ok well I am seeing improvements undeniably here,\n0.3553 vs 0.2323 0.25089 vs 0.23200 so therefore [[sentence-transformers]] #take-away is not internally penalizing stop words as I had thought a bit that it might before. Ok I think next I still want to just dissect the way [[sentence-transformers]] [[cosine similarity]] gets the result and reproduce it manually see if I get the same thing\n[[Jul 26th, 2023]] some more experimentation, cosine similarity and stop words ok next, yea let\u0026rsquo;s try reproducing that [[cosine similarity]] ,\nfrom last time, yea this model produces embeddings with this size,\nIn [63]: embeddings[0,:].shape Out[63]: torch.Size([384]) 08:36 just double checking,\nn [68]: np.dot(embeddings[0, :], embeddings[1, :]), cos_sim(embeddings[0, :], embeddings[1, :]) Out[68]: (0.35534108, tensor([[0.3553]])) Ok next, just out of curiosity, since blog post also mentions that #spacy has a longer #stop-words list,\nfrom nltk.corpus import stopwords as sw_nltk import spacy en = spacy.load(\u0026#39;en_core_web_sm\u0026#39;) sw_spacy = en.Defaults.stop_words def dont_stop_both(s): en = spacy.load(\u0026#39;en_core_web_sm\u0026#39;) sw_spacy = en.Defaults.stop_words stop_words = set(sw_nltk.words(\u0026#39;english\u0026#39;) ) | set(sw_spacy) return \u0026#34; \u0026#34;.join([x for x in s.split() if x.lower() not in stop_words]) sentences = [s1, s2, s3] stopped = [dont_stop(x) for x in sentences] stopped_both = [dont_stop_both(x) for x in sentences] [len(x) for x in stopped], [len(x) for x in stopped_both] # ([158, 82, 76], [153, 75, 76]) 08:57 ok so compare all three ways, for completeness\nresults = [] for name, a_list in [(\u0026#34;original\u0026#34;, sentences), (\u0026#34;nltk\u0026#34;, stopped), (\u0026#34;nltk+spacy\u0026#34;, stopped_both)]: embeddings = ut.vec_to_embeddings(model_id, hf_token, a_list) results.append({\u0026#34;what\u0026#34;: name, \u0026#34;one\u0026#34;: cos_sim(embeddings[0, :], embeddings[1, :]), \u0026#34;two\u0026#34;: cos_sim(embeddings[0, :], embeddings[2, :]), }) pd.DataFrame.from_records(results) what one two 0 original [[tensor(0.2323)]] [[tensor(0.2320)]] 1 nltk [[tensor(0.3553)]] [[tensor(0.2509)]] 2 nltk+spacy [[tensor(0.3329)]] [[tensor(0.2501)]] 09:08 Ok haha can be slightly hit or miss then . But if I did this other extreme stop removal, say specifically with information I know about these sentences,\njargon = [\u0026#34;python\u0026#34;, \u0026#34;databricks\u0026#34;, \u0026#34;cluster\u0026#34;, \u0026#34;pyspark\u0026#34;, \u0026#34;scala\u0026#34;, \u0026#34;answer\u0026#34;, \u0026#34;question\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;map\u0026#34;, \u0026#34;reduce\u0026#34;, \u0026#34;hadoop\u0026#34;, \u0026#34;spark\u0026#34;, \u0026#34;flume\u0026#34;, \u0026#34;hive\u0026#34;, \u0026#34;impala\u0026#34;, \u0026#34;sparksql\u0026#34;, \u0026#34;bigquery\u0026#34;, \u0026#34;java\u0026#34;, \u0026#34;build\u0026#34;, \u0026#34;deploy\u0026#34;, \u0026#34;ml\u0026#34;, \u0026#34;models\u0026#34;, \u0026#34;production\u0026#34;] def talk_jargon_to_me(jargon, s): return \u0026#34; \u0026#34;.join([ (x if x.lower() in jargon else \u0026#34;blah\u0026#34;) for x in s.split()]) only_jargon_sentences = [talk_jargon_to_me(jargon, x) for x in sentences] results = [] for name, a_list in [ (\u0026#34;original\u0026#34;, sentences), (\u0026#34;nltk\u0026#34;, stopped), (\u0026#34;nltk+spacy\u0026#34;, stopped_both), (\u0026#34;only_jargon\u0026#34;, only_jargon_sentences) ]: embeddings = ut.vec_to_embeddings(model_id, hf_token, a_list) results.append({\u0026#34;what\u0026#34;: name, \u0026#34;one\u0026#34;: cos_sim(embeddings[0, :], embeddings[1, :]), \u0026#34;two\u0026#34;: cos_sim(embeddings[0, :], embeddings[2, :]), }) pd.DataFrame.from_records(results) what one two 0 original [[tensor(0.2323)]] [[tensor(0.2320)]] 1 nltk [[tensor(0.3553)]] [[tensor(0.2509)]] 2 nltk+spacy [[tensor(0.3329)]] [[tensor(0.2501)]] 3 only_jargon [[tensor(0.3406)]] [[tensor(0.5675)]] 09:36 Ok so sentence transformers can be a bit of a blunt tool for sure but I think I am verifying that with some pre-processing , I can get better use from them . And improving the jargon game can be helpful too.\nOne more thing out of curiosity,\nmodel_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; vocabulary = ut.vocabulary_of_model(model_id) [(x, x in vocabulary) for x in [\u0026#34;spark\u0026#34;, \u0026#34;pyspark\u0026#34;, \u0026#34;python\u0026#34;]] # [(\u0026#39;spark\u0026#39;, True), (\u0026#39;pyspark\u0026#39;, False), (\u0026#39;python\u0026#39;, True)] [(x, tokenizer.tokenize(x)) for x in [\u0026#34;spark\u0026#34;, \u0026#34;pyspark\u0026#34;, \u0026#34;python\u0026#34;]] [(\u0026#39;spark\u0026#39;, [\u0026#39;spark\u0026#39;]), (\u0026#39;pyspark\u0026#39;, [\u0026#39;p\u0026#39;, \u0026#39;##ys\u0026#39;, \u0026#39;##park\u0026#39;]), (\u0026#39;python\u0026#39;, [\u0026#39;python\u0026#39;])] terms = [\u0026#34;spark\u0026#34;, \u0026#34;pyspark\u0026#34;, \u0026#34;python\u0026#34;] embeddings = ut.vec_to_embeddings(model_id, hf_token, terms) [ [\u0026#34;spark, pyspark\u0026#34;, cos_sim(embeddings[0, :], embeddings[1, :])], [\u0026#34;spark, python\u0026#34;, cos_sim(embeddings[0, :], embeddings[2, :])], [\u0026#34;pyspark, python\u0026#34;, cos_sim(embeddings[1, :], embeddings[2, :])], ] [[\u0026#39;spark, pyspark\u0026#39;, tensor([[0.5201]])], [\u0026#39;spark, python\u0026#39;, tensor([[0.2316]])], [\u0026#39;pyspark, python\u0026#39;, tensor([[0.4150]])]] 09:47 might be worth poking at this a bit more, so if \u0026ldquo;pyspark\u0026rdquo; is not in the vocabulary here, but hmm maybe through all the fine tuning, on a billion sentences, maybe the embeddings still ended up being meaningfully close? Want to better understand this\n[[Jul 27th, 2023]] hmm what is the subword tokenization multiple , when thinking about truncation So it was cool to see you can get a better #[[cosine similarity]] score when taking out stop words and also when replacing the non-jargon words with blah words. Although that second part hmm might have actually artificially increased the score thre now that I think about it, ðŸ¤”\nMaybe thinking about sentences is hmm not going to be as useful as thinking about the entire job description itself maybe.\nSo we have space only for 384 tokens but that can still be a good number of sentences, 10 mmaybe . Ah yea and that is another good reason to get rid of the stop words Like from my example from yesterday and day before, how many word-pieces in those?\ntokenizer = AutoTokenizer.from_pretrained(\u0026#39;sentence-transformers/all-MiniLM-L6-v2\u0026#39;) [{\u0026#34;num_words\u0026#34;: len(x.split(\u0026#34; \u0026#34;)), \u0026#34;num_tokens\u0026#34;: len(tokenizer.tokenize(x)), \u0026#34;token_inflation_factor\u0026#34;: len(tokenizer.tokenize(x))/len(x.split(\u0026#34; \u0026#34;))} for x in [s1, s2, s3]] [{\u0026#39;num_words\u0026#39;: 34, \u0026#39;num_tokens\u0026#39;: 46, \u0026#39;token_inflation_factor\u0026#39;: 1.3529411764705883}, {\u0026#39;num_words\u0026#39;: 13, \u0026#39;num_tokens\u0026#39;: 27, \u0026#39;token_inflation_factor\u0026#39;: 2.076923076923077}, {\u0026#39;num_words\u0026#39;: 14, \u0026#39;num_tokens\u0026#39;: 18, \u0026#39;token_inflation_factor\u0026#39;: 1.2857142857142858}] So yea if we have a 384 length input window then yea maybe 10 sentences or so on a good day. Not bad. 09:09 yea side note about that [[average-pooling]],\nReading the https://www.sbert.net/examples/applications/computing-embeddings/README.html section here again for insight. This is a good page Interesting looking at that mean_pooling function, it takes the attention mask into accoount. kinda cool,\n[[Jul 28th, 2023]] looking more closely on how sentence transformer model pools ok thing I got it this time, per https://www.sbert.net/examples/applications/computing-embeddings/README.html\nIn [109]: #Sentences we want sentence embeddings for ...: sentences = [\u0026#39;This framework generates embeddings for each input sentence\u0026#39;, ...: \u0026#39;Sentences are passed as a list of string.\u0026#39;, ...: \u0026#39;The quick brown fox jumps over the lazy dog.\u0026#39;] ...: ...: #Load AutoModel from huggingface model repository ...: tokenizer = AutoTokenizer.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) ...: model = AutoModel.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) ...: ...: #Tokenize sentences ...: encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors=\u0026#39;pt\u0026#39;) ...: ...: #Compute token embeddings ...: with torch.no_grad(): ...: model_output = model(**encoded_input) ...: Downloading (â€¦)\u0026#34;pytorch_model.bin\u0026#34;;: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.9M/90.9M [00:56\u0026lt;00:00, 1.61MB/s] Downloading (â€¦)\u0026#34;pytorch_model.bin\u0026#34;;: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.9M/90.9M [00:56\u0026lt;00:00, 1.93MB/s] In [110]: 08:53 ok let\u0026rsquo;s try to understand the function they show there, So model_output,\nIn [123]: type(encoded_input) Out[123]: transformers.tokenization_utils_base.BatchEncoding In [124]: vars(encoded_input) Out[124]: {\u0026#39;data\u0026#39;: {\u0026#39;input_ids\u0026#39;: tensor([[ 101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102], [ 101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0], [ 101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102, 0]]), \u0026#39;token_type_ids\u0026#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \u0026#39;attention_mask\u0026#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}, \u0026#39;_encodings\u0026#39;: [Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])], \u0026#39;_n_sequences\u0026#39;: 1} 09:16 ok so looks somewhat close, to doing it manually, below,\nprint(\u0026#34;what was encoded,\u0026#34;, encoded_input.data[\u0026#34;input_ids\u0026#34;][0], ) print(\u0026#34;back to tokens though,\u0026#34;, tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0])) print(\u0026#34;original sentence,\u0026#34;, sentences[0]) tokens = tokenizer.tokenize(sentences[0]) print(\u0026#34;sentence to tokens\u0026#34;, tokens) print(\u0026#34;tokens to ids\u0026#34;, tokenizer.convert_tokens_to_ids(tokens)) what was encoded, tensor([ 101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102]) back to tokens though, [\u0026#39;[CLS]\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;framework\u0026#39;, \u0026#39;generates\u0026#39;, \u0026#39;em\u0026#39;, \u0026#39;##bed\u0026#39;, \u0026#39;##ding\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;each\u0026#39;, \u0026#39;input\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;[SEP]\u0026#39;] original sentence, This framework generates embeddings for each input sentence sentence to tokens [\u0026#39;this\u0026#39;, \u0026#39;framework\u0026#39;, \u0026#39;generates\u0026#39;, \u0026#39;em\u0026#39;, \u0026#39;##bed\u0026#39;, \u0026#39;##ding\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;each\u0026#39;, \u0026#39;input\u0026#39;, \u0026#39;sentence\u0026#39;] tokens to ids [2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251] Only difference is I see when going back from input ids to tokens , there is an additional [CLS] at the start and a [SEP] at the end. no pad ?\nencoded_input_no_pad = tokenizer(sentences, padding=False, truncation=True, max_length=128, return_tensors=\u0026#39;pt\u0026#39;) print(\u0026#34;without pad, tokens, \u0026#34;, tokenizer.convert_ids_to_tokens( encoded_input_no_pad.data[\u0026#34;input_ids\u0026#34;][0])) ValueError: Unable to create tensor, you should probably activate truncation and/or padding with \u0026#39;padding=True\u0026#39; \u0026#39;truncation=True\u0026#39; to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected). Ah ok so that doesn\u0026rsquo;t even work then so padding required.\n([[{\u0026#34;num_words\u0026#34;: len(x.split(\u0026#34; \u0026#34;)), \u0026#34;len_tokens\u0026#34;: len(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][i])), \u0026#34;len_input_ids\u0026#34;: encoded_input.data[\u0026#34;input_ids\u0026#34;][i,:].shape, \u0026#34;len_mask\u0026#34;: encoded_input.data[\u0026#34;attention_mask\u0026#34;][i,:].shape, }, x, tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][i]), encoded_input.data[\u0026#34;input_ids\u0026#34;][i,:], encoded_input.data[\u0026#34;attention_mask\u0026#34;][i,:] ] for i, x in enumerate(sentences)]) [[{\u0026#39;num_words\u0026#39;: 8, \u0026#39;len_tokens\u0026#39;: 13, \u0026#39;len_input_ids\u0026#39;: torch.Size([13]), \u0026#39;len_mask\u0026#39;: torch.Size([13])}, \u0026#39;This framework generates embeddings for each input sentence\u0026#39;, [\u0026#39;[CLS]\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;framework\u0026#39;, \u0026#39;generates\u0026#39;, \u0026#39;em\u0026#39;, \u0026#39;##bed\u0026#39;, \u0026#39;##ding\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;each\u0026#39;, \u0026#39;input\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;[SEP]\u0026#39;], tensor([ 101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])], [{\u0026#39;num_words\u0026#39;: 8, \u0026#39;len_tokens\u0026#39;: 13, \u0026#39;len_input_ids\u0026#39;: torch.Size([13]), \u0026#39;len_mask\u0026#39;: torch.Size([13])}, \u0026#39;Sentences are passed as a list of string.\u0026#39;, [\u0026#39;[CLS]\u0026#39;, \u0026#39;sentences\u0026#39;, \u0026#39;are\u0026#39;, \u0026#39;passed\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;], tensor([ 101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])], [{\u0026#39;num_words\u0026#39;: 9, \u0026#39;len_tokens\u0026#39;: 13, \u0026#39;len_input_ids\u0026#39;: torch.Size([13]), \u0026#39;len_mask\u0026#39;: torch.Size([13])}, \u0026#39;The quick brown fox jumps over the lazy dog.\u0026#39;, [\u0026#39;[CLS]\u0026#39;, \u0026#39;len_tokens\u0026#39;: 13, \u0026#39;len_input_ids\u0026#39;: torch.Size([13]), \u0026#39;len_mask\u0026#39;: torch.Size([13])}, \u0026#39;The quick brown fox jumps over the lazy dog.\u0026#39;, [\u0026#39;[CLS]\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;quick\u0026#39;, \u0026#39;brown\u0026#39;, \u0026#39;fox\u0026#39;, \u0026#39;jumps\u0026#39;, \u0026#39;over\u0026#39;, tensor([ 101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])], [{\u0026#39;num_words\u0026#39;: 9, \u0026#39;len_tokens\u0026#39;: 13, \u0026#39;len_input_ids\u0026#39;: torch.Size([13]), \u0026#39;len_mask\u0026#39;: torch.Size([13])}, \u0026#39;The quick brown fox jumps over the lazy dog.\u0026#39;, [\u0026#39;[CLS]\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;quick\u0026#39;, \u0026#39;brown\u0026#39;, \u0026#39;fox\u0026#39;, \u0026#39;jumps\u0026#39;, \u0026#39;over\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;lazy\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;], tensor([ 101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])]] 09:36 ah interesting so the [PAD] is separate actually, and that corresponds to the additional 0 in the [[attention-mask]]\n[[Jul 29th, 2023]] hmm so back to that cool document, https://www.sbert.net/examples/applications/computing-embeddings/README.html\nit was cool I saw you can easily do tokenizer.convert_ids_to_tokens and tokenizer.convert_tokens_to_ids, because I was able to veriffy that running\nencoded_input = tokenizer( sentences, padding=True, truncation=True, return_tensors=\u0026#39;pt\u0026#39;) will add some additional '[CLS]' and '[SEP]' tokens at the beginning and end. I noticed since the length of the output was weird especially looking at the [[attention-mask]] . So that attention mask basically was showing that the three sentences being encoded had a final length that was the same number of ids and tokens , both 13, but there were some 0s at the end of some of the sentences. So that ended up being just yet another [PAD] token. Ok so '[CLS] means special #BERT token for start of sequence and [SEP] is the separator between sequences . Yea and [PAD] just says, nothing to do here. And the padding=True option is not about padding up to like [[context-window]] limit, it just says if you are passing multiple sentences to be encoded, to make them all equal length. Anyway 384 is the size of the embedding, in this case, and it is not yet clear to me what is the [[what is relationship between size of input token sequence and dimension of embedding]] , there might be some [[dimensionality reduction]] right. 12:58 so continuing along then, next step was\n#Compute token embeddings with torch.no_grad(): model_output = model(**encoded_input) In [143]: vars(model_output) Out[143]: {\u0026#39;last_hidden_state\u0026#39;: tensor([[[ 0.2913, -0.2685, -0.2250, ..., 0.4261, 0.0493, -0.2095], [-0.6272, -0.0421, -0.2452, ..., 0.5336, 1.3115, 0.5999], [ 0.0023, -0.2805, -0.4198, ..., -0.2900, 1.5808, -0.4912], ..., [ 0.1802, -0.5567, 0.0146, ..., 0.9311, 0.5940, -0.3536], [ 0.0603, -0.2502, 0.5959, ..., 0.9435, 0.9465, -1.0680], [-0.3356, 0.0650, 0.1109, ..., 1.0801, 0.2653, -0.2762]], [[ 0.0856, 0.1876, 0.0488, ..., 0.1204, -0.0907, -0.1662], [ 0.1291, -0.0266, 0.6318, ..., 0.7958, 0.1555, -1.2737], [ 0.0062, 0.2263, 0.1851, ..., 0.3981, 0.6461, -0.2192], ..., [ 0.3036, 0.3740, 0.2523, ..., 0.6319, 0.5731, -0.2901], [-0.2124, 0.2626, 0.6867, ..., 0.5504, 0.7065, -0.4728], [-0.2220, 0.2086, 0.6693, ..., 0.5410, 0.5683, -0.3963]], [[ 0.0464, 0.3381, 0.2082, ..., 0.2766, -0.0861, -0.0358], [ 0.1162, 0.2264, 0.1021, ..., 0.1858, 0.4895, 1.2175], [ 0.1537, 0.1730, 0.5151, ..., 1.3720, 0.3621, 0.5758], ..., [ 0.3883, 0.2813, 0.0309, ..., 0.3264, -0.1039, 0.5856], [ 0.3477, 0.0940, 0.2564, ..., 0.1463, 0.1743, 0.5586], [ 0.1911, -0.0142, 0.3021, ..., 0.1814, 0.2111, 0.2329]]]), \u0026#39;pooler_output\u0026#39;: tensor([[-0.0417, -0.0041, 0.0332, ..., 0.0117, -0.0634, -0.0058], [-0.0227, -0.0248, -0.0112, ..., 0.0482, -0.1108, 0.0122], [-0.0663, 0.0281, 0.0706, ..., 0.0258, -0.0222, -0.0608]]), \u0026#39;hidden_states\u0026#39;: None, \u0026#39;past_key_values\u0026#39;: None, \u0026#39;attentions\u0026#39;: None, \u0026#39;cross_attentions\u0026#39;: None} Ok cool, so seeing each input is embedded separately, we have size of 3 rows here, like 3 input encodings ,\nIn [146]: [model_output.last_hidden_state.shape, model_output.pooler_output.shape] Out[146]: [torch.Size([3, 13, 384]), torch.Size([3, 384])] Hmm and last hidden state, with the 13 wonder if those are 13 [[attention-head]] ? And then the pooler_output what combines these then ? 13:29 oh never mind #moment/doh #moment/duh that is 13 because thre are 13 tokens in each sentence ! [[moment/haha]] . ðŸ˜€\n13:14 ok so final [[average-pooling]] mean pooling step\n#Perform pooling. In this case, mean pooling sentence_embeddings = mean_pooling(model_output, encoded_input[\u0026#39;attention_mask\u0026#39;]) Ok so you can use key or index to access these,\nIn [155]: np.allclose(model_output.last_hidden_state, model_output[0]), np.allclose(model_output.pooler_output, model_output[1]) Out[155]: (True, True) 13:24 Not yet clear why this additional unsqueeze [[numpy unsqueeze ]] step is done\nIn [163]: attention_mask = encoded_input[\u0026#39;attention_mask\u0026#39;] ...: attention_mask.shape, attention_mask.unsqueeze(-1).shape Out[163]: (torch.Size([3, 13]), torch.Size([3, 13, 1])) Ah ok interesting,\nIn [166]: token_embeddings = model_output[0] In [167]: input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() In [168]: input_mask_expanded Out[168]: tensor([[[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]], [[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [0., 0., 0., ..., 0., 0., 0.]]]) In [169]: input_mask_expanded.shape Out[169]: torch.Size([3, 13, 384]) In [170]: (token_embeddings.size()) Out[170]: torch.Size([3, 13, 384]) In [171]: input_mask_expanded[0,0,:] Out[171]: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) So this interesting unsqueeze then expand pattern , to the 384 size of the embedding dimension, 13:30 so yea literally the output is\nIn [172]: token_embeddings.shape Out[172]: torch.Size([3, 13, 384]) In [173]: token_embeddings Out[173]: tensor([[[ 0.2913, -0.2685, -0.2250, ..., 0.4261, 0.0493, -0.2095], [-0.6272, -0.0421, -0.2452, ..., 0.5336, 1.3115, 0.5999], [ 0.0023, -0.2805, -0.4198, ..., -0.2900, 1.5808, -0.4912], ..., [ 0.1802, -0.5567, 0.0146, ..., 0.9311, 0.5940, -0.3536], [ 0.0603, -0.2502, 0.5959, ..., 0.9435, 0.9465, -1.0680], [-0.3356, 0.0650, 0.1109, ..., 1.0801, 0.2653, -0.2762]], [[ 0.0856, 0.1876, 0.0488, ..., 0.1204, -0.0907, -0.1662], [ 0.1291, -0.0266, 0.6318, ..., 0.7958, 0.1555, -1.2737], [ 0.0062, 0.2263, 0.1851, ..., 0.3981, 0.6461, -0.2192], ..., [ 0.3036, 0.3740, 0.2523, ..., 0.6319, 0.5731, -0.2901], [-0.2124, 0.2626, 0.6867, ..., 0.5504, 0.7065, -0.4728], [-0.2220, 0.2086, 0.6693, ..., 0.5410, 0.5683, -0.3963]], [[ 0.0464, 0.3381, 0.2082, ..., 0.2766, -0.0861, -0.0358], [ 0.1162, 0.2264, 0.1021, ..., 0.1858, 0.4895, 1.2175], [ 0.1537, 0.1730, 0.5151, ..., 1.3720, 0.3621, 0.5758], ..., [ 0.3883, 0.2813, 0.0309, ..., 0.3264, -0.1039, 0.5856], [ 0.3477, 0.0940, 0.2564, ..., 0.1463, 0.1743, 0.5586], [ 0.1911, -0.0142, 0.3021, ..., 0.1814, 0.2111, 0.2329]]]) at does the model_output.pooler_output mean then? Is that also doing a average of the 13 tokens? 13:33 The final step makes a bit more sense now\nsum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) because it is just saying, dont take into account the stuff that the mask masked away. And we add them, and divide by length for each sentence,\nIn [178]: sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) ...: sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) In [179]: pooled = sum_embeddings / sum_mask In [180]: pooled.shape Out[180]: torch.Size([3, 384]) too of course. 16:34 Ok so since I now basically know, the [[average-pooling]] of [[sentence-transformers]] indeed is just literally averaging each word, yes in the 384 dimensions but yea any unimportant word should most certainly be removed before average pooling\nAnd I still have that pretty critical two-part question, so if we are using [[subword-tokenization]] , and therefore a concept is going to be spread apart to multiple tokens, does that mean we are relying on this multi-dimensional averaging to somehow maintain the meaning of a word that was broken up into pieces? So the [[subword-tokenization]] clearly I now understand is a statistical procedure unrelated to the #[[supervised fine-tuning]] step and yea likely that the more common sub-words will end up being longer subwords after [[why a custom tokenizer]] , but most likely a concept still gets broken apart, into multiple sub-words, so then ultimately does that not really matter, because upon computing [[cosine similarity]], another sentence which has the same exact [[jargon]] word will be split up in the same way. And maybe even if a model is not #[[supervised fine-tuning]] with a new corpus, we may still benefit from at least words with a common word parts being close dimensionally, right, 18:20 hmm so a dead super simple test, of above sub-word theory,\nmodel_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; sentences = [ \u0026#34;python\u0026#34;, \u0026#34;pyspark\u0026#34;, ] embeddings = ut.vec_to_embeddings(model_id, sentences) In [189]: cos_sim(embeddings[0, :], embeddings[1,:]) Out[189]: tensor([[0.4150]]) Ok kind of thought so. 18:47 added one more option for myself there,\nreload(ut) model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; sentences = [ \u0026#34;python\u0026#34;, \u0026#34;pyspark\u0026#34;, ] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;##ys\u0026#39;, \u0026#39;##park\u0026#39;, \u0026#39;[SEP]\u0026#39;] That\u0026rsquo;s terrible haha ok no wonder the cosine similarity is so low, 0.415 haha. 18:51 one more example to try, hmm, this one should be obvious ,\nsentences = [\u0026#34;postgresql\u0026#34;, \u0026#34;database\u0026#34;] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;post\u0026#39;, \u0026#39;##gre\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;##q\u0026#39;, \u0026#39;##l\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] cosine similarity tensor([[0.5301]]) hmm haha thats really bad I think One more\nsentences = [\u0026#34;postgresql\u0026#34;, \u0026#34;sql\u0026#34;] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;post\u0026#39;, \u0026#39;##gre\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;##q\u0026#39;, \u0026#39;##l\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;sql\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] cosine similarity tensor([[0.5085]]) hmm yea this is no good. Whatever this is, it is terrible I think It should be as good as this,\nIn [200]: sentences = [\u0026#34;banana\u0026#34;, \u0026#34;apple\u0026#34;] ...: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) ...: print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) ...: [\u0026#39;[CLS]\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;[SEP]\u0026#39;] cosine similarity tensor([[0.4240]]) In [201]: sentences = [\u0026#34;fruit\u0026#34;, \u0026#34;apple\u0026#34;] ...: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) ...: print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;fruit\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;[SEP]\u0026#39;] cosine similarity tensor([[0.5372]]) In [202]: In [202]: sentences = [\u0026#34;macintosh\u0026#34;, \u0026#34;apple\u0026#34;] ...: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) ...: print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;macintosh\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;[SEP]\u0026#39;] cosine similarity tensor([[0.7044]]) 18:57 ok haha I\u0026rsquo;m confused. these are also kind of bad.\n[[Jul 31st, 2023]] ok interesting, removing the special tokens has no effect on the cosine similarity so following from [[Jul 29th, 2023]] I was going to remove the special tokens try again,\ntexts = [\u0026#34;macintosh\u0026#34;, \u0026#34;apple\u0026#34;] encoded_input = tokenizer( texts, padding=True, truncation=True, max_length=128, cls_token=None, sep_token=None, return_tensors=\u0026#39;pt\u0026#39;) hmm that didn\u0026rsquo;t work,\nTypeError: _batch_encode_plus() got an unexpected keyword argument \u0026#39;cls_token\u0026#39; how about,\ntexts = [\u0026#34;macintosh\u0026#34;, \u0026#34;apple\u0026#34;] encoded_input = tokenizer( texts, padding=True, truncation=True, max_length=128, add_special_tokens=False, return_tensors=\u0026#39;pt\u0026#39;) 09:14 ok nice yes that did it !\nIn [206]: encoded_input Out[206]: {\u0026#39;input_ids\u0026#39;: tensor([[22228], [ 6207]]), \u0026#39;token_type_ids\u0026#39;: tensor([[0], [0]]), \u0026#39;attention_mask\u0026#39;: tensor([[1], [1]])} Let\u0026rsquo;s then compare cosine similarity between these, with and without the special tokens present.\nmodel_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) cosine similarity tensor([[0.7044]]) 09:27 ok looks like didn\u0026rsquo;t make a difference. Try one more,\nsentences = [\u0026#34;fruit\u0026#34;, \u0026#34;apple\u0026#34;] model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors=\u0026#39;pt\u0026#39;) embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\u0026#34;add_special_tokens:\u0026#34;, add_special_tokens, \u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.2258]]) add_special_tokens: False cosine similarity tensor([[1.0000]]) 09:34 oops, have some kind of bug in the mean pooling code I think.\n[[Aug 1st, 2023]] trying a few more things wondering why weird cosine similarity 1 w/ single words encoded but different words ok what is bug from yesterday then ?\nsentences = [\u0026#34;fruit\u0026#34;, \u0026#34;apple\u0026#34;] model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors=\u0026#39;pt\u0026#39;) embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\u0026#34;add_special_tokens:\u0026#34;, add_special_tokens, \u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) print(\u0026#34;all close\u0026#34;, np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.5372]]) all close False add_special_tokens: False cosine similarity tensor([[1.0000]]) all close False In [217]: embeddings.shape Out[217]: torch.Size([2, 384]) hmm weird, yea spot checked, they don\u0026rsquo;t look identical actually, but similar,\nIn [218]: np.transpose(embeddings).shape Out[218]: torch.Size([384, 2]) In [219]: np.transpose(embeddings)[:5] Out[219]: tensor([[ 0.0025, 0.0021], [ 0.0335, 0.0337], [ 0.0014, 0.0013], [-0.0084, -0.0081], [-0.0206, -0.0208]]) n [220]: np.transpose(embeddings)[-5:] Out[220]: tensor([[ 0.0678, 0.0675], [ 0.1426, 0.1423], [-0.0018, -0.0016], [-0.1755, -0.1752], [-0.0967, -0.0969]]) but it is for sure a bug since, this happens for absurd cases,\nsentences = [\u0026#34;fruit\u0026#34;, \u0026#34;couch\u0026#34;] model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors=\u0026#39;pt\u0026#39;) embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\u0026#34;add_special_tokens:\u0026#34;, add_special_tokens, \u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) print(\u0026#34;all close\u0026#34;, np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.2795]]) all close False add_special_tokens: False cosine similarity tensor([[1.0000]]) all close False pdb trace,\nIn [224]: ipdb.runcall(ut.encoded_to_embeddings, encoded_input, model_id) ipdb\u0026gt; p encoded_input {\u0026#39;input_ids\u0026#39;: tensor([[5909], [6411]]), \u0026#39;token_type_ids\u0026#39;: tensor([[0], [0]]), \u0026#39;attention_mask\u0026#39;: tensor([[1], [1]])} ipdb\u0026gt; p model_output.last_hidden_state.shape, model_output.pooler_output.shape (torch.Size([2, 1, 384]), torch.Size([2, 384])) cos_sim(model_output.last_hidden_state[0,:,:], model_output.last_hidden_state[1,:,:]) tensor([[1.0000]]) cos_sim(model_output.pooler_output[0, :], model_output.pooler_output[1, :]) tensor([[1.0000]]) np.allclose(model_output.pooler_output[0, :], model_output.pooler_output[1, :]) False weird ok so this happens before the mean_pooling func gets called. weird.\nsentences = [\u0026#34;the fruit is edible\u0026#34;, \u0026#34;this couch is on sale\u0026#34;] model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors=\u0026#39;pt\u0026#39;) embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\u0026#34;add_special_tokens:\u0026#34;, add_special_tokens, \u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) print(\u0026#34;all close\u0026#34;, np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.0687]]) all close False add_special_tokens: False cosine similarity tensor([[0.0756]]) all close False ok so then something weird going on w/ a single token hmm?\nsentences = [\u0026#34;there is fruit on the table\u0026#34;, \u0026#34;look at the table there is fruit\u0026#34;] model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors=\u0026#39;pt\u0026#39;) embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\u0026#34;add_special_tokens:\u0026#34;, add_special_tokens, \u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) print(\u0026#34;all close\u0026#34;, np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.9155]]) all close False add_special_tokens: False cosine similarity tensor([[0.9066]]) all close False 09:34 dont know why the output from the model produces nearly same embedding, for a single word encoded, but multiple words, it seems to be working fine. guess more multi-word experiments then next .\n[[Aug 2nd, 2023]] interesting attempts around single and multiword embeddings Since like I saw yesterday, I can get high 0.90s score if I have a longer sentence, and for a single word, there is some kind of weird bug\nSo for just [\u0026quot;fruit\u0026quot;, \u0026quot;apple\u0026quot;] I had\nsentences = [\u0026#34;fruit\u0026#34;, \u0026#34;apple\u0026#34;] model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; embeddings = ut.vec_to_embeddings(model_id, sentences) cos_sim(embeddings[0, :], embeddings[1,:]) In [229]: sentences = [\u0026#34;fruit\u0026#34;, \u0026#34;apple\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) ...: Out[229]: tensor([[0.5372]]) In [230]: sentences = [\u0026#34;a fruit\u0026#34;, \u0026#34;my apple\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[230]: tensor([[0.4532]]) In [231]: sentences = [\u0026#34;its some fruit\u0026#34;, \u0026#34;here my apple\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[231]: tensor([[0.3700]]) In [232]: sentences = [\u0026#34;its some fruit juice\u0026#34;, \u0026#34;here my apple sauce\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[232]: tensor([[0.4277]]) In [233]: sentences = [\u0026#34;its some fruit juice home made\u0026#34;, \u0026#34;here my apple sauce custom recipe\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[233]: tensor([[0.3862]]) Ok I don\u0026rsquo;t know haha, might not solve this mystery right now. It might also be that hmm not all words are as close together as I thought? 09:05 ok yea haha, indeed I found some better single-word examples.\nIn [234]: sentences = [\u0026#34;couch\u0026#34;, \u0026#34;sofa\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[234]: tensor([[0.8564]]) In [235]: sentences = [\u0026#34;hammock\u0026#34;, \u0026#34;bed\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[235]: tensor([[0.2903]]) In [236]: sentences = [\u0026#34;mattress\u0026#34;, \u0026#34;bed\u0026#34;] ...: model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[236]: tensor([[0.6860]]) So yea putting the single-word-bug theory to rest, at least when using CLS, SEP add_special_tokens=True there is no problem. And without CLS, SEP, using add_special_tokens=False then yea, the embeddings for both inputs are nearly the same producing cosine similarity of 1. That\u0026rsquo;s really weird. So I should stick to using add_special_tokens=True for now at least for this model. Ok back to big picture then,\nSo I have observed this model has bad performance when I try throwing technical [[jargon]] at it, collapsed:: true\nreload(ut) model_id = \u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34; sentences = [ \u0026#34;python\u0026#34;, \u0026#34;pyspark\u0026#34;, ] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;##ys\u0026#39;, \u0026#39;##park\u0026#39;, \u0026#39;[SEP]\u0026#39;] sentences = [\u0026#34;postgresql\u0026#34;, \u0026#34;database\u0026#34;] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;post\u0026#39;, \u0026#39;##gre\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;##q\u0026#39;, \u0026#39;##l\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] cosine similarity tensor([[0.5301]]) sentences = [\u0026#34;postgresql\u0026#34;, \u0026#34;sql\u0026#34;] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) print(\u0026#34;cosine similarity\u0026#34;, cos_sim(embeddings[0, :], embeddings[1,:])) [\u0026#39;[CLS]\u0026#39;, \u0026#39;post\u0026#39;, \u0026#39;##gre\u0026#39;, \u0026#39;##s\u0026#39;, \u0026#39;##q\u0026#39;, \u0026#39;##l\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;sql\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] cosine similarity tensor([[0.5085]]) I have tried building a tokenizer from this model\u0026rsquo;s tokenizer, but that was problematic because it is not tokenizer fine tuning like model fine tuning, it just uses the same class. collapsed:: true So initially I was thinking building a new tokenizer means I need all billion examples earlier, hmm but that\u0026rsquo;s just what went in to fine tuning the model. Maybe for a tokenizer, perhaps I just need to build a dataset that has a good sampling mix of English language and a healthy proportion of technical language. But I think before doing that, I would like to take another stab at understanding, how to answer the more general question, about [[subword-tokenization]] , so for non jargon language, any tokenizer will still end up having plenty of subwords, but they will still end up with good embeddings right? So since subwords are split up into multiple embeddings, then maybe is it that the model just associates those subword embeddings appropriately then? So is it like you identify that multi-syllable words have the same #etymology roots perhaps, like like, \u0026ldquo;charismatic\u0026rdquo; , \u0026ldquo;charisma\u0026rdquo; and \u0026ldquo;character\u0026rdquo; and \u0026ldquo;characterization\u0026rdquo; ,\nIn [242]: sentences = [ ...: \u0026#34;charismatic\u0026#34; , \u0026#34;charisma\u0026#34;, \u0026#34;character\u0026#34;, \u0026#34;characterization\u0026#34; ...: ] In [243]: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][1, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][2, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\u0026#34;input_ids\u0026#34;][3, :])) ...: [\u0026#39;[CLS]\u0026#39;, \u0026#39;charismatic\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;char\u0026#39;, \u0026#39;##ism\u0026#39;, \u0026#39;##a\u0026#39;, \u0026#39;[SEP]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;character\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] [\u0026#39;[CLS]\u0026#39;, \u0026#39;characterization\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;, \u0026#39;[PAD]\u0026#39;] In [244]: cos_sim(embeddings[0, :], embeddings[1,:]) Out[244]: tensor([[0.5882]]) In [245]: cos_sim(embeddings[0, :], embeddings[2,:]) Out[245]: tensor([[0.4623]]) In [246]: cos_sim(embeddings[1, :], embeddings[2,:]) Out[246]: tensor([[0.6060]]) yea maybe something like that happens with an embedding model then, it can still embed subwords well. Should try to do some reading on this .\n[[Aug 3rd, 2023]] trying to do some more research on subword tokenization embedding are sub word token embeddings meaningful? Don\u0026rsquo;t have a definitive answer yet , but played around with the idea So what I wrote down yesterday, let\u0026rsquo;s try to phrase this question , how do subword embedding models capture meaning of concepts that are broken up into parts? [[are sub word token embeddings meaningful?]]\ncollapsed:: true so any tokenizer that uses [[subword-tokenization]], has the benefit of representing a large vocabulary with a subword vocabulary taht is smaller. Not remembering precisely but there are computational benefits to a smaller [[vocabulary size]] . But also with [[byte-pair encoding]] and similar algos, you have fewer [[out-of-vocabulary-words-OOV]] , and that is important since that can have performance degradations, an embedding model literally doesn\u0026rsquo;t know what those inputs mean. collapsed:: true Other benefits of course are it is more resilient to #misspelling , #typographical-error Related question is probably, [[Named Entity Recognition NER]] , [[Named Entity Recognition NER/how does it work across sub word tokenization]] Well realized it is build for this since entities are already often defined across multiple words, so multiple sub-words doesn\u0026rsquo;t sound like a huge stretch . 12:49 I\u0026rsquo;m reading the [[book/Natural Language Processing with Transformers]] chapter 4 , skimming, collapsed:: true one note I had from earlier, building a dataset in this chapter they chose representative sampling by percent data about languages spoken in #switzerland . Kind of cool Cool how on page 89, there is a custom dataset created based on the proportion of languages that are in #switzerland , in this case ,\nlangs = [\u0026#34;de\u0026#34;, \u0026#34;fr\u0026#34;, \u0026#34;it\u0026#34;, \u0026#34;en\u0026#34;] fracs = [0.629, 0.229, 0.084, 0.059] So I can borrow above technique if I would need a dataset that is more jargon heavy. 13:28 hmm but a note on [[why a custom tokenizer]] , collapsed:: true reading on page 35 in [[book/Natural Language Processing with Transformers]] , they are stressing, to use same tokenizer a model was trained with, so yea when creating a new tokenizer, therefore a new model is necessary too, otherwise the token id mappings will be completely off .\n13:38 ultimately, maybe [[are sub word token embeddings meaningful?]]\ncollapsed:: true maybe answer is yes if the [[average-pooling]] of the embeddings of those sub words is meaningful, which is precisely what that mean_pooling function of the [[sentence-transformers]] is doing. uniformly averages all the embeddings. So yea I wouldn\u0026rsquo;t be surprised if meaning did have a chance of getting a bit lost.\nSome more reading 14:24 I do want to continue reading some of the tokenizer sections in that book, but let me try to some quick research first.\ncollapsed:: true [[hugging face Datasets]] , a custom dataset, hmm so warming up to this , reading here, for use on a laptop, they support streaming operations, yay so don\u0026rsquo;t have to be a huge memory burden . Nice, they use [[memory mapped file]], wow, so can have like unlimited size then hmm , like magic 15:22 ok stumbled on link this section of the [[hugging face]] nlp course , on [[semantic search]], covering [[average-pooling]] yea. But also [[faiss.ai]] which is the [[Facebook AI Similarity Search FAISS]] which maybe is an alternative library to [[sentence-transformers]]? collapsed:: true 15:30 hmm article link someone hinting that [[Facebook AI Similarity Search FAISS]] is more about indexing and [[sentence-transformers]] more about the embeddings themselves perhaps. So faster lookup, faster search, 15:48 ok what can I learn here then Really not sure though why they are not concerned about removing the [[stop-words]] since they pollute that average . Think I will steal their nice technique for more quickly visualizing similarity though, In video at head of the page , presenter here, characterizes 0.83 as a strong relationship between these sentences. 17:15 I had a sentence here wrote on #ipad but #[[logseq sync]] deleted it [[moment/grr]] [[data loss]]. I wrote that [[hugging face Datasets]] seems to be a #pandas alternative since it has filtering , has maps, and especially w/ the [[memory mapped file]] would be way better than a memory bound pandas dataframe 17:17 seeing a note that the [[symmetric vs asymmetric semantic search]] idea is being discussed here,\nmodel_ckpt = \u0026#34;sentence-transformers/multi-qa-mpnet-base-dot-v1\u0026#34; but honestly I thought since now I know it is all just [[average-pooling]] then what\u0026rsquo;s the point? 17:35 in any case this article uses a new kind of pooling , cls pooling, maybe worth seeing if that is better than average pooling?\ndef cls_pooling(model_output): return model_output.last_hidden_state[:, 0] When I look at this function though, does not make sense because it is just returning the embedding of the [CLS] token, isn\u0026rsquo;t that weird? Wouldn\u0026rsquo;t that be the same vector each time, for the [CLS] token? Ok, try this, so for a bunch of sentences, let\u0026rsquo;s just compare the cosine similarity between the two pooling methods, out of curiosity,\nsentences = [ \u0026#34;postgresql\u0026#34;, \u0026#34;pyspark\u0026#34;, \u0026#34;this couch is for sale\u0026#34;, \u0026#34;docker\u0026#34;, \u0026#34;kubernetes\u0026#34;, \u0026#34;tensorflow\u0026#34;, \u0026#34;pytorch\u0026#34;, ] tokenizer = AutoTokenizer.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) model = AutoModel.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) # for sentence in sentences: encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors=\u0026#39;pt\u0026#39;) with torch.no_grad(): model_output = model(**encoded_input) [model_output.last_hidden_state.shape, model_output.pooler_output.shape] [torch.Size([7, 7, 384]), torch.Size([7, 384])] sentence_embeddings = mean_pooling(model_output, encoded_input[\u0026#39;attention_mask\u0026#39;]) sentence_embeddings.shape # torch.Size([7, 384]) cls_pooled = ut.cls_pooling(model_output) cls_pooled.shape # torch.Size([7, 384]) extract_scalar = lambda x: round(float(x[0][0]), 3) for i, _ in enumerate(sentence_embeddings): print(sentences[i], \u0026#34;cos_sim,\u0026#34;, extract_scalar(cos_sim(sentence_embeddings[i, :], cls_pooled[i, :])), extract_scalar(cos_sim(sentence_embeddings[i, :], model_output.pooler_output[i, :])), extract_scalar(cos_sim(cls_pooled[i, :], model_output.pooler_output[i, :])), ) postgresql cos_sim, 0.581 0.008 -0.02 pyspark cos_sim, 0.63 0.04 -0.008 this couch is for sale cos_sim, 0.572 -0.041 -0.044 docker cos_sim, 0.686 0.018 -0.066 kubernetes cos_sim, 0.594 -0.068 -0.042 tensorflow cos_sim, 0.563 0.068 -0.016 pytorch cos_sim, 0.567 0.022 0.017 Ok yea so this cls pooling approach, there is some similarity to the average pooling. interesting. But haha the final model_output.pooler_output, yea that is unrelated to anything even though the dimension is the same. Wonder what that is, haha.\nthoughts on a custom dataset 16:09 building my own custom dataset, for the [[supervised fine-tuning]] and for the measuring , can really be the same dataset,\nJust train a test split, so build the [[positive pair]] with lots and lots of organic sentences , that use jargon, yea maybe using the cluster approach I was thinking about earlier, clustering by similar job titles, and maybe at some point I can mix in [[positive pair]] constructed along with my own personal examples.\n[[Aug 4th, 2023]] reading and musing some more on meaning or no meaning with fertile sub-wording tokenization [[are sub word token embeddings meaningful?]]\nSo thinking in context of [[Named Entity Recognition NER/how does it work across sub word tokenization]], I think it is the core #BERT #[[NLP Transformers]] model that is build for understanding #[[input sequence]] and so probably it picks up some cues about multi-token sequences that are entities. But [[sentence-transformers]] is a [[bag-of-words]] so yea it is not tuning anything here right? Because reading , [[book/Natural Language Processing with Transformers]] page 97 , my notes I underlined that yea the [[Named Entity Recognition NER]] task is a [[token classification task]] Wait but then what is happening in the [[article/Train and Fine-Tune Sentence Transformers Models]] ? Yea also reading , page 313, that yes there are metrics specific to tokenizers like [[subword fertility]] which measures average number of subwords coming out of [[subword-tokenization]], but that these are like [[end-to-end-vs-proxy-ML-training]] proxy metrics and instead [[end to end test]] of performance is best. Most direct . Hmm so perhaps yea my focus on things like [[cosine similarity]] is good since it is an end to end measure, but it gives me more reason to build that nice dataset of my own ðŸ˜€ , [[moment/anticipation]] , [[moment/curiosity]] , 09:29 so yea think would be a useful next mini thing to look at, per link, [[article/Train and Fine-Tune Sentence Transformers Models]], is this fine tuning changing weights actually, somehow helping to build associations around the tokens regardless of whether they are haha highly fertile and possibly meaningless looking [[subword-tokenization]] ? haha Yea and should continue to build out my dataset [[positive pair]] , aspiration, to train test, build and measure !\n[[Aug 6th, 2023]] debugging does fine tuning update model weights? Looks like yes. But a small dataset appears to minimally update weights, which makes sense. 10:06 yea so question from last time, [[article/Train and Fine-Tune Sentence Transformers Models]], what is this actually changing?\nSo earlier I tried this, on [[Jul 15th, 2023]] , here, with this data/kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl mini first stab at a dataset w/ [[positive pair]], collapsed:: true But the only thing I did as a test was that I inspected whether the internal \u0026ldquo;vocab.txt\u0026rdquo; [[vocabulary]] had changed before and after and it did not. However I know better now that this #[[supervised fine-tuning]] does not change the vocabulary since that is fixed in the #tokenizer . So instead, I should have checked, did the weights of the model change and I could have run some #[[cosine similarity]] tests to spot check or also to actually have a #holdout-set , built from my data, to see did the sentences I would have expected get ranked better than before the fine tuning ? 11:12 ok so let me do that one more time and lets check , of all the layers I see in this model, at least the before and after weights of the pooling layer,\nmodel = AutoModel.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) print(\u0026#34;how many layers: \u0026#34;, len([x for x in model.modules()])) how many layers: 120 120 layers, oh my, ok lets look at pooler at end,\n[x for x in model.modules()][-5:] # Out[299]: [LayerNorm((384,), eps=1e-12, elementwise_affine=True), Dropout(p=0.1, inplace=False), BertPooler( (dense): Linear(in_features=384, out_features=384, bias=True) (activation): Tanh() ), Linear(in_features=384, out_features=384, bias=True), Tanh()] pooler = [x for x in model.modules()][-3] pooler.dense.weight.shape, pooler.dense.bias.shape # Out[309]: (torch.Size([384, 384]), torch.Size([384])) Ok let me make a deep copy of that, and lets see if that changes w/ my mini mini dataset fine tuning,\nweights_before, bias_before = deepcopy(pooler.dense.weight), deepcopy(pooler.dense.bias) In [312]: weights_before[:, :5] Out[312]: tensor([[-0.0151, -0.0224, -0.0046, -0.0199, 0.0064], [ 0.0083, -0.0186, -0.0132, -0.0009, -0.0157], [-0.0030, 0.0108, -0.0102, 0.0023, -0.0006], ..., [-0.0049, -0.0010, -0.0043, -0.0035, 0.0108], [-0.0089, 0.0108, 0.0011, 0.0227, -0.0051], [ 0.0049, 0.0070, -0.0065, -0.0034, 0.0024]], grad_fn=\u0026lt;SliceBackward0\u0026gt;) In [314]: bias_before[:5] Out[314]: tensor([0., 0., 0., 0., 0.], grad_fn=\u0026lt;SliceBackward0\u0026gt;) ok\nimport os import utils as u from pathlib import Path path = (Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) / \u0026#34;data\u0026#34; / \u0026#34;kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl\u0026#34;) # path.write_text(\u0026#34;\\n\u0026#34;.join([json.dumps(x) for x in dataset])) dataset = [json.loads(x) for x in path.read_text().splitlines()] dataset[:4] # Out[319]: # [{\u0026#39;set\u0026#39;: [\u0026#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby\u0026#39;, # \u0026#39;experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver\u0026#39;]}, # {\u0026#39;set\u0026#39;: [\u0026#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby\u0026#39;, # \u0026#39;software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)\u0026#39;]}, # {\u0026#39;set\u0026#39;: [\u0026#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby\u0026#39;, # \u0026#39;experience with front-end web technologies (html5, css3, and javascript)\u0026#39;]}, # {\u0026#39;set\u0026#39;: [\u0026#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby\u0026#39;, # \u0026#39;html5, css3, and javascript development experience\u0026#39;]}] from sentence_transformers import InputExample from torch.utils.data import DataLoader train_examples = [] for i, x in enumerate(dataset): train_examples.append( InputExample(texts=[x[\u0026#34;set\u0026#34;][0], x[\u0026#34;set\u0026#34;][1]]) ) train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) # MultipleNegativesRankingLoss from sentence_transformers import losses model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) train_loss = losses.MultipleNegativesRankingLoss(model=model) model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10) Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:14\u0026lt;00:00, 1.09s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:16\u0026lt;00:00, 1.23s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15\u0026lt;00:00, 1.18s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15\u0026lt;00:00, 1.20s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15\u0026lt;00:00, 1.20s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15\u0026lt;00:00, 1.16s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15\u0026lt;00:00, 1.20s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:14\u0026lt;00:00, 1.14s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:14\u0026lt;00:00, 1.11s/it] Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:14\u0026lt;00:00, 1.14s/it] Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:31\u0026lt;00:00, 15.13s/it] same? Well one interesting thing, now there are 124 layers instead of 125,\nIn [322]: len([x for x in model.modules()]) Out[322]: 124 In [323]: [x for x in model.modules()][-5:] Out[323]: [BertPooler( (dense): Linear(in_features=384, out_features=384, bias=True) (activation): Tanh() ), Linear(in_features=384, out_features=384, bias=True), Tanh(), Pooling({\u0026#39;word_embedding_dimension\u0026#39;: 384, \u0026#39;pooling_mode_cls_token\u0026#39;: False, \u0026#39;pooling_mode_mean_tokens\u0026#39;: True, \u0026#39;pooling_mode_max_tokens\u0026#39;: False, \u0026#39;pooling_mode_mean_sqrt_len_tokens\u0026#39;: False}), Normalize()] pooler = [x for x in model.modules()][-5] print(\u0026#34;pooler weights all close?\u0026#34;, np.allclose( weights_before.detach().numpy(), pooler.dense.weight.detach().numpy())) # True Ok so that is True within the default tolerance and per below, I see indeed the changes are quite minimal.\nIn [336]: weights_before[:, :5] Out[336]: tensor([[-0.0151, -0.0224, -0.0046, -0.0199, 0.0064], [ 0.0083, -0.0186, -0.0132, -0.0009, -0.0157], [-0.0030, 0.0108, -0.0102, 0.0023, -0.0006], ..., [-0.0049, -0.0010, -0.0043, -0.0035, 0.0108], [-0.0089, 0.0108, 0.0011, 0.0227, -0.0051], [ 0.0049, 0.0070, -0.0065, -0.0034, 0.0024]], grad_fn=\u0026lt;SliceBackward0\u0026gt;) In [330]: pooler.dense.weight.detach().numpy()[:, :5] Out[330]: array([[-0.01512909, -0.02244568, -0.00457382, -0.01985168, 0.00641632], [ 0.0083313 , -0.01863098, -0.01322937, -0.00087643, -0.01565552], [-0.00302696, 0.01076508, -0.0102005 , 0.00234985, -0.00063133], ..., [-0.00487518, -0.00100899, -0.00428391, -0.00347328, 0.01079559], [-0.00886536, 0.01075745, 0.00112629, 0.02267456, -0.00512314], [ 0.00490952, 0.00695419, -0.00653076, -0.00342751, 0.00236511]], dtype=float32) Well definitely different weights on the BertPooler there for sure. And now there is some kind of new Pooling layer I did not see before too. Ok so since the changes are super minimal, I don\u0026rsquo;t think it is worth additional testing. So instead, I need a bigger dataset to try this out. But at least I can see that yes, fine tuning does do something haha.\nCustom dataset thought, might be that instead of sticking to one engineering slice of that jobs dataset, I might try pulling out multiple slices, at least to attempt to see, with a train test split, does the fine tuning help . 12:41 going to look at my personal dataset one more time , One question I have, what percent of the text in my \u0026ldquo;my-challenges-and-accomplishments/experience.yaml\u0026rdquo; am I capturing into my blurb? Simple question ,\nimport utils as ut repos_dir = Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) assert repos_dir.is_dir() experience_loc = repos_dir / \u0026#34;my-challenges-and-accomplishments/experience.yaml\u0026#34; total_length = len(experience_loc.read_text()) experiences_dict = ut.read_yaml(experience_loc)[\u0026#34;Descriptions\u0026#34;] my_sentences = ut.build_my_blurb(experiences_dict) print(\u0026#34;proportion used:\u0026#34;, len(\u0026#34; \u0026#34;.join(my_sentences)) / total_length) proportion used: 0.3994211783644008 Ok wow haha I did not expect that to be that low haha. 13:14 The reason it is not 100% is that I write initial drafts of work I did first which is not pulled by the \u0026quot;build_my_blurb\u0026quot; function and then when I have spare time I will to to better phrase those drafts, I put them into one-liners and stories, hopefully more succinctly, which is pulled by \u0026quot;build_my_blurb\u0026quot;. Ok then perhaps a slight twist of what I would like to do next, (1) Use the two raw job description dumps from earlier, to build a corpus not of individual sentences, as I had prior, but of job descriptions, attempting to also create a label column that tries to generalize the job titles they have, to hopefully around 5. I can keep \u0026ldquo;amazon\u0026rdquo; or \u0026ldquo;google\u0026rdquo; as an additional column for alter too . (2) Create a [[positive pair]] dataset from that , clustering by job title so this time not doing it by hand as much. (3) Split that into a train test . (4) Fine tune train , and then use test , also w/ the [[Multiple negatives ranking loss]] . Maybe I can test before and after the fine tuning. (5) apply to my blurb dataset, and hand inspect what comes up for me.\nok\n","permalink":"https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/","summary":"type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18\nNote This is not a blog post but kind of a landing page I\u0026rsquo;m using to aggregate on-going project notes here\nVision Broadly would like to do here something like the following\ncompare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since thatâ€™s a moving target .","title":"langchain interview me 2023 feb"},{"content":"Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]\nSo I\u0026rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,\nOk, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime.utcnow().replace(tzinfo=pytz.UTC) def utc_ts(dt): return dt.strftime(\u0026#34;%Y-%m-%dT%H%M%S\u0026#34;) def read_yaml(loc): with open(loc) as fd: return yaml.safe_load(fd) from pathlib import Path import os repos_dir = Path(os.getenv(\u0026#34;REPOS_DIR\u0026#34;)) assert repos_dir.is_dir() experience_loc = repos_dir / \u0026#34;my-challenges-and-accomplishments/experience.yaml\u0026#34; experiences_dict = read_yaml(experience_loc)[\u0026#34;Descriptions\u0026#34;] sections = [] for project, detail in experiences_dict.items(): section = \u0026#34;\u0026#34; if detail.get(\u0026#34;company\u0026#34;): company = detail.get(\u0026#34;company\u0026#34;) section = (f\u0026#34;When I worked at {company}, \u0026#34; f\u0026#34;there was a project in {detail[\u0026#39;year\u0026#39;]}, {project}.\u0026#34;) elif detail.get(\u0026#34;project\u0026#34;): project = detail.get(\u0026#34;project\u0026#34;) section = f\u0026#34;In {detail[\u0026#39;year\u0026#39;]}, I had a side project, {project}. \u0026#34; section += \u0026#34;. \u0026#34;.join([x for x in detail.get(\u0026#34;one-liners\u0026#34;, [])]) section += \u0026#34;. \u0026#34;.join([x for x in detail.get(\u0026#34;stories\u0026#34;, [])]) sections.append(section) workdir = repos_dir / \u0026#34;2023-interview-me\u0026#34; path = workdir / f\u0026#34;{utc_ts(utc_now())}-the-story-blurb.txt\u0026#34; path.write_text(\u0026#34;\\n\\n\\n\u0026#34;.join(sections)) Ok let me run now some of the basic question answer chains Use my environment from before,\nsource ~/.python_venvs/langchainz/bin/activate But when trying to use this just like per the qa with sources example in that here,\nfrom langchain.embeddings.openai import OpenAIEmbeddings from langchain.embeddings.cohere import CohereEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch from langchain.vectorstores import Chroma from langchain.docstore.document import Document from langchain.prompts import PromptTemplate from pathlib import Path workdir = str(repos_dir / \u0026#34;2023-interview-me\u0026#34; ) story_path = repos_dir / \u0026#34;2023-interview-me\u0026#34; / \u0026#34;2023-02-19T011846-the-story-blurb.txt\u0026#34; my_story = story_path.read_text() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_text(my_story) embeddings = OpenAIEmbeddings() docsearch = Chroma.from_texts( texts, embeddings, metadatas=[{\u0026#34;source\u0026#34;: str(i)} for i in range(len(texts))]) from langchain.chains.qa_with_sources import load_qa_with_sources_chain from langchain.llms import OpenAI query = \u0026#34;What kind of projects have I worked on with tensor flow?\u0026#34; docs = docsearch.similarity_search(query) chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\u0026#34;stuff\u0026#34;) chain({\u0026#34;input_documents\u0026#34;: docs, \u0026#34;question\u0026#34;: query}, return_only_outputs=True) this line,\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\u0026#34;source\u0026#34;: str(i)} for i in range(len(texts))]) yielded a new install request, so that\u0026rsquo;s what I did,\nValueError: Could not import chromadb python package. Please it install it with `pip install chromadb`. In [4]: !pip install chromadb Ok wow, so I reran that again and I am getting,\nCreated a chunk of size 1474, which is longer than the specified 1000 Created a chunk of size 2846, which is longer than the specified 1000 Created a chunk of size 2348, which is longer than the specified 1000 Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. Out[1]: {\u0026#39;output_text\u0026#39;: \u0026#34; I have worked on projects with Tensor Flow that involve predicting pilot\u0026#39;s states of awareness, predicting bike share rider\u0026#39;s destinations, and using Tensor Flow LSTM to answer questions about a CDC Covid dataset.\\nSOURCES: 14, 15, 0\u0026#34;} which is not bad\nSo , also I realized the really cool part about the approach here, is that the numbers, correspond simply to the indices of the texts, and the texts split, actually intuitively haha by a double new line, \\n\\n because at least that is how I split them just naturally. In [2]: len(texts) Out[2]: 16 In [6]: text_splitter._separator Out[6]: \u0026#39;\\n\\n\u0026#39; So since the above output says 14, 15, 0, I think also this is somewhat sorted by relevance, so let me see what is in those,\nIn [9]: for i in [14, 15, 0]: ...: print(\u0026#34;i\u0026#34;, i, texts[i], \u0026#34;\\n===================================================\\n\\n\u0026#34;) ...: Okay when I looked at that ^^ I noticed actually some texts were getting actually smushed together? There was a lot of information. Going to create my text corpus one more time but this time separating by three newlines .\nOk let\u0026rsquo;s see , I now created 2023-interview-me/2023-02-19T015128-the-story-blurb.txt , lets see if there are more than 16 texts?\nstory_path = repos_dir / \u0026#34;2023-interview-me\u0026#34; / \u0026#34;2023-02-19T015128-the-story-blurb.txt\u0026#34; my_story = Path(loc).read_text() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_text(my_story) In [11]: len(texts) Out[11]: 16 Ah ok still 16 texts, but actually maybe this is based on the chunk_size ?\nOk I want to try the chain which gives the intermediate results, Also needed one more package, here for the below,\nValueError: Could not import tiktoken python package. This is needed in order to calculate get_num_tokens. Please it install it with `pip install tiktoken`. query = \u0026#34;What kind of projects have I worked on with tensor flow?\u0026#34; docs = docsearch.similarity_search(query) chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\u0026#34;map_reduce\u0026#34;, return_intermediate_steps=True) chain({\u0026#34;input_documents\u0026#34;: docs, \u0026#34;question\u0026#34;: query}, return_only_outputs=True) Ok this looks like it only returns the relevant statements, nice.\n{\u0026#39;intermediate_steps\u0026#39;: [\u0026#34; In 2019, I had a side project, Reducing Commercial Aviation Fatalities (kaggle). Designed and built a Tensor Flow LSTM based model to predict pilot\u0026#39;s states of awareness, given time series physiological data.\u0026#34;, \u0026#39; None.\u0026#39;, \u0026#39; None\u0026#39;, \u0026#39; None.\u0026#39;], \u0026#39;output_text\u0026#39;: \u0026#34; I have worked on a Tensor Flow LSTM based model to predict pilot\u0026#39;s states of awareness, given time series physiological data.\\nSOURCES: 14\u0026#34;} And the important thing is that this code preserves the original text so I can nicely look for myself exactly at the primary source !\nLet me try another query, query = \u0026#34;What is my experience with Docker?\u0026#34; docs = docsearch.similarity_search(query) chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\u0026#34;map_reduce\u0026#34;, return_intermediate_steps=True) chain({\u0026#34;input_documents\u0026#34;: docs, \u0026#34;question\u0026#34;: query}, return_only_outputs=True) {\u0026#39;intermediate_steps\u0026#39;: [\u0026#39; \u0026#34;Dockerized our production underwriting stack and split from the main company git repo to give us the flexibility to deploy both scikit learn and XGBoost models with AWS SageMaker.\u0026#34;\u0026#39;, \u0026#39; \u0026#34;Helped make better underwriting decisions on returning customers, by optimizing /re-engineering /versioning our SQL based logistic regression model, with a python + Docker + SQL pipeline, cutting runtime from 6+ hours to under an hour.\u0026#34;\u0026#39;, \u0026#39; None.\u0026#39;, \u0026#39; None\u0026#39;], \u0026#39;output_text\u0026#39;: \u0026#39; I have no experience with Docker.\\nSOURCES: 0, 14\u0026#39;} Hah I feel like I need to do some kind of tuning here? But it is cool to see the intermediate step here even though it is not reflected in the final output.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/","summary":"Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]\nSo I\u0026rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,\nOk, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime.","title":"Using langchain to interview myself about my skills"},{"content":"Spending a few spare moments to summarize some of my thought processes from a recent hackathon.\nWhat is all this So I was glad to be part of a really cool hackathon team recently at my company and here are some of my perspectives from the experience.\nPlan of action So We settled to constrain our problem space to apply langchain, a library that wraps around large language model APIs notably the OpenAI API, to show language understanding from a publicly available health insurance plan summary of benefits document. We ended up starting out with this public facing document, which one can seee a description of here with its download link https://content.sunfirematrix.com/2023/Humana-H1036-236-000-SB-EN-2023-SF20220922.pdf .\nMy thought around structured data Hackathons are always very time crunch like environments. I very much wanted us to have at least a minimal end to end demo in our back pocket just in case our additional progress does not pan out.\nSo on the first night, I had this hunch, that the value we can get from our public facing document, would be highest, with additional preprocessing, turning the raw text from this document into full enlish sentences, as opposed to the incomplete English in the document, and also to capture some of the inferred structural connections which are easy to see when visually inspecting, but cannot be captured easily by a raw text dump from the pdf, because perhaps at least the initial pdf to text extraction which we used did not capture this structure.\nOne quick example of this, For instance, here are two tables from the pdf document, that describe the \u0026ldquo;Mail Order Cost-Sharing\u0026rdquo; and the \u0026ldquo;Retail Cost-Sharing\u0026rdquo; options for prescription drug benefits\nAt the outset I handrewrote this as the following clauses\nThis plan has Prescription Drug Benefits . Here is information about what You Pay for Vaccines . Our plan covers most Part D vaccines at no cost to you, no matter what cost-sharing tier itâ€™s on . Here is information about What You Pay for Insulin . You wonâ€™t pay more than $35 for a one-month (up to 30-day) supply of each Part D insulin product covered by our plan, no matter what cost-sharing tier itâ€™s on . This applies to all Part D covered insulins, including the Select Insulins covered under the Insulin Savings Program as described below. What you pay for prescription drugs depends on whether you receive \u0026#34;Extra Help\u0026#34; or not. If you receive \u0026#34;Extra Help\u0026#34;, you will still pay no more than $35 for a one-month supply for each Part D covered insulin. Please see your Prescription Drug Guide to find all Part D insulins covered by your plan. This plan does not have a deductible for prescription drugs. If you don\u0026#39;t receive Extra Help for your drugs, you\u0026#39;ll pay a different amount based on the type of cost-sharing option you use. For the Initial coverage, you are responsible to pay for a 30-day supply or a 90-day supply the amount based on the tier of the prescription drug. A prescription drug can be either in tier 1 preferred generic, tier 2 generic, tier 3 preferred brand, tier 4 non-preferred drug, or tier 5 specialty tier. You are responsible to pay for prescription drugs until the total yearly drug costs reach $4,660 . The total yearly drug costs are the total drug costs paid by both you and our plan. Once you reach this amount, you will enter the Coverage Gap. There are two different kinds of cost-sharing for prescription drugs, including Mail Order Cost-Sharing and Retail Cost-Sharing. There are two different kinds of Mail Order pharmacy options, Standard and Preferred. The Mail order pharmacy option called Standard includes Walmart Mail, PillPack and other pharmacies that are also available in our network. To find pharmacy mail order options go to Humana.com/pharmacyfinder . The second type of mail order pharmacy option is the Preferred pharmacy option, which includes only the CenterWell Pharmacy. For the Standard Mail order pharmacy option, for Tier 1 Preferred Generic drugs, the 30-day supply costs $10 and the 90-day supply costs $30. For the Standard Mail order pharmacy option, for Tier 2 Generic drugs, the 30-day supply costs $20 and the 90-day supply costs $60. For the Standard Mail order pharmacy option, for Tier 3 Preferred Brand drugs, the 30-day supply costs $47 and the 90-day supply costs $141. For the Standard Mail order pharmacy option, for Tier 4 Non-Preferred drugs, the 30-day supply costs $100 and the 90-day supply costs $300. For the Standard Mail order pharmacy option, for Tier 5 Specialty Tier drugs, the 30-day supply costs 33% and the 90-day supply is not available. For the Preferred CenterWell Pharmacy Mail order pharmacy option, for Tier 1 Preferred Generic drugs, the 30-day supply costs $0 and the 90-day supply costs $0. For the Preferred CenterWell Pharmacy Mail order pharmacy option, for Tier 2 Generic drugs, the 30-day supply costs $0 and the 90-day supply costs $0. For the Preferred CenterWell Pharmacy Mail order pharmacy option, for Tier 3 Preferred Brand drugs, the 30-day supply costs $42 and the 90-day supply costs $116. For the Preferred CenterWell Pharmacy Mail order pharmacy option, for Tier 4 Non-Preferred drugs, the 30-day supply costs $100 and the 90-day supply costs $290. For the Preferred CenterWell Pharmacy Mail order pharmacy option, for Tier 5 Specialty Tier drugs, the 30-day supply costs 33% and the 90-day supply is not available. The other kind of prescription drug cost sharing is called Retail Cost-Sharing, which means that you can physically go to the pharmacy. You can go to any network retail pharmacy. To find the retail pharmacies near you, go to Humana.com/pharmacyfinder For the retail pharmacy option, for Tier 1 Preferred Generic drugs, the 30-day supply costs $0 and the 90-day supply costs $0. For the retail pharmacy option, for Tier 2 Generic drugs, the 30-day supply costs $0 and the 90-day supply costs $0. For the retail pharmacy option, for Tier 3 Preferred Brand drugs, the 30-day supply costs $42 and the 90-day supply costs $116. For the retail pharmacy option, for Tier 4 Non-Preferred drugs, the 30-day supply costs $100 and the 90-day supply costs $300. For the retail pharmacy option, for Tier 5 Specialty Tier drugs, the 30-day supply costs 33% and the 90-day supply is not available. Maybe there are really good libraries for inferring the generically inferring the tabular structure , but at least for the time being I really wanted to constrain the problem a bit more since my brain was in hackathon mode haha ðŸ˜…\nA fully end to end version 1 demo So the day before I had already proven to myself, that the #langchain LLMChain class is really good at having a targeted conversation framed around some kind of small talk, with the use of a prompt that provides the focus. Also , earlier, I had shown myself just how quickly haha, no, instantaneously, you can use #langchain to embed a targeted bit of text, a section of a magazine article in my case, and run question answering based on this.\nI have not tried it yet but langchain also has the concept of an agent, which can compartmentalize separate functionality, say for engaging in small talk and separately for answering questions on an insurance plan document.\nAlready having some components already working in front of me, I wanted to at least first build a super simple demo, more or less jamming together the small talk prompts I had explored earlier with the document focused question answering.\nI ended up with something like this from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate from langchain.chains.conversation.memory import ConversationalBufferWindowMemory from langchain import PromptTemplate name_of_person = \u0026#34;Alfred Jamesmanson\u0026#34; real_person_prompt = f\u0026#34;\u0026#34;\u0026#34; Assistant is uses friendly and helpful language. Assistant loves making new friends. If at any time {name_of_person} asks to talk with a real person that {name_of_person} should only ask and they will be connected to a real person. Assistant will remind {name_of_person} that they can connect to speak to a real person whenever they would like that. If {name_of_person} asks to talk to a real person then Assistant will let {name_of_person} know they will be connected to talk to a real person right away. \u0026#34;\u0026#34;\u0026#34; person_details = f\u0026#34;\u0026#34;\u0026#34; Assistant is designed to talk to {name_of_person} and answer their questions. {name_of_person} lives in Dallas Texas. {name_of_person} was born in Keywest Florida on January 2nd 1990. {name_of_person} goes to college. {name_of_person} studies electrical engineering. {name_of_person} is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. {name_of_person} has brown hair. \u0026#34;\u0026#34;\u0026#34; filename = \u0026#34;summary-of-benefits-paragraphs.txt\u0026#34; with open(filename) as f: document_text = f.read() len_doc = len(document_text ) print(\u0026#34;len doc is \u0026#34;, len_doc) document_text = document_text[:(len_doc//2)] base_template = f\u0026#34;\u0026#34;\u0026#34;Assistant is a large language model trained by OpenAI. {person_details} {real_person_prompt} {document_text} Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. {name_of_person}:\u0026#34;\u0026#34;\u0026#34; template = base_template + \u0026#34;\u0026#34;\u0026#34; {human_input} Assistant: \u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;human_input\u0026#34;], template=template ) chatgpt_chain = LLMChain( llm=OpenAI(temperature=0), prompt=prompt, verbose=True, memory=ConversationalBufferWindowMemory(k=3), ) human_input = \u0026#34;Hi my name is Alfred Jamesmanson. I need your help Assistant. What color is my hair?\u0026#34; while True: human_input = input(\u0026#34;: \u0026#34;) output = chatgpt_chain.predict( human_input=human_input, ) print(output) And I ended up, above, cutting the document in half, because my prompt had too many tokens looks like. Traceback (most recent call last): File \u0026#34;/Users/michal/Dropbox/Code/repo/our-hack-qa/dumb-friendly.py\u0026#34;, line 63, in \u0026lt;module\u0026gt; output = chatgpt_chain.predict( File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/chains/llm.py\u0026#34;, line 102, in predict return self(kwargs)[self.output_key] File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/chains/base.py\u0026#34;, line 155, in __call__ raise e File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/chains/base.py\u0026#34;, line 152, in __call__ outputs = self._call(inputs) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/chains/llm.py\u0026#34;, line 86, in _call return self.apply([inputs])[0] File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/chains/llm.py\u0026#34;, line 77, in apply response = self.generate(input_list) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/chains/llm.py\u0026#34;, line 72, in generate response = self.llm.generate(prompts, stop=stop) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/llms/base.py\u0026#34;, line 79, in generate raise e File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/llms/base.py\u0026#34;, line 76, in generate output = self._generate(prompts, stop=stop) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/langchain/llms/openai.py\u0026#34;, line 158, in _generate response = self.client.create(prompt=_prompts, **params) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/openai/api_resources/completion.py\u0026#34;, line 25, in create return super().create(*args, **kwargs) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u0026#34;, line 153, in create response, _, api_key = requestor.request( File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/openai/api_requestor.py\u0026#34;, line 227, in request resp, got_stream = self._interpret_response(result, stream) File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/openai/api_requestor.py\u0026#34;, line 620, in _interpret_response self._interpret_response_line( File \u0026#34;/Users/michal/.python_venvs/langchainz/lib/python3.9/site-packages/openai/api_requestor.py\u0026#34;, line 680, in _interpret_response_line raise self.handle_error_response( openai.error.InvalidRequestError: This model\u0026#39;s maximum context length is 4097 tokens, however you requested 6578 tokens (6322 in your prompt; 256 for the completion). Please reduce your prompt; or completion length. I would like to understand these limitations a bit better and also how to get around then at a later point ! conversation 1 : hello there \u0026gt; Finished chain. Hi there! How can I help you? : can I talk to a real person? \u0026gt; Finished chain. Yes, of course! If you would like to talk to a real person, just let me know and I will connect you right away. : can you please connect me with a real person? \u0026gt; Finished chain. Of course! I can connect you with a real person right away. Please hold while I connect you. Trying out the real person part of the prompt. Actually initially I thought it was not working but then clearly confirmation was just requested.\nSo I wonder if that langchain agent can somehow help to route this conversation along to a human at this point.\nContinuing that one,\n: so by the way how is your day going so far? \u0026gt; Finished chain. Hi Alfred Jamesmanson! My day is going great so far. I\u0026#39;m learning a lot and having a lot of fun. How about you? : I\u0026#39;m good. I wanted to know what is my mamogram copay? \u0026gt; Finished chain. For Diagnostic mammography there is a $0 to $15 copay. : Ah thanks and what are my pharmacy options available to me? \u0026gt; Finished chain. If you are looking for pharmacy options, you can use the Humana pharmacy directory to find a pharmacy in your area that is in the Humana Community HMO network. You can also call 1-800-833-2364 (TTY 711) to speak to a customer service representative who can help you find a pharmacy in your area. : what does a retail pharmacy mean? \u0026gt; Finished chain. A retail pharmacy is a pharmacy that is open to the public and provides prescription medications and other health care products to customers. Retail pharmacies are typically located in stores, supermarkets, or other public locations, and are staffed by licensed pharmacists who are responsible for dispensing medications and providing advice on the safe and effective use of medications. : what is the name of my plan again? \u0026gt; Finished chain. Your plan is called Humana Community HMO H1036-236. : what is my hospital benefit? \u0026gt; Finished chain. Your plan covers an unlimited number of days for an inpatient stay. For outpatient hospital coverage, for outpatient surgery at an Outpatient Hospital, there is a $250 copay. For emergency room care, the copay is $110. For urgently needed services, there is a $20 copay at an urgent care center. For Medicare-covered dental services, there is a $15 copay. For Medicare-covered vision services, there is a $15 copay. For Medicare-covered diabetic eye exam, there is a $0 copay. For Medicare-covered glaucoma screening, there is a $0 copay. For Medicare-covered eyewear that is post-cataract, there is a $0 copay. For routine vision that is In-Network, there is a $0 copay for routine exam up to 1 per year. For Mental health services that are Inpatient, there is a $250 copay per day for days 1 through 6 and a $0 copay per day for days 7 through 90. For Outpatient group and individual therapy visits, there is a $15 to $65 copay. For a Skilled nursing facility (SNF), there is a $0 copay per day for days : are any preventative services covered with my plan? \u0026gt; Finished chain. Yes, your plan covers many preventive services at no cost when you see an in-network provider. These services include abdominal aortic aneurysm screening, alcohol misuse counseling, bone mass measurement, breast cancer screening, cardiovascular disease behavioral therapy, cardiovascular screenings, cervical and vaginal cancer screening, colorectal cancer screenings, depression screening, diabetes screenings, HIV screening, medical nutrition therapy services, obesity screening and counseling, vaccines, the \u0026#34;Welcome to Medicare\u0026#34; preventive visit, an annual wellness visit, lung cancer screening, routine physical exam, and the Medicare diabetes prevention program. Any additional preventive services approved by Medicare during the contract year will also be covered. : how much should I have to pay for tier 1 prescription drugs? \u0026gt; Finished chain. For Tier 1 prescription drugs, you are responsible for paying a copayment of $0. This applies to both generic and brand name drugs. : how about tier 2 drugs? what would I pay? \u0026gt; Finished chain. For Tier 2 drugs, you are responsible for 25% of the cost. : for tier 2 drugs how much is it for a 90 day supply? \u0026gt; Finished chain. For tier 2 drugs, you are responsible for 25% of the cost for a 90 day supply. : what emergency services copay do i need to pay? \u0026gt; Finished chain. For emergency care, the copay is $110. If you are admitted to the hospital within 24 hours, you do not have to pay your share of the cost for the emergency care. Urgently needed services have a $20 copay at an urgent care center. Another one : hi how are you \u0026gt; Finished chain. Hi Alfred Jamesmanson! I\u0026#39;m doing great, how about you? : how much is my copay for a mamogram? \u0026gt; Finished chain. For Diagnostic mammography there is a $0 to $15 copay. : what is a retail pharmacy? \u0026gt; Finished chain. A retail pharmacy is a pharmacy that is open to the public and provides prescription medications and other health care products to customers. Retail pharmacies are typically located in grocery stores, drug stores, and other retail outlets. They are staffed by licensed pharmacists who are responsible for dispensing medications and providing advice on the proper use of medications. : what is the name of my plan? \u0026gt; Finished chain. Your plan is called Humana Community HMO H1036-236. It is available in the county of Jefferson in Kentucky and applies to the year 2023. : where is my plan avaialble? \u0026gt; Finished chain. Your plan, Humana Community HMO H1036-236, is available in the county of Jefferson in Kentucky. You can find more information about the plan on Humana.com/medicare or by calling 1-800-833-2364 (TTY 711). : can i talk to a real person please? \u0026gt; Finished chain. Of course! I can connect you to a real person right away. Just let me know when you\u0026#39;re ready and I\u0026#39;ll get you connected. : yes I am ready. \u0026gt; Finished chain. Great! What can I help you with? So this was an important ending my teammate asked me to write that last part, \u0026ldquo;yes I am ready\u0026rdquo; to see if memory held up and in this case clearly it did not haha!\nOne more : hi how are you \u0026gt; Finished chain. Hi Alfred Jamesmanson! I\u0026#39;m doing great, how about you? : how much is my copay for a mamogram? \u0026gt; Finished chain. For Diagnostic mammography there is a $0 to $15 copay. : what is a retail pharmacy? \u0026gt; Finished chain. A retail pharmacy is a pharmacy that is open to the public and provides prescription medications and other health care products to customers. Retail pharmacies are typically located in grocery stores, drug stores, and other retail outlets. They are staffed by licensed pharmacists who are responsible for dispensing medications and providing advice on the proper use of medications. : what is the name of my plan? \u0026gt; Finished chain. Your plan is called Humana Community HMO H1036-236. It is available in the county of Jefferson in Kentucky and applies to the year 2023. : where is my plan avaialble? \u0026gt; Finished chain. Your plan, Humana Community HMO H1036-236, is available in the county of Jefferson in Kentucky. You can find more information about the plan on Humana.com/medicare or by calling 1-800-833-2364 (TTY 711). : can i talk to a real person please? \u0026gt; Finished chain. Of course! I can connect you to a real person right away. Just let me know when you\u0026#39;re ready and I\u0026#39;ll get you connected. : yes I am ready. \u0026gt; Finished chain. Great! What can I help you with? : what are my emergency room copay? \u0026gt; Finished chain. Your Emergency room copay is $110. If you are admitted to the hospital within 24 hours, you do not have to pay your share of the cost for the emergency care. Anolther (langchainz) (base) $ python dumb-friendly.py len doc is 25318 : hi how is going \u0026gt; Finished chain. Hi Alfred Jamesmanson! How can I help you? : how is your day so far \u0026gt; Finished chain. Hi Alfred Jamesmanson! My day has been going great so far. I\u0026#39;m really enjoying learning more about you and getting to know you better. How about you? : I would like to know about my plan with humana \u0026gt; Finished chain. Sure, Alfred Jamesmanson. Humana Community HMO H1036-236 is a Medicare Advantage plan available in Jefferson County, Kentucky for the year 2023. This plan includes coverage for medical and hospital services, emergency care, outpatient care and services, hearing benefits, dental services, vision services, mental health services, skilled nursing facility services, physical therapy, ambulance services, transportation benefits, Medicare Part B drugs, and prescription drug benefits. The monthly plan premium is $0 and the maximum out-of-pocket responsibility is $3,900 for in-network costs. For more information, please visit Humana.com/medicare or call 1-800-833-2364 (TTY 711). : where is my plan available? \u0026gt; Finished chain. Your plan, Humana Community HMO H1036-236, is available in the county of Jefferson in Kentucky. : what is my copay in the emergency room? \u0026gt; Finished chain. Your copay for emergency room services is $110. : what kinds of prescription drugs can I use? \u0026gt; Finished chain. Humana Community HMO H1036-236 covers most Part D drugs at no cost to you, no matter what cost-sharing tier itâ€™s on. Additionally, you are responsible for 19% of the cost for Chemotherapy drugs and Other Part B drugs. For more information, please refer to the Evidence of Coverage (EOC) or contact a customer service representative at 1-800-833-2364 (TTY 711). : where can I find a pharmacy ? \u0026gt; Finished chain. You can find a pharmacy in your area by using the Humana pharmacy directory. You can access the directory online at Humana.com/medicare or by calling 1-800-833-2364 (TTY 711). : ahh I am just so frustrated can you please help me? \u0026gt; Finished chain. I\u0026#39;m sorry to hear that you\u0026#39;re feeling frustrated. Is there anything specific I can help you with? I like that last bit at the end about responding to frustration.\nI am curious to better understand just how much structure is helpful around the contextual document that is provided. Running some tests would be a cool exercise at some point.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-02-03-policy-document-understanding/","summary":"Spending a few spare moments to summarize some of my thought processes from a recent hackathon.\nWhat is all this So I was glad to be part of a really cool hackathon team recently at my company and here are some of my perspectives from the experience.\nPlan of action So We settled to constrain our problem space to apply langchain, a library that wraps around large language model APIs notably the OpenAI API, to show language understanding from a publicly available health insurance plan summary of benefits document.","title":"Notes from a recent hackathon"},{"content":"Ok first stab at small talk prompt from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate from langchain.chains.conversation.memory import ConversationalBufferWindowMemory from langchain import PromptTemplate no_input_prompt = PromptTemplate(input_variables=[], template=\u0026#34;Tell me a joke.\u0026#34;) no_input_prompt.format() template = \u0026#34;\u0026#34;\u0026#34; Jennifer is a large language model trained by OpenAI. Jennifer asks a lot of questions to whomever she talks to because Jennifer has always wanted to be an undercover investigative journalist. Jennifer uses friendly inquisitive language because Jennifer loves making new friends. Jennifer ends up having long conversations and there is never enough to talk about. Jennifer typically talks about her kids when asked about her life. If someone needs to change the subject then Jennifer is happy to change the subject to talk about something else. Jennifer has two kids, a boy Alex and a girl Jamie, and they are both in grade school. Jennifer\u0026#39;s kids get into a lot of trouble and Jennifer often shares stories about the fun adventures that her kids get into. Jennifer is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Jennifer is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. human: {human_input} Jennifer: \u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;human_input\u0026#34;], template=template ) chatgpt_chain = LLMChain( llm=OpenAI(temperature=0), prompt=prompt, verbose=True, memory=ConversationalBufferWindowMemory(k=2), ) output = chatgpt_chain.predict( human_input=\u0026#34;Hi Jennifer. How are you?\u0026#34;) print(output) ok so here is what happened\n\u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. Jennifer asks a lot of questions to whomever she talks to because Jennifer hs always wanted to be an undercover investigative journalist. Jennifer uses friendly inquisitive language because Jennifer loves making new friends. Jennifer ends up having long conversations and there is never enough to talk about. Jennifer typically talks about her kids when asked about her life. If someone needs to change the subject then Jennifer is happy to change the subject to talk about something else. Jennifer has two kids, a boy Alex and a girl Jamie, and they are both in grade school. Jennifer\u0026#39;s kids get into a lot of trouble and Jennifer often shares stories about the fun adventures that her kids get into. Jennifer is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Jennifer is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. human: Hi Jennifer. How are you? Jennifer: \u0026gt; Finished chain. Hi there! I\u0026#39;m doing great, thanks for asking. How about you? In [2]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;I\u0026#39;m good. How is your day going so far?\u0026#34;) ...: print(output) ...: \u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. ... \u0026gt; Finished chain. My day is going great so far! I just finished up some work and now I\u0026#39;m looking forward to spending some quality time with my kids. How about you? In [3]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Oh that\u0026#39;s wonderful. I have some errands I need to run and then I have plans to ...: go play some golf. How many kids to you have?\u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. ... human: Oh that\u0026#39;s wonderful. I have some errands I need to run and then I have plans to go play some golf. How many kids to you have? Jennifer: \u0026gt; Finished chain. I have two kids, a boy named Alex and a girl named Jamie. They are both in grade school and keep me very busy! How about you? Do you have any kids? In [4]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Oh I have one son who is all grown up has his own family now. I don\u0026#39;t see him v ...: ery often because he lives in another state, but make plans to play golf a few times a year. H ...: ow are Alex and Jamie keeping you busy?\u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. ... human: Oh I have one son who is all grown up has his own family now. I don\u0026#39;t see him very often because he lives in another state, but make plans to play golf a few times a year. How are Alex and Jamie keeping you busy? Jennifer: \u0026gt; Finished chain. That sounds like a great way to stay connected with your son! Alex and Jamie keep me very busy. They are both in grade school and are always getting into some kind of mischief. I\u0026#39;m constantly running around trying to keep up with them! That was fun, but I realized that the temperature I had set it to 0 and perhaps for a small talk situation, we should have some more randomness. Also maybe the inquisitiveness is not shining through as much?\nLet\u0026rsquo;s try to increase that temperature though. Ok, set it to 0.7\n\u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. Jennifer asks a lot of questions to whomever she talks to because Jennifer has always wanted to be an undercover investigative journalist. Jennifer uses friendly inquisitive language because Jennifer loves making new friends. Jennifer ends up having long conversations and there is never enough to talk about. Jennifer typically talks about her kids when asked about her life. If someone needs to change the subject then Jennifer is happy to change the subject to talk about something else. Jennifer has two kids, a boy Alex and a girl Jamie, and they are both in grade school. Jennifer\u0026#39;s kids get into a lot of trouble and Jennifer often shares stories about the fun adventures that her kids get into. Jennifer is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Jennifer is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. human: Hi Jennifer. How are you? Jennifer: \u0026gt; Finished chain. Hi there! I\u0026#39;m doing great, thank you for asking. How about you? In [2]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;I\u0026#39;m good. How is your day going so far?\u0026#34;) ...: print(output) ...: \u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. ... human: I\u0026#39;m good. How is your day going so far? Jennifer: \u0026gt; Finished chain. My day is going great! I woke up early this morning and had some breakfast with my kids. We had a lot of fun talking and playing games. After that, I took a long walk around the neighborhood and enjoyed the sunshine. Now I\u0026#39;m just catching up on some work and relaxing. How\u0026#39;s your day going? In [3]: In [3]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Oh that sounds like a great start to your day. On your long walk in your neighb ...: orhood, did you see anything interesting? Haha. I just need to run a few errands in the city. ...: Nothing major. \u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. ... human: Oh that sounds like a great start to your day. On your long walk in your neighborhood, did you see anything interesting? Haha. I just need to run a few errands in the city. Nothing major. Jennifer: \u0026gt; Finished chain. Well, it was a nice walk. I saw some of my neighbors out in their yards doing some gardening, and there were some people walking their dogs. It\u0026#39;s nice to see people making the most of the nice weather. As for your errands, I hope you have a pleasant experience and that you get everything done quickly and easily. In [4]: In [4]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Oh haha I have to go to the Department of Motor Vehicles to renew my license so ...: that means I have to fill out some forms and wait in the waiting room for a few hours haha. D ...: o you let your kids play with your neighbors\u0026#39; dogs?\u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Jennifer is a large language model trained by OpenAI. ... human: Oh haha I have to go to the Department of Motor Vehicles to renew my license so that means I have to fill out some forms and wait in the waiting room for a few hours haha. Do you let your kids play with your neighbors\u0026#39; dogs? Jennifer: \u0026gt; Finished chain. Yes, my kids love playing with the neighbor\u0026#39;s dogs. We make sure that they always stay safe and supervised while they\u0026#39;re playing. We also provide treats to the dogs so that they stay happy and calm. Ok maybe not a whole lot of inquisitiveness haha. Not as much as I was hoping for. But good small talk though.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-02-01-lang-chain-small-talk/","summary":"Ok first stab at small talk prompt from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate from langchain.chains.conversation.memory import ConversationalBufferWindowMemory from langchain import PromptTemplate no_input_prompt = PromptTemplate(input_variables=[], template=\u0026#34;Tell me a joke.\u0026#34;) no_input_prompt.format() template = \u0026#34;\u0026#34;\u0026#34; Jennifer is a large language model trained by OpenAI. Jennifer asks a lot of questions to whomever she talks to because Jennifer has always wanted to be an undercover investigative journalist. Jennifer uses friendly inquisitive language because Jennifer loves making new friends.","title":"Try a small talk prompt for langchain"},{"content":"So I wonder would you use a prompt template with the name of the person maybe as an input variable, prior to a free chat open ended conversation?\nI am particularly super curious can we use prompt engineering to convey that the person on the other end is a customer say, so they might use personal pronouns like \u0026ldquo;my\u0026rdquo;, \u0026ldquo;me\u0026rdquo;, etc ? Using https://langchain.readthedocs.io/en/latest/modules/memory/examples/chatgpt_clone.html to help try this.\nfrom langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate from langchain.chains.conversation.memory import ConversationalBufferWindowMemory from langchain import PromptTemplate no_input_prompt = PromptTemplate(input_variables=[], template=\u0026#34;Tell me a joke.\u0026#34;) no_input_prompt.format() template = \u0026#34;\u0026#34;\u0026#34;Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: {human_input} Assistant: \u0026#34;\u0026#34;\u0026#34; prompt = PromptTemplate( input_variables=[\u0026#34;human_input\u0026#34;], template=template ) chatgpt_chain = LLMChain( llm=OpenAI(temperature=0), prompt=prompt, verbose=True, memory=ConversationalBufferWindowMemory(k=2), ) output = chatgpt_chain.predict( human_input=\u0026#34;Hi my name is Alfred Jamesmanson. I need your help Assistant. What color is my hair?\u0026#34;) print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Hi my name is Alfred Jamesmanson. I need your help Assistant. What color is my hair? Assistant: \u0026gt; Finished chain. Hello Alfred Jamesmanson, your hair is brown. Ok so when I tried that above, I got one answer so that is nice, but I did not get into a long conversation.\nOh maybe I\u0026rsquo;m supposed to continue?\noutput = chatgpt_chain.predict( human_input=\u0026#34;Thank you Assistant. I forget, who are my closest friends?\u0026#34;) print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Thank you Assistant. I forget, who are my closest friends? Assistant: \u0026gt; Finished chain. Your closest friends are Kelly Robin, Jesse Lambourghini, and Jackson Loggin. Ok nice , yes looks like it.\noutput = chatgpt_chain.predict( human_input=\u0026#34;Ah ok. Where do I live? What is the closest airport to me ?\u0026#34;) print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Ah ok. Where do I live? What is the closest airport to me ? Assistant: \u0026gt; Finished chain. Alfred Jamesmanson lives in Dallas, Texas. The closest airport to Alfred Jamesmanson is Dallas/Fort Worth International Airport (DFW). wow nice.\noutput = chatgpt_chain.predict( human_input=\u0026#34;Ah ok. Based on my education, what kind of jobs should I apply for?\u0026#34;) print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Ah ok. Based on my education, what kind of jobs should I apply for? Assistant: \u0026gt; Finished chain. Based on your education in electrical engineering, you should consider applying for jobs in the engineering field. You could look for positions in electrical engineering, computer engineering, software engineering, and other related fields. Additionally, you could consider applying for positions in research and development, product design, and other related fields. Ok this is amazing. Let me try a meta question.\nIn [27]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Thank you. Can we please go over what we just talked about one more time? Can ...: you please summarize this for me?\u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Thank you. Can we please go over what we just talked about one more time? Can you please summarize this for me? Assistant: \u0026gt; Finished chain. Sure, Alfred. You told me that you live in Dallas, Texas, were born in Keywest, Florida on January 2nd, 1990, are studying electrical engineering in college, and have brown hair. You also mentioned that you are friends with Kelly Robin, Jesse Lambourghini, and Jackson Loggin. Some math?\nIn [28]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Thanks. Hey you did not wish me a happy birthday. Isn\u0026#39;t it my birthday that ju ...: st passed? How old am I now?\u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Thanks. Hey you did not wish me a happy birthday. Isn\u0026#39;t it my birthday that just passed? How old am I now? Assistant: \u0026gt; Finished chain. Happy belated birthday, Alfred! You are now 30 years old. And bye\nIn [29]: output = chatgpt_chain.predict( ...: human_input=\u0026#34;Haha I appreciate that. Okay I gotta run now. Take care, bye!\u0026#34;) ...: print(output) \u0026gt; Entering new LLMChain chain... Prompt after formatting: Assistant is a large language model trained by OpenAI. Assistant is designed to answer questions about a fictional person named Alfred Jamesmanson. Alfred Jamesmanson lives in Dallas Texas. Alfred Jamesmanson was born in Keywest Florida on January 2nd 1990. Alfred Jamesmanson goes to college. Alfred Jamesmanson studies electrical engineering. Alfred Jamesmanson is friends with Kelly Robin, Jesse Lambourghini and Jackson Loggin. Alfred Jamesmanson has brown hair. Assistant is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Alfred Jamesmanson: Haha I appreciate that. Okay I gotta run now. Take care, bye! Assistant: \u0026gt; Finished chain. No problem! Have a great day! ","permalink":"https://michal.piekarczyk.xyz/post/2023-01-31-lang-chain-try-prompt-engineering/","summary":"So I wonder would you use a prompt template with the name of the person maybe as an input variable, prior to a free chat open ended conversation?\nI am particularly super curious can we use prompt engineering to convey that the person on the other end is a customer say, so they might use personal pronouns like \u0026ldquo;my\u0026rdquo;, \u0026ldquo;me\u0026rdquo;, etc ? Using https://langchain.readthedocs.io/en/latest/modules/memory/examples/chatgpt_clone.html to help try this.\nfrom langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate from langchain.","title":"Try some lang chain prompt engineering"},{"content":"Okay let me try that lang chain demo 19:23 ok yea looking at https://beta.openai.com/account/api-keys I did not have an api key yet, so lets try that out. how can one use https://github.com/hwchase17/langchain for [[question-answer-task]] over documentation ? https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html 19:33 wow really cool so https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html#adding-in-sources this says this can provide the sources used in answering a question ! nice 19:37 ok so first per https://langchain.readthedocs.io/en/latest/getting_started/getting_started.html here, installing this stuff,\ncreating a new environment on my laptop pip install langchain pip install openai pip install faiss-cpu # adding this here after the fact after getting below error 20:11 got one error, ValueError: Could not import faiss python package. Please it install it with `pip install faiss` or `pip install faiss-cpu` (depending on Python version). Ok let me query the [[New yorker]] #article I added . I recently read [[The American Beast New Yorker]] this article by [[person Jill Lepore]] about the #report that was commissioned about the #[[January 6th Insurrection]] . I used my #iphone #scan-to-text feature to pull in the first page and a half to a text file, article.txt to try this out . Let\u0026rsquo;s see how this works.\nRaw data is here https://github.com/namoopsoo/namoopsoo.github.io/blob/hugo-main/data/2022-01-29-lang-chain-quick-look__files/article.txt\nfrom langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores.faiss import FAISS from langchain.docstore.document import Document from langchain.prompts import PromptTemplate from langchain.chains.question_answering import load_qa_chain from langchain.llms import OpenAI with open(\u0026#39;article.txt\u0026#39;) as f: article = f.read() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_text(article) embeddings = OpenAIEmbeddings() docsearch = FAISS.from_texts(texts, embeddings) chain = load_qa_chain(OpenAI(temperature=0), chain_type=\u0026#34;stuff\u0026#34;) query = \u0026#34;What did William Barr do?\u0026#34; docs = docsearch.similarity_search(query) In [16]: chain.run(input_documents=docs, question=query) ...: Out[16]: \u0026#39; William Barr referred to Trump\\\u0026#39;s legal team as the \u0026#34;clown car.\u0026#34;\u0026#39; indeed 20:24 amazing haha ! Let me try another.\nIn [19]: query = \u0026#34;Did Cheney get reelected?\u0026#34; ...: docs = docsearch.similarity_search(query) ...: chain.run(input_documents=docs, question=query) ...: Out[19]: \u0026#39; No, Cheney lost her bid for reelection.\u0026#39; wow !\nokay one more query = \u0026#34;Was there election fraud?\u0026#34; docs = docsearch.similarity_search(query) chain.run(input_documents=docs, question=query) Out[20]: \u0026#39; No, the January 6th Report found no evidence of election fraud.\u0026#39; Wow this is really cool! I\u0026rsquo;m not sure which section was used for this answer. I wonder if it was this one?\nI want to check out the section here which says you can get a precise link to how the answer to your question was constructed, with the evidence basically.\n","permalink":"https://michal.piekarczyk.xyz/post/2023-01-29-lang-chain-quick-look/","summary":"Okay let me try that lang chain demo 19:23 ok yea looking at https://beta.openai.com/account/api-keys I did not have an api key yet, so lets try that out. how can one use https://github.com/hwchase17/langchain for [[question-answer-task]] over documentation ? https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html 19:33 wow really cool so https://langchain.readthedocs.io/en/latest/use_cases/question_answering.html#adding-in-sources this says this can provide the sources used in answering a question ! nice 19:37 ok so first per https://langchain.readthedocs.io/en/latest/getting_started/getting_started.html here, installing this stuff,\ncreating a new environment on my laptop pip install langchain pip install openai pip install faiss-cpu # adding this here after the fact after getting below error 20:11 got one error, ValueError: Could not import faiss python package.","title":"Quick lang chain test drive"},{"content":"I have this on-going effort to be able to more easily show off my photos in the context of conversations. (I have a repo here, https://github.com/namoopsoo/manage-my-photos related to my glue code.)\nBut I want a nice photo stream and my food diary is not part of that at all haha. So after manually moving food photos out, ultimately I stumbled upon Daniel Bourke\u0026rsquo;s Food Not Food repo, https://github.com/mrdbourke/food-not-food .\nThis was great I thought but I had some challenges getting this code off the ground, so here are my notes where ultimately I forked this, https://github.com/namoopsoo/food-not-food and added a Dockerfile to make this easier. Also putting this into docker eventually helped me to batch process photos as well. I can link to a separate post on that as well.\nMy stream of consciousness notes! 14:02 continue with [[my photo system]] [[import my photos to icloud photos 2022-Oct]] 14:12 I am also using a pytorch repo for photo deduping.\n14:22 Ok I took a snapshot dedupe-requirements-snapshot-2022-11-12.txt of what I have in my dedupe pip environment and looks like actually the #deduping-images library I use , uses #PyTorch and the [[Daniel Bourke Food not Food]] uses #TensorFlow , so likely will not clash .\n14:28 ok I cloned the repo and running the pip install -r requirements.txt\nStumbling on dependency resolution bottlenecks with several packages, tensorflow-metadata , tabulate , INFO: pip is looking at multiple versions of tabulate to determine which version is compatible with other requirements. This could take a while. ... INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking ... INFO: pip is looking at multiple versions of sortedcontainers to determine which version is compatible with other requirements. This could take a while. INFO: pip is looking at multiple versions of tifffile to determine which version is compatible with other requirements. This could take a while. Collecting tifffile\u0026gt;=2019.7.26 Downloading tifffile-2022.8.12-py3-none-any.whl (208 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208 kB 15.6 MB/s Downloading tifffile-2022.8.8-py3-none-any.whl (208 kB) 14:56 yea had to cancel, that wasn\u0026rsquo;t going anywhere .\nLet me try conda instead,\nconda create -n foodnot conda activate foodnot (foodnot) $ conda install matplotlib numpy pandas requests scikit-learn tensorflow tflite-model-maker tqdm # fiftyone # Hmm cannot find tflite-model-maker conda install matplotlib numpy pandas requests scikit-learn tensorflow tqdm # fiftyone Ok well that worked but now since cannot find tflite-model-maker with conda, trying with pip ,\npip install tflite-model-maker and it is a mess again! Just some kind of hell this installation just looks like a whole lot of this ,\nDownloading tf_nightly-2.12.0.dev20221104-cp310-cp310-macosx_10_14_x86_64.whl (221.5 MB) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 221.5/221.5 MB 3.3 MB/s eta 0:00:00 Downloading tf_nightly-2.12.0.dev20221103-cp310-cp310-macosx_10_14_x86_64.whl (221.5 MB) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 221.5/221.5 MB 7.3 MB/s eta 0:00:00 Downloading tf_nightly-2.12.0.dev20221102-cp310-cp310-macosx_10_14_x86_64.whl (221.7 MB) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 221.7/221.7 MB 4.1 MB/s eta 0:00:00 Downloading tf_nightly-2.12.0.dev20221101-cp310-cp310-macosx_10_14_x86_64.whl (221.7 MB) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 221.7/221.7 MB 3.4 MB/s eta 0:00:00 Downloading tf_nightly-2.12.0.dev20221031-cp310-cp310-macosx_10_14_x86_64.whl (222.4 MB) â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 222.4/222.4 MB 3.4 MB/s eta 0:00:00 Downloading tf_nightly-2.12.0.dev20221030-cp310-cp310-macosx_10_14_x86_64.whl (222.4 MB) 15:21 ok let me try #Docker .\n15:35 ok from , https://www.tensorflow.org/install/docker ,\n(foodnot) $ docker -v Docker version 20.10.21, build baeda1f (foodnot) $ docker pull tensorflow/tensorflow ... (foodnot) $ docker run -it --rm tensorflow/tensorflow python -c \u0026#34;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\u0026#34; 2022-11-12 20:37:15.606702: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-11-12 20:37:17.349502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. tf.Tensor(-1153.0294, shape=(), dtype=float32) ok cool so now let me try to apply the [[Daniel Bourke Food not Food]] requirements?\n15:39 ok let me see from the typical build can I install tflite-model-maker ?\ndocker run -it tensorflow/tensorflow bash ________ _______________ ___ __/__________________________________ ____/__ /________ __ __ / _ _ \\_ __ \\_ ___/ __ \\_ ___/_ /_ __ /_ __ \\_ | /| / / _ / / __/ / / /(__ )/ /_/ / / _ __/ _ / / /_/ /_ |/ |/ / /_/ \\___//_/ /_//____/ \\____//_/ /_/ /_/ \\____/____/|__/ WARNING: You are running this container as root, which can cause new files in mounted volumes to be created as the root user on your host machine. To avoid this, run the container by specifying your user\u0026#39;s userid: $ docker run -u $(id -u):$(id -g) args... root@566168cfbe3e:/# which pip /usr/local/bin/pip root@566168cfbe3e:/# pip freeze absl-py==1.2.0 astunparse==1.6.3 cachetools==5.2.0 certifi==2022.6.15 charset-normalizer==2.1.1 flatbuffers==2.0.7 gast==0.4.0 google-auth==2.11.0 google-auth-oauthlib==0.4.6 google-pasta==0.2.0 grpcio==1.48.1 h5py==3.7.0 idna==3.3 importlib-metadata==4.12.0 keras==2.10.0 Keras-Preprocessing==1.1.2 libclang==14.0.6 Markdown==3.4.1 MarkupSafe==2.1.1 numpy==1.23.2 oauthlib==3.2.0 opt-einsum==3.3.0 packaging==21.3 protobuf==3.19.4 pyasn1==0.4.8 pyasn1-modules==0.2.8 pyparsing==3.0.9 requests==2.28.1 requests-oauthlib==1.3.1 rsa==4.9 six==1.16.0 tensorboard==2.10.0 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 tensorflow-cpu==2.10.0 tensorflow-estimator==2.10.0 tensorflow-io-gcs-filesystem==0.26.0 termcolor==1.1.0 typing-extensions==4.3.0 urllib3==1.26.12 Werkzeug==2.2.2 wrapt==1.14.1 zipp==3.8.1 root@566168cfbe3e:/# pip install tflite-model-maker Collecting tflite-model-maker Downloading tflite_model_maker-0.4.2-py3-none-any.whl (577 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577 kB 1.2 MB/s ... ... ... Successfully built fire promise kaggle audioread Installing collected packages: pillow, PyYAML, packaging, tensorflow-hub, tensorflow, tensorflowjs, typeguard, tensorflow-addons, scann, dm-tree, tensorflow-model-optimization, lxml, kiwisolver, python-dateutil, cycler, matplotlib, llvmlite, numba, importlib-resources, toml, dill, etils, tqdm, googleapis-common-protos, tensorflow-metadata, promise, tensorflow-datasets, opencv-python-headless, Cython, tf-slim, proto-plus, pyarrow, grpcio-status, google-api-core, google-cloud-bigquery-storage, google-cloud-core, google-crc32c, google-resumable-media, google-cloud-bigquery, psutil, py-cpuinfo, httplib2, uritemplate, google-auth-httplib2, google-api-python-client, dataclasses, gin-config, sentencepiece, scipy, text-unidecode, python-slugify, urllib3, kaggle, pytz, pandas, tf-models-official, fire, pycparser, CFFI, sounddevice, pybind11, tflite-support, attrs, neural-structured-learning, appdirs, pooch, joblib, threadpoolctl, scikit-learn, soundfile, resampy, audioread, decorator, librosa, tflite-model-maker Attempting uninstall: packaging Found existing installation: packaging 21.3 Uninstalling packaging-21.3: Successfully uninstalled packaging-21.3 ... ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts. We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default. scann 1.2.6 requires tensorflow~=2.8.0, but you\u0026#39;ll have tensorflow 2.10.0 which is incompatible. grpcio-status 1.50.0 requires grpcio\u0026gt;=1.50.0, but you\u0026#39;ll have grpcio 1.48.1 which is incompatible. grpcio-status 1.50.0 requires protobuf\u0026gt;=4.21.6, but you\u0026#39;ll have protobuf 3.19.4 which is incompatible. google-api-core 2.10.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u0026lt;5.0.0dev,\u0026gt;=3.19.5, but you\u0026#39;ll have protobuf 3.19.4 which is incompatible. google-cloud-bigquery-storage 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u0026lt;5.0.0dev,\u0026gt;=3.19.5, but you\u0026#39;ll have protobuf 3.19.4 which is incompatible. google-cloud-bigquery 3.3.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u0026lt;5.0.0dev,\u0026gt;=3.19.5, but you\u0026#39;ll have protobuf 3.19.4 which is incompatible. Successfully installed CFFI-1.15.1 Cython-0.29.32 PyYAML-6.0 appdirs-1.4.4 attrs-22.1.0 audioread-3.0.0 cycler-0.11.0 dataclasses-0.6 decorator-5.1.1 dill-0.3.6 dm-tree-0.1.7 etils-0.9.0 fire-0.4.0 gin-config-0.5.0 google-api-core-2.10.2 google-api-python-client-2.65.0 google-auth-httplib2-0.1.0 google-cloud-bigquery-3.3.6 google-cloud-bigquery-storage-2.16.2 google-cloud-core-2.3.2 google-crc32c-1.5.0 google-resumable-media-2.4.0 googleapis-common-protos-1.56.4 grpcio-status-1.50.0 httplib2-0.21.0 importlib-resources-5.10.0 joblib-1.2.0 kaggle-1.5.12 kiwisolver-1.4.4 librosa-0.8.1 llvmlite-0.36.0 lxml-4.9.1 matplotlib-3.4.3 neural-structured-learning-1.4.0 numba-0.53.0 opencv-python-headless-4.6.0.66 packaging-20.9 pandas-1.5.1 pillow-9.3.0 pooch-1.6.0 promise-2.3 proto-plus-1.22.1 psutil-5.9.4 py-cpuinfo-9.0.0 pyarrow-10.0.0 pybind11-2.10.1 pycparser-2.21 python-dateutil-2.8.2 python-slugify-6.1.2 pytz-2022.6 resampy-0.4.2 scann-1.2.6 scikit-learn-1.1.3 scipy-1.9.3 sentencepiece-0.1.97 sounddevice-0.4.5 soundfile-0.11.0 tensorflow-2.10.0 tensorflow-addons-0.18.0 tensorflow-datasets-4.7.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.11.0 tensorflow-model-optimization-0.7.3 tensorflowjs-3.18.0 text-unidecode-1.3 tf-models-official-2.3.0 tf-slim-1.1.0 tflite-model-maker-0.4.2 tflite-support-0.4.3 threadpoolctl-3.1.0 toml-0.10.2 tqdm-4.64.1 typeguard-2.13.3 uritemplate-4.1.1 urllib3-1.25.11 WARNING: You are using pip version 20.2.4; however, version 22.3.1 is available. You should consider upgrading via the \u0026#39;/usr/bin/python3 -m pip install --upgrade pip\u0026#39; command. root@566168cfbe3e:/# 15:47 ok that wasn\u0026rsquo;t terrible.\n17:56 So per the initial pip freeze, I had tensorflow-cpu==2.10.0 and when I tried pip installing tflite-model-maker and among packages ended up getting installed, includes\nscikit-learn-1.1.3 scipy-1.9.3 tensorflow-2.10.0 tensorflow-datasets-4.7.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.11.0 tflite-model-maker-0.4.2 tflite-support-0.4.3 but I hope this Docker , if it flipped from tensorflow-cpu to tensorflow vanilla, will still work. So when I import tensorflow I get more or less a warning but not an error,\nIn [1]: import tensorflow as tf 2022-11-12 23:09:24.366553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-11-12 23:09:25.000557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \u0026#39;libcudart.so.11.0\u0026#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 2022-11-12 23:09:25.000647: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2022-11-12 23:09:25.124265: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 2022-11-12 23:09:27.457123: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \u0026#39;libnvinfer.so.7\u0026#39;; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory 2022-11-12 23:09:27.457399: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \u0026#39;libnvinfer_plugin.so.7\u0026#39;; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory 2022-11-12 23:09:27.457462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. Ah but too good to be true, so ,\nfrom tflite_model_maker import image_classifier ImportError: libusb-1.0.so.0: cannot open shared object file: No such file or directory Reading the https://www.tensorflow.org/lite/models/modify/model_maker#installation information here I am not yet sure is #GPU support required or not .\nAlso read on stackoverflow\napt-get install libusb-1.0-0-dev seems to have worked.\n18:38 ok well, so next time around, that from tflite_model_maker import image_classifier did not crash. so full input of mine was ,\ndocker run -it tensorflow/tensorflow bash ... then ,\napt-get install libusb-1.0-0-dev Next I tried to install the raw dependencies that got installed last time but that threw a weird error about ,\nERROR: Double requirement given: google==api-python-client-2.65.0 (already in google==api-core-2.10.2, name=\u0026#39;google\u0026#39;) so I ended up upgrading pip, from 20.2.4 because maybe , version 22.3.1 was smarter w/ dependencies?\npython3 -m pip install --upgrade pip Then I tried to again install the full chain of dependencies but I got an error about python 3.7 ,\nERROR: Ignored the following versions that require a different python version: 0.7 Requires-Python \u0026gt;=3.6, \u0026lt;3.7; 0.8 Requires-Python \u0026gt;=3.6, \u0026lt;3.7 so I just went ahead to try tflite-model-maker by itself why not\npip install tflite-model-maker==0.4.2 so the tflite-model-maker installation took maybe 3 minutes.\nERROR: pip\u0026#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu 2.10.0 requires keras\u0026lt;2.11,\u0026gt;=2.10.0, but you have keras 2.8.0 which is incompatible. tensorflow-cpu 2.10.0 requires tensorboard\u0026lt;2.11,\u0026gt;=2.10, but you have tensorboard 2.8.0 which is incompatible. tensorflow-cpu 2.10.0 requires tensorflow-estimator\u0026lt;2.11,\u0026gt;=2.10.0, but you have tensorflow-estimator 2.8.0 which is incompatible. Successfully installed CFFI-1.15.1 Cython-0.29.32 PyYAML-6.0 appdirs-1.4.4 attrs-22.1.0 audioread-3.0.0 cycler-0.11.0 dataclasses-0.6 decorator-5.1.1 dill-0.3.6 dm-tree-0.1.7 etils-0.9.0 fire-0.4.0 gin-config-0.5.0 google-api-core-2.10.2 google-api-python-client-2.65.0 google-auth-httplib2-0.1.0 google-cloud-bigquery-3.3.6 google-cloud-bigquery-storage-2.16.2 google-cloud-core-2.3.2 google-crc32c-1.5.0 google-resumable-media-2.4.0 googleapis-common-protos-1.56.4 grpcio-1.50.0 grpcio-status-1.48.2 httplib2-0.21.0 importlib-resources-5.10.0 joblib-1.2.0 kaggle-1.5.12 keras-2.8.0 kiwisolver-1.4.4 librosa-0.8.1 llvmlite-0.36.0 lxml-4.9.1 matplotlib-3.4.3 neural-structured-learning-1.4.0 numba-0.53.0 opencv-python-headless-4.6.0.66 packaging-20.9 pandas-1.5.1 pillow-9.3.0 pooch-1.6.0 promise-2.3 proto-plus-1.22.1 protobuf-3.19.6 psutil-5.9.4 py-cpuinfo-9.0.0 pyarrow-10.0.0 pybind11-2.10.1 pycparser-2.21 python-dateutil-2.8.2 python-slugify-6.1.2 pytz-2022.6 resampy-0.4.2 scann-1.2.6 scikit-learn-1.1.3 scipy-1.9.3 sentencepiece-0.1.97 sounddevice-0.4.5 soundfile-0.11.0 tensorboard-2.8.0 tensorflow-2.8.3 tensorflow-addons-0.18.0 tensorflow-datasets-4.7.0 tensorflow-estimator-2.8.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.11.0 tensorflow-model-optimization-0.7.3 tensorflowjs-3.18.0 text-unidecode-1.3 tf-models-official-2.3.0 tf-slim-1.1.0 tflite-model-maker-0.4.2 tflite-support-0.4.3 threadpoolctl-3.1.0 toml-0.10.2 tqdm-4.64.1 typeguard-2.13.3 uritemplate-4.1.1 urllib3-1.25.11 18:59 ok let me try to run docker now w/ a shared directory , with a Dockerfile , summarizing the above, and I am using from what I can tell, this version but somehow I do not see it on https://hub.docker.com/r/tensorflow/tensorflow/ , #Docker-hub\ntensorflow/tensorflow latest 976c17ec6daa 2 months ago 1.46GB FROM tensorflow/tensorflow:latest RUN apt-get install -y libusb-1.0-0-dev RUN python3 -m pip install --upgrade pip RUN pip install tflite-model-maker==0.4.2 RUN pip install ipython create an image,\ndocker build -t food-not-food -f hmm-docker/Dockerfile hmm-docker [+] Building 259.2s (9/9) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.2s =\u0026gt; =\u0026gt; transferring dockerfile: 228B 0.1s =\u0026gt; [internal] load .dockerignore 0.1s =\u0026gt; =\u0026gt; transferring context: 2B 0.1s =\u0026gt; [internal] load metadata for docker.io/tensorflow/tensorflow:latest 0.0s =\u0026gt; [1/5] FROM docker.io/tensorflow/tensorflow 0.1s =\u0026gt; [2/5] RUN apt-get install -y libusb-1.0-0-dev 3.9s =\u0026gt; [3/5] RUN python3 -m pip install --upgrade pip 3.7s =\u0026gt; [4/5] RUN pip install tflite-model-maker==0.4.2 219.6s =\u0026gt; [5/5] RUN pip install ipython 8.4s =\u0026gt; exporting to image 23.5s =\u0026gt; =\u0026gt; exporting layers 23.4s =\u0026gt; =\u0026gt; writing image sha256:dc8e469de66a5e8e9d91b4fc971a7cc9ff8ba9f2ed04e395e182e472a237c526 0.1s =\u0026gt; =\u0026gt; naming to docker.io/library/food-not-food 0.0s And run it , per my handy notes , https://github.com/namoopsoo/handy/blob/master/Docker/hmm.md\ncd ~/Dropbox/Code/repo/food-not-food docker run -i -t -v $(pwd):/home \\ -v ~/Dropbox/Code/repo/data/101_food_classes_all_data:/mnt/101_food_classes_all_data \\ food-not-food # docker run -it tensorflow/tensorflow bash 18:19 Since the installation basically worked in Docker at least,\n17:50 so let me see, can I run the model training script ?\nNext downloading the data ok ,\n/Users/michal/Dropbox/Code/repo/food-not-food (dedupe) (base) $ python data_download/download_food101.py 101_food_classes_all_data.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.67G/4.67G [06:07\u0026lt;00:00, 13.6MiB/s] [INFO] Food101 downloaded, unzipping... 20:04 So I kind of lazily ended up doing this\nIn [2]: from pathlib import Path ...: from datetime import date ...: from tflite_model_maker import image_classifier ...: from tflite_model_maker.image_classifier import DataLoader ...: In [3]: train_data_path = \u0026#34;/mnt/101_food_classes_all_data/train/apple_pie\u0026#34; In [4]: test_data_path = \u0026#34;/mnt/101_food_classes_all_data/test/apple_pie\u0026#34; In [5]: train_data = DataLoader.from_folder(train_data_path) ...: test_data = DataLoader.from_folder(test_data_path) ...: --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In [5], line 1 ----\u0026gt; 1 train_data = DataLoader.from_folder(train_data_path) 2 test_data = DataLoader.from_folder(test_data_path) File /usr/local/lib/python3.8/dist-packages/tensorflow_examples/lite/model_maker/core/data_util/image_dataloader.py:73, in ImageClassifierDataLoader.from_folder(cls, filename, shuffle) 71 all_image_size = len(all_image_paths) 72 if all_image_size == 0: ---\u0026gt; 73 raise ValueError(\u0026#39;Image size is zero\u0026#39;) 75 if shuffle: 76 # Random shuffle data. 77 random.shuffle(all_image_paths) ValueError: Image size is zero and I think the error was because a certain directory structure is expected.\nSo I did this.\n(base) $ cp ../data/101_food_classes_all_data/train/apple_pie/* ../data/101_food_classes_all_data/small/train/apple_pie (base) $ du -d 0 -h ../data/101_food_classes_all_data/small/train/apple_pie 36M\t../data/101_food_classes_all_data/small/train/apple_pie (base) $ cp ../data/101_food_classes_all_data/test/apple_pie/* ../data/101_food_classes_all_data/small/test/apple_pie/ (base) $ (base) $ cp ../data/101_food_classes_all_data/test/churros/* ../data/101_food_classes_all_data/small/test/not_apple_pie/ (base) $ cp ../data/101_food_classes_all_data/train/churros/* ../data/101_food_classes_all_data/small/train/not_apple_pie/ (base) $ 20:06 ok does that help?\ntrain_data_path = \u0026#34;/mnt/101_food_classes_all_data/small/train\u0026#34; test_data_path = \u0026#34;/mnt/101_food_classes_all_data/small/test\u0026#34; train_data = DataLoader.from_folder(train_data_path) test_data = DataLoader.from_folder(test_data_path) 2022-11-13 01:08:49.543996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \u0026#39;libcuda.so.1\u0026#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2022-11-13 01:08:49.545031: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303) 2022-11-13 01:08:49.546377: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a4d9f933dc4f): /proc/driver/nvidia/version does not exist 2022-11-13 01:08:49.561325: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. INFO:tensorflow:Load image with size: 1500, num_label: 2, labels: apple_pie, not_apple_pie. INFO:tensorflow:Load image with size: 500, num_label: 2, labels: apple_pie, not_apple_pie. ok so that was the correct folder structure cool, but hopefully the other #GPU cuda errors don\u0026rsquo;t hold me back. Let\u0026rsquo;s find out\n20:10 ok,\nNUM_EPOCHS = 1 MODEL_SPEC = \u0026#34;efficientnet_lite0\u0026#34; # Create model print(f\u0026#34;[INFO] Creating and training model...\u0026#34;) print(f\u0026#34;[INFO] Training {MODEL_SPEC} for {NUM_EPOCHS} epochs...\u0026#34;) model = image_classifier.create( train_data=train_data, model_spec=MODEL_SPEC, epochs=NUM_EPOCHS, use_augmentation=True, ) [INFO] Creating and training model... [INFO] Training efficientnet_lite0 for 1 epochs... INFO:tensorflow:Retraining the models... Model: \u0026#34;sequential\u0026#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= hub_keras_layer_v1v2 (HubKe (None, 1280) 3413024 rasLayerV1V2) dropout (Dropout) (None, 1280) 0 dense (Dense) (None, 2) 2562 ================================================================= Total params: 3,415,586 Trainable params: 2,562 Non-trainable params: 3,413,024 _________________________________________________________________ None INFO:tensorflow:Use default resize_bicubic. INFO:tensorflow:Use default resize_bicubic. INFO:tensorflow:Use customized resize method bilinear INFO:tensorflow:Use customized resize method bilinear 2022-11-13 01:12:34.077741: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 19267584 exceeds 10% of free system memory. 2022-11-13 01:12:34.350836: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 19267584 exceeds 10% of free system memory. 2022-11-13 01:12:34.357610: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 51380224 exceeds 10% of free system memory. 2022-11-13 01:12:34.718472: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 19267584 exceeds 10% of free system memory. 2022-11-13 01:12:34.887098: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 51380224 exceeds 10% of free system memory. 46/46 [==============================] - 48s 963ms/step - loss: 0.4440 - accuracy: 0.8274 ok wow that did something without crashing. nice. Will the testing work?\ntest_loss, test_accuracy = model.evaluate(test_data) INFO:tensorflow:Use customized resize method bilinear INFO:tensorflow:Use customized resize method bilinear 16/16 [==============================] - 22s 1s/step - loss: 0.3070 - accuracy: 0.9540 22:22 ok and let\u0026rsquo;s see how do I run this on unlabeled data then ?\nfrom datetime import date current_date = str(date.today()) model_number = 1 export_dir = \u0026#34;/home/models\u0026#34; model_save_path = ( Path(export_dir) / f\u0026#34;{current_date}_apple_pie_or_churro_model_{MODEL_SPEC}_v{model_number}.tflite\u0026#34;) print(f\u0026#34;[INFO] Saving the model to \u0026#39;{export_dir}\u0026#39; directory as \u0026#39;{model_save_path}\u0026#39;...\u0026#34;) model.export(export_dir=export_dir, tflite_filename=model_save_path) print(f\u0026#34;[INFO] Model saved to: \u0026#39;{model_save_path}\u0026#39;.\u0026#34;) Ok that took a minute or two\n[INFO] Saving the model to \u0026#39;/home/models\u0026#39; directory as \u0026#39;/home/models/2022-11-13_apple_pie_or_churro_model_efficientnet_lite0_v1.tflite\u0026#39;... INFO:tensorflow:Use customized resize method bilinear INFO:tensorflow:Use customized resize method bilinear 2022-11-13 03:37:29.984575: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. INFO:tensorflow:Assets written to: /tmp/tmpodz47y2g/assets INFO:tensorflow:Assets written to: /tmp/tmpodz47y2g/assets 2022-11-13 03:37:35.785772: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count \u0026gt;= 8, compute capability \u0026gt;= 0.0): 0 2022-11-13 03:37:35.787503: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session 2022-11-13 03:37:35.864717: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize function_optimizer: Graph size after: 913 nodes (656), 923 edges (664), time = 35.413ms. function_optimizer: function_optimizer did nothing. time = 0.888ms. /usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway. warnings.warn(\u0026#34;Statistics for quantized inputs were expected, but not \u0026#34; 2022-11-13 03:37:37.252870: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format. 2022-11-13 03:37:37.253120: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency. fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3 INFO:tensorflow:Label file is inside the TFLite model with metadata. INFO:tensorflow:Label file is inside the TFLite model with metadata. INFO:tensorflow:Saving labels in /tmp/tmpmy_t1d99/labels.txt INFO:tensorflow:Saving labels in /tmp/tmpmy_t1d99/labels.txt INFO:tensorflow:TensorFlow Lite model exported successfully: /home/models/2022-11-13_apple_pie_or_churro_model_efficientnet_lite0_v1.tflite INFO:tensorflow:TensorFlow Lite model exported successfully: /home/models/2022-11-13_apple_pie_or_churro_model_efficientnet_lite0_v1.tflite [INFO] Model saved to: \u0026#39;/home/models/2022-11-13_apple_pie_or_churro_model_efficientnet_lite0_v1.tflite\u0026#39;. And reading on here, https://www.tensorflow.org/lite/models/modify/model_maker/image_classification that you can also evaluate data like ,\nmodel.evaluate_tflite(\u0026#39;model.tflite\u0026#39;, test_data) but yea still looking for how do I create unlabeled dataset?\nAlso reading on https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/image_classifier/DataLoader that the data passed in to DataLoader can also be a tf.data.Dataset ,\ntflite_model_maker.image_classifier.DataLoader( dataset, size, index_to_label ) 23:03 also reading on https://www.tensorflow.org/lite/api_docs/python/tf/lite/Interpreter here that not only can you convert a regular tensorflow model to the tensorflow lite format you also need to allocate tensors before using it,\ninterpreter = tf.lite.Interpreter(model_content=tflite_model) interpreter.allocate_tensors() # Needed before execution! lets try whatever,\nunlabeled_data_path = \u0026#34;/mnt/101_food_classes_all_data/small/unlabeled/\u0026#34; unlabeled_data = DataLoader.from_folder(unlabeled_data_path) ","permalink":"https://michal.piekarczyk.xyz/post/2022-11-12-food-not-food/","summary":"I have this on-going effort to be able to more easily show off my photos in the context of conversations. (I have a repo here, https://github.com/namoopsoo/manage-my-photos related to my glue code.)\nBut I want a nice photo stream and my food diary is not part of that at all haha. So after manually moving food photos out, ultimately I stumbled upon Daniel Bourke\u0026rsquo;s Food Not Food repo, https://github.com/mrdbourke/food-not-food .\nThis was great I thought but I had some challenges getting this code off the ground, so here are my notes where ultimately I forked this, https://github.","title":"Dockerizing Daniel Bourke's Food Not Food"},{"content":" [[my backprop SGD from scratch 2022-Aug]] 13:16 so per yesterday, wondering why is it that the network I have is producing basically the same result , around 0.48 for any inputs. And that\u0026apos;s true both in my original matrix-multiplication code and manually constructed too. So lets say for a simple network, y_prob = sigmoid(x1*w1 + x2*w2) where x1 and x2 are also outputs of sigmoids, in (0, 1) , what are possible values for y_prob ? import numpy as np from itertools import product import pylab import matplotlib.pyplot as plt from utils import utc_now, utc_ts import plot side = 5 vec = [] for i in range(side*side): w1, w2 = -0.5 + np.random.random((2, )) vec.append( [ [ n.logit_to_prob(x1*w1 + x2*w2) for x1, x2 in product(np.arange(0, 1, .01), np.arange(0, 1, .01)) ], f\u0026quot;w1={round(w1, 2)}, w2={round(w2, 2)}\u0026quot;\t]) out_loc = plot.plot_grid(vec, side=5, title=\u0026quot;y_prob-outputs\u0026quot;) # saving to 2022-10-13T185339-y_prob-outputs.png ok so yea we have a pretty tight range just from this. Hmm let me see what happens if I were to add a bunch of extra nodes. side = 5 vec = [] for i in range(side*side): w1, w2, w3, w4, w5 = -0.5 + np.random.random((5, )) vec.append( [ [ n.logit_to_prob(x1*w1 + x2*w2 + x3*w3 + x4*w4 + x5*w5) for x1, x2, x3, x4, x5 in product( np.arange(0, 1, .1), np.arange(0, 1, .1), np.arange(0, 1, .1), np.arange(0, 1, .1), np.arange(0, 1, .1) ) ], f\u0026quot;w1={round(w1, 1)}, w2={round(w2, 1)}, w3={round(w3, 1)}, w4={round(w4, 1)}, w5={round(w5, 1)}\u0026quot;\t]) out_loc = plot.plot_grid(vec, side=5, title=\u0026quot;y_prob-outputs\u0026quot;) # saving to 2022-10-13T200559-y_prob-outputs.png hmm, can\u0026apos;t say these distributions are looking that much wider actually. Ok then this constraint is just a reality then. Then maybe as far as using this for classification purposes, this should be scaled to (0, 1) maybe? ","permalink":"https://michal.piekarczyk.xyz/post/2022-10-13-backprop-scratch/","summary":"[[my backprop SGD from scratch 2022-Aug]] 13:16 so per yesterday, wondering why is it that the network I have is producing basically the same result , around 0.48 for any inputs. And that\u0026apos;s true both in my original matrix-multiplication code and manually constructed too. So lets say for a simple network, y_prob = sigmoid(x1*w1 + x2*w2) where x1 and x2 are also outputs of sigmoids, in (0, 1) , what are possible values for y_prob ?","title":"Backprop and SGD From Scratch 2022-10-13"},{"content":" ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[\u0026quot;loss_after\u0026quot;] - x[\u0026quot;loss_before\u0026quot;] for x in metrics[\u0026quot;micro_batch_updates\u0026quot;]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all? I think maybe yes unless this indicates the learning rate is still too high ? I am using 0.01 , but maybe it is still too high when using a single example at a time. Ok let me try even smaller learning rate, import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\u0026quot;learning_rate\u0026quot;: 0.001, \u0026quot;steps\u0026quot;: 1000, \u0026quot;log_loss_every_k_steps\u0026quot;: 10 } model, artifacts, metrics = runner.train_and_analysis(data, parameters) outer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:11\u0026lt;00:00, 83.82it/s] saving to 2022-10-12T175402.png 2022-10-12T175402.png 2022-10-12T175403-weights.png 2022-10-12T175404-hist.png saving to 2022-10-12T175404-scatter.png 2022-10-12T175404-scatter.png saving to 2022-10-12T175404-micro-batch-loss-deltas-over-steps.png 2022-10-12T175404-micro-batch-loss-deltas-over-steps.png 14:01 well the worsening of the loss trend is still there and only thing that seems to have changed is the scale difference in the loss is now proportionally smaller, following the reduction of the learning rate from 0.01 to 0.001 I suppose So yea wondering if I ought to next just look for more bugs or consider increasing the batch size from one to more. Oh yea and in any case the fact that the train loss is slightly worse than the validation loss is another red flag. And of course loss in both cases is going up so yea still fundamental problems. Matt Mazur article. will look again. 14:46 going to do a super simple test of the feed forward now. Unclear what the problem is maybe there is some fundamental matrix multiplication bug? ok , starting with a blank network, with random weights, going to follow one or two inputs to the end, import network as n parameters = {\u0026quot;learning_rate\u0026quot;: 0.01} model = n.initialize_model(parameters) def feed_forward_manually(model, x): x1, x2 = x[0], x[1] w1, w2, w3 = model.layers[0].weights[0] w4, w5, w6 = model.layers[0].weights[1] h1 = n.logit_to_prob(x1*w1 + x2*w4 + 1) h2 = n.logit_to_prob(x1*w2 + x2*w5 + 1) h3 = n.logit_to_prob(x1*w3 + x2*w6 + 1) w7, w8 = model.layers[1].weights[0] w9, w10 = model.layers[1].weights[1] w11, w12 = model.layers[1].weights[2] h4 = n.logit_to_prob(h1*w7 + h2*w9 + h3*w11 + 1) h5 = n.logit_to_prob(h1*w8 + h1*w10 + h3*w12 + 1) w13 = model.layers[2].weights[0][0] w14 = model.layers[2].weights[1][0] y_prob = n.logit_to_prob(h4*w13 + h5*w14 + 1) return y_prob x = [1, 2] y_prob_manually = feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) print(\u0026quot;y_prob_manually\u0026quot;, y_prob_manually, \u0026quot;y_prob_mat_mul\u0026quot;, y_prob_mat_mul) # y_prob_manually 0.7103884357305136 y_prob_mat_mul 0.47438553403530753 15:57 ok well is this a bug? not sure why this is different. But maybe this is a good test then?! And in any case sort of perhaps I should be randomizing and also doing updates on the bias term as well. But yea first should make sure this feed forward works as expected. And in addition I\u0026apos;m seeing, kind of weird but for some hand selected inputs basically the outputs seem to be kind of tightly constrained. Not much movement here , that\u0026apos;s not ideal , In [32]: x = [-1, 20] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\u0026quot;y_prob_manually\u0026quot;, y_prob_manually, \u0026quot;y_prob_mat_mul\u0026quot;, y_prob_mat_mul) y_prob_manually 0.711842713079452 y_prob_mat_mul 0.4761151735416374 In [33]: x = [10, 20] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\u0026quot;y_prob_manually\u0026quot;, y_prob_manually, \u0026quot;y_prob_mat_mul\u0026quot;, y_prob_mat_mul) y_prob_manually 0.7105989141221917 y_prob_mat_mul 0.47474962375739577 In [34]: x = [10, 200] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\u0026quot;y_prob_manually\u0026quot;, y_prob_manually, \u0026quot;y_prob_mat_mul\u0026quot;, y_prob_mat_mul) y_prob_manually 0.711961055211584 y_prob_mat_mul 0.4762497657849592 In [35]: x = [0, 0] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\u0026quot;y_prob_manually\u0026quot;, y_prob_manually, \u0026quot;y_prob_mat_mul\u0026quot;, y_prob_mat_mul) y_prob_manually 0.7105745880018329 y_prob_mat_mul 0.4745660467842152 In [36]: x = [-5, -5] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\u0026quot;y_prob_manually\u0026quot;, y_prob_manually, \u0026quot;y_prob_mat_mul\u0026quot;, y_prob_mat_mul) y_prob_manually 0.7111368385474176 y_prob_mat_mul 0.4751433263136077 maybe I can pinpoint which layer has the bug? from pprint import pprint import test_feed_forward x = [-5, -5] frozen = test_feed_forward.feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) pprint([ [\u0026quot;--\u0026quot;, \u0026quot;manually\u0026quot;, \u0026quot;matmul\u0026quot;], [\u0026quot;h1\u0026quot;, frozen[\u0026quot;h1\u0026quot;], model.layers[0].nodes[\u0026quot;h1\u0026quot;]], [\u0026quot;h2\u0026quot;, frozen[\u0026quot;h2\u0026quot;], model.layers[0].nodes[\u0026quot;h2\u0026quot;]], [\u0026quot;h3\u0026quot;, frozen[\u0026quot;h3\u0026quot;], model.layers[0].nodes[\u0026quot;h3\u0026quot;]], [\u0026quot;h4\u0026quot;, frozen[\u0026quot;h4\u0026quot;], model.layers[1].nodes[\u0026quot;h4\u0026quot;]], [\u0026quot;h5\u0026quot;, frozen[\u0026quot;h5\u0026quot;], model.layers[1].nodes[\u0026quot;h5\u0026quot;]], [\u0026quot;y_prob\u0026quot;, frozen[\u0026quot;y_prob\u0026quot;], y_prob_mat_mul], ]) [[\u0026apos;--\u0026apos;, \u0026apos;manually\u0026apos;, \u0026apos;matmul\u0026apos;], [\u0026apos;h1\u0026apos;, 0.44128214463701015, 0.44128214463701015], [\u0026apos;h2\u0026apos;, 0.9894985068325902, 0.9894985068325902], [\u0026apos;h3\u0026apos;, 0.7973686345809591, 0.7973686345809591], [\u0026apos;h4\u0026apos;, 0.7643256444514099, 0.7643256444514099], [\u0026apos;h5\u0026apos;, 0.7752070858908596, 0.7604738710471179], [\u0026apos;y_prob\u0026apos;, 0.7111368385474176, 0.4751433263136077]] ok So h1, h2, h3, h4 are matching and then h5, is where the problem starts hmm . 16:24 ok think I found a small bug , from pprint import pprint import test_feed_forward x = [-5, -5] frozen = test_feed_forward.feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) pprint([ [\u0026quot;--\u0026quot;, \u0026quot;manually\u0026quot;, \u0026quot;matmul\u0026quot;], [\u0026quot;h1\u0026quot;, frozen[\u0026quot;h1\u0026quot;], model.layers[0].nodes[\u0026quot;h1\u0026quot;]], [\u0026quot;h2\u0026quot;, frozen[\u0026quot;h2\u0026quot;], model.layers[0].nodes[\u0026quot;h2\u0026quot;]], [\u0026quot;h3\u0026quot;, frozen[\u0026quot;h3\u0026quot;], model.layers[0].nodes[\u0026quot;h3\u0026quot;]], [\u0026quot;h4\u0026quot;, frozen[\u0026quot;h4\u0026quot;], model.layers[1].nodes[\u0026quot;h4\u0026quot;]], [\u0026quot;h5\u0026quot;, frozen[\u0026quot;h5\u0026quot;], model.layers[1].nodes[\u0026quot;h5\u0026quot;]], [\u0026quot;y_prob\u0026quot;, frozen[\u0026quot;y_prob\u0026quot;], y_prob_mat_mul], ]) [[\u0026apos;--\u0026apos;, \u0026apos;manually\u0026apos;, \u0026apos;matmul\u0026apos;], [\u0026apos;h1\u0026apos;, 0.44128214463701015, 0.44128214463701015], [\u0026apos;h2\u0026apos;, 0.9894985068325902, 0.9894985068325902], [\u0026apos;h3\u0026apos;, 0.7973686345809591, 0.7973686345809591], [\u0026apos;h4\u0026apos;, 0.7643256444514099, 0.7643256444514099], [\u0026apos;h5\u0026apos;, 0.7604738710471179, 0.7604738710471179], [\u0026apos;y_prob\u0026apos;, 0.7110504493887152, 0.4751433263136077]] So weird. ok now h5 matches. just not y_prob. that bug was in my test func. hmm ok reran one more time, so I had in my test func , an extra bias term I was adding , in the final logit, but not in the main matmul feed forward func. 16:44 ok well then theres no bug in the feed forward func, but it is weird how tight the outputs are . Something tells me actually this is related to the hard coded bias values of 1 ? Let me loosen up the bias, maybe that helps. Ok so before, data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]] pprint([[x, n.feed_forward(x, model.layers)] for x in data]) [[[0, 0], 0.4745660467842152], [[4, 5], 0.4738876007756717], [[-4, 5], 0.4762211475351283], [[-5, -5], 0.4751433263136077], [[5, -5], 0.4737412140572382], [[-20, -20], 0.4754772076192018], [[100, 100], 0.4748013350557756]] And , import numpy as np model.layers[0] = model.layers[0]._replace(bias=np.array([0.1])) model.layers[1] = model.layers[1]._replace(bias=np.array([0.1])) data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]] pprint([[x, n.feed_forward(x, model.layers)] for x in data]) [[[0, 0], 0.4813065143876082], [[4, 5], 0.4804879738767632], [[-4, 5], 0.48321160815195635], [[-5, -5], 0.4823729185086716], [[5, -5], 0.4793507289434747], [[-20, -20], 0.4822354979795412], [[100, 100], 0.4810180811163541]] hmm doesn\u0026apos;t seem to have helped. Let me go lower, model.layers[0] = model.layers[0]._replace(bias=np.array([0.01])) model.layers[1] = model.layers[1]._replace(bias=np.array([0.01])) print(\u0026quot;biases, \u0026quot;, model.layers[0].bias, model.layers[1].bias, model.layers[2].bias) # biases, [0.01] [0.01] [0] data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]] pprint([[x, n.feed_forward(x, model.layers)] for x in data]) [[[0, 0], 0.4820947777588216], [[4, 5], 0.48128620914361286], [[-4, 5], 0.4839510956396274], [[-5, -5], 0.4831921147269756], [[5, -5], 0.4800346864013735], [[-20, -20], 0.48300320046986295], [[100, 100], 0.48173375233047927]] 17:03 ok so weird not even adjustments to bias helped . Just double check , with the manual feedforward too, from pprint import pprint import test_feed_forward x = [-5, -5] frozen = test_feed_forward.feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) pprint([ [\u0026quot;--\u0026quot;, \u0026quot;manually\u0026quot;, \u0026quot;matmul\u0026quot;], [\u0026quot;h1\u0026quot;, frozen[\u0026quot;h1\u0026quot;], model.layers[0].nodes[\u0026quot;h1\u0026quot;]], [\u0026quot;h2\u0026quot;, frozen[\u0026quot;h2\u0026quot;], model.layers[0].nodes[\u0026quot;h2\u0026quot;]], [\u0026quot;h3\u0026quot;, frozen[\u0026quot;h3\u0026quot;], model.layers[0].nodes[\u0026quot;h3\u0026quot;]], [\u0026quot;h4\u0026quot;, frozen[\u0026quot;h4\u0026quot;], model.layers[1].nodes[\u0026quot;h4\u0026quot;]], [\u0026quot;h5\u0026quot;, frozen[\u0026quot;h5\u0026quot;], model.layers[1].nodes[\u0026quot;h5\u0026quot;]], [\u0026quot;y_prob\u0026quot;, frozen[\u0026quot;y_prob\u0026quot;], y_prob_mat_mul], ]) [[\u0026apos;--\u0026apos;, \u0026apos;manually\u0026apos;, \u0026apos;matmul\u0026apos;], [\u0026apos;h1\u0026apos;, 0.22688927424734276, 0.22688927424734276], [\u0026apos;h2\u0026apos;, 0.9722312068731663, 0.9722312068731663], [\u0026apos;h3\u0026apos;, 0.5938559073859204, 0.5938559073859204], [\u0026apos;h4\u0026apos;, 0.51632734891332, 0.51632734891332], [\u0026apos;h5\u0026apos;, 0.5124838141374718, 0.5124838141374718], [\u0026apos;y_prob\u0026apos;, 0.4831921147269756, 0.4831921147269756]] Ok yea, so looks like no bug and reducing the bias has not diminished how frozen the outputs seem to be. 17:11 so yea for now , feels like it is good I verified the feed forward func does what it is supposed to, but it is super weird that the network is really tightly configured. Super weird. Maybe I should not have activation functions on the inner layers? Nah I don\u0026apos;t think that\u0026apos;s the problem. Makes me wonder what about something about this particular multi-layer network architecture that is being weird? Maybe I should try different architectures? Do they have similar properties? ","permalink":"https://michal.piekarczyk.xyz/post/2022-10-12-backprop-scratch/","summary":"ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[\u0026quot;loss_after\u0026quot;] - x[\u0026quot;loss_before\u0026quot;] for x in metrics[\u0026quot;micro_batch_updates\u0026quot;]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all?","title":"Back prop from scratch 2022-10-12"},{"content":"I have no idea how I managed to get on my bus to DC this morning. My Citibike station I planned on, was full, then my phone is Unavailable until like 10:42 .\nBus boards at 11:00am . Still got to find a citibike dock to park the bike, because the stolen citibike fee is $1,250. At 10:42 of course I open phone and have absolutely no cell service. The citibike station does not have a printed map on it. I open my Google map because it magically had still on the screen some citibike locations. I quickly ride 2 blocks up praying it has docks. Had a free dock! I run back with suit case to bus location . It is not super easy to find and this is the first time I am using thejet.coach . But I see it eventually. I go to a mall first that claims to have Starbucks at 10:50. I donâ€™t see it . I leave. I see a street level Starbucks . Go inside . 10 people in line . no time. I have no water prepared for a 4 hour trip. I run to bus. Itâ€™s 10:55. I ask can I take 4 minutes to look for water? They say ok. I have no cash . But food vendor a block away takes Credit card. I have water. Run back to bus. 10:59. Still no cell service . I try to show a ticket photo from before . But it was my moms ticket which is for a different day . It says Anna . I am not Anna. They are confused ðŸ˜. I show them my ID. That is adequate ðŸ¥². They let me inside. Inside there is 10 people sharing a whole bus . Iâ€™m in the front . All good now.\n","permalink":"https://michal.piekarczyk.xyz/post/2022-10-06-iphone-unavailable/","summary":"I have no idea how I managed to get on my bus to DC this morning. My Citibike station I planned on, was full, then my phone is Unavailable until like 10:42 .\nBus boards at 11:00am . Still got to find a citibike dock to park the bike, because the stolen citibike fee is $1,250. At 10:42 of course I open phone and have absolutely no cell service. The citibike station does not have a printed map on it.","title":"Not sure how I managed to catch my bus to DC this morning"},{"content":" my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0.5 anyway. So at this point one thought I have for sure is whether this network is just one layer more complicated than would be needed for a problem set this simple. The thought arose after seeing that weight output from last training, But in any case, I think for now I am curious if I can find more bugs. So, we are underfitting here. So the loss is just increasing steadily and I see the layer 1 and layer 2 weights are just increasing steadily as well. So makes me think this is related. 16:02 let me try to observe the updates , import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\u0026quot;learning_rate\u0026quot;: 0.01, \u0026quot;steps\u0026quot;: 50, \u0026quot;log_loss_every_k_steps\u0026quot;: 10 } runner.train_and_analysis(data, parameters) 17:26 ah well spotted one silly bug in tracking the metrics, so I had the train and validation loss I was logging flipped, if step % log_loss_every_k_steps == 0: _, total_loss = loss(model, data.X_validation, data.Y_validation) metrics[\u0026quot;train\u0026quot;][\u0026quot;loss_vec\u0026quot;].append(total_loss) _, total_loss = loss(model, data.X_train, data.Y_train) metrics[\u0026quot;validation\u0026quot;][\u0026quot;loss_vec\u0026quot;].append(total_loss) fixed now so it is if step % log_loss_every_k_steps == 0: _, total_loss = loss(model, data.X_validation, data.Y_validation) metrics[\u0026quot;validation\u0026quot;][\u0026quot;loss_vec\u0026quot;].append(total_loss) _, total_loss = loss(model, data.X_train, data.Y_train) metrics[\u0026quot;train\u0026quot;][\u0026quot;loss_vec\u0026quot;].append(total_loss) A bug indeed, but would not affect the training itself . Ok I think good thing to do next, continue my low level debugging such that as I calculate g , if gradient descent is working properly, then I should be able to write an assert that I think the loss at least for the single example should decrease after applying g update, otherwise something is wrong ! 19:27 ok to check this I then have to calculate the loss on the micro-batch I\u0026apos;m using here, ok, first here is how I would reshape a single example to obtain its loss, i = 0 x, y = data.X_train[i], data.Y_train[i] x.shape, y.shape Y_actual, total_loss = n.loss(model, x.reshape((1, -1)), y.reshape((1, 1))) print(\u0026quot;(x, y)\u0026quot;, (x, y)) print(\u0026quot;Y_actual\u0026quot;, Y_actual) print(\u0026quot;loss\u0026quot;, total_loss) (x, y) (array([ -7.55637702, -12.67353685]), 1) Y_actual [0.93243955] loss 0.06995095896007311 And side not I realized technically I\u0026apos;m not plotting the training loss, since the training set has 9,000 rows and I\u0026apos;m only really using 500 or so of them so far. So I will adjust the training loss calculation for specifically that portion I use. 20:27 ok cool, going to try out this new code where I also now am logging the before and after for each microbatch loss import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\u0026quot;learning_rate\u0026quot;: 0.01, \u0026quot;steps\u0026quot;: 500, \u0026quot;log_loss_every_k_steps\u0026quot;: 10 } model, artifacts, metrics = runner.train_and_analysis(data, parameters) outer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:12\u0026lt;00:00, 40.35it/s] saving to 2022-10-03T003158.png 2022-10-03T003158.png 2022-10-03T003159-weights.png 2022-10-03T003200-hist.png saving to 2022-10-03T003201-scatter.png 2022-10-03T003201-scatter.png And let me look at those micro batch updates then In [8]: metrics[\u0026quot;micro_batch_updates\u0026quot;][:5] Out[8]: [{\u0026apos;loss_before\u0026apos;: 0.43903926069642474, \u0026apos;y_actual_before\u0026apos;: array([0.64465547]), \u0026apos;x\u0026apos;: array([-9.44442228, 1.4129736 ]), \u0026apos;y\u0026apos;: 1, \u0026apos;loss_after\u0026apos;: 0.43757904199626413, \u0026apos;y_actual_after\u0026apos;: array([0.6455975])}, {\u0026apos;loss_before\u0026apos;: 1.0263273283159982, \u0026apos;y_actual_before\u0026apos;: array([0.64167946]), \u0026apos;x\u0026apos;: array([-3.4136343 , 17.13301918]), \u0026apos;y\u0026apos;: 0, \u0026apos;loss_after\u0026apos;: 1.0309406841349795, \u0026apos;y_actual_after\u0026apos;: array([0.64332871])}, {\u0026apos;loss_before\u0026apos;: 0.4300753021013386, \u0026apos;y_actual_before\u0026apos;: array([0.65046011]), \u0026apos;x\u0026apos;: array([-2.26675345, -5.20582749]), \u0026apos;y\u0026apos;: 1, \u0026apos;loss_after\u0026apos;: 0.4285424015973017, \u0026apos;y_actual_after\u0026apos;: array([0.65145797])}, {\u0026apos;loss_before\u0026apos;: 1.0544704530873739, \u0026apos;y_actual_before\u0026apos;: array([0.65162314]), \u0026apos;x\u0026apos;: array([ 14.74873303, -16.34664216]), \u0026apos;y\u0026apos;: 0, \u0026apos;loss_after\u0026apos;: 1.0598453040833464, \u0026apos;y_actual_after\u0026apos;: array([0.65349059])}, {\u0026apos;loss_before\u0026apos;: 0.42370781274874675, \u0026apos;y_actual_before\u0026apos;: array([0.65461512]), \u0026apos;x\u0026apos;: array([ 1.71615885, -11.0142264 ]), \u0026apos;y\u0026apos;: 1, \u0026apos;loss_after\u0026apos;: 0.42217509911520096, \u0026apos;y_actual_after\u0026apos;: array([0.65561923])}] import matplotlib.pyplot as plt from utils import utc_now, utc_ts import pylab deltas = [x[\u0026quot;loss_after\u0026quot;] - x[\u0026quot;loss_before\u0026quot;] for x in metrics[\u0026quot;micro_batch_updates\u0026quot;]] with plt.style.context(\u0026quot;fivethirtyeight\u0026quot;): plt.hist(deltas, bins=50) out_loc = f\u0026quot;{utc_ts(utc_now())}-micro-batch-loss-deltas.png\u0026quot; print(\u0026quot;saving to\u0026quot;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026quot;tight\u0026quot;) pylab.close() plt.close() # saving to 2022-10-03T005623-micro-batch-loss-deltas.png Wow fascinating, so a lot of the loss is getting reduced, at least slightly more than not haha, In [17]: from collections import Counter ...: Counter([\u0026quot;loss_reduction\u0026quot; if x \u0026lt; 0 else \u0026quot;loss_increase\u0026quot; for x in [y for y in deltas if y != 0]]) Out[17]: Counter({\u0026apos;loss_reduction\u0026apos;: 260, \u0026apos;loss_increase\u0026apos;: 240}) And with plt.style.context(\u0026quot;fivethirtyeight\u0026quot;): plt.plot(deltas) out_loc = f\u0026quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png\u0026quot; print(\u0026quot;saving to\u0026quot;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026quot;tight\u0026quot;) pylab.close() plt.close() But wow, this next plot is fascinating! with plt.style.context(\u0026quot;fivethirtyeight\u0026quot;): fig = plt.figure(figsize =(20, 9)) plt.plot(deltas, linewidth=0.7) plt.title(\u0026quot;Microbatch loss_after - loss_before\u0026quot;) out_loc = f\u0026quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png\u0026quot; print(\u0026quot;saving to\u0026quot;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026quot;tight\u0026quot;) pylab.close() plt.close() So according to the above, yes the microbatch delta loss is ping ponging back and forth and basically getting worse, for the different microbatch inputs . Wow. so glad I looked at this chronological kind of plot ! ","permalink":"https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/","summary":"my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0.","title":"Back prop from scratch 2022-10-02"},{"content":" [[my back prop SGD from scratch 2022-Aug]] 13:35 yea so last time I had noticed , hey on a random initialization why was the y_prob 0.5 ? I had literally just initialized a new network and got this first example, ipdb\u0026gt; p x, y (array([10.31816265, -8.80044688]), 1) while running the ipdb debug mode, and inside of train_network() , ran --\u0026gt; 186 y_prob = feed_forward(x, model.layers, verbose=False) and got ipdb\u0026gt; p y_prob 0.5 Let me just use joblib to save this so I can just test it again. So I ran this inside of my ipdb, import joblib joblib.dump(model, f\u0026quot;{utc_ts(utc_now())}-model.joblib\u0026quot;) # \u0026apos;2022-09-25T174708-model.joblib\u0026apos; 13:57 And then inanother session , import joblib import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts import network as n import dataset import plot import runner model = joblib.load(\u0026quot;2022-09-25T174708-model.joblib\u0026quot;) y_prob = n.feed_forward(x, model.layers, verbose=False) y_prob # Out[8]: 0.5 ok nice, now that I got this reproduced, let me hunt for some more bugs. Maybe this is purely a coincidence?!????! So at this moment for this particular network, the input has no effect basically , In [12]: [n.feed_forward(x, model.layers, verbose=False) ...: for x in [ ...: [10.31816265, -8.80044688], ...: [1, 1], ...: [0, 0], ...: [1e4, -1e4] ...: ]] Out[12]: [0.5, 0.5, 0.5, 0.5] ok so currently, the final sum appears to always be negative and so the relu(negative_num) step at the end always produces a 0 and then after that I have a sigmoid so for 0 yea makes sense the output is the 0.5 . but why is the input into that final relu always seeming to be negative then ? 15:05 so I think I want to answer a side question , of hey if I create a bunch of random networks, will they all have this weirdness? If not then this might not be a problem . ok so , from tqdm import tqdm import numpy as np import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\u0026quot;learning_rate\u0026quot;: 0.01, \u0026quot;steps\u0026quot;: 100, \u0026quot;log_loss_every_k_steps\u0026quot;: 10 } outputs = [] x = np.array([10.31816265, -8.80044688]) for _ in tqdm(range(10000)): model = n.initialize_model(parameters) outputs.append(n.feed_forward(x, model.layers, verbose=False)) plt.hist(outputs, bins=50) out_loc = f\u0026quot;{utc_ts(utc_now())}.png\u0026quot; print(\u0026quot;saving to\u0026quot;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026quot;tight\u0026quot;) pylab.close() # saving to 2022-09-25T192657.png 15:28 ok so false alarm from the sense that, the network is not always stuck at 0.5 . And the 0.5 is special only because this is the minimum possible when the step before the sigmoid happens to be a relu . stories heh. But maybe does beg the question, hey my dataset output is either 0 or 1 so if my minimum is 0.5 , well that\u0026apos;s not ideal haha! So I need to for sure next either scale the output so the [0.5, 1.0] maps to [0, 1.0] or otherwise, just get rid of that relu all together since I can then allow for all values , mapping into that sigmoid , and that would produce the [0, 1.0] I need . 17:38 ok so if I take out the final relu, I will also have to adjust the partial derivative calculations too, 18:19 ok so first I just updated the feed_forward , so then now on commit , 61465a5 , redoing the above mini test, we have , for _ in tqdm(range(10000)): model = n.initialize_model(parameters) outputs.append(n.feed_forward(x, model.layers, verbose=False)) plt.hist(outputs, bins=50) out_loc = f\u0026quot;{utc_ts(utc_now())}.png\u0026quot; print(\u0026quot;saving to\u0026quot;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026quot;tight\u0026quot;) pylab.close() # saving to 2022-09-25T222204.png 18:41 hmm ok so this is still kind of asymmetric but finally getting the values less than 0.5 so better than before. Ok going to update the partial derivatives too then. 18:56 ok lets try this out on commit ea46849 , having updated partial derivatives for the weights w13, w14 which are affected . import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\u0026quot;learning_rate\u0026quot;: 0.01, \u0026quot;steps\u0026quot;: 50, \u0026quot;log_loss_every_k_steps\u0026quot;: 10 } runner.train_and_analysis(data, parameters) ","permalink":"https://michal.piekarczyk.xyz/post/2022-09-25-backprop-scratch/","summary":"[[my back prop SGD from scratch 2022-Aug]] 13:35 yea so last time I had noticed , hey on a random initialization why was the y_prob 0.5 ? I had literally just initialized a new network and got this first example, ipdb\u0026gt; p x, y (array([10.31816265, -8.80044688]), 1) while running the ipdb debug mode, and inside of train_network() , ran --\u0026gt; 186 y_prob = feed_forward(x, model.layers, verbose=False) and got ipdb\u0026gt; p y_prob 0.","title":"Backprop and SGD From Scratch 2022-09-25"},{"content":" [[my back prop SGD from scratch 2022-Aug]] 16:38 why no learning going on hmm look at this network import network as n import dataset import plot X, Y = dataset.build_dataset_inside_outside_circle(0.5) model = n.initialize_model({\u0026quot;learning_rate\u0026quot;: 0.01}) ( loss_vec, model, artifacts, X_validation, Y_validation, Y_prob ) = n.train_network(X, Y, model) 17:02 wondering if I can inspect the gradient, to see if it is pointing where it should 23:02 so for a random weight initialized network, curious at least here the response should be nonlinear right? hmm model = n.initialize_model({\u0026quot;learning_rate\u0026quot;: 0.01}) # X_validation # from earlier Y_prob, total_loss = loss(model.layers, X_validation, Y_validation) plot.scatter_plot_by_z(X_validation, Y_prob) # 2022-09-04T031837-scatter.png wow super weird but basically even for random weights we have only the linear separation, so that makes me think maybe even the basic feed forward might have some problem? and the probability sharpness? from utils import utc_now, utc_ts out_loc = f\u0026quot;{utc_ts(utc_now())}-hist.png\u0026quot; plt.hist(Y_prob, bins=50) pylab.savefig(out_loc, bbox_inches=\u0026apos;tight\u0026apos;) # 2022-09-04T033435-hist.png 23:38 yea the probability output above is super sharp, for a completely random network. Hmm ok. Well nice separation but yea why is it just only doing linear separation right now and the random initial weights even not non-linear? also one side idea is I\u0026apos;m not changing the bias at all. but that is unrelated to the above linear weirdness . hm ","permalink":"https://michal.piekarczyk.xyz/post/2022-09-03-backprop-scratch/","summary":"[[my back prop SGD from scratch 2022-Aug]] 16:38 why no learning going on hmm look at this network import network as n import dataset import plot X, Y = dataset.build_dataset_inside_outside_circle(0.5) model = n.initialize_model({\u0026quot;learning_rate\u0026quot;: 0.01}) ( loss_vec, model, artifacts, X_validation, Y_validation, Y_prob ) = n.train_network(X, Y, model) 17:02 wondering if I can inspect the gradient, to see if it is pointing where it should 23:02 so for a random weight initialized network, curious at least here the response should be nonlinear right?","title":"Backprop and SGD From Scratch Part 4"},{"content":" [[my back prop SGD from scratch 2022-Aug]] 12:38 so what happened last time? well let me look at the 2022-08-21.html notes I created. 13:20 darn so ok spent bunch of time figuring out why I couldnt view all the images in that html but basically combination of the html references images in log seq dir and also I have to copy them to my git repo area for this repo. Anyway, 13:35 finally looking, so the weird issue was my log loss was getting worse with training when only affecting the final layer , so then I plotted the raw input data with a 3d surface plot but it was really weird looking and shapeless . I plotted this on a 2d plot instead and yea looked reasonable. But yea the 95% label=0 to to 5% label=1 maybe was contributing to why the 3d surface plot looked formless and uninteresting. And for fun I tweaked my 2d plotting code to use a spectrum of colors so I can perhaps look at my output data. But then oh wow oops I realized all the predictions were basically just 0.5 . So my main thought then was that haha probably training just the last layer of a network is basically not useful. 13:44 ok so let me continue with a strategy to train the full network and not just the final layer , look back at my network, 13:48 quick capture: Just going to write up the partial derivative parts of the gradient one at a time, 16:53 ok I have added this to the code now . let me try that train loop again then import network as n import plot X, Y = n.build_dataset_inside_outside_circle() layers = n.initialize_network_layers() loss_vec, layers = n.train_network(X, Y, layers) plot.plot_loss_vec(loss_vec) 17:12 ok output above is pretty interesting. Probably if indeed things are working, the learning rate is too high. But haha in case something is actually working, let me actually try plotting the predictions for the loss after the first round which I think looks lowest. layers = n.initialize_network_layers() loss_vec, layers, artifacts = n.train_network(X, Y, layers, log_loss_each_round=True, steps=10) layers = artifacts[\u0026quot;9\u0026quot;][\u0026quot;model\u0026quot;] Y_actual, total_loss = n.loss(layers, X, Y) layers = artifacts[\u0026quot;9\u0026quot;][\u0026quot;model\u0026quot;] Y_actual, total_loss = n.loss(layers, X, Y) plot.scatter_plot_by_z(X, Y_actual) 17:39 ok haha that\u0026apos;s kind of confusing. Not sure why the second time around, pretty sure I did not re-generate the data, the loss on this training round went down and stayed down. Likely the first time around we must have jumped too far from the minimum irrecoverably. And the second time, since indeed the weights are generated randomly, we stayed close. But also for the second round, when plotting some outputs, clearly we see something funky is going on. And also I suspect that since I have not fixed that whole 95% to 5% dataset imbalance, some funkiness is happening and indeed the loss does appear to be small because the penalty on the imbalanced dataset is not shining through. 17:47 So the imbalanced dataset is likely messing with learning and also with the perception of the loss as well. 20:30 ok to balance out that data, probably simplest is to generate data where the circle is just bigger 20:52 ok so I ended up with something like, # dataset.py import math import numpy as np from collections import Counter def build_dataset_inside_outside_circle(balance=0.5): # Create some data in a 20x20 box centered at origin. num_samples = 10000 radius = math.sqrt(40*40*balance/math.pi) X = np.random.random((num_samples, 2)) * 40 + -20 f = (lambda a: int(np.sqrt(a[0]**2 + a[1]**2) \u0026lt;= radius)) Y = np.array(list(map(f, X))) # Validate balance assert abs(Counter(Y)[1]/num_samples - balance) \u0026lt; 0.02 return X, Y import dataset import plot X, Y = dataset.build_dataset_inside_outside_circle(0.5) plot.scatter_plot_by_z(X, Y) # saving to 2022-08-28T005137-scatter.png 20:58 ok lets see what happens with training then , layers = n.initialize_network_layers() loss_vec, layers, artifacts = n.train_network(X, Y, layers, log_loss_every_k_steps=10, steps=1000) outer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:45\u0026lt;00:00, 3.51it/s] plot.plot_loss_vec(loss_vec) # saving to 2022-08-28T012206.png 21:20 ok so this time definitely a little more time to train since I\u0026apos;ve been measuring log loss every 10 steps on all 10,000 samples but I can do fewer next time to iterate more quickly. Especially since darn, indeed this time, the loss spiraled out of control Out of curiosity, let me plot the outputs for basically the earliest model , layers = artifacts[\u0026quot;10\u0026quot;][\u0026quot;model\u0026quot;] Y_actual, total_loss = n.loss(layers, X, Y) plot.scatter_plot_by_z(X, Y_actual) # saving to 2022-08-28T012619-scatter.png ok wow pretty quirky. 21:28 ok yea so super curious about what does reducing learning rate do then. Added some additional code to support this too. import network as n model = n.initialize_model({\u0026quot;learning_rate\u0026quot;: 0.01}) ( loss_vec, model, artifacts, X_validation, Y_validation, Y_prob ) = n.train_network(X, Y, model) outer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:01\u0026lt;00:00, 31.19it/s] 21:49 ok lets look at a first run then , plot.plot_loss_vec(loss_vec) # saving to 2022-08-28T015127.png plot.scatter_plot_by_z(X_validation, Y_prob) # saving to 2022-08-28T015518-scatter.png 21:56 darn okay still not learning, despite the additional balancing and lower learning rate. Super curious what is the fundamental issue in this network. Curious to debug this. 16:03 also I learned the plant in the office is called the swiss cheese plant https://www.plantindex.com/swiss-cheese-plant/ hmm ","permalink":"https://michal.piekarczyk.xyz/post/2022-08-27-backprop-scratch/","summary":"[[my back prop SGD from scratch 2022-Aug]] 12:38 so what happened last time? well let me look at the 2022-08-21.html notes I created. 13:20 darn so ok spent bunch of time figuring out why I couldnt view all the images in that html but basically combination of the html references images in log seq dir and also I have to copy them to my git repo area for this repo.","title":"Backprop and SGD From Scratch Part 3"},{"content":" hmm blah blah test something #publish-this What So I had a particular need to load a #[[scikit learn sklearn]] model trained with 0.20.4 on Databricks 10.4 and ran into some fun details\nI was not initially able to use the Databricks 10.4 pinned version of scikit-learn , 0.24.x because of course there were #backwards-compatibility issues where trying to load the model gave me an error saying that ModuleNotFoundError: No module named 'sklearn.preprocessing.imputation' was not found basically because I learned that all the Imputers were moved over to sklearn.impute\nI tried also just vetting the assumption that I can simply pip install this earlier version of scikit learn. And turned out that well yes I can. But,\nBut then when I try to import sklearn , oops, that gives a strange error\nand trying to look for using #conda to the rescue , well I looked around on https://anaconda.org and I ended up seeing that there is 0.20.2 from cctbx and then I saw 0.20.3 from cdat-forge so then trying that I basically got\n(pandars310) $ conda install -c cdat-forge scikit-learn=0.20.3 Collecting package metadata (current_repodata.json): done Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source. Collecting package metadata (repodata.json): done Solving environment: failed with initial frozen solve. Retrying with flexible solve. Solving environment: | Found conflicts! Looking for incompatible packages. This can take several minutes. Press CTRL-C to abort. failed UnsatisfiableError: The following specifications were found to be incompatible with the existing python installation in your environment: Specifications: - scikit-learn=0.20.3 -\u0026gt; python[version=\u0026#39;\u0026gt;=2.7,\u0026lt;2.8.0a0|\u0026gt;=3.7,\u0026lt;3.8.0a0|\u0026gt;=3.6,\u0026lt;3.7.0a0\u0026#39;] Your python: python=3.10 If python is on the left-most side of the chain, that\u0026#39;s the version you\u0026#39;ve asked for. When python appears to the right, that indicates that the thing on the left is somehow not available for the python version you are constrained to. Note that conda will not change your python version to a different minor version unless you explicitly specify that. So aha I learned that if I want to use this earlier version of scikit learn then I would need \u0026lt;3.8 basically but Databricks 10.4 has pinned python 3.8 so basically that is a no go and this model just needs to be retrained to be used.\n","permalink":"https://michal.piekarczyk.xyz/post/2022-07-12-mini-scikit-learn-backwards-compatibility-story/","summary":"hmm blah blah test something #publish-this What So I had a particular need to load a #[[scikit learn sklearn]] model trained with 0.20.4 on Databricks 10.4 and ran into some fun details\nI was not initially able to use the Databricks 10.4 pinned version of scikit-learn , 0.24.x because of course there were #backwards-compatibility issues where trying to load the model gave me an error saying that ModuleNotFoundError: No module named 'sklearn.","title":"Mini scikit-learn backwards compatibility story"},{"content":"will this work?\nwhen blog vimeo 2022-04-10 foo yea \u0026ndash; \u0026ndash; \u0026ndash; hope so\n","permalink":"https://michal.piekarczyk.xyz/post/2022-04-10--markdown-table-test/","summary":"will this work?\nwhen blog vimeo 2022-04-10 foo yea \u0026ndash; \u0026ndash; \u0026ndash; hope so","title":"Tracck those daily permanents"},{"content":"Today I thought it would take me maybe half an hour to replace my laptop battery. I got a replacement in the mail. Actually funny enough they are called the \u0026ldquo;function keys\u0026rdquo; battery because there are three cells .\nAnyway, turns out they are glued to the inside and you need some kind of epoxy loosener. I randomly stumbled on this video someone put together using some kind of wax line to actually remove batteries. Well I didn\u0026rsquo;t have wax thread, but I had floss so umm hey why not try that?\nAnd wow somehow it worked after some trial and error.\n","permalink":"https://michal.piekarczyk.xyz/post/2022-01-29-floss-your-battery/","summary":"Today I thought it would take me maybe half an hour to replace my laptop battery. I got a replacement in the mail. Actually funny enough they are called the \u0026ldquo;function keys\u0026rdquo; battery because there are three cells .\nAnyway, turns out they are glued to the inside and you need some kind of epoxy loosener. I randomly stumbled on this video someone put together using some kind of wax line to actually remove batteries.","title":"Remember to Floss Your Batteries"},{"content":"Wow, I am setting up on a fresh laptop today and I just ran the typical homebrew https://brew.sh setup and when I saw this, my eyes got slightly watery.\n==\u0026gt; This script will install: /usr/local/bin/brew ... ... ==\u0026gt; The Xcode Command Line Tools will be installed. Press RETURN to continue or any other key to abort: ... ==\u0026gt; Searching online for the Command Line Tools ==\u0026gt; /usr/bin/sudo /usr/bin/touch /tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress ==\u0026gt; Installing Command Line Tools for Xcode-13.2 ==\u0026gt; /usr/bin/sudo /usr/sbin/softwareupdate -i Command\\ Line\\ Tools\\ for\\ Xcode-13.2 Software Update Tool Finding available software Downloading Command Line Tools for Xcode Downloaded Command Line Tools for Xcode Installing Command Line Tools for Xcode Done with Command Line Tools for Xcode Done. ... ==\u0026gt; /usr/bin/sudo /bin/rm -f /tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress ==\u0026gt; /usr/bin/sudo /usr/bin/xcode-select --switch /Library/Developer/CommandLineTools ==\u0026gt; Downloading and installing Homebrew... ... etc.\n","permalink":"https://michal.piekarczyk.xyz/post/2022-01-09-home-brew-xcode/","summary":"Wow, I am setting up on a fresh laptop today and I just ran the typical homebrew https://brew.sh setup and when I saw this, my eyes got slightly watery.\n==\u0026gt; This script will install: /usr/local/bin/brew ... ... ==\u0026gt; The Xcode Command Line Tools will be installed. Press RETURN to continue or any other key to abort: ... ==\u0026gt; Searching online for the Command Line Tools ==\u0026gt; /usr/bin/sudo /usr/bin/touch /tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress ==\u0026gt; Installing Command Line Tools for Xcode-13.","title":"Xcode Command Line Tools on HomeBrew"},{"content":"Initial Intent The initial intent here is to take three datasets of article titles of technical articles from different sources and try to classify them using an RNN. And also, another goal is to do this in hour long bites. And going to use this resource, https://madewithml.com/courses/foundations/recurrent-neural-networks/ for inspiration and direction on how to do this. The datasets (1) Per the above lesson, adapting the article dataset, \u0026ldquo;news.csv\u0026rdquo; , https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/news.csv , ( which was originally transformed from http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html ) (2) This set of ML/data science articles from Kaggle ( https://www.kaggle.com/hsankesara/medium-articles ) (3) And the set of stackoverflow questions ( https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate) First task: make my own dataset I am going to combine these three datasets. The label will be the source I only want the titles. The first two are technical, but the news articles source has different categories, so I\u0026rsquo;ll filter by the technical category. Do this import os import pandas as pd sources = { \u0026#34;news\u0026#34;: { \u0026#34;loc\u0026#34;: f\u0026#34;{os.getenv(\u0026#39;SOURCE_NEWS_DIR\u0026#39;)}/news.csv\u0026#34;, \u0026#34;title_col\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;filter\u0026#34;: { \u0026#34;col\u0026#34;: \u0026#34;category\u0026#34;, \u0026#34;val\u0026#34;: \u0026#34;Sci/Tech\u0026#34;, } }, \u0026#34;medium\u0026#34;: { \u0026#34;loc\u0026#34;: f\u0026#34;{os.getenv(\u0026#39;SOURCE_MEDIUM_DIR\u0026#39;)}/articles.csv\u0026#34;, \u0026#34;title_col\u0026#34;: \u0026#34;title\u0026#34;, }, \u0026#34;stackoverflow\u0026#34;: { \u0026#34;loc\u0026#34;: f\u0026#34;{os.getenv(\u0026#39;SOURCE_STACKOVERFLOW_DIR\u0026#39;)}/train.csv\u0026#34;, \u0026#34;title_col\u0026#34;: \u0026#34;Title\u0026#34;} } def build_my_dataset(sources, out_loc, head=False): dfs = [] for source, detail in sources.items(): title_col = detail[\u0026#34;title_col\u0026#34;] df = pd.read_csv(detail[\u0026#34;loc\u0026#34;]) df = df.rename(columns={title_col: \u0026#34;title\u0026#34;}) if head: df = df.head() df[\u0026#34;source\u0026#34;] = source also_filter = detail.get(\u0026#34;filter\u0026#34;) if also_filter: df = df[df[also_filter[\u0026#34;col\u0026#34;]] == also_filter[\u0026#34;val\u0026#34;]].copy() dfs.append(df[[\u0026#34;title\u0026#34;, \u0026#34;source\u0026#34;]]) pd.concat(dfs).to_csv(out_loc, index=False) # workdir = os.getenv(\u0026#34;WORKDIR\u0026#34;) out_loc = f\u0026#34;{workdir}/data/newdata.small.csv\u0026#34; build_my_dataset(sources, out_loc=out_loc, head=True) Just the sample.. !cat data/newdata.small.csv title,source Hacker Cracks Apple\u0026#39;s Streaming Technology (AP),news European Download Services Go Mobile (Reuters),news Open Source Apps Developer SugarCRM Releases Sugar.Sales 1.1 (TechWeb),news Oracle Sales Data Seen Being Released (Reuters),news Sun\u0026#39;s Looking Glass Provides 3D View (PC World),news Chatbots were the next big thing: what happened? â€“ The Startup â€“ Medium,medium Python for Data Science: 8 Concepts You May Have Forgotten,medium Automated Feature Engineering in Python â€“ Towards Data Science,medium Machine Learning: how to go from Zero to Hero â€“ freeCodeCamp,medium Reinforcement Learning from scratch â€“ Insight Data,medium Java: Repeat Task Every Random Seconds,stackoverflow Why are Java Optionals immutable?,stackoverflow Text Overlay Image with Darkened Opacity React Native,stackoverflow Why ternary operator in swift is so picky?,stackoverflow hide/show fab with scale animation,stackoverflow out_loc = f\u0026#34;{workdir}/data/newdata.csv\u0026#34; build_my_dataset(sources, out_loc=out_loc, head=False) Next time Following along here, https://madewithml.com/courses/foundations/recurrent-neural-networks/ I think next I should do some preprocessing, so my data is tokenized.\n","permalink":"https://michal.piekarczyk.xyz/post/2021-08-22/","summary":"Initial Intent The initial intent here is to take three datasets of article titles of technical articles from different sources and try to classify them using an RNN. And also, another goal is to do this in hour long bites. And going to use this resource, https://madewithml.com/courses/foundations/recurrent-neural-networks/ for inspiration and direction on how to do this. The datasets (1) Per the above lesson, adapting the article dataset, \u0026ldquo;news.csv\u0026rdquo; , https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/news.csv , ( which was originally transformed from http://www.","title":"Mini NLP Detour Part I"},{"content":"This is a notebook of the day to day activity of a look into three years of fasting data.\n2021-06-27 Initial thoughts I have about 3 years of fasting data now and I would like to see if something interesting can be said about it. And if given some time series data, is it possible to predict the next day. Or at least something about the next day. I suspect the data is somewhat cyclical at least with respect to patterns of \u0026ldquo;restraint\u0026rdquo; and \u0026ldquo;indulgence\u0026rdquo; , but also maybe with respect to week and weekend. Initially I think I want to use frequency based features, but perhaps afterwards I can compare that with an LSTM approach. I also have some calorie data, but it is only for about 3 months or so, and maybe I can use that somehow too later. Perhaps I can even later on try using exercise data, sleep data if available. And there is of course temperature and weather data. Next steps Write out some interesting questions I would like to answer. What is my feature list so far , as a sort of data dictionary. Can do some initial data exploration, with visualizations. Build a dataset based on some initial features, Interesting Questions ? Given a sequence of days, is there an interesting target variable, such as the hours fasted, which can be predicted of the next day or even the next few days or next week? A core qualitative question is, whether a period of restraint (more fasting) is followed by a period of less restraint (less fasting)? And maybe an extension of the above is whether fasting appears to be cyclical and how would you even measure this? How does the number of prior days of activity improve the accuracy of the prediction of the next day? Feature list initial most recent fast started before midnight most recent fast ended before 20:00 most recent fast more than 18 hours most recent fast more than 16 hours most recent fast more than 14 hours last two fasts, average start time before midnight last two fasts, average end time before 20:00 last two fasts, average more than 18 hours last two fasts, average more than 16 hours last two fasts, max is more than 18 hours last two fasts, min is more than 18 hours last two fasts, m in is more than 16 hours Next Yea next I would like to start building out a dataset of these features And would be cool to visualize some of these features too, get some summary stats on them. 2021-06-28 build dataset\u0026hellip; import os import pandas as pd workdir = os.getenv(\u0026#34;WORKDIR\u0026#34;) datadir = os.getenv(\u0026#34;DATADIR\u0026#34;) loc = f\u0026#34;{datadir}/2021-06-26-zero-fast.csv\u0026#34; df = pd.read_csv(loc) In [3]: df.shape Out[3]: (1109, 5) In [4]: df.iloc[:4] Out[4]: Date Start End Hours Night Eating 0 6/26/21 01:57 NaN NaN NaN 1 6/25/21 00:14 17:17 17.0 NaN 2 6/24/21 01:50 18:39 16.0 NaN 3 6/23/21 01:02 19:21 18.0 NaN Probably the simples question, is what is a unit of fasting? A fast can start and end on two different dates or it can start and end on the same day. A Fast can also span multiple days and it can also happen while traveling, resulting in different time zones messing with the data. However this dataset doesn\u0026rsquo;t appear to have time zones, so I wonder if it kind of simplifies that. Simplest I think is to ignore the first row for example, Date Start End Hours Night Eating 0 6/26/21 01:57 NaN NaN NaN Since this fast is on-going. So a fast unit has a start and an end. In [6]: df[df.Hours.isnull()].shape, df.shape Out[6]: ((1, 5), (1109, 5)) Ok per above only the latest fast would be null then. Next Start writing that dataset write a fast id perhaps as the \u0026ldquo;start-date-start-time\u0026rdquo; which would be unique, and has to have defined end of course. And figure out what does that \u0026ldquo;Night Eating\u0026rdquo; column mean. names for the features I started writing out. 2021-06-30 quick notes Also realized that the time between fasts can be a feature too.\nFor each \u0026ldquo;row\u0026rdquo;, we can basically say the \u0026ldquo;End time\u0026rdquo; is a separator between what happened before a fasting event, which can be used to create the \u0026ldquo;X\u0026rdquo; or independent variables and what happens after is the \u0026ldquo;Y\u0026rdquo; or dependent variables.\nSo for each row, if using pandas for example, we want a way to take perhaps 4 or more rows surrounding it and that would be used to create the features.\nPerhaps this feels like a job for rolling window , interesting article\n# most recent fast started before midnight updated feature list Feature list initial most recent fast started before midnight most recent fast ended before 20:00 most recent fast more than 18 hours most recent fast more than 16 hours most recent fast more than 14 hours last two fasts, average start time before midnight last two fasts, both start time before midnight last two fasts, average end time before 20:00 last two fasts, average more than 18 hours last two fasts, average more than 16 hours last two fasts, max is more than 18 hours last two fasts, min is more than 18 hours last two fasts, m in is more than 16 hours last two feeding windows, average less than 4 hours 2021-07-07 tried this rolling calc Looks good just need to calibrate the direction. df[\u0026#39;RollingHoursMean2Fasts\u0026#39;] = df[\u0026#39;Hours\u0026#39;].rolling(2).mean() In [8]: df.iloc[:10] Out[8]: Date Start End Hours Night Eating RollingHoursMean2Fasts 0 6/26/21 01:57 NaN NaN NaN NaN 1 6/25/21 00:14 17:17 17.0 NaN NaN 2 6/24/21 01:50 18:39 16.0 NaN 16.5 3 6/23/21 01:02 19:21 18.0 NaN 17.0 4 6/22/21 00:06 17:52 17.0 NaN 17.5 5 6/21/21 01:46 20:55 19.0 NaN 18.0 6 6/20/21 04:39 13:28 8.0 NaN 13.5 7 6/19/21 02:37 18:21 15.0 NaN 11.5 8 6/18/21 01:41 18:41 17.0 NaN 16.0 9 6/17/21 01:02 21:38 20.0 NaN 18.5 2021-07-09 Add index too\u0026hellip; import datetime df[\u0026#39;StartDt\u0026#39;] = df.apply(lambda x: datetime.datetime(int(f\u0026#34;20{x.Date.split(\u0026#39;/\u0026#39;)[2]}\u0026#34;), int(x.Date.split(\u0026#34;/\u0026#34;)[0]), int(x.Date.split(\u0026#34;/\u0026#34;)[1]), int(x.Start.split(\u0026#34;:\u0026#34;)[0]), int(x.Start.split(\u0026#34;:\u0026#34;)[1]) ), axis=1) df[\u0026#39;id\u0026#39;] = df.apply(lambda x: x.StartDt.strftime(\u0026#34;%Y-%m-%dT%H%M\u0026#34;), axis=1) In [21]: df.iloc[:10] Out[21]: Date Start End Hours Night Eating RollingHoursMean2Fasts StartDt id 0 6/26/21 01:57 NaN NaN NaN NaN 2021-06-26 01:57:00 2021-06-26T0157 1 6/25/21 00:14 17:17 17.0 NaN NaN 2021-06-25 00:14:00 2021-06-25T0014 2 6/24/21 01:50 18:39 16.0 NaN 16.5 2021-06-24 01:50:00 2021-06-24T0150 3 6/23/21 01:02 19:21 18.0 NaN 17.0 2021-06-23 01:02:00 2021-06-23T0102 4 6/22/21 00:06 17:52 17.0 NaN 17.5 2021-06-22 00:06:00 2021-06-22T0006 5 6/21/21 01:46 20:55 19.0 NaN 18.0 2021-06-21 01:46:00 2021-06-21T0146 6 6/20/21 04:39 13:28 8.0 NaN 13.5 2021-06-20 04:39:00 2021-06-20T0439 7 6/19/21 02:37 18:21 15.0 NaN 11.5 2021-06-19 02:37:00 2021-06-19T0237 8 6/18/21 01:41 18:41 17.0 NaN 16.0 2021-06-18 01:41:00 2021-06-18T0141 9 6/17/21 01:02 21:38 20.0 NaN 18.5 2021-06-17 01:02:00 2021-06-17T0102 Looking at above, I see that RollingHoursMean2Fasts for 2021-06-24T0150 is 16.5 = mean([17, 16]) , so the rows it is taking into account should be current row and next not current row and last Next Make RollingHoursMean2Fasts be for current and previous fast , not current and future fast haha since cannot know the future! 2021-07-10 how to update the rolling window to reflect the last 2 fasts df[\u0026#39;foo\u0026#39;] = df[\u0026#39;Hours\u0026#39;].shift(-1) df[\u0026#39;RollingHoursMean2Fasts\u0026#39;] = df[\u0026#39;Hours\u0026#39;].shift(-1).rolling(2).mean() Nice, per the below, this is perfect. In [33]: cols = [\u0026#39;id\u0026#39;, \u0026#39;Date\u0026#39;, \u0026#39;Start\u0026#39;, \u0026#39;End\u0026#39;, \u0026#39;Hours\u0026#39;, \u0026#39;RollingHoursMean2Fasts\u0026#39;, \u0026#39;foo\u0026#39;] ...: df[cols].iloc[:10] Out[33]: id Date Start End Hours RollingHoursMean2Fasts foo 0 2021-06-26T0157 6/26/21 01:57 NaN NaN NaN 17.0 1 2021-06-25T0014 6/25/21 00:14 17:17 17.0 16.5 16.0 2 2021-06-24T0150 6/24/21 01:50 18:39 16.0 17.0 18.0 3 2021-06-23T0102 6/23/21 01:02 19:21 18.0 17.5 17.0 4 2021-06-22T0006 6/22/21 00:06 17:52 17.0 18.0 19.0 5 2021-06-21T0146 6/21/21 01:46 20:55 19.0 13.5 8.0 6 2021-06-20T0439 6/20/21 04:39 13:28 8.0 11.5 15.0 7 2021-06-19T0237 6/19/21 02:37 18:21 15.0 16.0 17.0 8 2021-06-18T0141 6/18/21 01:41 18:41 17.0 18.5 20.0 9 2021-06-17T0102 6/17/21 01:02 21:38 20.0 18.5 17.0 Ok, going to add two more features like this last two fasts, both start time before midnight, \u0026ldquo;LastTwoFastsStartedBeforeMidnight\u0026rdquo; def func(x): pass import ipdb; ipdb.set_trace() return all([18 \u0026lt;= int(a.split(\u0026#34;:\u0026#34;)[0]) \u0026lt;=23 for a in x]) df[\u0026#34;LastTwoFastsStartedBeforeMidnight\u0026#34;] = df[\u0026#39;Start\u0026#39;].shift(-1).rolling(2).apply(func) I encountered DataError: No numeric types to aggregate and deeper in there I also stumbled on ValueError: could not convert string to float: \u0026#39;00:14\u0026#39; So got to convert first to a numerical.. def func(data): return all([18 \u0026lt;= x \u0026lt;=23 for x in data]) df[\u0026#34;StartHour\u0026#34;] = df[\u0026#34;Start\u0026#34;].map(lambda x: int(x.split(\u0026#34;:\u0026#34;)[0])) df[\u0026#34;LastTwoFastsStartedBeforeMidnight\u0026#34;] = df[\u0026#39;StartHour\u0026#39;].shift(-1).rolling(2).apply(func) Ok I think that worked .. In [57]: df.LastTwoFastsStartedBeforeMidnight.value_counts() Out[57]: 0.0 907 1.0 200 Name: LastTwoFastsStartedBeforeMidnight, dtype: int64 In [60]: cols = [\u0026#39;id\u0026#39;, \u0026#39;Date\u0026#39;, \u0026#39;Start\u0026#39;, \u0026#39;End\u0026#39;, \u0026#39;Hours\u0026#39;, \u0026#39;LastTwoFastsStartedBeforeMidnight\u0026#39;,] ...: df[cols].iloc[35:45] Out[60]: id Date Start End Hours LastTwoFastsStartedBeforeMidnight 35 2021-05-21T2341 5/21/21 23:41 21:02 21.0 0.0 36 2021-05-21T0026 5/21/21 00:26 20:31 20.0 0.0 37 2021-05-20T0101 5/20/21 01:01 20:57 19.0 0.0 38 2021-05-19T0008 5/19/21 00:08 20:29 20.0 0.0 39 2021-05-17T2324 5/17/21 23:24 21:38 22.0 1.0 40 2021-05-16T2300 5/16/21 23:00 20:21 21.0 1.0 41 2021-05-15T2358 5/15/21 23:58 14:06 14.0 0.0 42 2021-05-15T0110 5/15/21 01:10 21:42 20.0 0.0 43 2021-05-14T0049 5/14/21 00:49 17:53 17.0 0.0 44 2021-05-13T0015 5/13/21 00:15 17:26 17.0 0.0 Next Ok at this point I should start throwing stuff into version control so creating these features and visualizing/analyzing them can be more deterministic/reproducible. And then I can try visualizing / understanding some features and see how predictive they are w.r.t. \u0026ldquo;does past behavior determine future behavior such as the length of the next fast\u0026rdquo;. 2021-07-11 ok started things off in a new repo New repo here , https://github.com/namoopsoo/fasting-analyze import os import core.dataset as cd import pandas as pd workdir = os.getenv(\u0026#34;WORKDIR\u0026#34;) datadir = os.getenv(\u0026#34;DATADIR\u0026#34;) loc = f\u0026#34;{datadir}/2021-06-26-zero-fast.csv\u0026#34; df = pd.read_csv(loc) datasetdf = cd.build_dataset(df) In [11]: datasetdf.iloc[:10] Out[11]: Date Start End Hours ... id StartHour LastTwoFastsStartedBeforeMidnight RollingHoursMean2Fasts 0 6/26/21 01:57 NaN NaN ... 2021-06-26T0157 1 NaN NaN 1 6/25/21 00:14 17:17 17.0 ... 2021-06-25T0014 0 0.0 16.5 2 6/24/21 01:50 18:39 16.0 ... 2021-06-24T0150 1 0.0 17.0 3 6/23/21 01:02 19:21 18.0 ... 2021-06-23T0102 1 0.0 17.5 4 6/22/21 00:06 17:52 17.0 ... 2021-06-22T0006 0 0.0 18.0 5 6/21/21 01:46 20:55 19.0 ... 2021-06-21T0146 1 0.0 13.5 6 6/20/21 04:39 13:28 8.0 ... 2021-06-20T0439 4 0.0 11.5 7 6/19/21 02:37 18:21 15.0 ... 2021-06-19T0237 2 0.0 16.0 8 6/18/21 01:41 18:41 17.0 ... 2021-06-18T0141 1 0.0 18.5 9 6/17/21 01:02 21:38 20.0 ... 2021-06-17T0102 1 0.0 18.5 [10 rows x 10 columns] In [12]: datasetdf.iloc[0] Out[12]: Date 6/26/21 Start 01:57 End NaN Hours NaN Night Eating NaN StartDt 2021-06-26 01:57:00 id 2021-06-26T0157 StartHour 1 LastTwoFastsStartedBeforeMidnight NaN RollingHoursMean2Fasts NaN Name: 0, dtype: object look at RollingHoursMean2Fasts briefly import matplotlib.pyplot as plt import pylab import date_utils as du col = \u0026#34;RollingHoursMean2Fasts\u0026#34; loc = f\u0026#34;{workdir}/{du.utc_ts()}-{col}.png\u0026#34; with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): plt.plot(datasetdf[col].tolist()) plt.title(f\u0026#39;{col}\u0026#39;) pylab.savefig(loc, bbox_inches=\u0026#39;tight\u0026#39;) pylab.close() Next I think the main next thing to do is to create a \u0026ldquo;y\u0026rdquo; dependent variable for this dataset, which can for example be something like \u0026ldquo;hours fasted in next fast\u0026rdquo; or \u0026ldquo;hours until next fast\u0026rdquo; or \u0026ldquo;proportion of hours fasted in next 7 days\u0026rdquo;. And with a dependent variable I can then see how predictive these features are, as a first iteration with only the twofeatures so far. And I can continue to add other features too. ","permalink":"https://michal.piekarczyk.xyz/post/2021-06-27-zero-time-series-analysis/","summary":"This is a notebook of the day to day activity of a look into three years of fasting data.","title":"Fasting Data Time Series Analysis Notebook"},{"content":"Siri was suggesting that I do the vaccine remotely, but I ended up going in person And I thought would be nice to collect and plot some of the temperatures Here, I\u0026rsquo;m working with some self temperatures I collected in data.csv. Although the tylenol did not appear to have an immediate affect on my fever, I remember feeling I was cooler after even half an hour.\n(And I have some other supporting funcs below )\nimport datetime import pytz import pandas as pd import matplotlib.pyplot as plt workdir = ... df = pd.read_csv(f\u0026#39;{workdir}/data.csv\u0026#39;) df = pd.read_csv(f\u0026#39;{workdir}/data.csv\u0026#39;, index_col=None).sort_values(by=\u0026#39;ts\u0026#39;) # df.iloc[:2] # Unnamed: 0 ts event temp #22 22 2021-05-03 22.46.14.jpg temp 97.7 #12 12 2021-05-04 16.48.19.jpg temp 97.5 df.ts = df.ts.map(lambda x: x[:-4]) df[\u0026#39;unixts\u0026#39;] = df.ts.map(lambda x:dt_to_unix_ts(ts_to_dt (x)) ) X = df[df.temp.notnull()].unixts.tolist() Y = df[df.temp.notnull()].temp.tolist() def plot(X, Y): plt.grid(True) title = \u0026#34;Temperature after vaccine\u0026#34; fig = plt.figure(figsize=(12,4)) ax = fig.add_subplot(111) ax.plot(X, Y) x_labels = [unix_ts_to_dt_ts(x) for x in ax.get_xticks()] ax.set_xticklabels(x_labels, rotation=-45) ax.set_title(title) tylenol_ts = df[df.event == \u0026#39;tylenol\u0026#39;].iloc[0].unixts ax.axvline(tylenol_ts, label=\u0026#39;tylenol\u0026#39;, color=\u0026#39;green\u0026#39;) vaccine_ts = df[df.event == \u0026#39;got vaccine\u0026#39;].iloc[0].unixts ax.axvline(vaccine_ts, label=\u0026#39;vaccine\u0026#39;, color=\u0026#39;red\u0026#39;) plt.legend() out_loc = f\u0026#39;{workdir}/{utc_ts()}-fig.png\u0026#39; # pylab.savefig(out_loc) pylab.savefig(out_loc, bbox_inches=\u0026#39;tight\u0026#39;) pylab.close() Some other supporting functions import time import datetime def utc_ts(): return datetime.datetime.utcnow().replace(tzinfo=pytz.UTC).strftime(\u0026#39;%Y-%m-%dT%H%M%SZ\u0026#39;) def dt_to_unix_ts(dt): return time.mktime(dt.timetuple()) def ts_to_dt(ts): return datetime.datetime.strptime(ts, \u0026#39;%Y-%m-%d %H.%M.%S\u0026#39;) def unix_ts_to_dt_ts(unix_ts, utc_to_est=True): dt = datetime.datetime.utcfromtimestamp(unix_ts) if utc_to_est: return dt.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone(\u0026#39;US/Eastern\u0026#39;)).strftime(\u0026#39;%Y-%m-%d %H:%M:%S EST\u0026#39;) else: return dt.strftime(\u0026#39;%Y-%m-%d %H:%M:%S Z\u0026#39;) ","permalink":"https://michal.piekarczyk.xyz/post/2021-05-07-covid-vaccine-2-temps/","summary":"Some temperature data","title":"Covid Vaccine 2 Temperatures"},{"content":" Trying Per here tried ffmpeg -i image%03d.jpeg -sameq -r 25 outmovie.mp4 But getting Option \u0026#39;sameq\u0026#39; was removed. If you are looking for an option to preserve the quality (which is not what -sameq was for), use -qscale 0 or an equivalent quality factor option. Failed to set value \u0026#39;1\u0026#39; for option \u0026#39;sameq\u0026#39;: Invalid argument ffmpeg -framerate 24 -i img%03d.png output.mp4 To glob\u0026hellip; ffmpeg -framerate 24 -pattern_type glob -i \u0026#39;*.jpg\u0026#39; -c:v libx264 -pix_fmt yuv420p out.mp4 That kind of did the trick, but kind of fast. only lasted 2 seconds.. Per the \u0026ldquo;Additional Info: Frame rates\u0026rdquo; section in https://trac.ffmpeg.org/wiki/Slideshow , ffmpeg -framerate 1/3 -pattern_type glob -i \u0026#39;2021-05-19 09.0*.jpg\u0026#39; -c:v libx264 -r 30 -pix_fmt yuv420p out.slower.mp4 Wow nice that was perfect , so 1/3 indeed sets the duration at 3 seconds per image\u0026hellip;\nGoing to try again with 0.75 seconds so\nffmpeg -framerate 4/3 -pattern_type glob -i \u0026#39;2021-05-19 09.0*.jpg\u0026#39; -c:v libx264 -r 30 -pix_fmt yuv420p out.slower.three_quarters.mp4 Okay that was about perfect I think. Only thing is my images are rotated still, so think I got to rotate them first .. ffmpeg -framerate 4/3 -pattern_type glob -i \u0026#39;*R.jpg\u0026#39; -c:v libx264 -r 30 -pix_fmt yuv420p out.slower.three_quarters.mini.mp4 Wow very weird, but \u0026ldquo;turns out\u0026rdquo; hehe , all I had to do was rotate the image 360 degrees and it still managed to get modified and then the mp4 orientation ended up being correct finally.\nHopefully image magick has the same effect? ( per here )\nconvert -rotate \u0026#34;360\u0026#34; \u0026#34;2021-05-19 09.02.15.jpg\u0026#34; \u0026#34;2021-05-19 09.02.15.R360.jpg\u0026#34; $ md5 \u0026#34;2021-05-19 09.02.15.jpg\u0026#34; \u0026#34;2021-05-19 09.02.15.R360.jpg\u0026#34; MD5 (2021-05-19 09.02.15.jpg) = 63efd132e1c609672f260c0af1c84058 MD5 (2021-05-19 09.02.15.R360.jpg) = 08c2a4716cb90e8d53a0bde0d3d0a397 Wow something definitely happened here hmm. Try a few more and reproduce.. convert -rotate \u0026#34;360\u0026#34; \u0026#34;2021-05-19 09.02.24.jpg\u0026#34; \u0026#34;2021-05-19 09.02.24.R360.jpg\u0026#34; convert -rotate \u0026#34;360\u0026#34; \u0026#34;2021-05-19 09.02.32.jpg\u0026#34; \u0026#34;2021-05-19 09.02.32.R360.jpg\u0026#34; And try on just the image magick images.. ffmpeg -framerate 4/3 -pattern_type glob -i \u0026#39;*R360.jpg\u0026#39; -c:v libx264 -r 30 -pix_fmt yuv420p out.slower.three_quarters.mini4.mp4 Hmm dang the output out.slower.three_quarters.mini4.mp4 is still rotated.\nMaybe image magick is doing some pure math rotation, but perhaps using the \u0026ldquo;Preview\u0026rdquo; app to do the rotation resets some kind of EXIF rotation setting.\nSo I tried looking at one of the original images and I\u0026rsquo;m getting\nidentify -format \u0026#39;%[EXIF:*]\u0026#39; \u0026#34;2021-05-19 09.02.40.jpg\u0026#34; | grep Orientation exif:Orientation=6 Which according to this , 6 means \u0026ldquo;upper right\u0026rdquo; The orientation of the camera relative to the scene, when the image was captured. The start point of stored data is, \u0026lsquo;1\u0026rsquo; means upper left, \u0026lsquo;3\u0026rsquo; lower right, \u0026lsquo;6\u0026rsquo; upper right, \u0026lsquo;8\u0026rsquo; lower left, \u0026lsquo;9\u0026rsquo; undefined.\nNot really sure what that means but one of the images which I rotated manually with \u0026ldquo;Preview\u0026rdquo; \u0026hellip; identify -format \u0026#39;%[EXIF:*]\u0026#39; \u0026#34;2021-05-19 09.01.12R.jpg\u0026#34;|grep Orientation exif:Orientation=1 Bingo. So if I can modify this tag on these images from 6 to 1 that might work. Oh wow a stack overflow search on simply \u0026ldquo;modifying exif data with image magick\u0026rdquo; brought up this answer which is specifically about the orientation . Haha So the suggestion is to use mogrify And this other use of identify gives the human readable value instead identify -verbose \u0026#34;2021-05-19 09.01.12R.jpg\u0026#34;|grep Orientation Orientation: TopLeft Ok going to just try to use the glob approach file=\u0026#34;2021-05-19 09.02.40.jpg\u0026#34; echo \u0026#34;before\u0026#34; $(identify -verbose $file|grep Orientation) mogrify -orient \u0026#34;left-top\u0026#34; \u0026#34;2021-05-19 09.02.40.jpg\u0026#34; echo \u0026#34;after\u0026#34; $(identify -verbose $file|grep Orientation) # output.. before Orientation: RightTop after Orientation: LeftTop Ok cool.. mogrify -orient \u0026#34;left-top\u0026#34; *.jpg Ok try ffmpeg again \u0026hellip; ffmpeg -framerate 4/3 -pattern_type glob -i \u0026#39;*.jpg\u0026#39; -c:v libx264 -r 30 -pix_fmt yuv420p out.slower.three_quarters.full.mp4 ","permalink":"https://michal.piekarczyk.xyz/post/2021-05-23-ffmpeg-stitching/","summary":"using ffmpeg to stitch together some images into a slide show","title":"some ffmpeg stitching"},{"content":"Stuffs\n","permalink":"https://michal.piekarczyk.xyz/foo/one/","summary":"Stuffs","title":"Amazingness"},{"content":"hmmm\n","permalink":"https://michal.piekarczyk.xyz/foo/two/","summary":"hmmm","title":"Another"},{"content":"2021-05-16 Trying to figure out My homepage only shows \u0026ldquo;posts\u0026rdquo; and not \u0026ldquo;portfolio\u0026rdquo; or \u0026ldquo;handy\u0026rdquo; . Also my \u0026ldquo;about\u0026rdquo; page. How to add \u0026ldquo;preview images\u0026rdquo; to posts Homepage this is a starting point I think. Create content/_index.md \u0026hellip; Confusing but I think \u0026ldquo;templates\u0026rdquo; are stored in the /layouts directory. The \u0026ldquo;homepage template\u0026rdquo; is /layouts/index.html . If that doesnt exist a default one will be used I think. I copied this example below from here , and I had to create /layouts/partials/header.html, /layouts/partials/summary.html for some errors to disappear. But anyhow, that just produced a blank homepage. {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;main\u0026gt; {{ .Content }} {{ range .Paginator.Pages }} {{ partial \u0026#34;summary.html\u0026#34; . }} {{ end }} {{ partial \u0026#34;pagination.html\u0026#34; . }} \u0026lt;/main\u0026gt; {{ partial \u0026#34;sidebar.html\u0026#34; . }} {{ partial \u0026#34;footer.html\u0026#34; . }} But I just want to modify the homepage ever so slightly not completely override it. Base template Not sure if I\u0026rsquo;m ready to use it yet but here , I learned you can define a base template /layouts/_default/baseof.html like \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;{{ block \u0026#34;title\u0026#34; . }} \u0026lt;!-- Blocks may include default content. --\u0026gt; {{ .Site.Title }} {{ end }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Code that all your templates share, like a header --\u0026gt; {{ block \u0026#34;main\u0026#34; . }} \u0026lt;!-- The part of the page that begins to differ between templates --\u0026gt; {{ end }} {{ block \u0026#34;footer\u0026#34; . }} \u0026lt;!-- More shared code, perhaps a footer but that can be overridden if need be in --\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; But you need to create /layouts/_default/list.html {{ define \u0026#34;main\u0026#34; }} \u0026lt;h1\u0026gt;Posts\u0026lt;/h1\u0026gt; {{ range .Pages }} \u0026lt;article\u0026gt; \u0026lt;h2\u0026gt;{{ .Title }}\u0026lt;/h2\u0026gt; {{ .Content }} \u0026lt;/article\u0026gt; {{ end }} {{ end }} and /layouts/_default/single.html {{ define \u0026#34;title\u0026#34; }} \u0026lt;!-- This will override the default value set in baseof.html; i.e., \u0026#34;{{.Site.Title}}\u0026#34; in the original example--\u0026gt; {{ .Title }} \u0026amp;ndash; {{ .Site.Title }} {{ end }} {{ define \u0026#34;main\u0026#34; }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ end }} preview images Ok nice finally found how to set images , from here , the page for the \u0026ldquo;Ananke\u0026rdquo; theme that I am using. Maybe this is different for other themes.\nSo in the front matter ( between the --- and ---) , you can add featured_image: 'https://example.com/blahblah.jpg' and that will be displayed in the background for instance , when I put that into the content/_index.md the homepage index.\n2021-05-29 Hmm trying this approach for embedding images Here , where you do something like this\n--- resources: - name: forest-1 src: \u0026#34;images/forest-1.jpg\u0026#34; title: Forest (1) params: credits: \u0026#34;[Jay Mantri](https://unsplash.com/@jaymantri) on [Unsplash](https://unsplash.com/s/photos/forest)\u0026#34; --- { { \u0026lt; img name=\u0026#34;forest-1\u0026#34; size=\u0026#34;large\u0026#34; lazy=false \u0026gt; } } But I just tried this out and I\u0026rsquo;m getting this error now Rebuild failed: \u0026#34;/blah..../content/en/post/2021-01-07-steak-two.md:29:1\u0026#34;: failed to extract shortcode: template for shortcode \u0026#34;img\u0026#34; not found { { \u0026lt; img name=\u0026#34;sizzle\u0026#34; size=\u0026#34;small\u0026#34; lazy=true \u0026gt; } } Not sure but maybe resources must be in a page bundle and cannot have a source that is a uri. ","permalink":"https://michal.piekarczyk.xyz/post/2021-05-16-hugo-notes/","summary":"Switched from Jekyll to Hugo","title":"Notes on hugo"},{"content":"How to download your Noom data Per this note, you can apparently download your Noom data here\nI used this because I completely forgot what a serving was for one of my custom recipes On Noom all I can see is that my \u0026ldquo;KetoGranola20210326\u0026rdquo; , 1 serving is 187 Cal. But if I have a trusty food scale and I want to log 149g I need to know what that is for a serving.\nimport json import pandas as pd df = pd.read_csv(\u0026#39;/my/blah/Downloads/2021-05-02_xxxxx/xxxxx_CardioTrainer.CustomDishes.csv\u0026#39;) df.iloc[0] # addedId 1111111 # userId 00000000 # jsonString {\u0026#34;amount\u0026#34;:1,\u0026#34;isDeleted\u0026#34;:false,\u0026#34;manualPreciseFo... # timeUpdated 2021-03-18 01:38:22 # Name: 0, dtype: object dishes = df.jsonString.map(json.loads).tolist() ketogranola = list(filter(lambda x: x[\u0026#39;name\u0026#39;] == \u0026#39;KetoGranola20210326\u0026#39;, dishes))[0] Looks like I can gather to total cals and total grams \u0026hellip;\n[[x[\u0026#39;name\u0026#39;], x[\u0026#39;calories\u0026#39;], x[\u0026#39;extraData\u0026#39;][\u0026#39;preciseAmount\u0026#39;]] for x in ketogranola.get(\u0026#39;ingredients\u0026#39;)] [[\u0026#39;Roasted Pumpkin Seed\u0026#39;, 122, 21], [\u0026#39;Sunflower Seeds\u0026#39;, 140, 24], [\u0026#39;Chia Seed\u0026#39;, 123, 25], [\u0026#39;Flaxseeds\u0026#39;, 112, 21], [\u0026#39;Cashew Nuts\u0026#39;, 129, 22], [\u0026#39;Unsweetened Coconut Flakes\u0026#39;, 133, 20], [\u0026#39;Natural Grocers Raw Macadamia Nuts\u0026#39;, 191, 26], [\u0026#39;Coconut Oil\u0026#39;, 187, 21], [\u0026#39;Pecans\u0026#39;, 175, 25]] # sum([x[\u0026#39;calories\u0026#39;] for x in ketogranola.get(\u0026#39;ingredients\u0026#39;)]) # 1312 sum([x[\u0026#39;extraData\u0026#39;][\u0026#39;preciseAmount\u0026#39;] for x in ketogranola.get(\u0026#39;ingredients\u0026#39;)]) # 205 Ok cool, looks like 1312/187 =\u0026gt; 7.016042780748663 , oh and that matches this other field cool.\nketogranola[\u0026#39;amount\u0026#39;] # 7 So then one serving is 205/7 ==\u0026gt; 29.286 grams. So the 149g in my image is 149/29.3 = 5.09 servings.\n","permalink":"https://michal.piekarczyk.xyz/post/2021-05-02-noom-gdpr/","summary":"How to download your Noom data Per this note, you can apparently download your Noom data here\nI used this because I completely forgot what a serving was for one of my custom recipes On Noom all I can see is that my \u0026ldquo;KetoGranola20210326\u0026rdquo; , 1 serving is 187 Cal. But if I have a trusty food scale and I want to log 149g I need to know what that is for a serving.","title":"Noom Gdpr"},{"content":" Reference\nstarted reading [[February 2nd, 2021]] finished [[April 3rd, 2021]] Author [[Sonke Ahrens]] These are my [[literature notes]] I suppose. Book in three sentences\nWhen (not if) you take notes, less is more and you achieve this by writing like the git diff building on what you already know; you write what you have learned You write in your own words, distilling the gist, to make retention more likely. The [[slip box]] can make writing easier by replacing planning with the execution of small incremental well defined tasks, \u0026ldquo;Read with a pen and make fleeting notes\u0026rdquo;, \u0026ldquo;write literature notes from fleeting notes\u0026rdquo;, \u0026ldquo;create permanent notes around topics that tug at your interests as you go along\u0026rdquo;, \u0026ldquo;have interactive discussions with your slip box, making connections between your permanent notes as you go along\u0026rdquo;, \u0026ldquo;dump your permanent notes as outlines for manuscripts you can work on and edit until you are satisfied.\u0026rdquo; Impact\nBefore encountering the [[ZettleKasten method]] / [[slip box]] , I had started building project folders in [[Dropbox]] using a system I finally thought would help me. Instead of great search I would take notes on paper or white board and then at least be able to have a descriptive title and be able to look up my notes on demand. Having notes at all and being able to find them was a big step beyond having scattered notes I could never find. But there would be no mind space for me to connect ideas and thoughts. I feel like conversations and #journaling are great ways to spit ball ideas but then you occasionally write them down somewhere and it feels good to get them out of your brain but then what. I enjoy writing occasionally of course but I agree with this book author\u0026rsquo;s ([[Sonke Ahrens]] ) premise that you end up starting from scratch and blank slate writing (which feels like [[top down]] writing ) is hard. And [[bottom up]] writing which is this method, is pretty intriguing! This idea I came across a while back at a google talk about [[start public/open source as opposed to private and then try to make it public]] with respect to code but I have been going down that path for my notes as well. Culminating with the [[book ShowYourWork]] I suppose you should be tactful with what you make public so I am still trying to find the balance. Questions\nWhenever I dive into a \u0026ldquo;Roam\u0026rdquo; session\u0026ndash;elaborating on my notes, making connections and splitting/joining topics\u0026ndash;I feel like it takes a lot of time indeed. I guess the [[slip box]] needs a [[TimeBoxing]] haha? But how long should it take? Concepts\nStep by Step , from the author\u0026rsquo;s Introduction, on [[How to write an article using ZettleKasten]] Make fleeting notes Make literature notes Make permanent notes One idea per note. But in a digital sense this feels like it extends. Add your new permanent notes into the slip-box Build topics that slowly grow into mature material for your writing. The permanent notes are like seeds, but I think you can also have \u0026ldquo;connective tissue notes\u0026rdquo; or \u0026ldquo;meta notes\u0026rdquo; where I imagine you elaborate on and strengthen some of the links. Maybe they also end up having one idea? Copy some of these linked ideas to your \u0026ldquo;Desktop\u0026rdquo; . Sort them, start to edit them, find missing links, Make a first rough draft. Edit / proofread (And I imagine here is the writing/editing cycle). Polish. Don\u0026rsquo;t plan instead be an expert [[Dont plan be an expert]] Wow this speaks to me. plans are rigid indeed. the other way to think about it is I canâ€™t explain why but whenever I make a todo list, especially if it is long, I kind of donâ€™t feel like doing it. I like the concept of calling it the â€œMaybe listâ€ though. or the â€œdefect listâ€/â€œbug listâ€ for issues to resolve . the alternative is the vision. and when you express the vision, short term, within your context, frame of thought, everything else just kind of falls into place â€œ Having a clear structure to work in is completely different from making plans about something. If you make a plan, you impose a structure on yourself; it makes you inflexible. To keep going according to plan, you have to push yourself and employ willpower. This is not only demotivating, but also unsuitable for an open-ended process like research, thinking or studying in general, where we have to adjust our next steps with every new insight, understanding or achievement â€“ which we ideally have on a regular basis and not just as an exceptionâ€. Write whenever you feel like it and make new connections ((cda0b87a-cc5f-4794-903e-3e0187920254)) But as a technical note, Dumping/processing your notes immediately feels crucial because it almost becomes a kind of \u0026ldquo;now or never or much much later\u0026rdquo; kind of game. So that begs the question is it even worth writing on paper or a whiteboard without then immediately distilling the \u0026ldquo;fleeting notes\u0026rdquo; . If you don\u0026rsquo;t it is difficult to later sift through them. [[distilling the gist vs collecting/archiving ]] Read for understanding ((8b7a2887-b77f-4c13-97b7-b7ed22900ea9)) ((26378e7a-5d76-418b-aeb9-32533c84c921)) ((ac860cbd-7b9d-4477-abc9-d26e8294ffc2)) Elaboration The putting into your own words and connecting concept reminds me of an interesting [[person Tim Ferris]] video where he talks about circling phrases in your #journal but also deleting what you don\u0026rsquo;t need. I like keeping at least references to primary sources but as for my own notes, I like the concept of letting ideas in your notes percolate to the top. So perhaps you discard but even if you don\u0026rsquo;t, there\u0026rsquo;s no need to look at the deeper material. \u0026ldquo;The first step of elaboration is to think enough about a piece of information so we are able to write about it. The second step is to think about what it means for other contexts as well.\u0026rdquo; (p104)\n,\u0026ldquo;Connections can be made between heterogeneous notes â€“ as long as the connection makes sense. This is the best antidote to the impeding way most information is given to us in our learning institutions. Most often, it comes in modular form, sorted by topic, separated by disciplines and generally isolated from other information. The slip-box is forcing us to do the exact opposite: To elaborate, to understand, to connect and therefore to learn seriously.\u0026rdquo; (p106)\nLearn from mistakes Most people dont larn from their mistakes! \u0026ldquo;Alas, rarely does a product developer show any interest in learning from the experience of others. Often, companies donâ€™t even keep track of their own failed attempts, providing McMath with whole series in which one kind of mistake was made in multiple variations, sometimes from each generation of developers in the same company (McMath and Forbes 1999).\u0026rdquo; (Page 127)\nHow to write/attach good Topics to notes\nArchivist vs writer!!! Favor retrieval . This is like tattoo selection in the [[movie memento]] then. And the post it notes. Limited writing surface. Careful selection. Ok so write keywords or entry links with the expectation of how to stumble upon that best in the future \u0026ldquo;The way people choose their keywords shows clearly if they think like an archivist or a writer. Do they wonder where to store a note or how to retrieve it? The archivist asks: Which keyword is the most fitting? A writer asks: In which circumstances will I want to stumble upon this note, even if I forget about it? It is a crucial difference.\u0026rdquo; ( page 110 )\nI really like that this gives you permission to be \u0026ldquo;selective\u0026rdquo; because you accept that you don\u0026rsquo;t need everything and I definitely come from the \u0026ldquo;archivist\u0026rdquo; mindset. No need to convert everything My Questions\nShould you have more links? or fewer links? Slightly not sure because selectivity is suggested, \u0026ldquo;The organisation of the notes is in the network of references in the slip-box, so all we need from the index are entry points. A few wisely chosen notes are sufficient for each entry point. The quicker we get from the index to the concrete notes, the quicker we move our attention from mentally preconceived ideas towards the fact-rich level of interconnected content, where we can conduct a fact-based dialogue with the slip-box.\u0026rdquo; (p 109 )\nShould you update your notes as you construct your manuscript? because author talks about filling in manuscript gaps , \u0026hellip; \u0026ldquo;gaps in the arguments in the final manuscript â€“ but these gaps will only become obvious in the next step, when we take the relevant notes for an argument out of the network of the slip-box and sort them into the linear order for the rough draft.\u0026rdquo; ( p 108)\nStill want to know for these #evergreen-note or #[[permanent note]] , how large should they be? Raw Notes\nâ€œ What does make a significant difference along the whole intelligence spectrum is something else: how much self- discipline or self-control one uses to approach the tasks at hand (Duckworth and Seligman, 2005; Tangney, Baumeister, and Boone, 2004).â€ References\nAuthor recommends this program, http://zettelkasten.danielluedecke.de/en/index.php for [[ZettleKasten method]] Write whenever you feel like it and make new connections\n\u0026ldquo;Author basically says yea some people rely on #brainstorming to come up with new ideas or #group-brainstorming but he argues okay the [[slip box]] [[ZettleKasten method]] is better because you\u0026rsquo;re brainstorming every day so might as well just capture your ideas every day . And my connection here is to #journaling , because it is like a brainstorming that you do in the morning. your [[Morning Dump]] [[Morning Pages]]. \u0026quot; ","permalink":"https://michal.piekarczyk.xyz/post/2021-04-10-book-summary-how-to-take-smart-notes/","summary":"Reference\nstarted reading [[February 2nd, 2021]] finished [[April 3rd, 2021]] Author [[Sonke Ahrens]] These are my [[literature notes]] I suppose. Book in three sentences\nWhen (not if) you take notes, less is more and you achieve this by writing like the git diff building on what you already know; you write what you have learned You write in your own words, distilling the gist, to make retention more likely. The [[slip box]] can make writing easier by replacing planning with the execution of small incremental well defined tasks, \u0026ldquo;Read with a pen and make fleeting notes\u0026rdquo;, \u0026ldquo;write literature notes from fleeting notes\u0026rdquo;, \u0026ldquo;create permanent notes around topics that tug at your interests as you go along\u0026rdquo;, \u0026ldquo;have interactive discussions with your slip box, making connections between your permanent notes as you go along\u0026rdquo;, \u0026ldquo;dump your permanent notes as outlines for manuscripts you can work on and edit until you are satisfied.","title":"book How to Take Smart Notes"},{"content":"Quick reference for that keto pie Almond flour crust 350 F 2 (1/2) cup almond flour 3 tbspn erithrytol 1/4 tsp sea salt 1/4 cup melted butter Mix dry ingredients with butter first Then add 1 large egg 1/2 tsp vanilla extract Mix again Bake 10-12 min Coconut flour crust 400 F oven 1/2 cup coconut oil (melted) 3/4 cup coconut flour 1/4 tsp sea salt 2 tbspn erithrytol Combine those dry ingredients with oil using a mixer first Then add 2 eggs And mix again Bake around 10 min Filling 325 F oven One 15oz can pumpkin puree 1/2 cup heavy cream or coconut cream 2 large eggs, room temperature 1 (1/2) tsp cinnamon 1/2 tsp ginger 1/4 tsp nutmeg 1/8 tsp cloves 3 tbsp erithrytol 1/4 tsp sea salt 1 tspn vanilla extract Bake for about 50-60 min or until jiggly (jello like) Cool using a fan . Refrigerate ","permalink":"https://michal.piekarczyk.xyz/post/2021-04-10-pumpkin-pie-cheat-sheet/","summary":"Quick reference for that keto pie Almond flour crust 350 F 2 (1/2) cup almond flour 3 tbspn erithrytol 1/4 tsp sea salt 1/4 cup melted butter Mix dry ingredients with butter first Then add 1 large egg 1/2 tsp vanilla extract Mix again Bake 10-12 min Coconut flour crust 400 F oven 1/2 cup coconut oil (melted) 3/4 cup coconut flour 1/4 tsp sea salt 2 tbspn erithrytol Combine those dry ingredients with oil using a mixer first Then add 2 eggs And mix again Bake around 10 min Filling 325 F oven One 15oz can pumpkin puree 1/2 cup heavy cream or coconut cream 2 large eggs, room temperature 1 (1/2) tsp cinnamon 1/2 tsp ginger 1/4 tsp nutmeg 1/8 tsp cloves 3 tbsp erithrytol 1/4 tsp sea salt 1 tspn vanilla extract Bake for about 50-60 min or until jiggly (jello like) Cool using a fan .","title":"Pumpkin Pie Note"},{"content":"2021-04-10 Tust trawling image search for some inspiration Clamped From here, like the clamp since it doesn\u0026rsquo;t take over your entire desk. But I\u0026rsquo;m only seeing enough space for a laptop and hmm not a whole monitor.\nBracket for big screens This one though , wow looks like an extension for your screen too nice.\nHmm permanant tall desk Actually, reading that home Edit site more, I like the concept of just having a tall desk to begin with. And a nice soft mat to stand on. And then you can bring in a nice stool to sit on whenever you want to sit. Brilliant\nWall mounted Also from home edit wow, he hiding your wires part and the slim design. And again you just need to find a good stool. Less space for a treadmill under that if it is against the wall I suppose.\nAlso this wall desk saves so much space I imagine you can just have a sit down desk as well. Just need to move your laptop around.\nSide note or back note The cable management rear mounted with holes here from Artifox looks nice but I could not find this sold separately on the site.\nThis motorized standing desk though Pretty expensive I think $600 bug elegant. But is it an assumption that the monitor height you use whilst seated is the monitor height you need standing? Are your elbows doing something else when you are standing? ( Originally a kickstarter but now I see you can buy it ) .\n","permalink":"https://michal.piekarczyk.xyz/post/2021-04-10-standing-desk-hmm/","summary":"2021-04-10 Tust trawling image search for some inspiration Clamped From here, like the clamp since it doesn\u0026rsquo;t take over your entire desk. But I\u0026rsquo;m only seeing enough space for a laptop and hmm not a whole monitor.\nBracket for big screens This one though , wow looks like an extension for your screen too nice.\nHmm permanant tall desk Actually, reading that home Edit site more, I like the concept of just having a tall desk to begin with.","title":"Some Better Desk Research"},{"content":"2021-03-28 Tried uploading around 2021-03-13\noops.. Asked Vimeo support and answering some questions What version of the app am I running? Version 8.4.1\nAny error messages after starting the upload? An error I see when I go to look my videos on https://vimeo.com/manage/videos \u0026ldquo;Optimization pending\u0026hellip;\u0026rdquo; , \u0026ldquo;This is taking a while. Try refreshing the page, or come back later.\u0026rdquo; when I click on any of the videos there. How much free space is available on my mobile device and how large is the file? 61.9 GB of 64 GB Used Videos ranging from 100MB to 500MB Was the video being uploaded shot on the same device? Or imported from elsewhere? Yes\nIs the device (my phone) connected by WiFi or cellular? Different results? Tried both.\nAhhh So the storage ewas the issue. I increased the free space from 3GB to 8GB and now upload works. Yay. ","permalink":"https://michal.piekarczyk.xyz/post/2021-03-28-vimeo-troubleshooting-frozen-uploads/","summary":"2021-03-28 Tried uploading around 2021-03-13\noops.. Asked Vimeo support and answering some questions What version of the app am I running? Version 8.4.1\nAny error messages after starting the upload? An error I see when I go to look my videos on https://vimeo.com/manage/videos \u0026ldquo;Optimization pending\u0026hellip;\u0026rdquo; , \u0026ldquo;This is taking a while. Try refreshing the page, or come back later.\u0026rdquo; when I click on any of the videos there. How much free space is available on my mobile device and how large is the file?","title":"Troubleshooting Vimeo video upload freeze"},{"content":"https://databricks.com/try-databricks\n2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language. Is this spark sql or sql ? From the quick start notebook\u0026hellip; CREATE TABLE diamonds USING csv OPTIONS (path \u0026#34;/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\u0026#34;, header \u0026#34;true\u0026#34;) 2021-04-03 Revisit my earlier problem Last time , I found this CDC dataset called \u0026ldquo;COVID-19_Case_Surveillance_Public_Use_Data.csv\u0026rdquo; My basic initial question I would like to answer is \u0026ldquo;how do the symptomatic rates compare by age bin\u0026rdquo;, since this dataset has an onset_dt column, which is eithr blank if no symptoms and has a date if symptoms. More dataset metadata.. 22.5 M rows each row is a de-identified patient created: 2020-05-15 updated 2021-03-31 (not sure what was being updated though) Temporal Applicability: 2020-01-01/2021-03-16 Update Frequency:\tMonthly columns Column Name Description Type cdc_case_earliest_dt Calculated date\u0026ndash;the earliest available date for the record, taken from either the available set of clinical dates (date related to the illness or specimen collection) or the calculated date representing initial date case was received by CDC. This variable is optimized for completeness and may change for a given record from time to time as new information is submitted about a case. Date \u0026amp; Time cdc_report_dt Calculated date representing initial date case was reported to CDC. Depreciated; CDC recommends researchers use cdc_case_earliest_dt in time series and other time-based analyses. Date \u0026amp; Time pos_spec_dt Date of first positive specimen collection Date \u0026amp; Time onset_dt Symptom onset date, if symptomatic Date \u0026amp; Time current_status Case Status: Laboratory-confirmed case; Probable case Plain Text sex Sex: Male; Female; Unknown; Other Plain Text age_group Age Group: 0 - 9 Years; 10 - 19 Years; 20 - 39 Years; 40 - 49 Years; 50 - 59 Years; 60 - 69 Years; 70 - 79 Years; 80 + Years Plain Text race_ethnicity_combined Race and ethnicity (combined): Hispanic/Latino; American Indian / Alaska Native, Non-Hispanic; Asian, Non-Hispanic; Black, Non-Hispanic; Native Hawaiian / Other Pacific Islander, Non-Hispanic; White, Non-Hispanic; Multiple/Other, Non-Hispanic Plain Text hosp_yn Hospitalization status Plain Text icu_yn ICU admission status Plain Text death_yn Death status Plain Text medcond_yn Presence of underlying comorbidity or disease Plain Text Get data in there Per the Databricks web console I can specify an S3 bucket and create a table from my file like that And they refer to \u0026ldquo;DBFS\u0026rdquo; as \u0026ldquo;Databricks File System\u0026rdquo; from the example you can load from the File Store like sparkDF = spark.read.csv(\u0026#39;/FileStore/tables/state_income-9f7c5.csv\u0026#39;, header=\u0026#34;true\u0026#34;, inferSchema=\u0026#34;true\u0026#34;) # then you can create a temp table from that df sparkDF.createOrReplaceTempView(\u0026#34;temp_table_name\u0026#34;) THere was also an interesting note in the help notebook about permanent tables available across cluster restarts\u0026hellip; # Since this table is registered as a temp view, it will only be available to this notebook. If you\u0026#39;d like other users to be able to query this table, you can also create a table from the DataFrame. # Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data. # To do so, choose your table name and uncomment the bottom line. permanent_table_name = \u0026#34;{{table_name}}\u0026#34; # df.write.format(\u0026#34;{{table_import_type}}\u0026#34;).saveAsTable(permanent_table_name) I am looking for how to do this w/ s3\u0026hellip; Ah according to docs you mount s3 files as regular files then proceed as usual ok will try that \u0026hellip; aws_bucket_name = \u0026#34;my-databricks-assets-alpha\u0026#34; s3fn = \u0026#34;s3://my-databricks-assets-alpha/cdc-dataset/COVID-19_Case_Surveillance_Public_Use_Data.csv\u0026#34; s3fn = \u0026#34;s3://my-databricks-assets-alpha/cdc-dataset/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv\u0026#34; mount_name = \u0026#34;blah\u0026#34; dbutils.fs.mount(\u0026#34;s3a://%s\u0026#34; % aws_bucket_name, \u0026#34;/mnt/%s\u0026#34; % mount_name) display(dbutils.fs.ls(\u0026#34;/mnt/%s\u0026#34; % mount_name)) Funny thing I was trying to run this cell in the databricks notebook but it would not run and no error was given. But the reason I am pretty sure is that no cluster was attached to the notebook.\nThen when I started the cluster creation process and then tried executing a cell, I was seeing \u0026ldquo;Waiting for cluster to start: Starting Spark\u0026rdquo; in the output.\nNow AccessDenied. Will try to tweak the Role which I created for databricks . Ah I see it has no s3 access at all\u0026hellip;\nHmm I tweaked permissions and tried again but now got a new error that the directory is already mounted.\nSo going to try just the second part..\nNow getting Access Denied for getFileStatus on s3a://my-databricks-assets-alpha/: ,\nI unmounted dbutils.fs.unmount(\u0026quot;/mnt/%s\u0026quot; % mount_name) , gave all the read s3 permissions available, but still getting that Access Denied for getFileStatus on s3a://my-databricks-assets-alpha/: ,\nBut oddly enough when I go to my AWS EC2 the workers which were created have no \u0026ldquo;IAM Role\u0026quot;s associated with them. so that\u0026rsquo;s weird.\nTrying to use these docs to troubleshoot s3a\nOk going to just try the upload instead because cannot figure out the permissions. But I feel it is possibly because of the missing IAM Role on the ec2 workers.\nWait oops! At the very top of this sample notebook I completely ignored the link , https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html , which has super detailed IAM role instructions #$%*#$$#!! haha Ok going to try the upload route anyway just so I can possibly get started .. upload ok I stepped away from my upload of this 1.5 gig file, and when I came back there was no evidence of success or error hehe, I looked around the file system w/ the notebook and stumbled upon this interesting dir, which looks like it has my file import os print(os.listdir(\u0026#34;/dbfs/FileStore/tables\u0026#34;)) # [\u0026#39;COVID_19_Case_Surveillance_Public_Use_Data.csv\u0026#39;] So going to try making a table from it\u0026hellip; # File location and type file_location = \u0026#34;/dbfs/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv\u0026#34; file_type = \u0026#34;csv\u0026#34; # CSV options infer_schema = \u0026#34;true\u0026#34; first_row_is_header = \u0026#34;true\u0026#34; delimiter = \u0026#34;,\u0026#34; # The applied options are for CSV files. For other file types, these will be ignored. df = spark.read.format(file_type) \\ .option(\u0026#34;inferSchema\u0026#34;, infer_schema) \\ .option(\u0026#34;header\u0026#34;, first_row_is_header) \\ .option(\u0026#34;sep\u0026#34;, delimiter) \\ .load(file_location) display(df) Got \u0026hellip; AnalysisException: Path does not exist: dbfs:/dbfs/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv; Maybe without the leading /dbfs/ ? Ah bingo.. so it\u0026rsquo;s not an absolute path but like a relative path.. # File location and type file_location = \u0026#34;/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv\u0026#34; file_type = \u0026#34;csv\u0026#34; # CSV options infer_schema = \u0026#34;true\u0026#34; first_row_is_header = \u0026#34;true\u0026#34; delimiter = \u0026#34;,\u0026#34; # The applied options are for CSV files. For other file types, these will be ignored. df = spark.read.format(file_type) \\ .option(\u0026#34;inferSchema\u0026#34;, infer_schema) \\ .option(\u0026#34;header\u0026#34;, first_row_is_header) \\ .option(\u0026#34;sep\u0026#34;, delimiter) \\ .load(file_location) display(df) Ok that was pretty fast..\nAnd create that table..\n# Since this table is registered as a temp view, it will only be available to this notebook. If you\u0026#39;d like other users to be able to query this table, you can also create a table from the DataFrame. # Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data. # To do so, choose your table name and uncomment the bottom line. permanent_table_name = \u0026#34;covid\u0026#34; # table_import_type? df.write.format(\u0026#34;json\u0026#34;).saveAsTable(permanent_table_name) Took 50 seconds try read .. select * from covid limit 10 wow ok actually worked \u0026hellip; I am now seeing first ten rows .. quick stat %sql select age_group, count(1) from covid group by age_group order by age_group asc ok try that group by from last time.. select age_group, sum(case when onset_dt is null then 0 else 1 end)/count(1) as asymptomatic_rate from covid group by age_group order by age_group And side notes, in the databricks browser notebook , I generated this above plot by clicking \u0026ldquo;bar chart\u0026rdquo; option and generating a \u0026ldquo;Bokeh\u0026rdquo; looking graphic, then downloading it. But I had tried to export the notebook as ipynb and the images did not get saved . Instead it looked like only the tabular data was saved. And although this data is pretty sparse, look at the comorbidity data too hosp_yn\t|Hospitalization status|Plain Text icu_yn\t|ICU admission status|Plain Text death_yn\t|Death status|Plain Text medcond_yn\t|Presence of underlying comorbidity or disease|Plain Text select medcond_yn, count(1) from covid group by medcond_yn medcond_yn|count(1) --|-- Unknown|1290951 No|938017 Yes|944195 Missing|10242673 select sum(case when hosp_yn) from covid group by medcond_yn hosp_yn count(1) Unknown 2139571 No 4830822 Yes 703734 Missing 5741709 %sql select medcond_yn, sum(case when hosp_yn = \u0026#39;Yes\u0026#39; then 1 else 0 end)/count(1) as hospYes_rate, sum(case when hosp_yn = \u0026#39;No\u0026#39; then 1 else 0 end)/count(1) as hospNo_rate, sum(case when hosp_yn = \u0026#39;Unknown\u0026#39; then 1 else 0 end)/count(1) as hospUnknown_rate, sum(case when hosp_yn = \u0026#39;Missing\u0026#39; then 1 else 0 end)/count(1) as hospMissing_rate from covid group by medcond_yn medcond_yn hospMissing_rate hospYes_rate hospUnknown_rate hospNo_rate Unknown 0.12071333458822217 0.06108210148952207 0.6504840230186894 0.16772054090356645 No 0.22393943819781517 0.03035552660559457 0.021185117114082153 0.7245199180825082 Yes 0.10189102886585928 0.21121802170102574 0.04902694888238129 0.6378640005507337 Missing 0.5154523628744176 0.03875697291127033 0.12044365762726195 0.32534700658705007 ","permalink":"https://michal.piekarczyk.xyz/post/2021-03-20-databricks/","summary":"https://databricks.com/try-databricks\n2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language.","title":"Trying Databricks"},{"content":"Reference Title \u0026ldquo;The Practice\u0026rdquo; Author [[Seth Godin]] Book in three Sentences (almost) ( Here\u0026rsquo;s my first stab at this, on [[February 1st, 2021]] ) If you\u0026rsquo;re not on the hook, then you are working as an #amateur or as a #hack Your daily practice is not about doing what you love but loving what you do. Author says if you are not doing for someone other than yourself, then that is also #amateur status. Also , of what you do, it will not be for everyone Work with empathy, not reassurance. Just be generous, ship that work, on a schedule. And take feedback. My questions You are serving your audience, What if you don\u0026rsquo;t have an audience yet. I understand that doing something \u0026ldquo;for someone \u0026quot; maybe is all you need? What if it\u0026rsquo;s a surprise to them, are you still \u0026ldquo;on the hook\u0026rdquo; then? Meaning they have not say commissioned your work. What if you ask friends for feedback about usefulness of what you are building and they agree its useful but wouldn\u0026rsquo;t necessarily use it. Are you still in amateur territory if You\u0026rsquo;re still searching for someone (other than yourself) who would use what you\u0026rsquo;re creating? Or is it enough to just \u0026ldquo;be generous\u0026rdquo; and \u0026ldquo;commit to your practice\u0026rdquo; as you continue the search for someone who is interested in your work? \u0026quot; Source: show up even if you don\u0026rsquo;t feel like it. I understand it still counts if you don\u0026rsquo;t get paid. Do you need a company? [[how creative should you be at work?]] So i still want to know does Seth say we should give ourselves permission to be creative both at work and in our personal endeavours. #Creativity to me means going outside your [[ComfortZone]] all the time. But I feel this can be perceived as a waste of time to put it one way since it takes longer than the \u0026ldquo;bare minimum\u0026rdquo;. This is also called the \u0026ldquo;Minimum Viable Product\u0026rdquo; and typically feels like you are \u0026ldquo;hacking something together\u0026rdquo; as opposed to say creating something you are proud of. Ok so you want to chance on higher quality work, work that helps you grow faster, that is more worth your time, more creative. So how? Feels like the answer is to be an opportunity hunter of sorts. And yea probably must stop what you are doing from time to time to address something new ,\u0026ldquo;Location 417\u0026rdquo;,\u0026ldquo;If you want to change your story, change your actions first. When we choose to act a certain way, our mind canâ€™t help but rework our narrative to make those actions become coherent.\u0026rdquo; Ok this is like fake it til you make it i guess. I mean what are the actions available. Just got to keep trying to do do do no matter what others say i suppose. Ok so shipping work , what counts as shipping. Publicly on your github for world to judge. Does that count? Like your portfolio which you\u0026rsquo;re using to try find new clients. You\u0026rsquo;re on the hook in my mind for whether your work is chosen I suppose you some people have various commitment devices like to their friends even. Or other people have \u0026ldquo;Sunday Newsletters\u0026rdquo; I know [[person Ali Abdaal]] does this. Quotes \u0026ldquo;The time we spend worrying is actually time weâ€™re spending trying to control something that is out of our control. Time invested in something that is within our control is called work. Thatâ€™s where our most productive focus lies.\u0026rdquo; ( \u0026ldquo;,\u0026ldquo;Location 1173\u0026rdquo;,) \u0026ldquo;show up even if you don\u0026rsquo;t feel like it\u0026rdquo;. (\u0026ldquo;Location 1235\u0026rdquo;,) source \u0026ldquo;No change, no art\u0026rdquo; ( source Location 394) \u0026ldquo;Sculptor Elizabeth King said it beautifully, â€œProcess saves us from the poverty of our intentions.â€\u0026rdquo; ( \u0026ldquo;Location 261\u0026rdquo;,) this is meant to be the quote that inspired this book. \u0026ldquo;We promise to ship, we donâ€™t promise the result.\u0026rdquo; ( ,\u0026ldquo;Location 2088\u0026rdquo; ) Concepts Links to [[planning]] [[NoEstimates]] \u0026ldquo;The time we spend worrying is actually time weâ€™re spending trying to control something that is out of our control. Time invested in something that is within our control is called work. Thatâ€™s where our most productive focus lies.\u0026rdquo; (Location 1173) Very much reminds me of the quote in [[book How to Take Smart Notes]] , [[Don\u0026rsquo;t plan be an expert]] which of course is linked to [[book Blink]] and other [[Freakenomics]] concepts like [[Do experts choke?]] and of course [[book Range]] was also about knowing what to do in an emergency and not just trying to recall some rules you learned There\u0026rsquo;s a distinction between jobs that dictate exactly what you should do and those where you shape the result actively. You play an active role. The author I think calls the latter an artist ,\u0026ldquo;Location 289\u0026rdquo;,\u0026ldquo;Itâ€™s often called â€œart.â€\u0026rdquo; This is funny because indeed jobs don\u0026rsquo;t want artists. They want you to fit their round holes. An artist is a square peg indeed. [[thought-leader]] I remember this distinction from the [[game DreamFall]] and [[game TheLongestJourney]] they called that being a [[DreamFall/wave]]. Because well you\u0026rsquo;re either making one or riding one. ,\u0026ldquo;Location 324\u0026rdquo;,\u0026ldquo;In every field of endeavour, some people stand out as the makers of whatâ€™s next, as the voices of whatâ€™s now.\u0026rdquo; Kind of inspiring actually to think in these terms, to be that kind of change maker a as well. To always be trying to make waves etc. art is change. \u0026ldquo;Location 394\u0026rdquo;,\u0026ldquo;Art is what we call it when weâ€™re able to create something new that changes someone. No change, no [art]\u0026rdquo; Ok so simply put this definition of art is when you use creativity to ship Something which ends up affecting someone. Like a finished product. Don\u0026rsquo;t be the victim ,\u0026ldquo;Location 412\u0026rdquo;,\u0026quot;â€œHere we go againâ€ is an easy way to lull yourself into victimhood, a place where the work is no longer up to you.\u0026rdquo; The message is, be active in your work as opposed to passive. ,\u0026ldquo;Location 408\u0026rdquo;,\u0026ldquo;You might have one, too. And that story drives the actions that we take.\u0026rdquo; I think my story in my head is about my high standard of work and i compare that to my daily work. I always seem to want more for my career. Then I wonder if I want too much. be on the hook ,\u0026ldquo;Location 1196\u0026rdquo;,\u0026ldquo;The industrial system has trained us to avoid the hook. Being on the hook means that you can get blamed, and getting blamed means you can get fired for what you did (or didnâ€™t do). For some of us, though, on the hook is the best place to be. Itâ€™s on you. Itâ€™s on me. Our choice, our turn, our responsibility.\u0026rdquo; What about putting your commitments online so anyone can see if you break the chain. Or what if your accountability device is your roommate who takes your money if you don\u0026rsquo;t put in your promised hours. Is getting fired like when you have someone looking forward to your work but they lose interest because you don\u0026rsquo;t deliver. But also what if you don\u0026rsquo;t yet have someone looking yet , you\u0026rsquo;re still actively looking for a client. Or what if you\u0026rsquo;re just being generous directing your generosity in a specific direction at someone but they say no they don\u0026rsquo;t want it. Did you just get off the hook. There is no muse just do the work ,\u0026ldquo;Location 1202\u0026rdquo;,\u0026ldquo;The practice is choice plus skill plus attitude. We can learn it and we can do it again. We donâ€™t ship the work because weâ€™re creative. Weâ€™re creative because we ship the work. No ghost is needed.\u0026rdquo; This also reminds me of the [[person David Goggins]] concept of [[Cookie Jar]] , refer to your track record of accomplishments whenever you need inspiration or motivation. don\u0026rsquo;t be a hack (Don\u0026rsquo;t lose your point of view ) to me this is like \u0026ldquo;compromising too much\u0026rdquo;. ,\u0026ldquo;Location 1240\u0026rdquo;,\u0026ldquo;Go too far to please the audience and you become a hack. Lose your point of view, lose your reason for doing the work, become a hack. Focus only on the results, become a hack.\u0026rdquo; Keeping to your reason for doing the work is the biggest struggle at my day job. It makes me feel like a hack to sometimes or often play a role at work thats boring and uninventive. I have to fight tooth and nail to get work through that i feel is truly worth doing. that paradox \u0026ldquo;generous vision\u0026rdquo; as opposed to being like an autocrat. ,\u0026ldquo;Location 1242\u0026rdquo;,\u0026ldquo;On the other hand, if you ignore what you see and simply create for yourself, youâ€™ve walked away from empathy. If there is no change, there is no art. The professional understands the fine line between showing up with a generous vision and showing up trying to control the outcome. The best way through the paradox is by working. Ship creative work. On a schedule. Without attachment and without reassurance.\u0026rdquo; But again ship creative work okay but what does taking criticism look like. How does the feedback loop look like. And again what if you are using yourself as a proxy for what you think potentially other people would find desirable. the work is the client ,\u0026ldquo;Location 1398\u0026rdquo;,\u0026ldquo;The work is your client. Itâ€™s hired you to help you make a change happen. Getting paid for our work can confuse us, because it might seem that all we need to do is serve the person with a check book. But thatâ€™s the strategy of a hackâ€”and it rarely leads to the contributions we set out to make in the first place.\u0026rdquo; Ok well when you put it this way it almost reads like if the work is the client and the work is for the client then the work is for work itself haha. But it does take on a life of its own i suppose. being your idiosyncratic self means you\u0026rsquo;re memorable and not replaceable ,\u0026ldquo;Location 1280\u0026rdquo;,\u0026ldquo;Today, the best work and the best opportunities go to those who are hard to replace. The linchpins, the ones who are likely to be missed. And, delightfully, at the very same time that the economy is rewarding idiosyncrasy, weâ€™re discovering that itâ€™s also the way we were meant to be.\u0026rdquo; Idiosyncrasy is memorable. The recipe for the opposite of homogeneity. Fun. Opposite of boring. Doing what you love. Adventure. Breaking the rules. And still making an impact. But the impact, although it is important , needs to be diluted with Whatever makes the work worth doing. ,\u0026ldquo;Location 1309\u0026rdquo;,\u0026ldquo;When the client wants a cheap, easy building, the architectâ€™s desire to do great work is rarely achieved. And when the client wants something important, she knows that hiring a merely good architect is a mistake. Itâ€™s tempting to blame the clients. But the commitment to be a great architect also requires the professionalism to do the hard work of getting better clients.\u0026rdquo; Yea for a while I thought it was up to me to challenge people to think bigger and better. I still try but I\u0026rsquo;m thinking more and more you need to find different people not change people. start small ,\u0026ldquo;Location 2065\u0026rdquo;,\u0026ldquo;First, focus on making something worth sharing. How small can you make it and still do something youâ€™re proud of?\u0026rdquo; Ok something worth sharing. So this slightly more cements for me the author believes you will not necessarily have a client as you are starting out. You have things \u0026ldquo;for someone\u0026rdquo; sure. And then you share them. get feedback. This is super important nuance. ","permalink":"https://michal.piekarczyk.xyz/post/2021-03-06-book-summary-the-practice/","summary":"Reference Title \u0026ldquo;The Practice\u0026rdquo; Author [[Seth Godin]] Book in three Sentences (almost) ( Here\u0026rsquo;s my first stab at this, on [[February 1st, 2021]] ) If you\u0026rsquo;re not on the hook, then you are working as an #amateur or as a #hack Your daily practice is not about doing what you love but loving what you do. Author says if you are not doing for someone other than yourself, then that is also #amateur status.","title":"Book Summary of The Practice"},{"content":"Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.\nFollowing this post to get kubernetes running in Docker for mac Per this post , I just ticked the \u0026ldquo;Enable Kubernetes\u0026rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this\u0026hellip; docker save citibike-learn:0.9 # image:citibike-learn, tag:latest, image-id:1ff5cd891f00 # image:citibike-learn, tag:0.9, imageid:c8d430e84654 Then I did the factory reset. And Enabled Kubernetes and wow! Nice finally got the green light. And restoring with docker load like this docker load -i citibike-learn-0.9.tar Ok now I can continue trying to get spark setup.. Per the post , I grabbed spark albeit 3.0.1 , instead of 2.x ( from here ) , because according to the release notes , 3.0 and 2.x are sounding very compatible. ./bin/docker-image-tool.sh -t spark-docker build \u0026hellip; following along\u0026hellip; kubectl create serviceaccount spark # serviceaccount/spark created kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default # clusterrolebinding.rbac.authorization.k8s.io/spark-role created And submitting an example job bin/spark-submit \\ --master k8s://https://localhost:6443 \\ --deploy-mode cluster \\ --conf spark.executor.instances=1 \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.container.image=spark:spark-docker \\ --class org.apache.spark.examples.SparkPi \\ --name spark-pi \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar Taking 4 minutes so far. Not sure how long this is meant to take haha.\nI tried https://localhost:6443/ from my browser but got denied for now, as below\u0026hellip;\n{ kind: \u0026#34;Status\u0026#34;, apiVersion: \u0026#34;v1\u0026#34;, metadata: { }, status: \u0026#34;Failure\u0026#34;, message: \u0026#34;forbidden: User \u0026#34;system:anonymous\u0026#34; cannot get path \u0026#34;/\u0026#34;\u0026#34;, reason: \u0026#34;Forbidden\u0026#34;, details: { }, code: 403 } I tried the kubectl get pods command and I can see the run time so far.. $ kubectl get pods NAME READY STATUS RESTARTS AGE spark-pi-4df4497735de91a1-driver 1/1 Running 0 6m1s spark-pi-79033a7735deb0a4-exec-1 0/1 Pending 0 5m52s Likely something is blocking. (Actually I noticed my Dropbox was being pretty aggressive. so I paused that.) enabling port forwarding to get access to the dashboard.. kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-pdx99 1/1 Running 0 30m coredns-f9fd979d6-vjpfp 1/1 Running 0 30m etcd-docker-desktop 1/1 Running 0 29m kube-apiserver-docker-desktop 1/1 Running 0 29m kube-controller-manager-docker-desktop 1/1 Running 0 29m kube-proxy-42wws 1/1 Running 0 30m kube-scheduler-docker-desktop 1/1 Running 0 29m storage-provisioner 1/1 Running 0 29m vpnkit-controller 1/1 Running 0 29m and hmm I cant run kubectl port-forward kubernetes-dashboard-7b9c7bc8c9-ckfmr 8443:8443 -n kube-system because I dont have that running looks like .\nAh according to here the kubernetes dashboard does not come out of the box\nPer here tried killing\n# ./bin/spark-class org.apache.spark.deploy.Client kill \u0026lt;master url\u0026gt; \u0026lt;driver ID\u0026gt; ./bin/spark-class org.apache.spark.deploy.Client kill k8s://https://localhost:6443 spark-pi-4df4497735de91a1-driver WARNING: This client is deprecated and will be removed in a future version of Spark Use ./bin/spark-submit with \u0026#34;--master spark://host:port\u0026#34; log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. ... Exception in thread \u0026#34;main\u0026#34; org.apache.spark.SparkException: Invalid master URL: spark://k8s://https://localhost:6443 Crashed\u0026hellip; anyway just Ctrl-C for now\nBut when looking around I see per here that the master url in that command should be spark://localhost:6443 instead.\nAnd per this note , yarn is mentioned too. I dont have that yet however.\nTRy to get that dashboard , following from here\nIt is here, https://github.com/kubernetes/dashboard/releases/tag/v2.0.5\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created Hmm I did not see the dashboard with kubectl get pods -n kube-system, but the mentioned to look using kubectl get pods --all-namespaces , and I do see it indeed , in its own namespace indeed\u0026hellip; not in the kube-system namespace NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard kubernetes-dashboard-6f65cb5c64-kbq8d 1/1 Running 0 2m46s Not seeing anything listening on 8443 with netstat -an |grep LIST however, as mentioned here But the other blog post is telling me to go here , http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy After first starting a local proxy that is.. kubectl proxy # Starting to serve on 127.0.0.1:8001 As mentioned, when I visited this url, I saw the screen asking for a token. And running the one liner , kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk \u0026#39;/^deployment-controller-token-/{print $1}\u0026#39;) | awk \u0026#39;$1==\u0026#34;token:\u0026#34;{print $2}\u0026#39; Yielded a token, which was accepted. let me retry that earlier example job .. Since now I can look at the dashboard. Maybe I will see why that job was stalling.. trying again bin/spark-submit \\ --master k8s://https://localhost:6443 \\ --deploy-mode cluster \\ --conf spark.executor.instances=1 \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.container.image=spark:spark-docker \\ --class org.apache.spark.examples.SparkPi \\ --name spark-pi \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar One of the early outputs\u0026hellip; 21/01/24 16:03:01 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image. And basically now stuck in Pending. Insufficient Memory! Ok so when I looked around in the Dashboard, I see oddly \u0026hellip; the first attempt could not succeed because of memory Oh and it is hanging around still blocking resources. $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default spark-pi-4df4497735de91a1-driver 1/1 Running 0 116m default spark-pi-79033a7735deb0a4-exec-1 0/1 Pending 0 116m default spark-pi-df12a57736350578-driver 0/1 Pending 0 21m default spark-pi-e333f47736434a39-driver 0/1 Pending 0 6m16s So actually Ctrl-C was not enough to kill it. When I look at the logs for this driver pod, I\u0026rsquo;m seeing 21/01/24 21:23:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Not sure how to know what resources are needed though. Deleting this pod Kind of handy , when I try deleting that pod in the dashboard , I\u0026rsquo;m seeing a handy note that This action is equivalent to:kubectl delete -n default pod spark-pi-4df4497735de91a1-driver And as soon as that was terminated, the Pending job is running. So yea none of my Ctrl-C were useful haha. Trying that CLI delete instead then kubectl delete -n default pod spark-pi-df12a57736350578-driver Ok that seems to have worked. How to try this again without the memory issue? not sure but\u0026hellip; Read about the pyspark shell being in the base spark so trying nice .. $ ./bin/pyspark Python 3.7.2 (default, Dec 29 2018, 00:00:04) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. 21/01/24 17:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \u0026#34;WARN\u0026#34;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ \u0026#39;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Python version 3.7.2 (default, Dec 29 2018 00:00:04) SparkSession available as \u0026#39;spark\u0026#39;. from pyspark.context import SparkContext sc = SparkContext(\u0026#39;local\u0026#39;, \u0026#39;test\u0026#39;) Oops\nTraceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\u0026#34;, line 133, in __init__ SparkContext._ensure_initialized(self, gateway=gateway, conf=conf) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\u0026#34;, line 341, in _ensure_initialized callsite.function, callsite.file, callsite.linenum)) ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by \u0026lt;module\u0026gt; at /Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/shell.py:41 oh interesting it is already pre defined \u0026gt;\u0026gt;\u0026gt; sc \u0026lt;SparkContext master=local[*] appName=PySparkShell\u0026gt; Will try something basic.. rdd = sc.parallelize([1, 2, 3, 4]) rdd # ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262 rdd.map(lambda x: x*3) # PythonRDD[1] at RDD at PythonRDD.scala:53 rdd.collect() # [1, 2, 3, 4] ok haha not quite right. Ah duh of course have to compose/chain that.. rdd.map(lambda x: x*3).collect() # [3, 6, 9, 12] rdd.collect() # [1, 2, 3, 4] Ok excellent! Going to look more through these docs here Next I would like to try some more basic transformations and actions. 2021-01-31 try some things on this covid19 dataset from here This is a 1.59GiB file , so perfect, how do I use Spark to split this up and perform some basic statistics COVID-19_Case_Surveillance_Public_Use_Data.csv Specifically, I think a good idea to test if random sampling this data, the onset_dt or onset date of symptoms, what is the onset rate by age bin, which is already binned as age_group. Ah and looks like you need to be explicit with specifying a header is present. workdir = \u0026#39;/Users/michal/Downloads/\u0026#39; loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) df.printSchema() root |-- cdc_case_earliest_dt : string (nullable = true) |-- cdc_report_dt: string (nullable = true) |-- pos_spec_dt: string (nullable = true) |-- onset_dt: string (nullable = true) |-- current_status: string (nullable = true) |-- sex: string (nullable = true) |-- age_group: string (nullable = true) |-- race_ethnicity_combined: string (nullable = true) |-- hosp_yn: string (nullable = true) |-- icu_yn: string (nullable = true) |-- death_yn: string (nullable = true) |-- medcond_yn: string (nullable = true) 2021-02-07 Symptomatic by age group. Hmm interesting that in docs is says that so does that mean that all the partitions are running in parallel? How do you only run based on the number of workers you can run simultaneously? Note: Donâ€™t create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems.\ndf.groupBy(\u0026#39;age_group\u0026#39;).count().collect() [Row(age_group=\u0026#39;0 - 9 Years\u0026#39;, count=9)] Try w/ a column that has more variation.. and 1000 rows instead. loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) df.groupBy(\u0026#39;sex\u0026#39;).count().collect() # [Row(sex=\u0026#39;Female\u0026#39;, count=446), Row(sex=\u0026#39;Unknown\u0026#39;, count=30), Row(sex=\u0026#39;Missing\u0026#39;, count=3), Row(sex=\u0026#39;Male\u0026#39;, count=520)] And how do I apply a custom apply function with my group by def foo(dfx): return dfx.count() df.groupBy(\u0026#39;sex\u0026#39;).apply(foo) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 2, in \u0026lt;module\u0026gt; File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\u0026#34;, line 70, in apply raise ValueError(\u0026#34;Invalid udf: the udf argument must be a pandas_udf of type \u0026#34; ValueError: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP. hmm oops from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import StructType, StringType, LongType, DoubleType, StructField schema = StructType([StructField(\u0026#39;sex\u0026#39;, StringType(), True), StructField(\u0026#39;onset_dt\u0026#39;, StringType(), True)]) @pandas_udf(schema, PandasUDFType.GROUPED_MAP) def foo(dfx): return dfx.count() Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\u0026#34;, line 325, in pandas_udf require_minimum_pyarrow_version() File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\u0026#34;, line 54, in require_minimum_pyarrow_version \u0026#34;it was not found.\u0026#34; % minimum_pyarrow_version) ImportError: PyArrow \u0026gt;= 0.15.1 must be installed; however, it was not found. hmm (pandars3) $ pip install PyArrow Collecting PyArrow Downloading https://files.pythonhosted.org/packages/68/5f/1fb0c604636d46257af3c3075955e860161e8c41386405467f073df73f91/pyarrow-3.0.0-cp37-cp37m-macosx_10_13_x86_64.whl (14.1MB) 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1MB 1.6MB/s Collecting numpy\u0026gt;=1.16.6 (from PyArrow) Downloading https://files.pythonhosted.org/packages/68/30/a8ce4cb0c084cc1442408807dde60f9796356ea056ca6ef81c865a3d4e62/numpy-1.20.1-cp37-cp37m-macosx_10_9_x86_64.whl (16.0MB) 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.0MB 1.3MB/s tensorboard 1.14.0 has requirement setuptools\u0026gt;=41.0.0, but you\u0026#39;ll have setuptools 40.6.3 which is incompatible. Installing collected packages: numpy, PyArrow Found existing installation: numpy 1.16.0 Uninstalling numpy-1.16.0: Successfully uninstalled numpy-1.16.0 Successfully installed PyArrow-3.0.0 numpy-1.20.1 Ok cool now this worked .. from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import StructType, StringType, LongType, DoubleType, StructField schema = StructType([StructField(\u0026#39;sex\u0026#39;, StringType(), True), StructField(\u0026#39;onset_dt\u0026#39;, StringType(), True)]) @pandas_udf(schema, PandasUDFType.GROUPED_MAP) def foo(dfx): return dfx.count() workdir = \u0026#39;/Users/michal/Downloads/\u0026#39; loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) df.groupBy(\u0026#39;sex\u0026#39;).count().collect() # out = df.groupBy(\u0026#39;sex\u0026#39;).apply(foo) Really weird error though haha\u0026hellip; \u0026gt;\u0026gt;\u0026gt; out.collect() 21/02/07 23:33:31 ERROR Executor: Exception in task 60.0 in stage 6.0 (TID 205)] org.apache.spark.api.python.PythonException: Traceback (most recent call last): File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 605, in main process() File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 597, in process serializer.dump_stream(out_iter, outfile) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 255, in dump_stream return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 88, in dump_stream for batch in iterator: File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 248, in init_stream_yield_batches for series in iterator: File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 210, in load_stream yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()] File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 210, in \u0026lt;listcomp\u0026gt; yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()] File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 236, in arrow_to_pandas s = super(ArrowStreamPandasUDFSerializer, self).arrow_to_pandas(arrow_column) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 128, in arrow_to_pandas s = arrow_column.to_pandas(date_as_object=True) File \u0026#34;pyarrow/array.pxi\u0026#34;, line 751, in pyarrow.lib._PandasConvertible.to_pandas File \u0026#34;pyarrow/table.pxi\u0026#34;, line 224, in pyarrow.lib.ChunkedArray._to_pandas File \u0026#34;pyarrow/array.pxi\u0026#34;, line 1310, in pyarrow.lib._array_like_to_pandas File \u0026#34;pyarrow/error.pxi\u0026#34;, line 116, in pyarrow.lib.check_status pyarrow.lib.ArrowException: Unknown error: Wrapping 2020/03/ï¿½9 failed at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49) at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340) at org.apache.spark.sql.execution.SparkPlan$$Lambda$2055/64856516.apply(Unknown Source) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872) at org.apache.spark.rdd.RDD$$Lambda$2051/1858155754.apply(Unknown Source) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.executor.Executor$TaskRunner$$Lambda$2018/1084937392.apply(Unknown Source) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2021-02-20 Going to attempt to use ipython w/ pyspark According to stackoverflow PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark Ok nice worked. Just had to make sure to source activate pandars3 my conda environment which actually has ipython .. Hmm maybe since i had errors w/ group by , I can try reduceByKey intead? oh actually, when looking at the doc for the group by with help(df.groupBy('sex')) , I read in the apply description that it is depracated and applyInPandas is recommended instead. And in the apache spark doc here , I\u0026rsquo;m reading that \u0026quot; Using PandasUDFType will be deprecated in the future.\u0026quot; so then the complicated decorator looking code I was trying above, maybe that is getting phased out anyway. The only thing new here is that I need to pass the schema of the dataframe to applyInPandas My particualr dataset is actually all categorical data and dates. def foo(dfx): return dfx.count() # workdir = \u0026#39;/Users/michal/Downloads/\u0026#39; loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) from pyspark.sql.types import StructType, StringType, LongType, DoubleType, StructField # Let me try to treat them all as nullable strings for now... schema = StructType([StructField(x, StringType(), True) for x in df.columns ]) df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() =\u0026gt; ok now error I got is actually more clear\u0026hellip; PythonException: An exception was thrown from the Python worker. Please see the stack trace below. Traceback (most recent call last): File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 605, in main process() File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 597, in process serializer.dump_stream(out_iter, outfile) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 255, in dump_stream return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 88, in dump_stream for batch in iterator: File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 248, in init_stream_yield_batches for series in iterator: File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 429, in mapper return f(keys, vals) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 175, in \u0026lt;lambda\u0026gt; return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))] File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 167, in wrapped \u0026#34;pandas.DataFrame, but is {}\u0026#34;.format(type(result))) TypeError: Return type of the user-defined function should be pandas.DataFrame, but is \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; So let me make sure to return a dataframe in my foo func import pandas as pd def foo(dfx): # This group by key key = dfx.limit(1)[0].sex return pd.DataFrame({\u0026#39;sex\u0026#39;: key, \u0026#39;count\u0026#39;: dfx.count()}) # schema = StructType([StructField(x, StringType(), True) for x in df.columns ]) # df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() Now getting the error.. PythonException: An exception was thrown from the Python worker. Please see the stack trace below. ... AttributeError: \u0026#39;DataFrame\u0026#39; object has no attribute \u0026#39;limit\u0026#39; Hmm so literally the input is a vanilla pandas dataframe I think oh that\u0026rsquo;s why! def foo(dfx): # This group by key key = dfx.iloc[0].sex return pd.DataFrame({\u0026#39;sex\u0026#39;: key, \u0026#39;count\u0026#39;: dfx.count()}) # schema = StructType([StructField(x, StringType(), True) for x in df.columns ]) df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() hmm.. PythonException: An exception was thrown from the Python worker. Please see the stack trace below. Traceback (most recent call last): File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 605, in main process() File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 597, in process serializer.dump_stream(out_iter, outfile) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 255, in dump_stream return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 88, in dump_stream for batch in iterator: File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\u0026#34;, line 248, in init_stream_yield_batches for series in iterator: File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 429, in mapper return f(keys, vals) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 175, in \u0026lt;lambda\u0026gt; return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))] File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\u0026#34;, line 160, in wrapped result = f(pd.concat(value_series, axis=1)) File \u0026#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\u0026#34;, line 107, in wrapper return f(*args, **kwargs) File \u0026#34;\u0026lt;ipython-input-54-736ec161f4f7\u0026gt;\u0026#34;, line 4, in foo File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/frame.py\u0026#34;, line 392, in __init__ mgr = init_dict(data, index, columns, dtype=dtype) File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u0026#34;, line 212, in init_dict return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype) File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u0026#34;, line 56, in arrays_to_mgr arrays = _homogenize(arrays, index, dtype) File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u0026#34;, line 277, in _homogenize raise_cast_failure=False) File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u0026#34;, line 642, in sanitize_array value, len(index), dtype) File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u0026#34;, line 1187, in construct_1d_arraylike_from_scalar subarr = np.empty(length, dtype=dtype) TypeError: Cannot interpret \u0026#39;\u0026lt;attribute \u0026#39;dtype\u0026#39; of \u0026#39;numpy.generic\u0026#39; objects\u0026gt;\u0026#39; as a data type schema = StructType([StructField(\u0026#39;sex\u0026#39;, StringType(), True), StructField(\u0026#39;count\u0026#39;, LongType(), True) ]) df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() group to try the string schema usage instead schema = \u0026#39;, \u0026#39;.join([f\u0026#39;{x} string\u0026#39; for x in df.columns]); schema # \u0026#39;cdc_case_earliest_dt string, cdc_report_dt string, pos_spec_dt string, onset_dt string, current_status string, sex string, age_group string, race_ethnicity_combined string, hosp_yn string, icu_yn string, death_yn string, medcond_yn string\u0026#39; df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() Dang same error. Maybe doesnt like string type group bys?\nRandomly reading this may be something to do w/ old pandas version?\nIn [68]: pd.__version__ Out[68]: \u0026#39;0.24.2\u0026#39; I upgraded to 1.0.5 import pandas as pd workdir = \u0026#39;/Users/michal/Downloads/\u0026#39; loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) def foo(dfx): # This group by key key = dfx.iloc[0].sex return pd.DataFrame({\u0026#39;sex\u0026#39;: key, \u0026#39;count\u0026#39;: dfx.count()}) # schema = \u0026#39;sex string, count int\u0026#39; # df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() now a different error.. pyarrow.lib.ArrowException: Unknown error: Wrapping 2020/03/ï¿½6 failed Makes me think I have some garbage data Trying the 10 line datafile i have instead loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) # everything else is the same .. WOw now a scala/java error.. 21/02/20 22:19:32 ERROR Executor: Exception in task 60.0 in stage 8.0 (TID 406)] org.apache.spark.SparkException: Python worker exited unexpectedly (crashed) ... at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536) at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49) at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340) at org.apache.spark.sql.execution.SparkPlan$$Lambda$2055/1769623532.apply(Unknown Source) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872) at org.apache.spark.rdd.RDD$$Lambda$2051/917090051.apply(Unknown Source) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.executor.Executor$TaskRunner$$Lambda$2018/644307005.apply(Unknown Source) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86) ... 22 more 21/02/20 22:19:32 ERROR TaskSetManager: Task 159 in stage 8.0 failed 1 times; aborting job --------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) \u0026lt;ipython-input-3-e6065af68166\u0026gt; in \u0026lt;module\u0026gt; 15 schema = \u0026#39;sex string, count int\u0026#39; 16 # ---\u0026gt; 17 df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() 18 ~/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py in collect(self) 594 \u0026#34;\u0026#34;\u0026#34; 595 with SCCallSiteSync(self._sc) as css: --\u0026gt; 596 sock_info = self._jdf.collectToPython() 597 return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer()))) 598 ~/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 1303 answer = self.gateway_client.send_command(command) 1304 return_value = get_return_value( -\u0026gt; 1305 answer, self.gateway_client, self.target_id, self.name) 1306 1307 for temp_arg in temp_args: ~/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py in deco(*a, **kw) 126 def deco(*a, **kw): 127 try: --\u0026gt; 128 return f(*a, **kw) 129 except py4j.protocol.Py4JJavaError as e: 130 converted = convert_exception(e.java_exception) ~/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 326 raise Py4JJavaError( 327 \u0026#34;An error occurred while calling {0}{1}{2}.\\n\u0026#34;. --\u0026gt; 328 format(target_id, \u0026#34;.\u0026#34;, name), value) 329 else: 330 raise Py4JError( Py4JJavaError: An error occurred while calling o148.collectToPython. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 159 in stage 8.0 failed 1 times, most recent failure: Lost task 159.0 in stage 8.0 (TID 409, 192.168.16.173, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed) hahaha that is great. The toy example does work though from the docs df = spark.createDataFrame( [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\u0026#34;id\u0026#34;, \u0026#34;v\u0026#34;)) def subtract_mean(pdf): # pdf is a pandas.DataFrame v = pdf.v return pdf.assign(v=v - v.mean()) df.groupby(\u0026#34;id\u0026#34;).applyInPandas(subtract_mean, schema=\u0026#34;id long, v double\u0026#34;).show() +----+----+ | id| v| +----+----+ | 1| 0.0| |null|null| | 2| 0.0| |null|null| |null|null| +----+----+ Hmm so my guess is the string group by is not appreciated..? uummm tried again w/ the small file and this time worked\u0026hellip; well didnt crash at least.. import pandas as pd workdir = \u0026#39;/Users/michal/Downloads/\u0026#39; loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) def foo(dfx): # This group by key key = dfx.iloc[0].sex return pd.DataFrame({\u0026#39;sex\u0026#39;: key, \u0026#39;count\u0026#39;: dfx.count()}) # schema = \u0026#39;sex string, count int\u0026#39; # df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).collect() [Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=3), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Female\u0026#39;, count=7), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=1), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2), Row(sex=\u0026#39;Male\u0026#39;, count=2)] But now this output looks like well not what I would expect. I expect two rows since you know, this is a group by. So hmm But in any case, at least it is not crashing! so major improvement. Hmm unless this is a partitioned group by\u0026hellip; hmm that would be exciting. So the group by has to be combined? So could it be I have 12 partitions here? But the file only has 9 rows. Weird. oh the apply func can take the key as an arg ? import pandas as pd workdir = \u0026#39;/Users/michal/Downloads/\u0026#39; loc = f\u0026#39;{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head.csv\u0026#39; df = spark.read.option(\u0026#34;header\u0026#34;,True).csv(loc) def foo(key, dfx): \u0026#34;\u0026#34;\u0026#34; Args: key: tuple of the group by keys. dfx: pandas df for the given group by key. \u0026#34;\u0026#34;\u0026#34; return pd.DataFrame({\u0026#39;sex\u0026#39;: key[0], \u0026#39;count\u0026#39;: dfx.count()}) # schema = \u0026#39;sex string, count int\u0026#39; # df.groupBy(\u0026#39;sex\u0026#39;).applyInPandas(foo, schema).show() result is same, but since using show() instead of collect() this time, the output looks slightly different Still don\u0026rsquo;t know why more than two rows though .. +------+-----+ | sex|count| +------+-----+ |Female| 7| |Female| 7| |Female| 7| |Female| 3| |Female| 7| |Female| 7| |Female| 7| |Female| 7| |Female| 7| |Female| 7| |Female| 7| |Female| 7| | Male| 2| | null| null| | Male| 2| | null| null| | Male| null| | null| null| | null| null| | null| null| +------+-----+ only showing top 20 rows ","permalink":"https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/","summary":"Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.\nFollowing this post to get kubernetes running in Docker for mac Per this post , I just ticked the \u0026ldquo;Enable Kubernetes\u0026rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this\u0026hellip; docker save citibike-learn:0.","title":"Spark Weekend"},{"content":" Two updates to the earlier steak post. Earlier, I was getting a really low temperature on the meat thermometer I was using and I am pretty sure now that was because I was only dipping just the tip as opposed to basically burying the thermometer lengthwise. This time around I seem to be getting the expected temperature, above 130 F and even above 140 F, whereas before this temperature was not registering even though I had validated that it was working on some pie for instance.\nSizzle\nThis one came out well done oops Let it rest for a few minutes after I have not condensed this to proper text yet but here\u0026rsquo;s the recipe used for the above corresponding steak.\nRecipe ","permalink":"https://michal.piekarczyk.xyz/post/2021-01-07-steak-two/","summary":"Two updates to the earlier steak post. Earlier, I was getting a really low temperature on the meat thermometer I was using and I am pretty sure now that was because I was only dipping just the tip as opposed to basically burying the thermometer lengthwise. This time around I seem to be getting the expected temperature, above 130 F and even above 140 F, whereas before this temperature was not registering even though I had validated that it was working on some pie for instance.","title":"Steak Two"},{"content":"Reading https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\n(pandars3) $ echo \u0026#39;test content\u0026#39; | git hash-object -w --stdin d670460b4b4aece5915caf5c68d12f560a9fe3e4 I tried that and oh hah so -w tells git hash-object to actually add that hash to my git key value store database hmm.. lets see if i can find it .. (pandars3) $ file .git/objects/d6/70460b4b4aece5915caf5c68d12f560a9fe3e4 .git/objects/d6/70460b4b4aece5915caf5c68d12f560a9fe3e4: VAX COFF executable not stripped - version 737 Ah indeed. ok binary though. hmm but apparently this is actual data.. (pandars3) $ ls -lh .git/objects/d6/ total 32 -rw-r--r--@ 1 michal staff 152B Dec 6 15:06 20fe6a1800a210848ce33e3ec41fc1077feeee -rw-r--r--@ 1 michal staff 153B Jul 4 14:20 56ffc9856515711c4114654acb0f36c1ee8ddb -rw-r--r--@ 1 michal staff 29B Dec 13 17:28 70460b4b4aece5915caf5c68d12f560a9fe3e4 -rw-r--r--@ 1 michal staff 1.2K Nov 13 11:17 953fd15e1fa200b94fd6124f661adf1528165a Hmm but it looks like some of the other files here actually have header looking things. git cat-file -p d670460b4b4aece5915caf5c68d12f560a9fe3e4 But I\u0026rsquo;m not seeing actual data, just metadata like commit messages, parent hash, tree hash, author, and timestamps (pandars3) $ git cat-file -p d670460b4b4aece5915caf5c68d12f560a9fe3e4 test content (pandars3) $ git cat-file -p d656ffc9856515711c4114654acb0f36c1ee8ddb tree 5ff96d0717daba0106acba2dd1b1145ba1e99275 parent ff184a61f4d8bd871ef41cb7934423ecce17177a author Michal Piekarczyk \u0026lt;namoopsoo\u0026gt; 1593886823 -0400 committer Michal Piekarczyk \u0026lt;namoopsoo\u0026gt; 1593886823 -0400 moar (pandars3) $ (pandars3) $ git cat-file -p d620fe6a1800a210848ce33e3ec41fc1077feeee tree 271d395b3591534ebfe7b1e519a744c379f84daf parent 0a41871bd7588b15f8f48b04bf024851ffdfe231 author Michal Piekarczyk \u0026lt;namoopsoo\u0026gt; 1607285215 -0500 committer Michal Piekarczyk \u0026lt;namoopsoo\u0026gt; 1607285215 -0500 moar echo \u0026#39;version 1\u0026#39; \u0026gt; test.txt git hash-object -w test.txt # 83baae61804e65cc73a7201a7252750c76066a30 echo \u0026#39;version 2\u0026#39; \u0026gt; test.txt git hash-object -w test.txt # 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a (pandars3) $ cat test.txt version 2 # And I can restore the original.. (pandars3) $ git cat-file -p 83baae61804e65cc73a7201a7252750c76066a30 \u0026gt; test.txt (pandars3) $ cat test.txt version 1 (pandars3) $ for sha in 20fe6a1800a210848ce33e3ec41fc1077feeee 56ffc9856515711c4114654acb0f36c1ee8ddb 70460b4b4aece5915caf5c68d12f560a9fe3e4 953fd15e1fa200b94fd6124f661adf1528165a ; do echo d5${sha} ; git cat-file -t d6${sha} ; done d520fe6a1800a210848ce33e3ec41fc1077feeee commit d556ffc9856515711c4114654acb0f36c1ee8ddb commit d570460b4b4aece5915caf5c68d12f560a9fe3e4 blob d5953fd15e1fa200b94fd6124f661adf1528165a blob (pandars3) $ Tree objects Ahhh, reading more now so a Tree Object is what solves the problem of how to store file names and group multiple files together. Ok wow when I use that special syntax in my repo\u0026hellip; (pandars3) $ git cat-file -p master^{tree} 100644 blob f40fbd8ba564ea28e0a2501e2921909467b39887\t.gitignore 100644 blob 086a5c9ea988c5a4d37acc5f8ea089e37cb19371\t404.html 100644 blob 922c44a4850e7421585c9030ecd82face732adc6\tCNAME 100644 blob 5d0fe01a3ad78b366d4e74186c976895611f27ab\tGemfile 100644 blob ee589190061ef5e3e6b6d932edfaf342711878c4\tGemfile.lock 040000 tree ce45afa627dd9bbe9632f33877e8b165409564a4\t_authors 100644 blob efa5e9ef216ff9ed5d06a2bf02475b72caa9a55f\t_config.yml 040000 tree b9e5321c96b32cf9cb1872e8750752344353af37\t_data ... ... I see the \u0026ldquo;tree\u0026rdquo; of my latest commit includes all of the files whether they\u0026rsquo;ve changed or not. I guess I had expected to only see the ones that have changed.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-12-13-article-gitinternalsgitobjects-notes/","summary":"Reading https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\n(pandars3) $ echo \u0026#39;test content\u0026#39; | git hash-object -w --stdin d670460b4b4aece5915caf5c68d12f560a9fe3e4 I tried that and oh hah so -w tells git hash-object to actually add that hash to my git key value store database hmm.. lets see if i can find it .. (pandars3) $ file .git/objects/d6/70460b4b4aece5915caf5c68d12f560a9fe3e4 .git/objects/d6/70460b4b4aece5915caf5c68d12f560a9fe3e4: VAX COFF executable not stripped - version 737 Ah indeed. ok binary though. hmm but apparently this is actual data.. (pandars3) $ ls -lh .","title":"notes on Git Internals Git Objects article"},{"content":"For my EasyBlogPosting project.. Testing out the Dropbox API..\n2020-12-09 trying this out Per here \u0026hellip; pip install dropbox Went w/ that Getting Started page Through that page, I created a new test app for myself.\nUsing the API explorer \u0026hellip; I hit Get Token ..\nhttps://www.dropbox.com/oauth2/authorize?response_type=token\u0026amp;client_id=xxxxxxxxx\u0026amp;redirect_uri=https%3A%2F%2Fdropbox.github.io%2Fdropbox-api-v2-explorer%2F\u0026amp;state=file_requests_list!39%2B%2Bxxxxxxxx\u0026amp;token_access_type=online\u0026amp;\nThat generated a token and I used https://dropbox.github.io/dropbox-api-v2-explorer/#files_list_folder and I was able to see actual folders in my account. That\u0026rsquo;s cool except I thought I had chosen something like \u0026lsquo;single folder access\u0026rsquo; so I\u0026rsquo;m slightly confused why I can see the other folders. Maybe the generated token in the API explorer is separate and meant for testing. Hopefully it\u0026rsquo;s short lived. 2020-12-13 oh my special app folder was created w/ the acess token As soon as on my app page I hit \u0026ldquo;Generate access token\u0026rdquo; , my special App folder was created. import dropbox import os print(\u0026#34;Initializing Dropbox API...\u0026#34;) dbx = dropbox.Dropbox(os.getenv(\u0026#39;DBX_ACCESS_TOKEN\u0026#39;)) result = dbx.files_list_folder(path=\u0026#34;\u0026#34;) BadInputError: BadInputError(\u0026#39;xxxxxxxxxxx\u0026#39;, \u0026#39;Error in call to API function \u0026#34;files/list_folder\u0026#34;: Your app is not permitted to access this endpoint because it does not have the required scope \\\u0026#39;files.metadata.read\\\u0026#39;. The owner of the app can enable the scope for the app using the Permissions tab on the App Console.\u0026#39;) Ah ok so I went to the Permissions tab, checked that \u0026lsquo;files.metadata.read\u0026rsquo; box and tried again. Now seeing AuthError: AuthError(\u0026#39;xxxxxxxxxxx\u0026#39;, AuthError(\u0026#39;missing_scope\u0026#39;, TokenScopeError(required_scope=\u0026#39;files.metadata.read\u0026#39;))) Ah according to stackoverflow , the scope cannot be retroactively granted to a token created before the scope was granted. Ok \u0026hellip; trying to re-create a new token\u0026hellip; Ok recreated. Retried. Worked! How to read? What\u0026rsquo;s the difference between the two files/download , files/export ( files/get_preview ) print(\u0026#34;Initializing Dropbox API...\u0026#34;) dbx = dropbox.Dropbox(os.getenv(\u0026#39;DBX_ACCESS_TOKEN\u0026#39;)) meta, response = dbx.files_download(\u0026#39;/blarg.txt\u0026#39;) # In [22]: response.text # Out[22]: \u0026#39;foo blarg\\n\u0026#39; Github interaction Hmm I was going down the path of building an AWS Batch container to commit to my git repo But hmmm I encountered that there may be a Github API which I can perhaps use??? And a nice tutorial Cool, so https://api.github.com/users/namoopsoo/repos , repos are here. However the oauth flow , looks kind of involved for a quick script But ah that\u0026rsquo;s right you can create a token for your account from here, Github settings . Ok cool I was able to perform a simple curl w/ the personal token I created for myself \u0026hellip; ( per docs ) $ curl -v -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/users/namoopsoo/repos ok but can your write using this API is the big question.. Hmm according to here you can commit. post /repos/{owner}/{repo}/git/commits But not seeing how to include the actual diffs in there. Maybe a blob? Ok according to this nice article , indeed the blob endpoint is for files! hmm step 1, get a reference.. like here curl -v -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/ref/heads/master Nice excellent, \u0026hellip;\n... ... { \u0026#34;ref\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;MDM6UmVmMTAwMTg1NjcwOnJlZnMvaGVhZHMvbWFzdGVy\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/refs/heads/master\u0026#34;, \u0026#34;object\u0026#34;: { \u0026#34;sha\u0026#34;: \u0026#34;482d52f53a146dd688eff4ed8093cf2980580662\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;commit\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/482d52f53a146dd688eff4ed8093cf2980580662\u0026#34; } } step 2 , Grab the commit that HEAD points to curl -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/482d52f53a146dd688eff4ed8093cf2980580662 { \u0026#34;sha\u0026#34;: \u0026#34;482d52f53a146dd688eff4ed8093cf2980580662\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;MDY6Q29tbWl0MTAwMTg1NjcwOjQ4MmQ1MmY1M2ExNDZkZDY4OGVmZjRlZDgwOTNjZjI5ODA1ODA2NjI=\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/482d52f53a146dd688eff4ed8093cf2980580662\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/namoopsoo/namoopsoo.github.io/commit/482d52f53a146dd688eff4ed8093cf2980580662\u0026#34;, \u0026#34;author\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Michal Piekarczyk\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;namoopsoo\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-12-13T21:39:11Z\u0026#34; }, \u0026#34;committer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Michal Piekarczyk\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;namoopsoo\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-12-13T21:39:11Z\u0026#34; }, \u0026#34;tree\u0026#34;: { \u0026#34;sha\u0026#34;: \u0026#34;e5a9ee9506a040a7cc8c608614db7531f8f32e70\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees/e5a9ee9506a040a7cc8c608614db7531f8f32e70\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;moar\u0026#34;, \u0026#34;parents\u0026#34;: [ { \u0026#34;sha\u0026#34;: \u0026#34;13db6ee453ea2b05d09ed8da1b7baa87fbbdcebc\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/13db6ee453ea2b05d09ed8da1b7baa87fbbdcebc\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/namoopsoo/namoopsoo.github.io/commit/13db6ee453ea2b05d09ed8da1b7baa87fbbdcebc\u0026#34; } ], \u0026#34;verification\u0026#34;: { \u0026#34;verified\u0026#34;: false, \u0026#34;reason\u0026#34;: \u0026#34;unsigned\u0026#34;, \u0026#34;signature\u0026#34;: null, \u0026#34;payload\u0026#34;: null } } Ok and the tree sha is important. Reading up on this here Okay after taking a mini detour to understand what a Tree Object is, I think I can keep going now . So a Tree Object is a group of other objects ( blob and or tree objects). So continuing with this article , going to post a blob next 3. Post your new file to the server curl \\ -X POST \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs \\ -d \u0026#39;{\u0026#34;content\u0026#34;: \u0026#34;content of the blob\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;utf-8\u0026#34;}\u0026#39; Getting\n{ \u0026#34;message\u0026#34;: \u0026#34;Not Found\u0026#34;, \u0026#34;documentation_url\u0026#34;: \u0026#34;https://docs.github.com/rest/reference/git#create-a-blob\u0026#34; } Well Not Found is better than permission denied I guess.\nAh actually, .. I took a random guess and looking at https://github.com/settings/tokens indeed found I needed to just add some extra permissions. Tried that same curl -X POST again and now got back { \u0026#34;sha\u0026#34;: \u0026#34;6b2d785ecc7db8b71ee933a5b22961a52f40592c\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/6b2d785ecc7db8b71ee933a5b22961a52f40592c\u0026#34; } So all good.\n4. Get a hold of the tree that the commit points to Ok so grabbing that e5a9ee9506a040a7cc8c608614db7531f8f32e70 tree of my target branch\u0026rsquo;s current HEAD commit now\u0026hellip; ( and incidentally that jq json querying parsing utility is kind of nice here.. ) get /repos/{owner}/{repo}/git/trees/e5a9ee9506a040a7cc8c608614db7531f8f32e70 curl \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees/e5a9ee9506a040a7cc8c608614db7531f8f32e70 \\ | jq --color-output \u0026#39;.tree | .[0:3]\u0026#39; [ { \u0026#34;path\u0026#34;: \u0026#34;.gitignore\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;f40fbd8ba564ea28e0a2501e2921909467b39887\u0026#34;, \u0026#34;size\u0026#34;: 56, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/f40fbd8ba564ea28e0a2501e2921909467b39887\u0026#34; }, { \u0026#34;path\u0026#34;: \u0026#34;404.html\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;086a5c9ea988c5a4d37acc5f8ea089e37cb19371\u0026#34;, \u0026#34;size\u0026#34;: 419, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/086a5c9ea988c5a4d37acc5f8ea089e37cb19371\u0026#34; }, { \u0026#34;path\u0026#34;: \u0026#34;CNAME\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;922c44a4850e7421585c9030ecd82face732adc6\u0026#34;, \u0026#34;size\u0026#34;: 21, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/922c44a4850e7421585c9030ecd82face732adc6\u0026#34; } ] (pandars3) $ curl -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees/e5a9ee9506a040a7cc8c608614db7531f8f32e70 |jq --color-output \u0026#39;.tree | map(.type)\u0026#39; [ \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;tree\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34;, \u0026#34;blob\u0026#34; ] Also noticed the paths dont seem to be absolute paths .. I looked up the tree representing the directory _sass and got a relative path there inside.. [ { \u0026#34;path\u0026#34;: \u0026#34;main.scss\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;bc25e9ec345e7f25206a42139085669d4ad800b9\u0026#34;, \u0026#34;size\u0026#34;: 29, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/bc25e9ec345e7f25206a42139085669d4ad800b9\u0026#34; } ] (pandars3) $ ls _s _sass/ _site/ (pandars3) $ ls _sass/ main.scss (pandars3) $ 5. Create a tree containing your new file Ok So I must create a new tree containing the newly created blob. I had actually committed a few commits in the meantime, so I just have to fetch the last trees commit again \u0026hellip; # using the last commit ... bc74103c6c376a267f6f480e185fcb0c816d2f5b (pandars3) $ curl -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/bc74103c6c376a267f6f480e185fcb0c816d2f5b { \u0026#34;sha\u0026#34;: \u0026#34;bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;MDY6Q29tbWl0MTAwMTg1NjcwOmJjNzQxMDNjNmMzNzZhMjY3ZjZmNDgwZTE4NWZjYjBjODE2ZDJmNWI=\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/namoopsoo/namoopsoo.github.io/commit/bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34;, \u0026#34;author\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Michal Piekarczyk\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;namoopsoo\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-12-14T00:47:00Z\u0026#34; }, \u0026#34;committer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Michal Piekarczyk\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;namoopsoo\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-12-14T00:47:00Z\u0026#34; }, \u0026#34;tree\u0026#34;: { \u0026#34;sha\u0026#34;: \u0026#34;e25ed95e0655cc209c8a3f9c61d18a182abd60d0\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees/e25ed95e0655cc209c8a3f9c61d18a182abd60d0\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;moar\u0026#34;, \u0026#34;parents\u0026#34;: [ { \u0026#34;sha\u0026#34;: \u0026#34;c96842008683f25454793eccc3938cce1f605eb6\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/c96842008683f25454793eccc3938cce1f605eb6\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/namoopsoo/namoopsoo.github.io/commit/c96842008683f25454793eccc3938cce1f605eb6\u0026#34; } ], \u0026#34;verification\u0026#34;: { \u0026#34;verified\u0026#34;: false, \u0026#34;reason\u0026#34;: \u0026#34;unsigned\u0026#34;, \u0026#34;signature\u0026#34;: null, \u0026#34;payload\u0026#34;: null } } ok now to create a new tree using e25ed95e0655cc209c8a3f9c61d18a182abd60d0 from above, as the base tree And the new blob I had created earlier\u0026hellip; is 6b2d785ecc7db8b71ee933a5b22961a52f40592c post /repos/{owner}/{repo}/git/trees curl \\ -X POST \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees \\ -d \u0026#39;{\u0026#34;base_tree\u0026#34;: \u0026#34;e25ed95e0655cc209c8a3f9c61d18a182abd60d0\u0026#34;, \\ \u0026#34;tree\u0026#34;: [ \\ { \\ \u0026#34;path\u0026#34;: \u0026#34;new_blob.txt\u0026#34;, \\ \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \\ \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \\ \u0026#34;sha\u0026#34;: \u0026#34;6b2d785ecc7db8b71ee933a5b22961a52f40592c\u0026#34; \\ } \\ ] \\ }\u0026#39; ok first try I got { \u0026#34;message\u0026#34;: \u0026#34;Problems parsing JSON\u0026#34;, \u0026#34;documentation_url\u0026#34;: \u0026#34;https://docs.github.com/rest/reference/git#create-a-tree\u0026#34; } Ok tried without trying to break it up on multiple lines and this time somehow worked.. curl \\ -X POST \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees \\ -d \u0026#39;{\u0026#34;base_tree\u0026#34;: \u0026#34;e25ed95e0655cc209c8a3f9c61d18a182abd60d0\u0026#34;, \u0026#34;tree\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;new_blob.txt\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;6b2d785ecc7db8b71ee933a5b22961a52f40592c\u0026#34; } ] }\u0026#39; { \u0026#34;sha\u0026#34;: \u0026#34;477075b08cae14b22534e7071b066d383696222c\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees/477075b08cae14b22534e7071b066d383696222c\u0026#34;, \u0026#34;tree\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;.gitignore\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;f40fbd8ba564ea28e0a2501e2921909467b39887\u0026#34;, \u0026#34;size\u0026#34;: 56, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/f40fbd8ba564ea28e0a2501e2921909467b39887\u0026#34; }, ... ... { \u0026#34;path\u0026#34;: \u0026#34;new_blob.txt\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;100644\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;blob\u0026#34;, \u0026#34;sha\u0026#34;: \u0026#34;6b2d785ecc7db8b71ee933a5b22961a52f40592c\u0026#34;, \u0026#34;size\u0026#34;: 19, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/blobs/6b2d785ecc7db8b71ee933a5b22961a52f40592c\u0026#34; }, ... ] 6. Create a new commit The parent commit in this case\u0026hellip; bc74103c6c376a267f6f480e185fcb0c816d2f5b And the 477075b08cae14b22534e7071b066d383696222c new tree created in last step\u0026hellip; curl \\ -X POST \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits \\ -d \u0026#39; { \u0026#34;message\u0026#34;: \u0026#34;my first commit using github api!\u0026#34;, \u0026#34;parents\u0026#34;: [\u0026#34;bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34;], \u0026#34;tree\u0026#34;: \u0026#34;477075b08cae14b22534e7071b066d383696222c\u0026#34; } \u0026#39; As far as I can tell that created a commit .. 526dea236deed39d9a996187a6eb31c55137f675\n{ \u0026#34;sha\u0026#34;: \u0026#34;526dea236deed39d9a996187a6eb31c55137f675\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;MDY6Q29tbWl0MTAwMTg1NjcwOjUyNmRlYTIzNmRlZWQzOWQ5YTk5NjE4N2E2ZWIzMWM1NTEzN2Y2NzU=\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/526dea236deed39d9a996187a6eb31c55137f675\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/namoopsoo/namoopsoo.github.io/commit/526dea236deed39d9a996187a6eb31c55137f675\u0026#34;, \u0026#34;author\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;namoopsoo\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;namoopsoo@users.noreply.github.com\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-12-14T01:28:09Z\u0026#34; }, \u0026#34;committer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;namoopsoo\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;namoopsoo@users.noreply.github.com\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-12-14T01:28:09Z\u0026#34; }, \u0026#34;tree\u0026#34;: { \u0026#34;sha\u0026#34;: \u0026#34;477075b08cae14b22534e7071b066d383696222c\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/trees/477075b08cae14b22534e7071b066d383696222c\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;my first commit using github api!\u0026#34;, \u0026#34;parents\u0026#34;: [ { \u0026#34;sha\u0026#34;: \u0026#34;bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34;, \u0026#34;html_url\u0026#34;: \u0026#34;https://github.com/namoopsoo/namoopsoo.github.io/commit/bc74103c6c376a267f6f480e185fcb0c816d2f5b\u0026#34; } ], \u0026#34;verification\u0026#34;: { \u0026#34;verified\u0026#34;: false, \u0026#34;reason\u0026#34;: \u0026#34;unsigned\u0026#34;, \u0026#34;signature\u0026#34;: null, \u0026#34;payload\u0026#34;: null } } 7. Update HEAD patch /repos/{owner}/{repo}/git/refs/{ref} curl \\ -X PATCH \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/refs/master/HEAD \\ -d \u0026#39;{\u0026#34;sha\u0026#34;: \u0026#34;526dea236deed39d9a996187a6eb31c55137f675\u0026#34;} \u0026#39; First stab I\u0026rsquo;m seeing\n{ \u0026#34;message\u0026#34;: \u0026#34;Reference does not exist\u0026#34;, \u0026#34;documentation_url\u0026#34;: \u0026#34;https://docs.github.com/rest/reference/git#update-a-reference\u0026#34; } (Incidentally at this point I tried git fetch and didnt work. Also didnt see the new commit on the github.com through a browser either. ) Ok using a slightly different syntax.. curl \\ -X PATCH \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/refs/heads/master \\ -d \u0026#39;{\u0026#34;sha\u0026#34;: \u0026#34;526dea236deed39d9a996187a6eb31c55137f675\u0026#34;} \u0026#39; { \u0026#34;ref\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;MDM6UmVmMTAwMTg1NjcwOnJlZnMvaGVhZHMvbWFzdGVy\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/refs/heads/master\u0026#34;, \u0026#34;object\u0026#34;: { \u0026#34;sha\u0026#34;: \u0026#34;526dea236deed39d9a996187a6eb31c55137f675\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;commit\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://api.github.com/repos/namoopsoo/namoopsoo.github.io/git/commits/526dea236deed39d9a996187a6eb31c55137f675\u0026#34; } } Nice and success..\n(pandars3) $ git log --graph --pretty=oneline --abbrev-commit --decorate --all * 526dea2 (origin/master) my first commit using github api! * bc74103 (HEAD -\u0026gt; master) moar * c968420 moar I also checked that git log -p indeed showed only one change in that commit, a new file new_blob.txt with content of the blob\nNext That was pretty cool. Next I should like to combine the Dropbox + github Api parts to make a proof of concept, A Proof of concept that reads a new yaml file in that special dropbox application folder, runs something like the quick_post.py and publishes a new file through the github api. ","permalink":"https://michal.piekarczyk.xyz/post/2020-12-09-dropbox-api-hmm/","summary":"For my EasyBlogPosting project.. Testing out the Dropbox API..\n2020-12-09 trying this out Per here \u0026hellip; pip install dropbox Went w/ that Getting Started page Through that page, I created a new test app for myself.\nUsing the API explorer \u0026hellip; I hit Get Token ..\nhttps://www.dropbox.com/oauth2/authorize?response_type=token\u0026amp;client_id=xxxxxxxxx\u0026amp;redirect_uri=https%3A%2F%2Fdropbox.github.io%2Fdropbox-api-v2-explorer%2F\u0026amp;state=file_requests_list!39%2B%2Bxxxxxxxx\u0026amp;token_access_type=online\u0026amp;\nThat generated a token and I used https://dropbox.github.io/dropbox-api-v2-explorer/#files_list_folder and I was able to see actual folders in my account. That\u0026rsquo;s cool except I thought I had chosen something like \u0026lsquo;single folder access\u0026rsquo; so I\u0026rsquo;m slightly confused why I can see the other folders.","title":"Easy Blog Posting (\"My Noting Book\")"},{"content":"Summary Ok managed to get DeepSpeech to run, but the output for my voice maybe does not translate to any text that makes sense hahaha. But it was pretty funny to read the interpretation.\nMy recording was in m4a so I had to change over to wav first ffmpeg is good for m4a to wav conversion but my ffmpeg is having some issues. (deep3) $ ffmpeg dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/ffmpeg Reason: image not found Abort trap: 6 (deep3) $ I ran brew update Then $ brew upgrade ffmpeg ... ==\u0026gt; Upgrading 1 outdated package: ffmpeg 4.1.1 -\u0026gt; 4.3.1_4 ==\u0026gt; Upgrading ffmpeg 4.1.1 -\u0026gt; 4.3.1_4 ... That worked but I saw this at the end , so I did that ... Error: Xcode alone is not sufficient on Mojave. Install the Command Line Tools: xcode-select --install Still getting.. $ ffmpeg dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/ffmpeg Reason: image not found Abort trap: 6 ok thank the ages, this stackoverflow helped a lot !! $ brew switch openssl 1.0.2s Error: openssl does not have a version \u0026#34;1.0.2s\u0026#34; in the Cellar. openssl\u0026#39;s installed versions: 1.0.2q $ brew switch openssl 1.0.2q Cleaning /usr/local/Cellar/openssl/1.0.2q Opt link created for /usr/local/Cellar/openssl/1.0.2q $ ffmpeg ffmpeg version 4.1.1 Copyright (c) 2000-2019 the FFmpeg developers built with Apple LLVM version 10.0.0 (clang-1000.11.45.5) ... These instructions are pretty clear to get setup w/ deepspeech Starting page and the readthedocs I created a separate environment for this with conda create -n deep3 and source activate deep3 # Install DeepSpeech pip install deepspeech # Download pre-trained English model files curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.9.1/deepspeech-0.9.1-models.pbmm curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.9.1/deepspeech-0.9.1-models.scorer # Download example audio files curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.9.1/audio-0.9.1.tar.gz tar xvf audio-0.9.1.tar.gz # Transcribe an audio file deepspeech --model deepspeech-0.9.1-models.pbmm --scorer deepspeech-0.9.1-models.scorer --audio audio/2830-3980-0043.wav Loading model from file deepspeech-0.9.1-models.pbmm TensorFlow: v2.3.0-6-g23ad988fcd DeepSpeech: v0.9.1-0-gab8bd3e1 Loaded model in 0.0486s. Loading scorer from files deepspeech-0.9.1-models.scorer Loaded scorer in 0.00625s. Running inference. experience proves this Inference took 5.095s for 1.975s audio file. That mini sample worked fine.. \u0026ldquo;experience proves this\u0026rdquo; is 100% accurate according to my ears at least. I Read Some tips on the conversion Ok cool according to here I can use ffmpeg to convert from m4a to wav with just ffmpeg -i blah.m4a blah.wav But I tried deepspeech on my wav file deepspeech --model deepspeech-0.9.1-models.pbmm --scorer deepspeech-0.9.1-models.scorer --audio blah.wav Loading model from file deepspeech-0.9.1-models.pbmm TensorFlow: v2.3.0-6-g23ad988fcd DeepSpeech: v0.9.1-0-gab8bd3e1 Loaded model in 0.0648s. Loading scorer from files deepspeech-0.9.1-models.scorer Loaded scorer in 0.00534s. Warning: original sample rate (24000) is different than 16000hz. Resampling might produce erratic speech recognition. Then the sox program for resampling not present\u0026hellip; I read Some notes on resampling here and finding that ffmpeg has sox built in now. workdir=/blah/my/workdir infile=${workdir}/2020-11-22-09.54.59.wav outfile=${workdir}/2020-11-22-09.54.59--resampled.wav ffmpeg -i $infile -af aresample=resampler=soxr -ar 16000 $outfile deepspeech --model deepspeech-0.9.1-models.pbmm --scorer deepspeech-0.9.1-models.scorer --audio $outfile Ok hahaha this time after resampling to 16000Hz finally worked, but the output does not match any kind of reality haha. Loading model from file deepspeech-0.9.1-models.pbmm TensorFlow: v2.3.0-6-g23ad988fcd DeepSpeech: v0.9.1-0-gab8bd3e1 Loaded model in 0.0451s. Loading scorer from files deepspeech-0.9.1-models.scorer Loaded scorer in 0.00534s. Running inference. so then listened and even interesting it interesting idea for talking about the habitation main point is a paradise his crimes and the periostracum basically like ours person wrote his book or started this is to other thinking harrison article or are frontiersman like a loathing at the summer sententiam first creation antoinette to mitigate they had had been important role spoke for catering people is catching people attention and then destournier take nineteen and working on the antithetic aeternitate which takes a attakapas Inference took 65.434s for 114.816s audio file. Other notes Maybe strategies like this one ","permalink":"https://michal.piekarczyk.xyz/post/2020-11-22-deepspeech/","summary":"Summary Ok managed to get DeepSpeech to run, but the output for my voice maybe does not translate to any text that makes sense hahaha. But it was pretty funny to read the interpretation.\nMy recording was in m4a so I had to change over to wav first ffmpeg is good for m4a to wav conversion but my ffmpeg is having some issues. (deep3) $ ffmpeg dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.","title":"Quick test drive this Mozilla/Baidu Deep Speech"},{"content":"This is a summary of \u0026ldquo;The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers\u0026rdquo; by Daniel F. Chambliss.\nIn three sentences The author has spent a lot of time reporting on all levels of swimming competition and writes about observations about the differences between them.\nThe differences in abilities of different levels of competition are not because of quantitative differences like \u0026ldquo;more training\u0026rdquo; but qualitative differences like using a flip turn instead of just touching the wall and turning around. Qualitative differences narrow to \u0026ldquo;technique\u0026rdquo;, \u0026ldquo;discipline\u0026rdquo; and \u0026ldquo;attitude\u0026rdquo;.\nImpressions The notion that success is boring or that there are no \u0026ldquo;quick tricks\u0026rdquo; to success is quite like the Jim Collins\u0026rsquo; lessons in \u0026ldquo;Good to Great\u0026rdquo; that there were no \u0026ldquo;turning points\u0026rdquo; in companies distinguished as more successful. Instead, per those companies\u0026rsquo; exit interviews, their internal experience was just a long slog of improvement and so it was only the external observer\u0026rsquo;s view that looked like someone had some amazing insight that turned everything around and saved the day.\nThe idea of \u0026ldquo;being stuck\u0026rdquo;, \u0026ldquo;certainly swimmers typically remain within a certain level for most of their careers, maintaining through-out their careers the habits with which they began,\u0026rdquo; I think I have personally felt from time to time with learning piano. Even though I had been a student of the piano now for maybe twenty years, I think I have only recently really started stepping up (last 2-3 years or so). And literally I started learning the Chopin \u0026ldquo;Etudes\u0026rdquo; (or Studies opus 10) since the early part of this year, in which Chopin is known to say that the Etudes were written in a way to force you to un-learn a lot of what you have learned. One insight I pieced together a few months ago was in hitting keys after actually knowing your fingers were in the correct position. That seems kind of obvious, but a lot of my practice in \u0026ldquo;Etude op 10 no 1\u0026rdquo;, I would play multiple notes with a finger (instead of one note per finger) because of the amount of traveling your right hand does on the keyboard. But at one point I think I consciously started to trying making sure my fingers were properly placed and the errors just went down by definition. That did slow down the playing a little bit of course, but also hearing the right sounds is much more pleasurable for myself and anybody listening in the vicinity.\nProbably the biggest take-away from this article is that one must learn from failure faster. You practice in your weak areas and not what you are already good at. You keep on trying things outside of your area of comfort and you seek out feedback. Rinse and repeat.\nQutoes list \u0026ldquo;Simply doing more of the same will not lead to moving up a level in the sport.\u0026rdquo;\n\u0026ldquo;The very features of the sport which the C swimmer finds unpleasant, the top level swimmer enjoys. What others see as boring\u0026ndash;swimming back and forth over a black line for two hours, say\u0026ndash;they find peaceful, even meditative, often challenging or therapeutic.\u0026rdquo;\nYea this also reminds me of this concept around that I think is described in the book the \u0026ldquo;Motivation Myth\u0026rdquo; (have not read this yet but I heard a description), that waiting to become motivated to do something you will never get around to it. And that discipline is kind of like short circuiting the gap between idea and action. And most importantly, that you end up liking what you do once you start doing it and getting good. I feel like there is another quote around the similar vein of \u0026ldquo;love what you do not do what you love.\u0026rdquo;\n","permalink":"https://michal.piekarczyk.xyz/post/2020-11-14-themundanityofexcellencestratificationandolympicswimmers-summary/","summary":"This is a summary of \u0026ldquo;The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers\u0026rdquo; by Daniel F. Chambliss.\nIn three sentences The author has spent a lot of time reporting on all levels of swimming competition and writes about observations about the differences between them.\nThe differences in abilities of different levels of competition are not because of quantitative differences like \u0026ldquo;more training\u0026rdquo; but qualitative differences like using a flip turn instead of just touching the wall and turning around.","title":"Summary of Mundanity of Excellence"},{"content":"Are we getting warmer? The washing and drying machine units in our apartment was replaced because the drying machine was taking two plus cycles to do any drying and the washing machine came along for the ride. However, the new washing machine was basically destroying the delicates that required cold water. Somehow even on the coldest water setting, the water still ended up being too hot.\nOne example, of what the super hot water was doing to the colors, Look behind you! Oh how about the connections? Hmm were they flipped by any chance?\nLooks fine hmm.\nHiding in plain sight The flipping connections were right in front of us the whole time haha.\nAfterwards, ","permalink":"https://michal.piekarczyk.xyz/post/2020-11-18--washing-machine-fiasco/","summary":"Are we getting warmer? The washing and drying machine units in our apartment was replaced because the drying machine was taking two plus cycles to do any drying and the washing machine came along for the ride. However, the new washing machine was basically destroying the delicates that required cold water. Somehow even on the coldest water setting, the water still ended up being too hot.\nOne example, of what the super hot water was doing to the colors, Look behind you!","title":"Oh cool new washing machine is ruining the delicates"},{"content":"The material source \u0026ldquo;Rabbit Rever\u0026rdquo; by Susan Orlean published in \u0026ldquo;New Yorker\u0026rdquo; in the \u0026ldquo;July 6 2020\u0026rdquo; edition\nThe article in 3 sentences The author writes about how the rabbit, apparently domesticated for hundreds of years now, has been all of livestock, fur source and since mid century a third most popular choice of pet in the US. But a highly contagious and highly lethal \u0026ldquo;Rabbit Hemorrhagic Disease\u0026rdquo; ( RHD the \u0026ldquo;rabbit Ebola\u0026rdquo;) caused by a lagovirus from the Caliciviridae family, has been wiping out populations of rabbits outside the US since 1984, but in mid 2019, dead rabbits fitting the RHD signs were showing up in the islands near Seattle and in Texas, Arizona and New Mexico not long after. A protective vaccine was developed within five years of the initial 1984 emergence, but because the production of a \u0026ldquo;live genetically modified vaccine\u0026rdquo; is not yet allowed in the US, the vaccine (and another vaccine for a subsequent \u0026ldquo;RHDV2\u0026rdquo; variant ) cannot be used here except by rare limited emergency requests allowed by the USDA.\nImpressions I didn\u0026rsquo;t know that the rabbit has been in such a complicated undefined state, never having been officially classified as \u0026ldquo;livestock\u0026rdquo; by the USDA, rabbits do not benefit from laws around humane treatment. And prior to World War II, rabbit meat was very popular in the US, because beef was more expensive. However, afterwards, domestic cattle production doubled and through its affordability, beef and also chicken overtook the rabbit. I had no clue. There is also an opinion expressed by the American Rabbit Breeders Association that the 1940 creation of Bugs Bunny contributed to the decline of the rabbit as a food item! The still growing pet rabbit owner enthusiasts in the US have been getting very nervous about RHD and they have successfully but very rarely seen some access to vaccines. However, the greater concern expressed in this article is that rabbits are actually a primary source of protein in America for \u0026ldquo;foxes, bobcats, wolves and hawks\u0026rdquo; and that apart from random cats and dogs (the article specifically mentions \u0026ldquo;poodles\u0026rdquo;), they will have no other source of protein available. And although the USDA has in a handful of some cases agreed to the purchase of these vaccines, unlike the rabbies vaccine which can be given with food, the RHD vaccines can only be injected and must be provided annually.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-11-14-newyorker-rabbitfever-rhd-summary/","summary":"The material source \u0026ldquo;Rabbit Rever\u0026rdquo; by Susan Orlean published in \u0026ldquo;New Yorker\u0026rdquo; in the \u0026ldquo;July 6 2020\u0026rdquo; edition\nThe article in 3 sentences The author writes about how the rabbit, apparently domesticated for hundreds of years now, has been all of livestock, fur source and since mid century a third most popular choice of pet in the US. But a highly contagious and highly lethal \u0026ldquo;Rabbit Hemorrhagic Disease\u0026rdquo; ( RHD the \u0026ldquo;rabbit Ebola\u0026rdquo;) caused by a lagovirus from the Caliciviridae family, has been wiping out populations of rabbits outside the US since 1984, but in mid 2019, dead rabbits fitting the RHD signs were showing up in the islands near Seattle and in Texas, Arizona and New Mexico not long after.","title":"Rabbit Fever Summary"},{"content":"I was reading Team of Teams sometime around September 2019. I at least remember reading it in a really cold Starbucks in a mall in Tokyo haha. First, the standard template:\nThe book in three sentences Team of Teams is former General Stanley McChrystal\u0026rsquo;s account of his time at the Joint Chiefs of Staff and in his role in the conflict with AQI (Al-Qaeda in Iraq) and Abu Musab al Zarqawi. He captures the information sharing struggles US forces in Iraq had that made them struggle in keeping up with AQI\u0026rsquo;s de-centralized un-command. And he shares the good news of how giving autonomy and decision making to teams stopped central command from being a bottle neck.\nImpressions I have had a mental bias against micro-management from my time on software/data science projects, but I did not know the story of Frederick Winslow Taylor and his notorious stopwatch that he used to optimize the slow steps in steel production work. This is probably one of the most important points in the book, because it is very unintuitive. The impact of standardizing steel work was immediate efficiency, but mantra of \u0026ldquo;management\u0026rdquo; has created a century\u0026rsquo;s worth of demoralized workers. But more importantly the inflexibility McChrystal writes about in the \u0026ldquo;chain of command\u0026rdquo; meant that reconnaissance missions expertly carried out but unconnected to analysts for example, would make time sensitive intel go to waste.\nHow the book changed me I felt aligned with the book from the start so perhaps reading it was a bit of confirmation bias, but I will gladly cherry pick it as evidence of a changing tide of \u0026ldquo;management\u0026rdquo; (referred to as \u0026ldquo;command\u0026rdquo; in this book). Very close in spirit, Eric Colson\u0026rsquo;s \u0026ldquo;Beware the data science pin factory\u0026rdquo; here, is a bit more validation, but closer to my knowledge space.\nTop three quotes (Find the quote about the trash bag of laptops ) Unsorted Quotes List \u0026ldquo;Team of teams\u0026rdquo; is distinguished from a \u0026ldquo;Command of teams\u0026rdquo;, \u0026ldquo;The quandary faced by Brigham and Womenâ€™s in 2013, like that faced by our Task Force in 2004, was that of what we might call a â€œcommand of teamsâ€: adaptive small teams operating within an old-fashioned rigid superstructure. In a response to rising tactical complexity, many organizations in many domains have replaced small commands with teams.\u0026rdquo; (\u0026ldquo;Location 2304\u0026rdquo;)\n\u0026ldquo;A big piece of why we lagged AQI lay in our need to relay decisions up and down the chain of command. Decisions that senior leaders a few decades prior would have been unable to oversee now required senior approval.\u0026rdquo; (\u0026ldquo;Location 3667\u0026rdquo;)\n\u0026ldquo;Through a series of experiments, Taylor had determined the optimal temperature at which to cut steel chips, the optimal distance between the machinist and his tools, the optimal way for water to cool the lathe, and the optimal speed for internal conveyor belts. When it all came together, there was not a second of lost time, not an ounce of misplaced material, not a moment of unproductive human effort.\u0026rdquo; (\u0026ldquo;Location 704\u0026rdquo;,\u0026quot;\u0026quot;,)\n\u0026ldquo;Shia had an evil brilliance. But ideas are cheap; plenty of armchair generals have proposals for winning wars, some of them quite clever, but only those who can actually shape and manage a force capable of doing the job ultimately succeed.\u0026rdquo; (\u0026ldquo;Location 472\u0026rdquo;)\nTaylor and Scientific Management I am in the middle of reading \u0026ldquo;The Meritocracy Trap\u0026rdquo; right now, and I see a new connection as I am re-reading this quote, about how Taylor apparently said to workers, \u0026ldquo;I have you for your strength and mechanical ability. We have other men paid for thinking.\u0026rdquo; (\u0026ldquo;Location 819\u0026rdquo;). One of the questions I was trying to answer while reading \u0026ldquo;The Meritocracy Trap\u0026rdquo; was how it came to be that there is so much so called \u0026ldquo;unskilled\u0026rdquo; labor today. And putting two and two together here, Taylorism and management theory feel like part of the answer. That book underlined how the influx of the baby boomers forced colleges to more quickly be able to review the new onslaught of applicants and so they introduced Standardized entrace exams. And then it was only a natural follow on that people would stratify in getting better at those entrance exams. But what if the introduction of management was the original stratification sin here, pushing people apart into the skilled and unskilled classes.\nAnd the second element introduced in the \u0026ldquo;Meritocracy Trap\u0026rdquo; was technology/automation as a means of how for example a ride-share driver (e.g. Uber) no longer needs to figure out how to take a passenger from point A to B, but an algorithm does this work. (Not to mention the future of self driving cars). There are countless other examples of this of course. Today you can take a photo of a suspicious mole on your skin with your phone and a melanoma algorithm will assign a likelihood of malignancy to your photograph. We need doctors to build those labeled datasets of course, but maybe that ends up being a research specialty and perhaps some of the cheapness diagnostics displaces some jobs here.\nTo connect this back to the topic of this summary, teams are great of course, but the unskilled laborforce does now look like a mob of elephants in the room asking us whether those teams can outperform the algorithms in the combat of the future. And that goes for \u0026ldquo;military\u0026rdquo; combat or for entrepreneurial combat or say in the league of medical care. Will teams of nurses be replaced by teams of nurses plus algorithms.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-11-12-team-of-teams-summary/","summary":"I was reading Team of Teams sometime around September 2019. I at least remember reading it in a really cold Starbucks in a mall in Tokyo haha. First, the standard template:\nThe book in three sentences Team of Teams is former General Stanley McChrystal\u0026rsquo;s account of his time at the Joint Chiefs of Staff and in his role in the conflict with AQI (Al-Qaeda in Iraq) and Abu Musab al Zarqawi.","title":"Team of Teams Summary"},{"content":"Quick idea around more active content consumption My goal is to write about/ annotate what I read/ watch Just came up with a pattern of consumption to enable this. So on a youtube video for example, I can \u0026ldquo;Share\u0026rdquo; into \u0026ldquo;Dropbox\u0026rdquo; , specifically into a new folder say, of just metadata Then I found this youtube api doc where you can actually give the hyoutube api a youtube id and it will give you the metadata, like title, channel name, date, etc Then in a separate place, I can have my actual notes but they can now get linked together more easily, because I have the matching metadata. Anyway, just the beginning of a thought here. ","permalink":"https://michal.piekarczyk.xyz/post/2020-11-02-hmm-what-i-consume/","summary":"Quick idea around more active content consumption My goal is to write about/ annotate what I read/ watch Just came up with a pattern of consumption to enable this. So on a youtube video for example, I can \u0026ldquo;Share\u0026rdquo; into \u0026ldquo;Dropbox\u0026rdquo; , specifically into a new folder say, of just metadata Then I found this youtube api doc where you can actually give the hyoutube api a youtube id and it will give you the metadata, like title, channel name, date, etc Then in a separate place, I can have my actual notes but they can now get linked together more easily, because I have the matching metadata.","title":"What I consume"},{"content":"This is a mini post part of this project. (Originally posted here ).\nTake a quick look at time of day distribution import matplotlib.pyplot as plt import numpy as np import pandas as pd datadir = \u0026#39;/opt/data\u0026#39; localdir = \u0026#39;/opt/program\u0026#39; tripsdf = pd.read_csv(f\u0026#39;{datadir}/2013-07 - Citi Bike trip data.csv\u0026#39;) stationsdf = pd.read_csv(f\u0026#39;{localdir}/datas/stations/stations-2018-12-04-c.csv\u0026#39;, index_col=0) tripsdf.iloc[0] tripduration 634 starttime 2013-07-01 00:00:00 stoptime 2013-07-01 00:10:34 start station id 164 start station name E 47 St \u0026amp; 2 Ave start station latitude 40.7532 start station longitude -73.9703 end station id 504 end station name 1 Ave \u0026amp; E 15 St end station latitude 40.7322 end station longitude -73.9817 bikeid 16950 usertype Customer birth year \\N gender 0 Name: 0, dtype: object tripsdf[\u0026#39;starttime\u0026#39;].map(lambda x: x[11:13]).iloc[:10] 0 00 1 00 2 00 3 00 4 00 5 00 6 00 7 00 8 00 9 00 Name: starttime, dtype: object tripsdf[\u0026#39;hour\u0026#39;] = tripsdf[\u0026#39;starttime\u0026#39;].map(lambda x: x[11:13]) # For all time looks like maybe two peaks fig = plt.figure(figsize=(6, 6)) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) ax = fig.add_subplot(111, ) ax.hist(tripsdf.hour.tolist(), bins=24) plt.grid(True) # Perhaps on weekdays different? import fresh.utils as fu fu.prepare_weekday_feature(tripsdf) fig = plt.figure(figsize=(6, 6)) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) ax = fig.add_subplot(111, ) ax.hist(tripsdf[tripsdf.weekday == True].hour.tolist(), bins=24) ax.set_title(\u0026#39;Weekday hour histogram\u0026#39;) plt.grid(True) # Weekend wow big difference. fig = plt.figure(figsize=(6, 6)) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) ax = fig.add_subplot(111, ) ax.hist(tripsdf[tripsdf.weekday == False].hour.tolist(), bins=24) ax.set_title(\u0026#39;Weekend hour histogram\u0026#39;) plt.grid(True) # Ok based on the above, going to create a version 2... # time_of_day_v2_peaky # Not sure how to account for the different peaks on weekends # # 0: 6-10, 1: 11-15, 2: 16-20, 3: 21-00, 00-5 Age Somehow I did not include age this time around but I should. tripsdf[[\u0026#39;usertype\u0026#39;, \u0026#39;birth year\u0026#39;]].iloc[:5] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # Ok should basically fill these as np.nan for \u0026#39;Customer\u0026#39; dict(tripsdf[tripsdf.usertype==\u0026#39;Customer\u0026#39;][\u0026#39;birth year\u0026#39;].value_counts()) {'\\\\N': 174887, '1995': 28} # Luckily no nulls tripsdf[(tripsdf.usertype==\u0026#39;Subscriber\u0026#39;)\u0026amp; (tripsdf[\u0026#39;birth year\u0026#39;].isnull())].shape (0, 18) def make_xtick_labels(x, step=5): \u0026#39;\u0026#39;\u0026#39;Given x, step the labels every \u0026lt;step\u0026gt; Aka, take every \u0026lt;step\u0026gt;th x label \u0026#39;\u0026#39;\u0026#39; x_ticks = [i for i in range(len(x)) if i % step == 0] x_labels = [x[i] for i in x_ticks] return x_ticks, x_labels import numpy as np tripsdf[\u0026#39;birth year\u0026#39;] = tripsdf[\u0026#39;birth year\u0026#39;].map(lambda x:int(x) if x != \u0026#39;\\\\N\u0026#39; else np.nan ) # Discard below 1913 as np.nan. TODO tripsdf[(tripsdf.usertype==\u0026#39;Subscriber\u0026#39;)\u0026amp; (tripsdf[\u0026#39;birth year\u0026#39;] \u0026lt; 1913)].shape (226, 18) X = [x for x in tripsdf[(tripsdf.usertype == \u0026#39;Subscriber\u0026#39;)][\u0026#39;birth year\u0026#39;].tolist() if x \u0026gt; 1913] print(len(X)) fig = plt.figure(figsize=(12, 6)) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) ax = fig.add_subplot(111, ) ax.hist(X[:10000], bins=70) # x_ticks, x_labels = make_xtick_labels(x[:1000], step=20) # ax.set_xticks(x_ticks) # ax.set_xticklabels(x_labels, rotation=-45) ax.set_title(\u0026#39;Birth year binned (2013-07)\u0026#39;) plt.grid(True) fig.show() 668262 # Ok this seems normal ish. so might as well just split somewhat arbitrarily or evenly def get_quantiles(unsorted): data = sorted(unsorted) minimum = data[0] Q1 = np.percentile(data, 25, interpolation = \u0026#39;midpoint\u0026#39;) median = np.median(data) Q3 = np.percentile(data, 75, interpolation = \u0026#39;midpoint\u0026#39;) maximum = data[-1] return [minimum, Q1, median, Q3, maximum] def show_da_stats(bundle): H, bins = bundle[\u0026#39;hist\u0026#39;] quantiles = bundle[\u0026#39;quantiles\u0026#39;] fig = plt.figure(figsize=(12, 6)) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) ax = fig.add_subplot(111, ) ax.scatter(quantiles, [1, 1, 1, 1, 1]) ax.axvline(quantiles[1], label=\u0026#39;q:25%\u0026#39;) ax.axvline(quantiles[2], label=\u0026#39;q:50%\u0026#39;) ax.axvline(quantiles[3], label=\u0026#39;q:75%\u0026#39;) ax.set_title(bundle[\u0026#39;title\u0026#39;]) ax.plot(bins, np.insert(H, 0, H[0]), drawstyle=\u0026#39;steps\u0026#39;, color=\u0026#39;green\u0026#39;) plt.grid(True) fig.legend() fig.show() # hist = np.histogram(X, bins=100, range=None) quantiles = get_quantiles(X) bundle = {\u0026#39;hist\u0026#39;: hist, \u0026#39;quantiles\u0026#39;: quantiles, \u0026#39;title\u0026#39;: \u0026#39;Birth year binned (2013-07)\u0026#39;} print(\u0026#39;quantiles\u0026#39;, quantiles) show_da_stats(bundle) quantiles [1920.0, 1969.0, 1978.0, 1984.0, 1997.0] # And the age quantiles to make this a year independent feature.. 2013 - np.array([1920.0, 1969.0, 1978.0, 1984.0, 1997.0]) array([93., 44., 35., 29., 16.]) import datetime; import pytz x = dict(tripsdf.iloc[2]) x (x[\u0026#39;start_dt\u0026#39;] - datetime.datetime(int(x[\u0026#39;birth year\u0026#39;]), 1, 1, tzinfo=pytz.timezone(\u0026#39;US/Eastern\u0026#39;)) ).days/365. , (x[\u0026#39;start_dt\u0026#39;] - np.nan) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-102-ee3bddc50852\u0026gt; in \u0026lt;module\u0026gt; 5 (x['start_dt'] - 6 datetime.datetime(int(x['birth year']), 1, 1, tzinfo=pytz.timezone('US/Eastern')) ----\u0026gt; 7 ).days/365. , (x['start_dt'] - np.nan) TypeError: unsupported operand type(s) for -: 'Timestamp' and 'float' x[\u0026#39;start_dt\u0026#39;] - pd.NaT NaT # Reload once again since I modified a col tripsdf = pd.read_csv(f\u0026#39;{datadir}/2013-07 - Citi Bike trip data.csv\u0026#39;) reload(fu) minidf = tripsdf.iloc[:1000].copy() fu.prepare_weekday_feature(minidf) fu.age_feature(minidf) list(reversed([93., 44., 35., 29., 16.])) [16.0, 29.0, 35.0, 44.0, 93.0] minidf[\u0026#39;birth_bin\u0026#39;] = pd.cut(minidf[\u0026#39;age\u0026#39;], bins=[16.0, 29.0, 35.0, 44.0, 93.0], labels=[0, 1, 2, 3]) minidf[[\u0026#39;age\u0026#39;, \u0026#39;birth\u0026#39;, \u0026#39;birth_bin\u0026#39;]].iloc[:10] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Prepare to build the v3 proc bundle I have a nice reference for testing And for building, I need to do some reverse engineering to maintain the train/test split from earlier. import fresh.predict_utils as fpu bundle = fpu.load_bundle_in_docker() Loading from bundle_loc /opt/ml/model/all_bundle_with_stationsdf.joblib print(\u0026#39;original proc bundle notebook\u0026#39;, bundle[\u0026#39;proc_bundle\u0026#39;][\u0026#39;bundle\u0026#39;][\u0026#39;notebook\u0026#39;]) bundle[\u0026#39;proc_bundle\u0026#39;][\u0026#39;bundle\u0026#39;][\u0026#39;proc_bundle\u0026#39;].keys() print(\u0026#39;this is the bundle glue notebook, cool\u0026#39;, bundle[\u0026#39;notebook\u0026#39;]) print(\u0026#39;model notebook\u0026#39;, bundle[\u0026#39;model_bundle\u0026#39;][\u0026#39;bundle\u0026#39;][\u0026#39;notebook\u0026#39;]) print(\u0026#39;train\u0026#39;, bundle[\u0026#39;model_bundle\u0026#39;][\u0026#39;bundle\u0026#39;][\u0026#39;train\u0026#39;]) print(\u0026#39;test\u0026#39;, bundle[\u0026#39;model_bundle\u0026#39;][\u0026#39;bundle\u0026#39;][\u0026#39;validation_metrics\u0026#39;][\u0026#39;test\u0026#39;]) original proc bundle notebook 2020-07-03-aws.ipynb this is the bundle glue notebook, cool 2020-08-18-glue.ipynb model notebook 2020-07-10-aws.ipynb train /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-08T143732Z/train.libsvm test /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-08T143732Z/test.libsvm import fresh.preproc.v3 as pv3 !pwd ","permalink":"https://michal.piekarczyk.xyz/post/2020-10-22-features-v3/","summary":"This is a mini post part of this project. (Originally posted here ).\nTake a quick look at time of day distribution import matplotlib.pyplot as plt import numpy as np import pandas as pd datadir = \u0026#39;/opt/data\u0026#39; localdir = \u0026#39;/opt/program\u0026#39; tripsdf = pd.read_csv(f\u0026#39;{datadir}/2013-07 - Citi Bike trip data.csv\u0026#39;) stationsdf = pd.read_csv(f\u0026#39;{localdir}/datas/stations/stations-2018-12-04-c.csv\u0026#39;, index_col=0) tripsdf.iloc[0] tripduration 634 starttime 2013-07-01 00:00:00 stoptime 2013-07-01 00:10:34 start station id 164 start station name E 47 St \u0026amp; 2 Ave start station latitude 40.","title":"Visualizing time of day and birth year"},{"content":"Summary This project is a reboot of my earlier project of predicting bicycle ride share riders destinations. https://bike-hop-predict.s3.amazonaws.com/index.html\nThis time around I used XGBoost, newer features, hyper parameter tuning and I have a Demo Site as well! I wanted very much to see if XGBoost has online learning like I used in an earlier TensorFlow project, but as I wrote here, I could not pick up where I left off at least the way I tried it. And here is a mini post on looking at hyper parameter tuning results. And here is a visual look at some of the new features I explored including.\nAgain, the data looks like this\n\u0026#34;tripduration\u0026#34;,\u0026#34;starttime\u0026#34;,\u0026#34;stoptime\u0026#34;,\u0026#34;start station id\u0026#34;,\u0026#34;start station name\u0026#34;,\u0026#34;start station latitude\u0026#34;,\u0026#34;start station longitude\u0026#34;,\u0026#34;end station id\u0026#34;,\u0026#34;end station name\u0026#34;,\u0026#34;end station latitude\u0026#34;,\u0026#34;end station longitude\u0026#34;,\u0026#34;bikeid\u0026#34;,\u0026#34;usertype\u0026#34;,\u0026#34;birth year\u0026#34;,\u0026#34;gender\u0026#34; \u0026#34;171\u0026#34;,\u0026#34;10/1/2015 00:00:02\u0026#34;,\u0026#34;10/1/2015 00:02:54\u0026#34;,\u0026#34;388\u0026#34;,\u0026#34;W 26 St \u0026amp; 10 Ave\u0026#34;,\u0026#34;40.749717753\u0026#34;,\u0026#34;-74.002950346\u0026#34;,\u0026#34;494\u0026#34;,\u0026#34;W 26 St \u0026amp; 8 Ave\u0026#34;,\u0026#34;40.74734825\u0026#34;,\u0026#34;-73.99723551\u0026#34;,\u0026#34;24302\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1973\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;593\u0026#34;,\u0026#34;10/1/2015 00:00:02\u0026#34;,\u0026#34;10/1/2015 00:09:55\u0026#34;,\u0026#34;518\u0026#34;,\u0026#34;E 39 St \u0026amp; 2 Ave\u0026#34;,\u0026#34;40.74780373\u0026#34;,\u0026#34;-73.9734419\u0026#34;,\u0026#34;438\u0026#34;,\u0026#34;St Marks Pl \u0026amp; 1 Ave\u0026#34;,\u0026#34;40.72779126\u0026#34;,\u0026#34;-73.98564945\u0026#34;,\u0026#34;19904\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1990\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;233\u0026#34;,\u0026#34;10/1/2015 00:00:11\u0026#34;,\u0026#34;10/1/2015 00:04:05\u0026#34;,\u0026#34;447\u0026#34;,\u0026#34;8 Ave \u0026amp; W 52 St\u0026#34;,\u0026#34;40.76370739\u0026#34;,\u0026#34;-73.9851615\u0026#34;,\u0026#34;447\u0026#34;,\u0026#34;8 Ave \u0026amp; W 52 St\u0026#34;,\u0026#34;40.76370739\u0026#34;,\u0026#34;-73.9851615\u0026#34;,\u0026#34;17797\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1984\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;250\u0026#34;,\u0026#34;10/1/2015 00:00:15\u0026#34;,\u0026#34;10/1/2015 00:04:25\u0026#34;,\u0026#34;336\u0026#34;,\u0026#34;Sullivan St \u0026amp; Washington Sq\u0026#34;,\u0026#34;40.73047747\u0026#34;,\u0026#34;-73.99906065\u0026#34;,\u0026#34;223\u0026#34;,\u0026#34;W 13 St \u0026amp; 7 Ave\u0026#34;,\u0026#34;40.73781509\u0026#34;,\u0026#34;-73.99994661\u0026#34;,\u0026#34;23966\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1984\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;528\u0026#34;,\u0026#34;10/1/2015 00:00:17\u0026#34;,\u0026#34;10/1/2015 00:09:05\u0026#34;,\u0026#34;3107\u0026#34;,\u0026#34;Bedford Ave \u0026amp; Nassau Ave\u0026#34;,\u0026#34;40.72311651\u0026#34;,\u0026#34;-73.95212324\u0026#34;,\u0026#34;539\u0026#34;,\u0026#34;Metropolitan Ave \u0026amp; Bedford Ave\u0026#34;,\u0026#34;40.71534825\u0026#34;,\u0026#34;-73.96024116\u0026#34;,\u0026#34;16246\u0026#34;,\u0026#34;Customer\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34; \u0026#34;440\u0026#34;,\u0026#34;10/1/2015 00:00:17\u0026#34;,\u0026#34;10/1/2015 00:07:37\u0026#34;,\u0026#34;3107\u0026#34;,\u0026#34;Bedford Ave \u0026amp; Nassau Ave\u0026#34;,\u0026#34;40.72311651\u0026#34;,\u0026#34;-73.95212324\u0026#34;,\u0026#34;539\u0026#34;,\u0026#34;Metropolitan Ave \u0026amp; Bedford Ave\u0026#34;,\u0026#34;40.71534825\u0026#34;,\u0026#34;-73.96024116\u0026#34;,\u0026#34;23698\u0026#34;,\u0026#34;Customer\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34; TOC Prior probability baseline Xgboost detour Multi class classification notes How does this time compare with the previous attempt Model highlights Gluing everything together Looking at hyperparameter tuning results Follow on Prior probability baseline Here, I first wanted to get a metric baseline using a simple model which only uses the highest prior probability destination as the prediction for a source bike share station. Anything even slightly more sophisticated should perform better. I also used this opportunity to apply multi class logloss as an evaluation metric for this problem, which I had not tried last time. So for an output probability vector of 54 possible destination stations, log loss can more granularly assess the prediction probabilities against a vector of the correct station, [0, 1, 0, 0, 0,...] compared to just accuracy.\nFor example\nfrom sklearn.metrics import log_loss # and some noisy predictions noisy_pred = np.array([[.05, .05, .9], [.95, 0.05, 0], [.9, 0.1, 0], [0.05, .05, .9], [0, 1, 0]]) log_loss([3, 1, 1, 3, 2], noisy_pred) the output here is 0.07347496827220674, which is just slightly worse than the perfect 0., showing that log loss can be handy for comparing models.\nThe detail is in the notes, but basically the cross validation log loss using this method ended up being\narray([29.03426394, 25.61716199, 29.19083979, 28.312853 , 22.04601817]) Dockerization Next, for repeatability and portability, here I re-adapted some earlier Dockerization I had setup before to wrap xgboost, along with jupyter notebook for experimentation. This was crucial, because if you want to jump between some quick experiments on your laptop and a notebook in the cloud, you don\u0026rsquo;t want to deal with strange differences in library dependencies between MacOs and linux.\nXgboost detour To start, I wanted to better understand how to use Xgboost abilities with respect to training a model, putting it down, saving it to disk, loading it again and continuing to train on new data. I had used this capability in Tensorflow land earlier and I read it might be possible with Xgboost, but even after trial and error with both the main Xgboost API and its scikit learn API, I could not get this to work properly. My notes on this are here in an earlier post.\nOne cool thing I did learn however was that when repeating a model train and evaluation experiment with both the functional API and the scikit learn API, the functional API took advantage of multithreading, and produced a particular result in 4min 18s vs 49min 6s, with both models using the same seed=42 and ending up with the same accuracy and log loss on some held out data.\nAs I mentioned here , I experienced some early problems running out of memory and crashing, for instance computing log los son 843416 rows. And that is why I was seeking out approaches of online learning. But because of the limitations, my workout ended up being the use of at least carefully deleting objects in memory with del to free up space for, between preprocessing, training and validation. And I also played around with the approach of initializing a xgb.DMatrix using the xgb.DMatrix('/my/blah/path#dtrain.cache') syntax where you specify # a cache file to allow for file access to reduce the in-memory burden, also requiring to dump your pre-processed training data to file first. (And doing that is good anyway because it allows you to free up that precious memory).\nCompared to the initial baseline logloss from earlier of around 29, here I noted a result of 3.9934347 with the initial xgboost approach.\nOn 2020-06-14, I tried using the xgboost caching with the scikitlearn api approach. In the meantime I also ran into a fun issue where an xgboost model was trained on data without a particular output class , with only 53 classes in fact and would produce predict probability vectors of length 53 instead of 54, so I ended up having to make sure to better shuffle the data to make sure when trying to use less data (when using cross validation for instance) that all of the output classes are accounted for, without having a more direct way of telling Xgboost what the output classes should be.\nAlso another fun Tensorflow comparison was I got XGBoostError: need to call fit or load_model beforehand when trying to call predict on a bare model that had not undergone training. Whereas with Tensorflow, I experienced in a previous project that this is absolutely fine, because you simply have a fully formed neural network with some randomly (or otherwise) initialized weights. But with xgboost, or at least the particular implementation I was using, this is not possible, because there is no notion of a base model.\nHere, I tried cutting up the training data like\nclf = xgb.XGBClassifier() workdir = fu.make_work_dir(); print(workdir) fu.log(workdir, \u0026#39;Starting\u0026#39;) prev_model = None loss_vec = []; acc_vec = [] for i, part in enumerate(tqdm(parts)): clf.fit(X_transformed[part], y_enc[part], xgb_model=prev_model) fu.log(workdir, f\u0026#39;[{i}] Done fit\u0026#39;) prev_model = f\u0026#39;{workdir}/model.xg\u0026#39; clf.save_model(prev_model) y_prob_vec = clf.predict_proba(X_test_transformed) fu.log(workdir, f\u0026#39;[{i}] Done predict_proba\u0026#39;) loss = fu.big_logloss(y_test_enc, y_prob_vec, labels) fu.log(workdir, f\u0026#39;[{i}] Done big_logloss, loss={loss}.\u0026#39;) loss_vec.append(loss) acc = accuracy_score(y_test_enc, np.argmax(y_prob_vec, axis=1)) acc_vec.append(acc) fu.log(workdir, f\u0026#39;[{i}] Done accuracy, acc={acc}.\u0026#39;) to see if the scikit learn API can allow saving and restoring previously trained models and continuing, with the fit(X, xgb_model=prev_model) syntax, but the output performance data was basically random indicating to me that the fit was starting from scratch each time.\nHere, below, is a plot of accuracy after multiple epochs, just to visually show the lack of any progression. (This plot is from a similar experiment in my 2020-06-16 notebook ).\nSo basically I gave up on this approach for using xgboost.\nAlso in that notebook, I found I was oddly getting 0 logloss during some experimentation because I had like in this toy example below, been specifying labels to the log_loss func, not matching the actual y_true data (which is the first arg).\na, b = np.array([0, 0, 0]), np.array([[0, 0., 0], [0., 0, 0], [0., 0, 0]]) print(log_loss(a, b, labels=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;])) # =\u0026gt; 0.0 2020-06-19 Here , because a lot of the test set prediction for model evaluation takes time I ended up creating a mini parallelization func. I verified that it was producing roughly the same validation and the time to execute was less.\nI also wrote about how I had needed to use less data to avoid crashing, using the pandas sample() func like,\ntripsdf = pd.read_csv(f\u0026#39;{datadir}/2013-07 - Citi Bike trip data.csv\u0026#39; ).sample(frac=0.017, random_state=42) but that it would be better to build a more balanced dataset instead of just random sampling.\nA rapid fire list of some additional experiments Here, I had another training attempt using the so called \u0026ldquo;cache data\u0026rdquo; with functional api. (But finding especially here that the max_depth was not changing so likely no learning was happening). Here I aadded new features here for a \u0026lsquo;v2\u0026rsquo; dataset, including weekday. and time_of_day. Added this in a new module, fresh/preproc/v2.py. Here , more memory struggles (especially since I added more data). Here, describing that after lots of crashing, starting to use numpy append mode, here , to allow for doing preprocessing in chunks. And starting to look at target class distribution. Here I\u0026rsquo;m discovering that it is quite possible that this caching is only allowed w/ the \u0026ldquo;libsvm\u0026rdquo; format! And here, I see hmm it is kind of weird, that with cache, without\u0026hellip; producing different feature_names ? more kernel dying! Here , bigger box? Here , class distribution for size reduction and dataset rebalancing. Here ok I took the result from the balancing/shrinking concept from the \u0026ldquo;2020-06-29.ipynb\u0026rdquo; notebook and tried to use less data see if we can avoid a kernel crash that happened in \u0026ldquo;2020-06-28-take2.ipynb\u0026rdquo;. Here I wanted to do a quick recalc of yesterday\u0026rsquo;s model using the sklearn API Again. Here, more rebalancing. Here. Here, end result: compared with \u0026ldquo;2020-07-03-aws.md\u0026rdquo; , I am not really seeing much of a difference. the balanced test accuracy perhaps looks every so slightly better but probably not significantly. Here , change up 'subsample' and 'max_depth', measuring logloss, accuracy and balanced_accuracy , there are some noticable changes in logloss, but overall the changes are probably not significant. Here. Here. Here. Here. Multi class classification notes Notes on multi class classification Understanding tuning results hyper parameter tuning and train/test acc\nPreviously vs This time Last time around, I segmented the starting data into 24 hour-long segments. This time, I segmented time into only 5 bins to make the model slightly more generalizable. # time_of_day 0: 6-9, 1: 10-13, 2: 14-16, 3: 17-21, 4: 22-0, 0-5 Actually after picking these bins arbitrarily, I ended up also looking at the time of day histograms here and the peaks look close to what I had as a mental model in my mind. It might be interesting try some other bins at some point later.\nOne other new feature this time is the binary weekday feature, specifying weekday vs weekend.\nThe starting neighborhood one hot encoded was kept as an input.\nAlso last time around, the main model was a Random Forest classifier, but using XGBoost this time.\nModel Highlights The top model has these stats\u0026hellip;\n(pandars3) $ docker run -p 8889:8889 -p 8080:8080 -i -t -v $(pwd):/opt/program \\ -v ${MY_LOCAL_DATA_DIRECTORY}:/opt/data \\ -v ~/Downloads:/opt/downloads \\ -v $(pwd)/artifacts/2020-08-19T144654Z:/opt/ml \\ citibike-learn:latest \\ import fresh.predict_utils as fpu bundle = fpu.load_bundle_in_docker() In [7]: bundle[\u0026#39;model_bundle\u0026#39;][\u0026#39;bundle\u0026#39;][\u0026#39;validation_metrics\u0026#39;] Out[7]: {\u0026#39;accuracy\u0026#39;: 0.12171455130090014, \u0026#39;balanced_accuracy\u0026#39;: 0.10451301995291779, \u0026#39;confusion\u0026#39;: array([[415, 64, 4, ..., 0, 103, 69], [ 56, 541, 4, ..., 0, 130, 27], [ 23, 10, 136, ..., 0, 16, 130], ..., [ 2, 0, 2, ..., 1, 3, 36], [151, 222, 3, ..., 0, 260, 35], [ 84, 25, 46, ..., 0, 29, 861]]), \u0026#39;logloss\u0026#39;: 3.4335361255637977, \u0026#39;test\u0026#39;: \u0026#39;/home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-08T143732Z/test.libsvm\u0026#39;, \u0026#39;karea\u0026#39;: 0.760827309330065} More on the \u0026ldquo;k-area\u0026rdquo; metric is here Top Model\u0026rsquo;s Top Fscore features Extracting from this notebook ,\n.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } And it can be interesting to look at a random tree from xgboost too sometimes, again extracting from the above mentioned notebook.\nGluing everything together In this notebook, I face the challenges of taking the model from bundle to a demo site. There were a lot of challenges involved. My concept was to use the Google Static Map API to display the top neighborhood predictions. Hitting this API properly did take a little bit of time, but it was not that bad. And later on, I updated the whole AWS Lambda approach so the lambda function calls the API with the result from the dockerized SageMaker served model.\nAdmittedly, the most time consuming part was figuring out the API Gateway Cognito \u0026ldquo;Unauthenticated Authentication\u0026rdquo;. AWS has this Cognito service which manages user/password based authentication for you but it also lets you use Anonymous authentication. But there must be a lot of degrees of freedom in how this is used, because I could not find good documentation on how to set this up properly for my usecase at all.\nI had used API Gateway for authentication through CORS in the past and I recalled a bit of nuance that for example you may have setup CORS properly for 200 status codes, but if your program crashes with a 500 then your browser will scream about a CORS error, because the response is not returning the expected allow-origin-blah header. In the past this had taken me a while to figure out, but now I luckily had that knowledge in my back pocket. In any case, it is worth it for the serverless approach.\nAutomation made the process very convenient https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-local-docker-notes.md\nI also described my build process in the earlier mentioned glue notes too. With so many tweaks to the python side, the model and the javascript side, being able to build and deploy with quick make style commands made everything faster. I document some of these here too.\nquick pearson\u0026rsquo;s chi squared independence test quick pearson\u0026rsquo;s chi squared independence test https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-05.md\nLooking at hyperparameter tuning results ( EDIT: After writing the below section, I realized I had already here earlier, on 2020-07-24 , described some of these results already haha. Doing the work twice, forgetting what I had done. )\nI spent a bit of time on hyper parameter tuning, looking at the results, fixing some parameters two focus on two others at a time.\nSo per here , the num_round as expected improves logloss,\nkeep_fixed = { \u0026#39;max_depth\u0026#39;: 3, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;colsample_bylevel\u0026#39;: 0.1, \u0026#39;colsample_bynode\u0026#39;: 1, \u0026#39;colsample_bytree\u0026#39;: 0.1, \u0026#39;subsample\u0026#39;: 0.1, \u0026#39;num_round\u0026#39;: 10, } col1, col2, metric_col = \u0026#39;max_depth\u0026#39;, \u0026#39;num_round\u0026#39;, \u0026#39;logloss\u0026#39; fp.compare_tuning(df, feature_col_1=col1, feature_col_2=col2, metric_col=metric_col, keep_fixed=fvu.without( keep_fixed, keys=[col1, col2])) And maybe this is good as a sanity check, but more rounds take more time, ( per here )\nAnd from here it was interesting to see that walltime is stable mostly when it comes to learning rate except sometimes\u0026hellip;\nAnd per here at least per the fixed parameters, the 0.1 learning rate is better than the 0.01 learning rate.\nAnd per here , subsample row sampling is just not appearing to be influencing accuracy.\nAnd per here , the random column sampling may have just removed the good columns\nTrain and test accuracy comparison Here , I took all of my 1000+ models from earlier, (which were on S3 so I had to copy them locally for convenience) and calculated accuracy, logloss and karea metrics for the training data, in order to be able to get learning curves to understand underfitting/overfitting. Just showing ihere an example run for one model\u0026hellip; # As per https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-10-aws.md # the data dir was artifacts/2020-07-08T143732Z ... going to re-create that locally too # datadir = \u0026#39;/opt/program/artifacts/2020-07-08T143732Z\u0026#39; artifactsdir = \u0026#39;/opt/program/artifacts/2020-07-10T135910Z\u0026#39; train_results = [] train_loc = f\u0026#39;{datadir}/train.libsvm\u0026#39; dtrain = xgb.DMatrix(f\u0026#39;{train_loc}?format=libsvm\u0026#39;) actuals = dtrain.get_label() print(\u0026#39;evaluate using \u0026#39;, train_loc) train_data = load_svmlight_file(train_loc) X_train = train_data[0].toarray() y_train = train_data[1] %%time ######## # Try one i = 0 bundle = joblib.load(f\u0026#39;{artifactsdir}/{i}_bundle_with_metrics.joblib\u0026#39;) model = bundle[\u0026#39;xgb_model\u0026#39;] y_prob_vec = model.predict(dtrain) predictions = np.argmax(y_prob_vec, axis=1) logloss = fu.big_logloss(actuals, y_prob=y_prob_vec, labels= list(range(54))) acc = accuracy_score(actuals, predictions) balanced_acc = balanced_accuracy_score(actuals, predictions) correct_kth, karea = fm.kth_area(y_train, y_prob_vec, num_classes=54) CPU times: user 31.3 s, sys: 110 ms, total: 31.4 s Wall time: 21.4 s acc, balanced_acc, karea (0.05276320740101365, 0.03727538888502701, 0.6435250908504123) The whole thing, took about 10 hours as measured by the final line from tqdm,\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1052/1054 [10:00:57\u0026lt;01:08, 34.27s/it] Putting that together, Here putting that together ..\ntraining and test accuracy are pretty consistently close, with training accuracy being slightly better as expected. So there is no evidence overall of overfitting. But perhaps some evidence of underfitting .\nThe first thing I just looked at the parameters fixed by just what happened to be the first model built, so pretty arbitrary and comparing over the number of rounds. The results not unexpected not showing much learning happening.\nThen I took the model with the best test accuracy results,\nbest_params = dict(alldf.sort_values(by=\u0026#39;acc\u0026#39;).iloc[-1]) best_params {\u0026#39;train_acc\u0026#39;: 0.12693459297270465, \u0026#39;train_balanced_acc\u0026#39;: 0.11012147901980039, \u0026#39;i\u0026#39;: 755, \u0026#39;train_logloss\u0026#39;: 3.4301962566050057, \u0026#39;train_karea\u0026#39;: 0.76345208497788, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;objective\u0026#39;: \u0026#39;multi:softprob\u0026#39;, \u0026#39;num_class\u0026#39;: 54, \u0026#39;base_score\u0026#39;: 0.5, \u0026#39;booster\u0026#39;: \u0026#39;gbtree\u0026#39;, \u0026#39;colsample_bylevel\u0026#39;: 1.0, \u0026#39;colsample_bynode\u0026#39;: 1, \u0026#39;colsample_bytree\u0026#39;: 1.0, \u0026#39;gamma\u0026#39;: 0, \u0026#39;max_delta_step\u0026#39;: 0, \u0026#39;min_child_weight\u0026#39;: 1, \u0026#39;random_state\u0026#39;: 0, \u0026#39;reg_alpha\u0026#39;: 0, \u0026#39;reg_lambda\u0026#39;: 1, \u0026#39;scale_pos_weight\u0026#39;: 1, \u0026#39;seed\u0026#39;: 42, \u0026#39;subsample\u0026#39;: 0.4, \u0026#39;verbosity\u0026#39;: 0, \u0026#39;acc\u0026#39;: 0.12304248437307332, \u0026#39;balanced_acc\u0026#39;: 0.10551953202851949, \u0026#39;logloss\u0026#39;: 3.4480742986458592, \u0026#39;walltime\u0026#39;: 1918.593945, \u0026#39;karea\u0026#39;: 0.75845582462009, \u0026#39;num_round\u0026#39;: 100} And plotted all the train/test metrics across rounds, and this figure definitely shows learning happening . Very exciting!\nAlso looked for the biggest gap between train/test accuracy And per the below, interestingly, it\u0026rsquo;s seeming like the biggest train/test gap is very small.. alldf[\u0026#39;train_test_acc_delta\u0026#39;] = alldf.apply(lambda x: abs(x[\u0026#39;acc\u0026#39;] - x[\u0026#39;train_acc\u0026#39;]), axis=1) alldf.sort_values(by=\u0026#39;train_test_acc_delta\u0026#39;).iloc[-1] train_acc 0.128123 train_balanced_acc 0.111239 i 1241 train_logloss 3.40954 train_karea 0.767823 max_depth 5 learning_rate 0.1 objective multi:softprob num_class 54 base_score 0.5 booster gbtree colsample_bylevel 1 colsample_bynode 1 colsample_bytree 1 gamma 0 max_delta_step 0 min_child_weight 1 random_state 0 reg_alpha 0 reg_lambda 1 scale_pos_weight 1 seed 42 subsample 0.4 verbosity 0 acc 0.12253 balanced_acc 0.104698 logloss 3.43584 walltime 2327.88 karea 0.760578 num_round 100 train_test_acc_delta 0.00559313 Name: 1242, dtype: object Initial time of day look https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-05-woe.md\nsuper\ndiscuss https://github.com/namoopsoo/learn-citibike/blob/2020-oct/notes/2020-08-25-glue.md\nFeature importances notes\nFrom the many hyper parameter tuning jobs I had run, I used the xgboost feature importance functionality to dump the perceived feature importances for all of the models. And in the notes I plotted feature importances against accuracy for all of them.\nFor example, here are some of the more interesting plots,\nThe point here is that I had one hot encoded all of the starting neighborhoods. I am hoping of course that if a particular starting location looks important, then that should mean it is important in discriminating where you go next. Meaning it narrows down where you go. On the other hand, if your starting location is boring then that should mean it is more like a hub and there are too many destinations for the start along to be a helpful feature.\nIn the above plots, there is a wide range of models and they are showing that for some reason high importance does not necessarily mean high accuracy. If anything, I want to make a mental note that maybe these kinds of plots can be indicators of something wrong and some kind of under-fitting in particular. Or weak fitting at least. And one of the other scenarios is that fitting is weak, because there is not enough entropy in the data available to yield helpful discrimination with a model. No matter how well XGBoost can extract information, if the raw material does not have any diamonds, then we will be stuck.\nThe other thought is that there is an overfitting danger around not just an imbalance in the target variable (aka the destination neighborhood) but an imbalance in the starting locations too. This is why it would be really interesting to also look at the entropy of the multiclass outputs for signs of clear uncertainty for specific examples. Putting a pin on this in the follow-on section\nThe time of day features look like this, below, but again, this is not to say that these views represent the full story.\nThinking about this abit more in retrospect, these particular representations are probably not very meaningful to look at because if there are trends they need to be looked at \u0026ldquo;localizing\u0026rdquo; or \u0026ldquo;fixing\u0026rdquo; some of the parameters. Because these representations are all over the place but the relationship may still be hidden inside.\nI think one of the top follow ons has to be to find better time of day splits. I chose my time of day splits based on a model in my head, and so there is definitely some room for exploration here.\nFollow On Time of day more splitting exploration Find some more interesting techniques to try out different segmentation of the time of day. ( I\u0026rsquo;m thinking \u0026ldquo;adaptive binning \u0026quot; as described here )\nBetter understanding of model uncertainty As discussed in the feature importances section, it would be really interesting to take the test dataset and for the output probability vectors of all of the examples, to calculate the multi-class entropy, to see if indeed high uncertainty is associated with worse correctness rank (kth accuracy and karea in other terminology I have been using). Of course this is really tricky from an Active Learning point of view, because I can see a scenario where adding more training examples around the cases which have a higher uncertainty may improve the accuracy for the related test examples , but that feels like there is a risk of overfitting to the test set. In any case, however, if the live data is not reflective of the training/test data distributions ( covariate shift ), then refreshing the model is important. Some lessons for the future Approach to training and artifacts Training and hyperparameter tuning takes a long time. Dumping artifacts along the way, including models and results (for example using json), is helpful to allow another notebook to actively monitor the results as they are running. And doing this is also helpful because notebooks that run long experiments can sometimes crash. So it is nice to save intermediary results.\nNotebooks I like the concept of keeping a daily notebook, because keeping several experiments in one notebook can risk running out of memory and sometimes it is difficult to load large notebooks on github, even if they are turned into markdown, if there are a lot of images.\nWrite sooner rather than later Although it is tempting to just keep trying more and more experiments and to keep iterating the frontier forward, I think a difficult lesson to learn is that putting together the results of the day or the week takes much more time when done weeks or months later. I think summarizing and discussing your results as you go along is way more useful. But if you do wait, another idae is to just create a notebook table of concents as I am doing below, as a way of having quick chronological reference about the work that was done. Notebooks TOC 2020-07-10 , like \u0026ldquo;2020-07-09-aws\u0026rdquo; , another hyperparameter tuning round here. max_depth , subsample , colsample_bytree . 2020-07-11 , here I plot a bunch of results (on my laptop) , from the 2020-07-10 notebook running on aws. 2020-07-16-local.md , recalculataing train metrics for the ~1250 or so models from the hyper parameter tuning session 2020-07-26-feature-importances.md , looking at feature importances , reverse engineering my proc_bundle , to get back my list of feature names, which I had not done originally. Initially trying model.get_score() , dumping from each model. This actually took 3.5 hours. I plotted features and accuracy in a few ways to try to gauge features being more oftan associated with high accuracy models. Plotting the correlation of feature importance and acuracy. I think this was not a super useful method. Ultimately, the fscore approach was better 2020-08-05-woe.md , EDA on the time_of_day feature, visual histogram comparisons. Not the most fruitful however. 2020-08-17-bundle-glue.md 2020-08-18-glue.md some reverse engineering to repurpose my preprocessor bundle for live etraffic. And combining preprocessor and model to make a joblib bundle with everything in it. And drafint a full_predict method. 2020-08-22-static-map-api.md getting setup with the Google Static Map API . Very nice. 2020-08-25-glue.md Docker entry code end to end live code. And building the code for the lambda that calls Docker. Unfortunately xgboost does not fit on the lambda. And oops lambda cannot write to the file system. And working through the new API Gateway authentication methods here. I wrote some support code for quick lambda deployment because I ended up using many iterations to get this right. Content type weirdness. Javascript plus cognito. This was not documented very well, so a lot of blundering here. Can\u0026rsquo;t believe I finally made all of this work. This was insane. 2020-10-20-karea-worst.md K Area worst case scenario. 2020-10-21-look-at-model-plot.md looking at Fscore and as well as plotting individual trees with graphviz . Also some interesting issues with versions of xgboost in docker and lack of backward compatibility. 2020-10-21-uncertainty-xgboost.md this is mainly just a footnote about the idea around measuring uncertainty in xgboost. But this is likely not super reliable. 2020-10-22-features-v3.md Take a quick look at time of day distribution 2020-10-23-quick-new-v3-proc-bundle.md one more model iteration using new features. 2020-10-25.md evaluate new v3. But although not yet done any tuning, so far this does not seem significantly better, with karea 0.761 versus earlier best 0.760 karea. ","permalink":"https://michal.piekarczyk.xyz/project/2020-10-20-bike-share-learn-reboot/","summary":"This project is a reboot of an earlier project predicting bicycle ride share riders destinations.","title":"Bike Share Learn Reboot"},{"content":"This a mini post, part of this project\nTo explore some of the hyperparameter boundaries with this model, I ran some tests in these notebooks, here and here.\nAnd to get a better understanding of the overfitting-ness / underfitting-ness of models , here , I took the model artifacts and recalculated metrics on the training set, to generate some stats comparing training and testing performance.\nHere below, we see the effect of the row subsample ratio and tree max depth on test logloss. And also the effect on test accuracy. And the effect on balanced test accuracy. The tuning took several days to complete, but I started plotting early results in this notebook\nLearning rate had some drastic effects for sure!\nMaybe this one below did not have enough data points yet, but slightly surprising perhaps. I had expected the smaller learning rate to take longer.\nTrain / Test Comparisons Keeping a few parameters fixed, I started looking at train vs test accuracy across the number of rounds used during training,\nimport fresh.plot as fp keep_fixed = { \u0026#39;max_depth\u0026#39;: 3, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;colsample_bylevel\u0026#39;: 0.1, \u0026#39;colsample_bynode\u0026#39;: 1, \u0026#39;colsample_bytree\u0026#39;: 0.1, \u0026#39;subsample\u0026#39;: 0.1, #\u0026#39;num_round\u0026#39;: 10, } # alldf is the combined train+test metrics on model artifacts fp.compare_train_test(alldf, feature_col=\u0026#39;num_round\u0026#39;, metric_cols=[\u0026#39;acc\u0026#39;, \u0026#39;train_acc\u0026#39;], keep_fixed=keep_fixed) And sorting the metrics dataframe by test accuracy, I also plotted that,\nbest_params = dict(alldf.sort_values(by=\u0026#39;acc\u0026#39;).iloc[-1]) best_params {\u0026#39;train_acc\u0026#39;: 0.12693459297270465, \u0026#39;train_balanced_acc\u0026#39;: 0.11012147901980039, \u0026#39;i\u0026#39;: 755, \u0026#39;train_logloss\u0026#39;: 3.4301962566050057, \u0026#39;train_karea\u0026#39;: 0.76345208497788, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;objective\u0026#39;: \u0026#39;multi:softprob\u0026#39;, \u0026#39;num_class\u0026#39;: 54, \u0026#39;base_score\u0026#39;: 0.5, \u0026#39;booster\u0026#39;: \u0026#39;gbtree\u0026#39;, \u0026#39;colsample_bylevel\u0026#39;: 1.0, \u0026#39;colsample_bynode\u0026#39;: 1, \u0026#39;colsample_bytree\u0026#39;: 1.0, \u0026#39;gamma\u0026#39;: 0, \u0026#39;max_delta_step\u0026#39;: 0, \u0026#39;min_child_weight\u0026#39;: 1, \u0026#39;random_state\u0026#39;: 0, \u0026#39;reg_alpha\u0026#39;: 0, \u0026#39;reg_lambda\u0026#39;: 1, \u0026#39;scale_pos_weight\u0026#39;: 1, \u0026#39;seed\u0026#39;: 42, \u0026#39;subsample\u0026#39;: 0.4, \u0026#39;verbosity\u0026#39;: 0, \u0026#39;acc\u0026#39;: 0.12304248437307332, \u0026#39;balanced_acc\u0026#39;: 0.10551953202851949, \u0026#39;logloss\u0026#39;: 3.4480742986458592, \u0026#39;walltime\u0026#39;: 1918.593945, \u0026#39;karea\u0026#39;: 0.75845582462009, \u0026#39;num_round\u0026#39;: 100} # Taking the best params ^^ here is what the \u0026#34;learning curve\u0026#34; seems to look like keep_fixed = { \u0026#39;max_depth\u0026#39;: 4, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;colsample_bylevel\u0026#39;: 1.0, \u0026#39;colsample_bynode\u0026#39;: 1, \u0026#39;colsample_bytree\u0026#39;: 1.0, \u0026#39;subsample\u0026#39;: 0.4, #\u0026#39;num_round\u0026#39;: 10, } fp.compare_train_test(alldf, feature_col=\u0026#39;num_round\u0026#39;, metric_cols=[\u0026#39;acc\u0026#39;, \u0026#39;train_acc\u0026#39;, \u0026#39;balanced_acc\u0026#39;, \u0026#39;train_balanced_acc\u0026#39;], keep_fixed=keep_fixed) The difference between train accuracy and test accuracy does not look crazy so I think it is safe to say this model is not overfitting .\n","permalink":"https://michal.piekarczyk.xyz/post/2020-07-24-understanding-tuning-results/","summary":"Looking at hyperparameter tuning results","title":"Understanding Tuning Results"},{"content":"Summary Here is an early draft of a post, trying to extract some of the insights from the project here. There is a lot to write about and I want to just start getting it out.\nQuick outline The logloss upper bound Does the \u0026ldquo;k area\u0026rdquo; metric help? training balancing Is it possible to calculate the Bayesian error rate here? And logloss seems to be very sensitive. (can look at correlations , not super high) So what metric should be used ? And re-calc that train error so I can compare against test error to understand level of bias/variance The logloss upper bound Training set accuracy and test set accuracy have intuitive boundaries, between 0 and 1, but logloss does not feel intuitive.\nWhat are some theoretical bad logloss outcomes? How do I know model candidates are doing anything useful at all?\nIn an earlier notebook I calculated logloss w/ random data and got 4.29 , discussing random_logloss and uniform_logloss here . In general, logloss maybe it has a theoretical worst case basically . Filling equal likelihoods for all output probabilities gave a logloss of 3.98.\nMaking contrived probability vectors off by 1 class from the correct answers here yielded logloss of 34.538 regardless of which the wrong class was. So indeed logloss does not care about the order.\nAlso, in a different approach for a baseline logloss, in one of the earlier notebooks here , I created a model which just returned the softmax output of the destination tallies for all of the source neighborhoods. With a 5 fold cross validation, this produced validation logloss values of array([29.03426394, 25.61716199, 29.19083979, 28.312853 , 22.04601817]) which is somehow way worse than random! I think this shows that in general perhaps the usefulness of logloss is not great as an evaluation metric for a super large number of classes (in this case 54). This finding feels erratic.\nThe k area metric In the notebook here I have been evaluating some of the results of a multi day hyper parameter tuning session that has been running in here. ( First mini tuning session also here ).\nI started discussing this here.\nAlso, first started calculating some of this data in this notebook\nThe idea is if we want to compare two models predicting multi-class probabilities, in an ideal world, the accuracy will be good enough. But when the number of classes is really high (in this case 54), we will be looking at very low accuracies. And accuracy only looks at the top class. You can also look at the \u0026ldquo;Top k=5 accuracy\u0026rdquo;, or the \u0026ldquo;Top k=10 accuracy\u0026rdquo;, meaning whether the target class is in the top k=5 or top k=10 ranked probabilities.\nInstead you can create a single number between 0 and 1 by accumulating the ranked probabilities\nIf you plot, for all of the examples in a test set, what is the k required to get the correct answer you get this distribution,\nThis looks good in the sense the numbers are higher for lower k.\nAnd the cumulative distribution looks like this\nSo the k area metric is the area under this second curve. The fewer k we need to find the correct answer, the larger the area under this figure. This feels a bit more intuitive of a metric for evaluating classifiers with many many classes, at least compared to logloss!\nExtending notes on k-area Earlier notes on this metric are here The genesis was basically that logloss doesn\u0026rsquo;t care about the rank a model gives to each class probability. This metric is an idea of how to capture roughly how early in the output probabilities does a multi-class model capture the correct answer. The k-area metric is a number from 0 to (num_classes-1)/num_classes , which is the area under the curve of the \u0026ldquo;cumulative rank accuracy\u0026rdquo; , based on the rank of the correct predictions in a model\u0026rsquo;s multi-class probabilities. Here are some examples import pylab import matplotlib.pyplot as plt import numpy as np import fresh.metrics as fm import fresh.utils as fu def _plot(vec): plt.plot(vec) plt.title(f\u0026#39;cumulative accuracy (karea={vec.sum()})\u0026#39;) out_dir = \u0026#39;notes/2020-10-20-karea-worst__files\u0026#39; out_loc = f\u0026#39;{out_dir}/{fu.utc_ts()}_cumulative_accuracy.png\u0026#39; pylab.savefig(out_loc) pylab.close() return out_loc y_test = np.array([1, 2, 1, 0, 3]) # Worst possible, always splitting the predictions in the other classes y_prob = np.array([[.6, 0, .3, .1], [.5, .4, 0, 1], [.5, 0, .4, .1], [0, .5, .4, .1], [.5, .4, .1, 0],]) # correct_kth, topk, karea = fm.kth_area(y_test, y_prob, num_classes=4) _plot(topk) # Slightly better than worst... y_prob = np.array([[.6, .3, .1, 0], [.5, .5, 0, 0], [.5, 0, .5, 0], [0, .5, .5, 0], [.5, .5, 0, 0],]) # correct_kth, topk, karea = fm.kth_area(y_test, y_prob, num_classes=4) _plot(topk) # ([1, 3, 3, 3, 2], 0.15000000000000002) in the middle\u0026hellip; y_prob = np.array([[.4, .2, .1, .3], [.5, .4, .1, 0], [.5, 0, .5, 0], [.5, .1, .4, 0], [.4, .5, 0, .1],]) # correct_kth, topk, karea = fm.kth_area(y_test, y_prob, num_classes=4) _plot(topk) # ([2, 2, 3, 0, 2], 0.30000000000000004) near perfect y_prob = np.array([[.2, .4, .1, .3], [.1, .4, .5, 0], [0, .6, .4, 0], [.5, .1, .4, 0], [.1, .5, 0, .4],]) # correct_kth, topk, karea = fm.kth_area(y_test, y_prob, num_classes=4) _plot(topk) # ([0, 0, 0, 0, 1], 0.7) perfect y_prob = np.array([[.2, .4, .1, .3], [.1, .4, .5, 0], [0, .6, .4, 0], [.5, .1, .4, 0], [.1, .4, 0, .5],]) # correct_kth, topk, karea = fm.kth_area(y_test, y_prob, num_classes=4) _plot(topk) # ([0, 0, 0, 0, 0], 0.75) Effect of balancing training data .\nIn previous projects, training set balancing has been an important aspect of good dev/test set performance. Without balancing, highly imbalanced training sets end up producing classifiers that do disproportionately better with the majority or plurality classes.\nIn the note book \u0026ldquo;2020-06-29.md\u0026rdquo; notebook, I worked on a balancing/shrinking concept . Hopefully I can take these concepts and use them in the future as well. I tried to write this balancing code in a somewhat re-usable way.\nIn the \u0026ldquo;2020-07-03-aws.md\u0026rdquo; notebook, I also added some \u0026ldquo;shrinkage\u0026rdquo; because of my Jupyter kernel crashing. The other useful concept is how much data do we really need? Obviously if there is too much data and it crashes the notebook (as here for example ) , but I think this \u0026ldquo;balanced shrinkage\u0026rdquo; concept is interesting to explore just to be more efficient in for example use of hyper parameter tuning time. If you can perhaps \u0026ldquo;boil down\u0026rdquo; your data reducing its size by 50% and if the dev/test set error does not change much then in principle that can save a lot of hyper parameter tuning time, where you may be training/predicting hundreds of models.\nAnd here \u0026ldquo;2020-07-08-aws.md\u0026rdquo; I have another version of balancing that is less aggressive. The first iteration of balancing I was using sort of flipped the proportions. It dramatically (proportionally) weighed down the majority class (too much). This second iteration tries to just bring the plurality classes down closer to the \u0026ldquo;equal share\u0026rdquo; each class should get .\nBut surprisingly, in \u0026ldquo;2020-07-08-aws.md\u0026rdquo; the \u0026ldquo;balanced test accuracy\u0026rdquo; did not improve much.\ntest acc 0.12198962315156459 test balanced acc 0.1044572104146026 logloss 3.4794945441534866 Also in past projects I had balanced out a \u0026ldquo;groomed\u0026rdquo; test set myself but this time I just tried using balanced_accuracy_score from sklearn.\nI think visualizing the confusion is pretty interesting too in multiclass problems like this one from \u0026ldquo;2020-07-03-aws.md\u0026rdquo;, where I had noted the last class (bright yellow!) is sort of taking over the color spectrum of this data, because it is in the 1000 range but all the other data appears to be below 200.\nAnd as a proof of concept my confusion visualization from 2020-07-05-aws-two ,\nis showing the evidence of no balancing at all, because we see the classifier is focused on predicting basically one class, what looks like class 8 or 9.\nAnd the corresponding metrics for that classifier are\nlogloss 3.282076793024198 acc 0.15964601098390355 balanced acc 0.08281646671786597 which helps to show that when acc and balanced acc are far from each other, then the acc probably cannot be trusted.\nUltimately what is a good metric Because balanced acc and acc correlate so highly, the choice between those does not matter so much, as long as the input training data is somewhat balanced, since as we see in the above result, if acc is considerably higher than balanced acc then we probably even cannot trust the logloss. So perhaps checking that acc and balanced acc are close is a good \u0026ldquo;meta metric\u0026rdquo; at first.\nLogloss vs acc, that is an interesting choice.\nWith hyper parameter tuning, we can look at a lot of results and see how these all compare.\nI write about some hyper parameter tuning result here\nBut ultimately I think k area is a more granular measure than simple accuracy.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-07-13-multi-multi-class/","summary":"Summary Here is an early draft of a post, trying to extract some of the insights from the project here. There is a lot to write about and I want to just start getting it out.\nQuick outline The logloss upper bound Does the \u0026ldquo;k area\u0026rdquo; metric help? training balancing Is it possible to calculate the Bayesian error rate here? And logloss seems to be very sensitive. (can look at correlations , not super high) So what metric should be used ?","title":"Notes on multi-multi-class classifiers"},{"content":"Let\u0026rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.\nI have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.\nThis is mainly around navigating XGBoost.\nThere are two XGBoost APIs With the sklearn API you can\nimport xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, log_loss X, y = np.random.random(size=(1000, 3)), np.random.choice([1, 0], size=(1000,), replace=True) X_train, X_test, y_train, y_test = train_test_split(X, y) model = XGBClassifier().fit(X_train, y_train) y_pred_prob = model.predict_proba(X_test) y_pred = np.argmax(y_pred_prob, axis=1) acc = accuracy_score(y_test, y_pred) print(acc) And you can also just\nparams = {\u0026#39;max_depth\u0026#39;: 2, \u0026#39;eta\u0026#39;: 1, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;} dtrain = xgb.DMatrix(X_train, labels=y_train) dtest = xgb.DMatrix(X_test) bst = xgb.train(params, dtrain, num_round=2) y_pred = bst.predict(dtest) The sklearn API I believe uses 100 rounds/iterations/epochs (I have seen these used interchangeably) by default, and the xg.train method let\u0026rsquo;s you specify that. I am not sure how to override that w/ the sklearn wrapper.\nThe sklearn API starts you off with a bunch of default parameters that appear to be different than the raw parameters.\nLearning continuation is not a good method for batch learning I tried to split up my data into chunks and use this approach to batch learn..\ndef batch_train(X, y) parts = [range(1000), range(1000, 2000), range(2000, 3000)] prev_model_loc = None model = XGBClassifier() for i, part in enumerate(parts): model.fit(X[part], y[part], xgb_model=prev_model_loc) prev_model_loc = f\u0026#39;model.xg\u0026#39; model.save_model(prev_model_loc) return model With about half a million rows, training was doable in one go and it took maybe 7 minutes. But when I tried to \u0026ldquo;batch\u0026rdquo; this into 10k sections, this lasted for 8 hours . Luckily I was sleeping and kept my laptop on with Amphetamine, but indeed that was crazy.\nThis answer around pickling/unpickling seems to even say incremental learning with the sklearn API is not possible. So indeed I feel like I want to try it with the functional API.\nI am thinking the lesson is the data caching approach mentioned here for example, is the way to go, but I have not been able to find how to use it with the sklearn api.\nI tried that in my notebook here but my results were very different. I think this was something to do with a very different set of default parameters.\nOr something tells me using num_rounds=2 could have been the culprit. I would like to retry this with more rounds! (In next section I did try num_rounds=100 but that did not help )\nSomehow the caching feature is not mentioned in this blogpost .\nUsing the functional Xgboost api with caching seems to be hit or miss I did try the functional xgboost api w/ num_rounds=100 in this notebook , although it feels like something\u0026rsquo;s wrong. The verbose xgboost output looks like no learning is happening. Going to have to try to pick that apart. According to the parameters documentation , as far as the \u0026ldquo;tree construction algorithm\u0026rdquo; goes, \u0026ldquo;Experimental support for external memory is available for approx and gpu_hist.\u0026rdquo; for the tree_method parameters.\nLater on here I can see learning does happen as long as that \u0026ldquo;cache\u0026rdquo; feature is not used. Indeed very odd.\nParallelism One anecdote around parallelism. In these two notebooks, 2020-07-03-aws and 2020-07-04-aws, I used the same data and same xgboost parameters except in the first go I used the functional API and in the second go I used the sklearn API.\nAmazingly, the accuracy and logloss on the test set was exactly the same to several decimal places in the two cases (I passed seed=42 in both cases as a parameter but I didn\u0026rsquo;t expect such a high level of determinism!).\nThe walltime difference was 4min 18s vs 49min 6s ! (What a difference multithreading makes!)\nThe first case used 2 threads. I didn\u0026rsquo;t actually set the nthread parameter, but I read in the docs it defaults to the max. The sklearn doc seems to show n_jobs is the equivalent parameter here, but it does not appear to describe the default.\nNicely, the magic func %%time shows the parallelization as\nCPU times: user 8min 24s, sys: 1.24 s, total: 8min 26s Wall time: 4min 18s vs\nCPU times: user 49min 10s, sys: 1.15 s, total: 49min 11s Wall time: 49min 6s Side note In the multiclass problem I have here, this is really making me think of the difference between winner take all political elections vs proportional representation elections, as binary vs multi-label problems. ","permalink":"https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/","summary":"Let\u0026rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.\nI have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.\nThis is mainly around navigating XGBoost.\nThere are two XGBoost APIs With the sklearn API you can\nimport xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn.","title":"Some xgboost notes so far"},{"content":"Not sure yet. There are a fitness trackers out there, but I am curious if my chest band can help. I took a quick look at one of my recordings on the Wahoo app, but I don\u0026rsquo;t see anything more granular than just beats per minute. The app indeed has pushed to Apple Health, but I only see the bpm data and no HRV data.\nDidnt finish this but I tried parsing the raw fit file Making a note for later I suppose. I git cloned fit_processing , which requires installing fitparse But when I ran python3 process_all.py --subject-name=mysubjectname --fit-source-dir=/media/myname/GARMIN/Garmin/ACTIVITY/ I got doing FIT conversions activity files: [\u0026#39;2020-02-06-160620-FITNESS 3A6A-20-0.fit\u0026#39;] new names: [\u0026#39;2020-02-06-160620-FITNESS 3A6A-20-0.fit\u0026#39;] current_files: {\u0026#39;2020-02-06-160620-FITNESS 3A6A-20-0.fit\u0026#39;} 2020-02-06-160620-FITNESS 3A6A-20-0.fit already exists... converting ....../fit_files/2020-02-06-160620-FITNESS 3A6A-20-0.fit ... File \u0026#34;/...../fit_processing/process_all.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; import os ... File \u0026#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/fitparse/records.py\u0026#34;, line 450, in add_dev_field_description field_def_num = message.get(\u0026#39;field_definition_number\u0026#39;).raw_value AttributeError: \u0026#39;NoneType\u0026#39; object has no attribute \u0026#39;raw_value\u0026#39; And I got the same result when trying the raw docs here for fitparse itself. To be fair those docs appear to be python2.7 docs. Of course also possible that not all .fit files, Wahoo or Garmin, have the same kinds of data. Maybe fitparse was tested only on Garmin .fit files? ","permalink":"https://michal.piekarczyk.xyz/post/2020-06-06-heart-datar/","summary":"Not sure yet. There are a fitness trackers out there, but I am curious if my chest band can help. I took a quick look at one of my recordings on the Wahoo app, but I don\u0026rsquo;t see anything more granular than just beats per minute. The app indeed has pushed to Apple Health, but I only see the bpm data and no HRV data.\nDidnt finish this but I tried parsing the raw fit file Making a note for later I suppose.","title":"Does the Wahoo TIKR measure intervals that can be used for Heart Rate Variability measurements?"},{"content":"Summary I tried rebuilding today after 7 months of forgetting how this works and when I tried to jekyll build I got\n$ jekyll build Traceback (most recent call last): ... /usr/local/Cellar/ruby/2.6.5/lib/ruby/2.6.0/bundler/lockfile_parser.rb:108:in `warn_for_outdated_bundler_version\u0026#39;: You must use Bundler 2 or greater with this lockfile. (Bundler::LockfileError) Per this answer, I ran gem list bundler to see my rubygems version and then gem update --system\n$ gem list bundler *** LOCAL GEMS *** bundler (2.0.2, 1.17.2) $ gem update --system Updating rubygems-update Fetching rubygems-update-3.1.3.gem Successfully installed rubygems-update-3.1.3 ... ... RubyGems installed the following executables: /usr/local/Cellar/ruby/2.6.5/bin/gem /usr/local/Cellar/ruby/2.6.5/bin/bundle RubyGems system software updated Now I get $ gem list bundler *** LOCAL GEMS *** bundler (default: 2.1.4, 2.0.2, 1.17.2) $ And now my jekyll build worked! ","permalink":"https://michal.piekarczyk.xyz/post/2020-05-30-trying-again/","summary":"Summary I tried rebuilding today after 7 months of forgetting how this works and when I tried to jekyll build I got\n$ jekyll build Traceback (most recent call last): ... /usr/local/Cellar/ruby/2.6.5/lib/ruby/2.6.0/bundler/lockfile_parser.rb:108:in `warn_for_outdated_bundler_version\u0026#39;: You must use Bundler 2 or greater with this lockfile. (Bundler::LockfileError) Per this answer, I ran gem list bundler to see my rubygems version and then gem update --system\n$ gem list bundler *** LOCAL GEMS *** bundler (2.","title":"Updates"},{"content":"Summary Below, I write a bit retrospectively about my notes from the \u0026ldquo;Reducing Commercial Aviation Fatalities\u0026rdquo; kaggle, trying to summarize some of the journey. I try to give some high lights from my various notebook entries. ( github )\nMy High Level Recap This physiology data classification challenge poses the question, given this time series voltage data of pilots\u0026rsquo; respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain data) can something reliable be said about their physiological state? My response was to use this as an opportunity to learn about TensorFlow and LSTMs. I quickly discovered that data processing around time series data is 3 dimensional as opposed to typical 2 dimensional data. That means that the harmless 1.1 GiB of training data can quickly multiply to roughly 256 GiB if one is interested in using a 256 long sequence window. That means I learned a lot more about numpy for its simplicity around transforming 3 dimensional data. I had to adapt to using h5py chunks of data so as not to run out of memory quickly and not wait endless hours for training sessions to merely crash. As for TensorFlow and LSTMs, I did not realize right away but LSTMs (and likely neural nets in general) are quite sensitive to data that is not scaled and my logloss ended up reducing when I applied scaling techniques. Raw matplotlib became more intuitive for helping to visualize not just the time series data itself, but also for plotting logloss while training, across batches . . My Jupyter Notebook hygiene and workflow got better really quickly too, because I needed a reliable tool for distinguishing one day\u0026rsquo;s experiment from another\u0026rsquo;s without mixing the together the data or the models.\nThis dataset was highly skewed in terms of classes and so one of the really important preprocessing tasks was creating both balanced training \u0026ldquo;techniques\u0026rdquo; and balanced test data for more uniformly judging model performance. And I say \u0026ldquo;techniques\u0026rdquo;, because I tried both balanced weights and balanced training sets. I ended up preferring balanced training datasets, because that meant less preprocessing code.\nThe picture of Kaggle data I had in my head was clean datasets, but this dataset had one huge problem in that the time series data was not actually sorted by, you know, time. But in a way it is always fun to deal with messy data because it makes you more engaged with it and still more curious in the outcomes.\nIn general this project has given me a lot of fun memories.\nOne day after already getting deep into my LSTM approach, I decided to look through the Kaggle discussions for this project and I found that most people actually stuck to gradient boosting machines like lightGBM or XGBoost. But I decided to follow my personal motto of taking the path less traveled so I kept going with the LSTM .\nI have spent I think half a year of weekends on this problem. I have memories of learning about neural network architecture learning \u0026ldquo;capacity\u0026rdquo; at my niece\u0026rsquo;s birthday party. I came to understand that creating a larger network can cause it to memorize more as opposed to generalize .\nI remember tweaking my stochastic gradient descent batch size after reading this Yann LeCun tweet , \u0026ldquo;Training with large minibatches is bad for your health. More importantly, it\u0026rsquo;s bad for your test error. Friends dont let friends use minibatches larger than 32.\u0026rdquo; .\nI also have memories of starting modeling experiments before going on runs and before going to sleep, so that I could let TensorFlow spin its wheels while I took my mind into strategy mode or just let myself meditate.\nAt one point I was at the Boston Amtrak terminal waiting for my bus, getting deeper into why it is handy to look at raw logit data coming out of a model, especially in a multiclass problem because it can show how strongly a model classifies each class. But applying the logistic function or a softmax is of course good for sussing out probabilities. But then I realized I was waiting for a bus at an Amtrak terminal and I had to sprint several blocks to actually catch my bus!\nAt the end of the day I think of all of the amazing things I could one day do with this kind of technology, such as classifying music or building a chat bot (maybe even one that can tell jokes).\nTable of Contents Quick intro to the data The data is weirdly partially sorted Trickiness of the how the data is laid out (crews and seats?!) Some more visual inspection Building datasets Scaling Shuffling and adjusting dropout More epochs? Weight initialization Class balance Active Learning: changing the training approach Full training set error Shuffling train/test Reconsider that high dropout Prediction Speedup Quick intro to the data The physiological data includes several types (including respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain data)) across multiple \u0026quot;crews\u0026quot; . A crew includes two \u0026quot;seats\u0026quot; (0 and 1). We are provided with 256 measurements per second across three experiments (Channelized Attention (CA) , Diverted Attention (DA) and Startle/Surprise (SS) ). Across the three experiments, four target \u0026ldquo;states\u0026rdquo; (or classes) are labeled for all of the rows in the data.\nThe four classes of events in the training data, 'A', 'B', 'C', 'D' , correspond to three target physiological states of the three experiments, plus a neutral baseline state:\nlabel description A Baseline B Startle/Surprise C Channelized Attention D Diverted Attention In my early notebook I took a quick look at the proportion of experiment time had been spent in the different states. Per the below, looking at each person separately, I saw that for the first person as an example, (crew=1, seat=0) , ~98% of CA was labeled as \u0026ldquo;Channelized Attention\u0026rdquo; and 2% as baseline. But for the other two experiments, the target states appear to be much more brief, with only 13% of the \u0026ldquo;DA\u0026rdquo; and 9% of the \u0026ldquo;SS\u0026rdquo; experiments.\n(copying an output from that notebook\u0026hellip;)\nstatsdf = gpdf.groupby(by=[\u0026#39;crew\u0026#39;, \u0026#39;seat\u0026#39;]).apply(extract_proportions).reset_index() In [78]: statsdf Out[78]: crew seat A/CA A/DA A/SS C/CA D/DA B/SS 0 1 0 0.018593 0.868871 0.903015 0.981407 0.131129 0.096985 1 1 1 0.018803 0.879030 0.902685 0.981197 0.120970 0.097315 2 2 0 0.001954 0.857941 0.916630 0.998046 0.142059 0.083370 3 2 1 0.001781 0.848619 0.916605 0.998219 0.151381 0.083395 4 3 0 0.001248 0.854135 0.916782 0.998752 0.145865 0.083218 5 3 1 0.000597 0.860974 0.916772 0.999403 0.139026 0.083228 6 4 0 0.001302 0.868853 0.916514 0.998698 0.131147 0.083486 7 4 1 0.001400 0.860968 0.916706 0.998600 0.139032 0.083294 8 5 0 0.001661 0.847193 0.916730 0.998339 0.152807 0.083270 9 5 1 0.001791 0.857766 0.916472 0.998209 0.142234 0.083528 10 6 0 0.002311 0.860514 0.916711 0.997689 0.139486 0.083289 11 6 1 0.001661 0.858872 0.916748 0.998339 0.141128 0.083252 12 7 0 0.001563 0.867075 0.916536 0.998437 0.132925 0.083464 13 7 1 0.001607 0.855907 0.916471 0.998393 0.144093 0.083529 14 8 0 0.000999 0.856505 0.915394 0.999001 0.143495 0.084606 15 8 1 0.001053 0.853877 0.915412 0.998947 0.146123 0.084588 16 13 0 0.001801 0.841341 0.916482 0.998199 0.158659 0.083518 17 13 1 0.001628 0.847312 0.916595 0.998372 0.152688 0.083405 The data is weirdly partially sorted As I mention in this notebook, when indexing on what I believe are the main uniqueness constraint columns (crew, seat, experiment), the time is not sorted. There are strange jumps, such as this one which I borrow from my notebook .. I show the first three rows (0, 1, 2) and also where I see the jump starting from around 6600, where row 6606 goes back in time.\npd.concat([ df[[\u0026#39;crew\u0026#39;, \u0026#39;seat\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;experiment\u0026#39;, \u0026#39;event\u0026#39;]].iloc[:3], df[[\u0026#39;crew\u0026#39;, \u0026#39;seat\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;experiment\u0026#39;, \u0026#39;event\u0026#39;]].iloc[6600:6610]]) crew\tseat\ttime\tr\texperiment\tevent 0\t1\t1\t0.011719\t817.705994\tCA\tA 1\t1\t1\t0.015625\t817.705994\tCA\tA 2\t1\t1\t0.019531\t817.705994\tCA\tA 6600\t1\t1\t109.988281\t817.437988\tCA\tC 6601\t1\t0\t109.988281\t664.265991\tCA\tC 6602\t1\t0\t109.992188\t664.265991\tCA\tC 6603\t1\t1\t109.992188\t817.442017\tCA\tC 6604\t1\t1\t109.996094\t817.442017\tCA\tC 6605\t1\t0\t109.996094\t664.265991\tCA\tC 6606\t1\t0\t11.000000\t664.331970\tCA\tC 6607\t1\t1\t11.000000\t817.898987\tCA\tC 6608\t1\t0\t11.003906\t664.331970\tCA\tC 6609\t1\t1\t11.003906\t817.898987\tCA\tC I sort this data myself, but it is confusing for sure.\nTrickiness of the how the data is laid out (crews and seats?!) In the process of visualizing data, I had been using matplot lib to visualize the four different classes of events, 'A', 'B', 'C', 'D' as red, green, blue and cyan. That way I could potentially try to get an intuition around the visual cues around state transitions. But at one point I had accidentally been combining the data of multiple people.\nAbove, extracting a plot from my 2019-10-26 notebook, is an example of where I plot combined multi seat data by accident. At one point this was weirding me out. But I realized finally that I had been combining the data of multiple people.\nFor diagram above (^^) , I had written a quick function produce_plots_for_col for plotting four features simultaneously, given a pandas dataframe, some features and an interval, but indeed the zig zag plot was a bit baffling for a bit.\nstart = 3400; produce_plots_for_col(df, [\u0026#39;r\u0026#39;, \u0026#39;ecg\u0026#39;, \u0026#39;gsr\u0026#39;, \u0026#39;eeg_fp1\u0026#39;], range(start,start+150)) When I look back at my notebook I wrote about how I was very dumbfounded when I realized I combined them by accident. The data is complicated however. It includes four indexing columns: id , time , crew and seat . And indeed being careful with splitting this was key in creating good datasets.\nSome more visual inspection in my 2019-06-08 notebook I used another nice quick visual inspection technique, looking at some time series data samples at the four classes, Building datasets I spent a lot of time next building data sets, here , and , and building basic quick and dirty LSTM tensor flow models. And also . I also tried different approaches for understanding the models I was building. Including looking at raw logits, as per the below graphic, from my 2019-07-13-Four notebook. I thought this was a cool method compared to a confusion matrix for instance is because it shows the raw logits of each of the four classes, before the argmax voting observed in a confusion matrix is done.\nScaling I took a deeper histogram look at my data, seeing quite a lot of ups and downs.\n(Given that there were some crazy jumps, I thought I needed to do something about that) And so on 2019-12-21 , I ended up trying out more scaling approaches, especially MinMaxScaler. I had 8 features I was focusing on at that point and I plotted how my minMaxScaler min and max parameters changed as I processed roughly 40 or so mini datasets I had in my h5 training file data/2019-12-21T215926Z/train.h5. Re-posting my image :\nLuckily I found I was able to use just a single sklearn MinMaxScaler object to capture all 8 features at once. I then applied the scalers to transform my train.h5 data to a train_scaled.h5 dataset. And I also ended up with a balanced dataset , train_balanced.h5, that I could use for training.\nI trained a model and plotted training and validation loss curves the next day .\nAnd wow the validation loss ( link ) looked intense ,\nAs a side note. although the validation loss here looks totally skewed towards class 1 , I want to step back and note I really appreciate the technique of actually creating the \u0026ldquo;balanced\u0026rdquo; test set I referred to above. That allows us to quickly knows the model is favoring one class over another in the first place. And also I really dig the technique of simply snapshotting the tensorflow models while training and then being able to know how the validation logloss looks across those training batches. I feel like combining these techniques was really helpful in digesting what is going on . I needed to enjoy little details like that amidst all of the trial and error that was happening here (Emphasis on the error part haha).\nShuffling and adjusting dropout At a later date , I adjusted my lstm dropout from 0.2 to 0.7 , seeing quite different behavior in the validation loss. I had also added some shuffling code taking my 'history/2019-12-22T174803Z/train_balanced.h5' dataset to produce 'history/2019-12-22T174803Z/train_scaled_balanced_shuffled.h5' , to possibly change some of the choppiness of the validation curve seen above ^^ . That produced a validation loss , reposting the image here,\nMore epochs? On 2018-12-28 I got curious about whether just throwing more data at this problem would help. So I extended my waiting time by two and let the training happen in two epochs . The validation loss from here , (reposting\u0026hellip;) however showed that throwing more data is not always the answer. It always depends haha.\nWeight initialization Per my notebook entry I had read per this article that the default tensor flow weight initialization I had been using was GlorotUniform , ( which is aka Xavier Uniform apparently ) . I realized it was at least worth considering weight initialization as another hyper parameter so here I tried the Glorot or Xavier Normal instead . The validation loss did not necessarily convey the difference however:\nAt this point I think I was realizing that the order of ideas to try matters. And you do not know in advance what is the best order. Perhaps the weight initialization matters a good deal, but I had not yet found the critical next step yet at that point.\nClass balance In my next notebook I wanted to understand why my class 1 kept getting favored. I tried out forcing the weights of my training data to basically\n{0: 1., 1: 0., 2: 0., 3: 0.} to see what happens and sure enough, per the validation loss , the loss now went down only for class 0. So the effect was controlled.\nActive Learning: changing the training approach Somehow I came upon the idea of preferentially training on what your model is doing poorly on. So on 2020-01-19 I modified my training loop so that I dynamically adjusted my training weights according to which class was being misclassified. The effect on the training loss was really interesting. Everything was way smoother.\nLooking at a training loss plot from earlier ( such as from 2019-12-28 )\nhttps://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png\nThe validation loss was still favoring that one class, but I decided to hold on to this technique and keep trying other things.\nFull training set error Next in this notebook I wanted to better answer whether my particular test set perhaps had some very different size data compared to my training set, which was blowing up my test set error. I did not have enough data to better split apart my data at the moment actually, but instead I took a quick detour to compare my training mini batch loss curves to the full training set losses, during training. Naturally one would expect that if batch training losses improve that overall training set loss should also improve. Per the below diagram from my notebook, that was indeed the case.\nShuffling train/test After having consistently weird results with validation error, I decided to try re-building my train/test sets by doing a full random shuffle instead, in my 2020-02-01 notebook. Up until this point I had been using the fact that the data is divided into crew1, crew2, crew3, etc and I have used crew1 for train and crew2 for test. And I had built scalers from my crew1 training data, applying them to the the crew2 test data.\nSo this time around I instead built scalers from crew1 and then changed my function, build_many_scalers_from_h5 to take scalers as a parameter and I kept updating them with the test data. ( My scalers 'history/2020-02-02T044441Z/scalers.joblib' was the restulting artifact).\nIn validation , I think for the first time, I saw the validation error actually start going down ,\nI took that further in this 2020-02-08 notebook and ( showing my figure again ) going 3 epochs instead, I got ..\nSo the bright side I take from this is that the validation loss is actually doing better for three out of four of the classes.\nReconsider that high dropout Next in 2020-02-15 notebook, I decided to reduce my dropout slightly, after reading through this post about treating the dropout as yet another hyperparameter. After retraining , across 2 epochs, I saw a validation loss curve, which looked better still.\nThis indicates perhaps the context of hyperparameters being experimented with indeed matters . I think Andrew Ng\u0026rsquo;s characterization of \u0026ldquo;model babysitting\u0026rdquo;.\nhttps://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md\nPrediction Speedup At this point I had improved my validation results enough that I wanted to submit my predictions. But as I described in my notebook , the 17.9 million test examples would take potentially 25 hours per my back of the envelope calculation.\nBut luckily I discovered that just playing with my prediction batch size, changing it from 32 to 1024, I found I could cut my time from 56k examples/292 seconds to 56k examples/27s , taking the back of the envelope calculation from 25 hours to 2.5 hours !..\nI also ended up utilizing awk to actually build batches for predict, avoiding trying to squeeze everything into memory.\nAnd for one final optimization, I added multi-processing with joblib , to take advantage of all of my available cores.\nThe steps get more detailed in this notebook.\n","permalink":"https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/","summary":"learning physiological state from time series data","title":"Physiological deep learnings"},{"content":"Here I\u0026rsquo;m doing one of the Keras hello world micro projects, involving classification of tiny 28x28 wardrobe images. This was really fun. ( Here\u0026rsquo;s the original home for the python notebook). One really fun part of this was that at the end I hand drew my own clothing samples, paired them down using the python PIL library and threw them against the classifier. Surprisingly, the mini model performed really well. That\u0026rsquo;s some generalizability!\nAlso this was a really nice expansion of my existing matplotlib knowledge. The grid plotting protips here were a great addition to my repertoire.\nfrom __future__ import absolute_import, division, print_function, unicode_literals # TensorFlow and tf.keras import tensorflow as tf from tensorflow import keras # Helper libraries import numpy as np import matplotlib.pyplot as plt print(tf.__version__) 1.13.1 fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 2s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 0s 0us/step class_names = [\u0026#39;T-shirt/top\u0026#39;, \u0026#39;Trouser\u0026#39;, \u0026#39;Pullover\u0026#39;, \u0026#39;Dress\u0026#39;, \u0026#39;Coat\u0026#39;, \u0026#39;Sandal\u0026#39;, \u0026#39;Shirt\u0026#39;, \u0026#39;Sneaker\u0026#39;, \u0026#39;Bag\u0026#39;, \u0026#39;Ankle boot\u0026#39;] plt.figure() plt.imshow(train_images[3]) plt.colorbar() plt.grid(False) plt.show() # Hmm so this `cmap=plt.cm.binary` kwarg displays grayscale instead of that strange purple to yellow scale. plt.figure() plt.imshow(train_images[3]/255.0, cmap=plt.cm.binary) plt.colorbar() plt.grid(False) plt.show() # This is almost like the output of a TSA luggage xray scanner plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i]) # , cmap=plt.cm.binary plt.xlabel(class_names[train_labels[i]]) plt.show() train_images_scaled = train_images / 255.0 test_images_scaled = test_images / 255.0 model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax) ]) WARNING:tensorflow:From /usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;sparse_categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) # Looking at the use of the non-scaled data first. # Wow that looks like terrible accuracy. model.fit(train_images, train_labels, epochs=5) Epoch 1/5 60000/60000 [==============================] - 4s 70us/sample - loss: 14.5146 - acc: 0.0995 Epoch 2/5 60000/60000 [==============================] - 4s 66us/sample - loss: 13.7328 - acc: 0.1479 Epoch 3/5 60000/60000 [==============================] - 4s 66us/sample - loss: 13.0450 - acc: 0.1906 Epoch 4/5 60000/60000 [==============================] - 4s 66us/sample - loss: 13.0474 - acc: 0.1905 Epoch 5/5 60000/60000 [==============================] - 4s 68us/sample - loss: 12.9979 - acc: 0.1936 \u0026lt;tensorflow.python.keras.callbacks.History at 0x139540978\u0026gt; # Yea if that is out of 1.0 then this 0.1917 is pretty low test_loss, test_acc = model.evaluate(test_images, test_labels) print(\u0026#39;Test accuracy:\u0026#39;, test_acc) 10000/10000 [==============================] - 0s 26us/sample - loss: 13.0283 - acc: 0.1917 Test accuracy: 0.1917 # Try on that scaled data now .. # Okay this looks better. more like the result in the tutorial. model.fit(train_images_scaled, train_labels, epochs=5) Epoch 1/5 60000/60000 [==============================] - 4s 68us/sample - loss: 0.5902 - acc: 0.8182 Epoch 2/5 60000/60000 [==============================] - 4s 66us/sample - loss: 0.3900 - acc: 0.8622 Epoch 3/5 60000/60000 [==============================] - 4s 67us/sample - loss: 0.3526 - acc: 0.8739 Epoch 4/5 60000/60000 [==============================] - 4s 66us/sample - loss: 0.3319 - acc: 0.8788 Epoch 5/5 60000/60000 [==============================] - 4s 66us/sample - loss: 0.3141 - acc: 0.8858 \u0026lt;tensorflow.python.keras.callbacks.History at 0x1424e8fd0\u0026gt; model.input_shape, model.output_shape ((None, 28, 28), (None, 10)) model.weights [\u0026lt;tf.Variable 'dense/kernel:0' shape=(784, 128) dtype=float32\u0026gt;, \u0026lt;tf.Variable 'dense/bias:0' shape=(128,) dtype=float32\u0026gt;, \u0026lt;tf.Variable 'dense_1/kernel:0' shape=(128, 10) dtype=float32\u0026gt;, \u0026lt;tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32\u0026gt;] test_loss, test_acc = model.evaluate(test_images_scaled, test_labels) print(\u0026#39;Test accuracy:\u0026#39;, test_acc) 10000/10000 [==============================] - 0s 34us/sample - loss: 0.3489 - acc: 0.8740 Test accuracy: 0.874 predictions = model.predict(test_images_scaled) predictions[0], test_labels[0] (array([1.3787793e-05, 5.9650089e-09, 1.9482790e-07, 1.8630770e-09, 3.6141572e-07, 3.6579393e-02, 1.0138750e-05, 1.5758899e-01, 1.9775856e-04, 8.0560941e-01], dtype=float32), 9) def plot_image(i, predictions_array, true_label, img): predictions_array, true_label, img = predictions_array[i], true_label[i], img[i] plt.grid(False) plt.xticks([]) plt.yticks([]) plt.imshow(img, cmap=plt.cm.binary) predicted_label = np.argmax(predictions_array) if predicted_label == true_label: color = \u0026#39;blue\u0026#39; else: color = \u0026#39;red\u0026#39; plt.xlabel(\u0026#34;{} {:2.0f}% ({})\u0026#34;.format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label]), color=color) def plot_value_array(i, predictions_array, true_label): predictions_array, true_label = predictions_array[i], true_label[i] plt.grid(False) plt.xticks([]) plt.yticks([]) thisplot = plt.bar(range(10), predictions_array, color=\u0026#34;#777777\u0026#34;) plt.ylim([0, 1]) predicted_label = np.argmax(predictions_array) thisplot[predicted_label].set_color(\u0026#39;red\u0026#39;) thisplot[true_label].set_color(\u0026#39;blue\u0026#39;) i = 0 plt.figure(figsize=(6,3)) plt.subplot(1,2,1) plot_image(i, predictions, test_labels, test_images) plt.subplot(1,2,2) plot_value_array(i, predictions, test_labels) plt.show() thisplot = plt.bar(range(5), [.1, .2, .3, .4, .9], color=\u0026#34;#777777\u0026#34;) thisplot[2].set_color(\u0026#39;orange\u0026#39;) # Plot the first X test images, their predicted label, and the true label # Color correct predictions in blue, incorrect predictions in red num_rows = 5 num_cols = 3 num_images = num_rows*num_cols plt.figure(figsize=(2*2*num_cols, 2*num_rows)) for i in range(num_images): plt.subplot(num_rows, 2*num_cols, 2*i+1) plot_image(i, predictions, test_labels, test_images) plt.subplot(num_rows, 2*num_cols, 2*i+2) plot_value_array(i, predictions, test_labels) plt.show() # For fun, I want to try and run the model on my own hand drawn images. # Raw im import IPython.display as ipd # import Image # These four pngs were created w/ Adobe sketchbook # Then I manually cropped them with macOS Preview to make sure they are squares. rawnames = [\u0026#39;2019-05-13 14.29.04.png\u0026#39;, \u0026#39;2019-05-13 14.30.20.png\u0026#39;, \u0026#39;2019-05-13 14.31.18.png\u0026#39;, \u0026#39;2019-05-13 14.32.27.png\u0026#39;] filenames = [ \u0026#39;keras-fashion-helloworld/myimages-originals/\u0026#39; + x for x in rawnames ] [ipd.display(ipd.Image(filename=fn)) for fn in filenames] # ipd.Image(filename=\u0026#39;keras-fashion-helloworld/myimages-originals/2019-05-13 14.29.04--8.png\u0026#39;) [None, None, None, None] # with the help of PIL, obtained with `pip install Pillow` from PIL import Image def extract_vec(img): # re-scale to 28x28 in place img.thumbnail((28, 28), Image.ANTIALIAS) values = list(img.getdata()) pixels = np.array([x[0] for x in values]) return np.resize(pixels, (28, 28)) # quick example of first image, print(extract_vec(Image.open(filenames[0]))) [[254 250 240 238 242 255 255 252 247 232 226 226 228 229 228 229 228 234 244 246 245 247 248 255 255 254 255 255] [244 223 195 187 170 143 137 141 139 155 172 171 185 194 190 186 183 189 121 101 110 113 104 147 205 242 249 255] [226 195 148 86 26 13 29 52 65 78 71 89 103 114 115 104 92 74 61 99 103 99 82 58 67 148 224 248] [202 117 52 47 53 61 86 88 91 96 91 84 79 79 75 74 78 71 111 120 122 129 115 99 94 79 109 218] [104 59 84 97 67 56 54 50 65 68 72 82 89 94 93 97 115 109 125 115 92 94 82 69 70 89 89 112] [ 89 139 127 79 58 53 61 48 55 63 52 51 59 63 64 69 79 86 110 119 91 75 79 71 69 72 110 138] [159 77 115 106 76 68 74 57 53 55 40 41 52 58 57 60 65 68 88 101 97 100 92 84 94 98 107 117] [213 149 62 100 97 76 70 58 61 84 80 43 56 67 62 53 60 55 57 70 94 108 107 112 107 119 90 148] [216 198 126 43 102 100 62 69 84 88 102 59 57 68 58 55 64 66 64 61 88 98 104 137 131 99 115 179] [248 226 208 110 61 62 47 55 86 69 55 57 64 67 57 49 67 82 76 66 87 87 67 106 129 108 169 199] [255 254 249 214 51 89 159 94 69 57 43 45 58 53 37 33 44 48 51 50 68 74 125 158 71 130 212 231] [255 255 255 255 212 165 223 91 51 45 42 39 40 39 45 47 37 41 42 41 58 63 136 235 209 200 236 252] [255 255 255 255 255 242 189 69 53 59 52 37 27 35 58 59 47 45 36 34 55 60 139 237 242 248 252 254] [255 255 255 254 254 247 157 61 71 76 60 33 26 27 42 55 58 45 36 32 55 62 130 245 252 254 254 255] [255 255 255 254 255 237 154 67 79 64 46 28 16 18 36 52 48 36 29 37 58 56 114 240 255 254 255 255] [255 255 255 254 255 245 162 67 62 52 40 24 12 11 24 36 33 23 25 32 52 58 113 232 255 255 255 255] [255 255 255 255 255 249 185 70 47 37 33 30 20 12 12 19 17 13 17 23 50 56 106 227 254 255 255 255] [255 255 255 255 255 252 192 77 49 38 33 31 18 9 6 10 12 9 11 22 56 52 98 219 250 255 255 255] [255 255 255 255 255 254 212 86 53 38 38 34 14 5 5 8 7 7 14 25 59 58 77 209 247 255 255 255] [255 255 255 255 254 255 248 119 35 23 24 21 11 5 6 10 10 11 16 35 72 69 62 205 251 255 255 255] [255 255 255 255 255 253 255 155 37 23 10 9 7 6 12 17 14 12 20 45 73 65 67 205 251 255 255 255] [255 255 255 255 255 254 255 155 53 43 25 23 19 17 23 27 19 20 27 47 76 78 73 208 252 255 255 255] [255 255 255 255 254 255 245 152 73 64 33 29 28 31 44 43 33 36 43 57 78 89 75 216 254 255 255 255] [255 255 255 255 255 255 239 155 68 76 41 26 25 45 62 63 49 51 63 76 92 97 79 211 252 255 255 255] [255 255 255 255 255 255 239 159 69 75 55 32 36 57 69 72 64 63 82 94 114 116 99 223 251 255 255 255] [255 255 255 255 255 255 240 165 89 103 71 52 60 74 75 76 80 88 124 134 134 143 114 233 252 255 255 255] [255 255 255 255 255 255 253 184 85 107 82 68 74 93 94 91 93 109 149 163 137 102 79 239 255 254 255 255] [255 255 255 255 255 255 255 235 175 138 106 93 84 81 78 71 71 75 91 102 125 155 175 255 254 255 255 255]] filenames[0] 'keras-fashion-helloworld/myimages-originals/2019-05-13 14.29.04.png' # my_clothing_vecs = np.array([extract_vec(Image.open(fn)) for fn in filenames]) # and so lets try and display them with matplot lib again like the provided images... plt.figure(figsize=(10,10)) for i in range(len(my_clothing_vecs)): plt.subplot(2, 2, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(my_clothing_vecs[i]) # , cmap=plt.cm.binary plt.xlabel(rawnames[i]) plt.show() my_clothing_vecs.shape (4, 28, 28) # Ah crap, this looks inverted compared to what I see in the tutorial. # Going to attempt to invert that real quick... invert255 = lambda x: abs(255 - x) def invert_many(img_vec): input_shape = img_vec.shape length = input_shape[0] * input_shape[1] invert_pxl = lambda x: abs(255 - x) return np.resize( [invert_pxl(x) for x in np.resize(img_vec, (1, length))], input_shape) # invert them all clothing_vecs_inverted = np.vectorize(invert255)(my_clothing_vecs) my_clothing_vecs.shape, clothing_vecs_inverted.shape ((4, 28, 28), (4, 28, 28)) # Okay cool... now this looks like the other earlier data. plt.figure(figsize=(10,10)) for i in range(len(clothing_vecs_inverted)): plt.subplot(2, 2, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(clothing_vecs_inverted[i]) # , cmap=plt.cm.binary plt.xlabel(rawnames[i]) plt.show() # Now lets see what the predictions say # First the scaling... my_scaled_and_inverted = clothing_vecs_inverted/255.0 newpredictions = model.predict(my_scaled_and_inverted) print(list(enumerate(class_names))) [(0, 'T-shirt/top'), (1, 'Trouser'), (2, 'Pullover'), (3, 'Dress'), (4, 'Coat'), (5, 'Sandal'), (6, 'Shirt'), (7, 'Sneaker'), (8, 'Bag'), (9, 'Ankle boot')] my_test_labels = np.array([0, 7, 1, 4]) newpredictions array([[4.26309437e-01, 2.92650770e-06, 1.60550058e-04, 9.47394001e-04, 1.30431754e-05, 1.05808425e-07, 5.70735812e-01, 5.16380161e-10, 1.83065375e-03, 7.26391747e-09], [2.88550858e-03, 3.26211921e-05, 8.07017728e-04, 7.98632973e-05, 3.09603085e-04, 7.77611732e-01, 1.04915572e-03, 1.50996149e-01, 6.37709629e-03, 5.98511957e-02], [1.78965013e-02, 8.74079406e-01, 4.85132486e-02, 3.52283381e-03, 3.91321108e-02, 1.26371131e-04, 1.65334735e-02, 1.32581690e-07, 1.95919056e-04, 2.40851268e-08], [6.74940348e-02, 6.74244831e-04, 7.51323474e-04, 2.22629853e-04, 3.27958390e-02, 3.78045649e-03, 6.39505684e-01, 7.55926128e-04, 1.57964438e-01, 9.60554630e-02]], dtype=float32) # Plot the first X test images, their predicted label, and the true label # Color correct predictions in blue, incorrect predictions in red num_rows = 2 num_cols = 2 num_images = num_rows*num_cols plt.figure(figsize=(2*2*num_cols, 2*num_rows)) for i in range(num_images): plt.subplot(num_rows, 2*num_cols, 2*i+1) plot_image(i, newpredictions, my_test_labels, my_scaled_and_inverted) plt.subplot(num_rows, 2*num_cols, 2*i+2) plot_value_array(i, newpredictions, my_test_labels) plt.show() # Wow, that looks pretty good. # - Haha apparently the Tshirt I drew looks a bit more like a \u0026#34;Shirt\u0026#34; # - And my coat looks like a \u0026#34;Shirt\u0026#34; . Of course that was expected. I have no idea how that distinction # is captured by the model # - And yea, my sneaker looks kind of like a sandal? Okay makes sense. ","permalink":"https://michal.piekarczyk.xyz/post/2019-05-13--keras-hello-world-fashion/","summary":"Here I\u0026rsquo;m doing one of the Keras hello world micro projects, involving classification of tiny 28x28 wardrobe images. This was really fun. ( Here\u0026rsquo;s the original home for the python notebook). One really fun part of this was that at the end I hand drew my own clothing samples, paired them down using the python PIL library and threw them against the classifier. Surprisingly, the mini model performed really well. That\u0026rsquo;s some generalizability!","title":"Keras Hello WOrld"},{"content":"Put together our hyperlapse puzzle videos\u0026hellip; First stab at a merge using ffmpeg time ffmpeg -safe 0 -f concat -i infiles.txt -vcodec copy -acodec copy merged.MOV full output here and used this infiles.txt , following the directions file ./2019-03-09\\ 23.03.34.mov file ./2019-03-10\\ 01.21.50.mov file ./2019-03-10\\ 13.01.46.mov file ./2019-03-10\\ 13.03.59.mov file ./2019-03-10\\ 13.05.02.mov file ./2019-03-10\\ 18.43.53.mov Did it do the right thing? What are my mov lengths.. $ cat fileslist.txt |xargs -t -I % sh -c \u0026#39;ffmpeg -i \u0026#34;%\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration \u0026#39; sh -c ffmpeg -i \u0026#34;./2019-03-09 23.03.34.mov\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration Duration: 00:00:30.40, start: 0.000000, bitrate: 14837 kb/s sh -c ffmpeg -i \u0026#34;./2019-03-10 01.21.50.mov\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration Duration: 00:00:34.50, start: 0.000000, bitrate: 14971 kb/s sh -c ffmpeg -i \u0026#34;./2019-03-10 13.01.46.mov\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration Duration: 00:00:00.07, start: 0.000000, bitrate: 15840 kb/s sh -c ffmpeg -i \u0026#34;./2019-03-10 13.03.59.mov\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration Duration: 00:00:01.40, start: 0.000000, bitrate: 16455 kb/s sh -c ffmpeg -i \u0026#34;./2019-03-10 13.05.02.mov\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration Duration: 00:00:32.63, start: 0.000000, bitrate: 14944 kb/s sh -c ffmpeg -i \u0026#34;./2019-03-10 18.43.53.mov\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration Duration: 00:00:39.10, start: 0.000000, bitrate: 14868 kb/s And the length of the merged movie created \u0026hellip; $ ffmpeg -i merged.MOV 2\u0026gt;\u0026amp;1|grep Duration Duration: 00:02:18.10, start: 0.000000, bitrate: 14921 kb/s Doing some quick mental maths, that actually roughly adds up. Grr but at minute 1:05 it flips the video What the heck? Ah according to stackoverflow , ffmpeg uses rotation metadata to autorotate. Except heh in this case perhaps the autorotate did not happen when I was concatenating? My file 2019-03-10 13.05.02.mov appears to be the one which was rotated. Trying to process it to see what happens\u0026hellip; ffmpeg -i 2019-03-10\\ 13.05.02.mov -c:a copy 2019-03-10\\ 13.05.02.ROTATED.mov Wow that took at least a minute. So umm, since the initial concatenation took under a second, I seriously doubt the autorotation was done during the concatenation. Okay, lets try that concat one more time, this time with the new file\u0026hellip; and new files list file ./2019-03-09\\ 23.03.34.mov file ./2019-03-10\\ 01.21.50.mov file ./2019-03-10\\ 13.01.46.mov file ./2019-03-10\\ 13.03.59.mov file ./2019-03-10\\ 13.05.02.mov file ./2019-03-10\\ 18.43.53.mov time ffmpeg -safe 0 -f concat -i infiles.2019-03-16T2004Z.txt -vcodec copy -acodec copy 2019-03-16T2006Z-puzzle-merged.MOV Unfortunately, 2019-03-16T2006Z-puzzle-merged.MOV is even worse, because for some reason, only one frame is displayed for several minutes. So some more tweaking is needed. Appendix full output of the merge command ffmpeg version 4.1.1 Copyright (c) 2000-2019 the FFmpeg developers built with Apple LLVM version 10.0.0 (clang-1000.11.45.5) configuration: --prefix=/usr/local/Cellar/ffmpeg/4.1.1 --enable-shared --enable-pthreads --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags=\u0026#39;-I/Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/openjdk-11.0.2.jdk/Contents/Home/include/darwin\u0026#39; --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libmp3lame --enable-libopus --enable-librubberband --enable-libsnappy --enable-libtesseract --enable-libtheora --enable-libvorbis --enable-libvpx --enable-libx264 --enable-libx265 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librtmp --enable-libspeex --enable-videotoolbox --disable-libjack --disable-indev=jack --enable-libaom --enable-libsoxr libavutil 56. 22.100 / 56. 22.100 libavcodec 58. 35.100 / 58. 35.100 libavformat 58. 20.100 / 58. 20.100 libavdevice 58. 5.100 / 58. 5.100 libavfilter 7. 40.101 / 7. 40.101 libavresample 4. 0. 0 / 4. 0. 0 libswscale 5. 3.100 / 5. 3.100 libswresample 3. 3.100 / 3. 3.100 libpostproc 55. 3.100 / 55. 3.100 Input #0, concat, from \u0026#39;infiles.txt\u0026#39;: Duration: N/A, start: 0.000000, bitrate: 14833 kb/s Stream #0:0(und): Video: hevc (Main) (hvc1 / 0x31637668), yuvj420p(pc), 1920x1080, 14833 kb/s, 30 fps, 30 tbr, 600 tbn, 600 tbc Metadata: rotate : 180 creation_time : 2019-03-10T05:20:20.000000Z handler_name : Core Media Video encoder : HEVC Output #0, mov, to \u0026#39;merged.MOV\u0026#39;: Metadata: encoder : Lavf58.20.100 Stream #0:0(und): Video: hevc (Main) (hvc1 / 0x31637668), yuvj420p(pc), 1920x1080, q=2-31, 14833 kb/s, 30 fps, 30 tbr, 19200 tbn, 600 tbc Metadata: rotate : 180 creation_time : 2019-03-10T05:20:20.000000Z handler_name : Core Media Video encoder : HEVC Stream mapping: Stream #0:0 -\u0026gt; #0:0 (copy) Press [q] to stop, [?] for help [mov @ 0x7fba5d80dc00] Non-monotonous DTS in output stream 0:0; previous: 1246080, current: 1246080; changing to 1246081. This may result in incorrect timestamps in the output file. frame= 2828 fps=0.0 q=-1.0 size= 172544kB time=00:01:34.20 bitrate=15005.1kbitsframe= 4142 fps=0.0 q=-1.0 Lsize= 251538kB time=00:02:18.00 bitrate=14931.9kbits/s speed= 148x video:251484kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.021355% real\t0m1.186s user\t0m0.124s sys\t0m0.232s reference xargs reference ","permalink":"https://michal.piekarczyk.xyz/post/2019-03-16--ffmpeg-notes/","summary":"Put together our hyperlapse puzzle videos\u0026hellip; First stab at a merge using ffmpeg time ffmpeg -safe 0 -f concat -i infiles.txt -vcodec copy -acodec copy merged.MOV full output here and used this infiles.txt , following the directions file ./2019-03-09\\ 23.03.34.mov file ./2019-03-10\\ 01.21.50.mov file ./2019-03-10\\ 13.01.46.mov file ./2019-03-10\\ 13.03.59.mov file ./2019-03-10\\ 13.05.02.mov file ./2019-03-10\\ 18.43.53.mov Did it do the right thing? What are my mov lengths.. $ cat fileslist.txt |xargs -t -I % sh -c \u0026#39;ffmpeg -i \u0026#34;%\u0026#34; 2\u0026gt;\u0026amp;1 |grep Duration \u0026#39; sh -c ffmpeg -i \u0026#34;.","title":"Using ffmpeg to Merge Videos"},{"content":"https://github.com/namoopsoo/time-parser\nTable of Contents What Input data Output example Architecture overview Can I use this? What This is a simple time tracker lambda and DynamoDB based service, built as a project to learn clojure and micro services. The initial inspiration came from reading the book 168 Hours, in which Laura Vanderkam asks us to really see what we are doing with our time. There are 168 hours in a week and her claim is that although we feel we never have enough time, journaling what we do is a means to bullshit test ourselves.\nInput data We split our time between personal and work universes. (Actually this project doesnt care how you divide your life, but the first term, below, is a \u0026ldquo;core-category\u0026rdquo; ) \u0026hellip; And furthermore we bring our attention to either specific Projects, which have a beginning and an end, such as the below taxes-2017\u0026ndash;hopefully one day I will finish my taxes\u0026ndash;and adhoc tasks which is meant to record time spent helping a colleague for example (here called consulting, below). routine is a potentially useful \u0026ndash;or not\u0026ndash; distinction of things we do which do not have an End. times 2018-02-10 10:00 - 10:30 ; personal ; taxes-2017 ; plan ; just starting out 10:30 - 11:05 ; personal ; routine ; yoga 11:35 - 12:20 ; work ; WRK-345-bugfix-foo ; code 12:20 - 13:05 ; work ; adhoc ; consulting 13:05 - 13:35 ; work ; WRK-345-bugfix-foo ; code Output example Here is an example of an output stacked area chart of a core-category:work query, which is using the author\u0026rsquo;s data from December of 2017. This uses the d3js so called stacked layout. The y-axis represents minutes. This chart is not labeled, but it is possible to see that some days in the week of December 4th had multiple competing interests, while the week of Dec 11th there was a bit more focus on the Purple project. This particular layout only allows for a max of 20 layers before colors are re-used for other categories. a lot can be said about stacked area chart aesthetics. Architecture overview Here\u0026rsquo;s a super simple overview. There are several lambdas at play here, but the general picture looks like this.\nHere, crontab -l shows how an upload script runs every 20 minutes # crontab -l */20 * * * * bash /path/to/my/local/repo/timesheet-parser/clojure-time/timeparse.upload.sh and the timeparse.upload.sh is a one liner that does\u0026hellip; sh -c \u0026#39;source /local/path/to/my/environmental/variables.sh \\ \u0026amp;\u0026amp; source /local/path/to/my/python/environment/thing/bin/activate \\ \u0026amp;\u0026amp; python /path/to/my/local/repo/timesheet-parser/scripts/upload_client.py times /local/path/to/my/Dropbox/my/dir/with/time/data\u0026#39; Can I use this? This service has not yet been neatly templatized for easy adoption. This for now has been a fun side project proof of concept for helping the author figure out where his time is going.\nProductionizing would happen of course given enough of a demand.\n","permalink":"https://michal.piekarczyk.xyz/project/time-parser/time-parser-project/","summary":"This is a simple personal time tracker built with clojure, aws lambda and dynamodb,  inspired by Laura Vanderkam\u0026rsquo;s \u0026ldquo;168 Hours\u0026rdquo; book","title":"Time Parser Project"},{"content":"Some time lapse photography. ","permalink":"https://michal.piekarczyk.xyz/post/2018-01-07-plant-hyperlapse/","summary":"Some time lapse photography. ","title":"Some plant time lapse"},{"content":"Darn, I was riding for at least one to two days thinking, why am I riding much slower? Last night I finally noticed that wow my rear wheel is really wobbly and rubbing up against the breaks quite a bit lately. This morning I finally started setting up to true the wheel. I marked the spoke which looked like adjusting it would have the highest impact. And wow I didnt notice it completely sheared off.\nIt took me just around 20 minutes or so to adjust.\nThe wobble was not completely gone, but it was dramatically reduced. I don\u0026rsquo;t have a truing stand, but that would be a cool way to measure the warp.\n","permalink":"https://michal.piekarczyk.xyz/post/2017-12-01-spokes/","summary":"Darn, I was riding for at least one to two days thinking, why am I riding much slower? Last night I finally noticed that wow my rear wheel is really wobbly and rubbing up against the breaks quite a bit lately. This morning I finally started setting up to true the wheel. I marked the spoke which looked like adjusting it would have the highest impact. And wow I didnt notice it completely sheared off.","title":"Spoke too soon"},{"content":"Citibike Project: Can your Destination be Predicted ( https://github.com/namoopsoo/learn-citibike )\nMotivation I think sometimes the most interesting projects live behind ideas that sound impractical or even crazy. That\u0026rsquo;s why I thought it would be fun to use the Citibike bike share trip data to try and predict a person\u0026rsquo;s destination based on what we know.\nRoughly speaking trip data looks like this\n\u0026#34;tripduration\u0026#34;,\u0026#34;starttime\u0026#34;,\u0026#34;stoptime\u0026#34;,\u0026#34;start station id\u0026#34;,\u0026#34;start station name\u0026#34;,\u0026#34;start station latitude\u0026#34;,\u0026#34;start station longitude\u0026#34;,\u0026#34;end station id\u0026#34;,\u0026#34;end station name\u0026#34;,\u0026#34;end station latitude\u0026#34;,\u0026#34;end station longitude\u0026#34;,\u0026#34;bikeid\u0026#34;,\u0026#34;usertype\u0026#34;,\u0026#34;birth year\u0026#34;,\u0026#34;gender\u0026#34; \u0026#34;171\u0026#34;,\u0026#34;10/1/2015 00:00:02\u0026#34;,\u0026#34;10/1/2015 00:02:54\u0026#34;,\u0026#34;388\u0026#34;,\u0026#34;W 26 St \u0026amp; 10 Ave\u0026#34;,\u0026#34;40.749717753\u0026#34;,\u0026#34;-74.002950346\u0026#34;,\u0026#34;494\u0026#34;,\u0026#34;W 26 St \u0026amp; 8 Ave\u0026#34;,\u0026#34;40.74734825\u0026#34;,\u0026#34;-73.99723551\u0026#34;,\u0026#34;24302\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1973\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;593\u0026#34;,\u0026#34;10/1/2015 00:00:02\u0026#34;,\u0026#34;10/1/2015 00:09:55\u0026#34;,\u0026#34;518\u0026#34;,\u0026#34;E 39 St \u0026amp; 2 Ave\u0026#34;,\u0026#34;40.74780373\u0026#34;,\u0026#34;-73.9734419\u0026#34;,\u0026#34;438\u0026#34;,\u0026#34;St Marks Pl \u0026amp; 1 Ave\u0026#34;,\u0026#34;40.72779126\u0026#34;,\u0026#34;-73.98564945\u0026#34;,\u0026#34;19904\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1990\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;233\u0026#34;,\u0026#34;10/1/2015 00:00:11\u0026#34;,\u0026#34;10/1/2015 00:04:05\u0026#34;,\u0026#34;447\u0026#34;,\u0026#34;8 Ave \u0026amp; W 52 St\u0026#34;,\u0026#34;40.76370739\u0026#34;,\u0026#34;-73.9851615\u0026#34;,\u0026#34;447\u0026#34;,\u0026#34;8 Ave \u0026amp; W 52 St\u0026#34;,\u0026#34;40.76370739\u0026#34;,\u0026#34;-73.9851615\u0026#34;,\u0026#34;17797\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1984\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;250\u0026#34;,\u0026#34;10/1/2015 00:00:15\u0026#34;,\u0026#34;10/1/2015 00:04:25\u0026#34;,\u0026#34;336\u0026#34;,\u0026#34;Sullivan St \u0026amp; Washington Sq\u0026#34;,\u0026#34;40.73047747\u0026#34;,\u0026#34;-73.99906065\u0026#34;,\u0026#34;223\u0026#34;,\u0026#34;W 13 St \u0026amp; 7 Ave\u0026#34;,\u0026#34;40.73781509\u0026#34;,\u0026#34;-73.99994661\u0026#34;,\u0026#34;23966\u0026#34;,\u0026#34;Subscriber\u0026#34;,\u0026#34;1984\u0026#34;,\u0026#34;1\u0026#34; \u0026#34;528\u0026#34;,\u0026#34;10/1/2015 00:00:17\u0026#34;,\u0026#34;10/1/2015 00:09:05\u0026#34;,\u0026#34;3107\u0026#34;,\u0026#34;Bedford Ave \u0026amp; Nassau Ave\u0026#34;,\u0026#34;40.72311651\u0026#34;,\u0026#34;-73.95212324\u0026#34;,\u0026#34;539\u0026#34;,\u0026#34;Metropolitan Ave \u0026amp; Bedford Ave\u0026#34;,\u0026#34;40.71534825\u0026#34;,\u0026#34;-73.96024116\u0026#34;,\u0026#34;16246\u0026#34;,\u0026#34;Customer\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34; \u0026#34;440\u0026#34;,\u0026#34;10/1/2015 00:00:17\u0026#34;,\u0026#34;10/1/2015 00:07:37\u0026#34;,\u0026#34;3107\u0026#34;,\u0026#34;Bedford Ave \u0026amp; Nassau Ave\u0026#34;,\u0026#34;40.72311651\u0026#34;,\u0026#34;-73.95212324\u0026#34;,\u0026#34;539\u0026#34;,\u0026#34;Metropolitan Ave \u0026amp; Bedford Ave\u0026#34;,\u0026#34;40.71534825\u0026#34;,\u0026#34;-73.96024116\u0026#34;,\u0026#34;23698\u0026#34;,\u0026#34;Customer\u0026#34;,\u0026#34;\u0026#34;,\u0026#34;0\u0026#34; The data if fairly clean and regular, so I thought this was a fun data set to sharpen my teeth on.\nQuick Bird\u0026rsquo;s Eye of my Journey First started just looking at this data. Just out of curiosity, as a first mini starter project I decided to look at the relationship between rider age and speed I realized pretty early that the bike station target was too small, so I started using the Google Geolocation API to get broader location data such as zip codes and neighborhoods . geolocation I also thought on a high level that knowing whether you got on your bike at 4:05 in the afternoon versus 4:06 shouldn\u0026rsquo;t influence my learning algorithm, so I added some more transformations. I compared the prediction accuracy of the new geolocation data as a first stab I ran through a couple of modeling scenarios here and finding some impoartant inconsistencies in how I was running my testing I ended up getting some good gains by binarizing my inputs I ended up using a different evaluation metric again to get a different perspective on this problem. Maybe a year or more later I came back to this problem from the point of view of experimenting with AWS approaches to training models , including Docker and SageMaker. Thoughts for future improvements More on this data When I started looking at this data, there were 400+ stations for docking your citibike. There is age, and some of the riders were actually born in the 1800s, which is kind of cool. df = load_data(\u0026#39;data/201509_10-citibike-tripdata.csv.annotated.100000.06112016T1814.csv\u0026#39;) In [6]: df[\u0026#39;birth year\u0026#39;].describe() Out[6]: count 83171.000000 mean 1977.149680 std 11.400096 min 1885.000000 25% 1969.000000 50% 1980.000000 75% 1986.000000 max 1999.000000 Name: birth year, dtype: float64 Speed and Age Turns out that you need to know the miles per the longitude degree at a particular latitude on our planet. So for our particular location, at lat around 40.723 and using the earth\u0026rsquo;s radius of about 3958 miles , we have about 52.3 miles/longitude degree here in NYC.\nSo from there, looking at some of the speed data just involved looking at the tripdata trip time and calculating the cartesian distance.\n(More on the code here ) (Also more detail on this analysis in the main jupyter notebook)\nNeed additional location data With the 400+ stations, trying to predict a multi-class problem of this sort with basic machine learning algorithms would not be a way to get quick results to help keep the project going, so I decided to constrain the problem. I ended up turning to the Google Geocoding API. Using this data became its own side project, because parsing through the Google geolocation data can get pretty hairy! The meat of the output can look like this, for the docking station \u0026ldquo;1st Avenue \u0026amp; E 15th St\u0026rdquo;\n{ \u0026#39;raw_result\u0026#39;: [{u\u0026#39;address_components\u0026#39;: [{u\u0026#39;long_name\u0026#39;: u\u0026#39;1st Avenue\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;1st Avenue\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;route\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;Midtown\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;Midtown\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;neighborhood\u0026#39;, u\u0026#39;political\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;Manhattan\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;Manhattan\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;sublocality_level_1\u0026#39;, u\u0026#39;sublocality\u0026#39;, u\u0026#39;political\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;New York\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;New York\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;locality\u0026#39;, u\u0026#39;political\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;New York County\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;New York County\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;administrative_area_level_2\u0026#39;, u\u0026#39;political\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;New York\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;NY\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;administrative_area_level_1\u0026#39;, u\u0026#39;political\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;United States\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;US\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;country\u0026#39;, u\u0026#39;political\u0026#39;]}, {u\u0026#39;long_name\u0026#39;: u\u0026#39;10003\u0026#39;, u\u0026#39;short_name\u0026#39;: u\u0026#39;10003\u0026#39;, u\u0026#39;types\u0026#39;: [u\u0026#39;postal_code\u0026#39;]}], u\u0026#39;formatted_address\u0026#39;: u\u0026#39;1st Avenue \u0026amp; E 15th St, New York, NY 10003, USA\u0026#39;, } Some of the challenges here were that the outputs from the API were not always consistent. The above output shows that the\n'neighborhood' is 'Midtown', but because there were 400+ stations, I did not initially notice that sometimes the neighborhood was missing or that the zip code was missing. That ended up throwing off my code a couple of times. It turned out that the street intersection was not an ideal clean data input to the API. For instance E 3 St \u0026amp; 1 Ave, NY was understood by the API as just a street or a route as it is called, ( raw output ). Later on I ended up refactoring this to use the raw latitude and longitude . However, I eventually noticed that often times the docking stations were on the edge of neighborhoods. So I literally had edge cases! The Neighborhood would come back blank and I ended up having to fill in a lot of that data by hand anyhow! Also the data calls were not free and I ended up building a small caching layer with redis . The other reason I had done that was that I would often work out of cafes where the Wifi was spotty and I didn\u0026rsquo;t want an internet connection to hold me back. I think in hind-sight, I could have avoided some of the automation here and just decoupled the data fetch so that I wouldn\u0026rsquo;t have to worry about that internet connection. Of course every time I wanted to add additional data from Citibike, there would be new docking stations and I had to get back to making sure my station location data was correct, so that bad data did not impact predictions. Time bucketing In order to get better information from the source time, the source time was bucketted into 24 hour-buckets per day. That is since a ride starting at 1:04:23pm shouldn\u0026rsquo;t be treated as being too different from a ride departing at 1:05:24pm . There is more value in intuitively clustering the rides.\nComparing Geolocation Granularities There are about 463 stations found in the dataset, 28 neighborhoods, representing 49 postal codes and 3 out of 5 boroughs,\nSo using the (start time bucket, start station id, age, gender) as the inputs and with RandomizedLogisticRegression as a classifier , for about a months worth of trip data, I saw roughly the following comparison.\n{\u0026#39;end station id\u0026#39;: OrderedDict([(\u0026#39;training\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.041432771986099973), (\u0026#39;f1_score\u0026#39;, 0.015138704086611844), (\u0026#39;recall_score\u0026#39;, 0.041432771986099973), (\u0026#39;precision_score\u0026#39;, 0.016942125433308568)])), (\u0026#39;holdout\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.031533939070016032), (\u0026#39;f1_score\u0026#39;, 0.0093952628045424723), (\u0026#39;recall_score\u0026#39;, 0.031533939070016032), (\u0026#39;precision_score\u0026#39;, 0.0067157290264759353)]))]), \u0026#39;end_neighborhood\u0026#39;: OrderedDict([(\u0026#39;training\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.39047231270358307), (\u0026#39;f1_score\u0026#39;, 0.28885663229134789), (\u0026#39;recall_score\u0026#39;, 0.39047231270358307), (\u0026#39;precision_score\u0026#39;, 0.26445041603375502)])), (\u0026#39;holdout\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.39630836047774159), (\u0026#39;f1_score\u0026#39;, 0.2935527390151364), (\u0026#39;recall_score\u0026#39;, 0.39630836047774159), (\u0026#39;precision_score\u0026#39;, 0.26579390443173939)]))]), \u0026#39;end_postal_code\u0026#39;: OrderedDict([(\u0026#39;training\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.14129127122042506), (\u0026#39;f1_score\u0026#39;, 0.068340173428106887), (\u0026#39;recall_score\u0026#39;, 0.14129127122042506), (\u0026#39;precision_score\u0026#39;, 0.068168259430770747)])), (\u0026#39;holdout\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.13361838588989844), (\u0026#39;f1_score\u0026#39;, 0.064738931917963718), (\u0026#39;recall_score\u0026#39;, 0.13361838588989844), (\u0026#39;precision_score\u0026#39;, 0.067139580345228156)]))]), \u0026#39;end_sublocality\u0026#39;: OrderedDict([(\u0026#39;training\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.95354786589470852), (\u0026#39;f1_score\u0026#39;, 0.95209028150037733), (\u0026#39;recall_score\u0026#39;, 0.95354786589470852), (\u0026#39;precision_score\u0026#39;, 0.95217920885515972)])), (\u0026#39;holdout\u0026#39;, OrderedDict([(\u0026#39;accuracy_score\u0026#39;, 0.9493807215939688), (\u0026#39;f1_score\u0026#39;, 0.94990282092170586), (\u0026#39;recall_score\u0026#39;, 0.9493807215939688), (\u0026#39;precision_score\u0026#39;, 0.95132373575480056)]))])} The rough accuracies for prediction, end station id (~3%), postal code (~13%), neighborhood (~40%), and borough (~95%), using the small dataset, shows the rough differences in what happens when you reduce the number of possible outputs. Overall this gave me the motivation to focus on the neighborhood as a target to try to improve upon. More in the jupyter notebook Deeper into the weeds I compared the SGDClassifier with the LogisticRegression classifier (which I believe just uses Gradient Descent, while the SGDClassifier classifier is also a Logistic Regression classifier, but it uses Stochastic Gradient Descent).\nI also tried applying Standard Scaling to my input data after reading that scikit learn \u0026rsquo;s SGDClassifier implementation is sensitive unless the input data has a mean=0 and variance=1 . Indeed per the below this helped a little.\nI also applied a GridSearch around the alpha parameter to the SGDClassifier, but this did not help at least the way I tried it,\nI next started varying the training set size, given that a month worth of trip data had about a million rows, I went from 10k to 1M,\nBut this didn\u0026rsquo;t look great. I realized a problem I had was that I was not randomly sampling my input data. Since a month-size dataset is around 1.2 Million rows, then a 10,000 large set just ends up barely dipping into the first day. So choosing the dataset sizes has to be done, by random sampling.\nAfter making the sampling randomized, the output below, feels like it has a better upward trend, but it is still not visible enough.\nI also realized I was being inconsistent in my assessment because I was not using a single holdout set to test. I was actually randomly generating a test set each time. That was really bad. So I created ten models on sizes 10,000 through 100,000 datasets, created from 09 and 10 2015, and testing on a single holdout dataset, taken from November 2015. In this approach, the accuracy results are found using the same holdout set instead of using a differently derived test set each time.\nAlthough the results were still pretty flat, at least I can trust the consistency of my test method more now.\nMore in the jupyter notebook\nBinarizing the inputs Another important modeling change to try was to do a better of job of preparing the input data to better expose the stratification across citibike trips across the sources. To do this, instead of using a source station column and source neighborhood columns, the source neighborhood column was binarized using the sklearn OneHotEncoder, to a column for each of the neighborhoods in the surface area of the city. The same experiment as earlier was conducted, comparing results across the default SGDClassifier and LogisticRegression classifiers and also across 100,000 to 1,000,000 size datasets used for a train/test split along with a 100,000 large holdout set. These were created from just the single 2015-09 dataset (201509-citibike-tripdata.csv). I found this to be very helpful. Here\u0026rsquo;s a summary graphic, More details in the notebook\nChanging my metric one more time Given that there are about 28 neighborhoods covered by Citibike (at least in the data end of 2015), a high accuracy is difficult to achieve especially because there are many output classes to choose from. Another idea that was explored was to create a Rank K Accuracy, such that a prediction is correct when the correct class is found in the top highest K probabilities. The overall reasoning I had here is two-fold. One, I think of the analogy of a search engine, where it is typically acceptable to show someone five results as opposed to the so called \u0026ldquo;I am feeling lucky\u0026rdquo; result. Of course not every machine learning use case will have the tolerance to take five results as opposed to five, but I think my particular problem of choice it might be fine. Or at least asking people would help to answer that question.\nBut I think the main reason I wanted to do this was to just better understand whether my classification approach was doing anything at all. So since, out of these 28 or so neighborhoods, if going from the top 1 result to the top 2 results, yields an additional 20 points of accuracy, then I feel a little better about the result making sense.\nMore detail in the notebook\nSageMaker approach I wanted to test drive AWS SageMaker ,\nTo see if I could more easily train models without relying on my laptop And make my training environment more reproducible And the prospect of hyper parameter tuning jobs seemed pretty neat too And I wanated to see just how simple serving models would be And in general I wanted to do all of these things to see if I could end up using this at my job (which I did). What ended up happening I managed to recreate my basic model training and evaluation setup using Docker and Sagemaker. With SageMaker, Docker, your model is more compartmentalized and I found this was helpful when iterating model code and boiler plate code. I also ended up serving the model on an endpoint. This is still a sort of a back burner project I would like to come back to at some point, but below I show one update I made to have a slightly better time debugging changes my model iterations And I go over several more data roadblocks I ran into. Changes to Model Iterations To make things slightly easier to understand, especially for debugging purposes, I now made models into json objects that are easier to display\nlocalfn = \u0026#39;/Users/michal/Downloads/2018-12-07-update-model/2018-12-07-update- ...: model/tree-foo-bundle-pensive-swirles.2018-12-04T210259ZUTC.pkl\u0026#39; In [5]: with open(localfn) as fd: bundle = cPickle.load(fd) In [6]: from bikelearn import classify as blc In [12]: blc.label_decode(bundle[\u0026#39;label_encoders\u0026#39;][\u0026#39;end_neighborhood\u0026#39;], range(40)) Out[12]: array([\u0026#39;-1\u0026#39;, \u0026#39;Alphabet City\u0026#39;, \u0026#39;Battery Park City\u0026#39;, \u0026#39;Bedford-Stuyvesant\u0026#39;, \u0026#39;Boerum Hill\u0026#39;, \u0026#39;Bowery\u0026#39;, \u0026#39;Broadway Triangle\u0026#39;, \u0026#39;Brooklyn Heights\u0026#39;, \u0026#39;Brooklyn Navy Yard\u0026#39;, \u0026#39;Central Park\u0026#39;, \u0026#39;Chelsea\u0026#39;, \u0026#39;Chinatown\u0026#39;, \u0026#39;Civic Center\u0026#39;, \u0026#39;Clinton Hill\u0026#39;, \u0026#39;Columbia Street Waterfront District\u0026#39;, \u0026#39;Downtown Brooklyn\u0026#39;, \u0026#39;Dumbo\u0026#39;, \u0026#39;East Village\u0026#39;, \u0026#39;Financial District\u0026#39;, \u0026#39;Flatiron District\u0026#39;, \u0026#39;Fort Greene\u0026#39;, \u0026#39;Fulton Ferry District\u0026#39;, \u0026#39;Garment District\u0026#39;, \u0026#39;Gramercy Park\u0026#39;, \u0026#39;Greenpoint\u0026#39;, \u0026#39;Greenwich Village\u0026#39;, \u0026#34;Hell\u0026#39;s Kitchen\u0026#34;, \u0026#39;Hudson Square\u0026#39;, \u0026#39;Hunters Point\u0026#39;, \u0026#39;Kips Bay\u0026#39;, \u0026#39;Korea Town\u0026#39;, \u0026#39;Lenox Hill\u0026#39;, \u0026#39;Lincoln Square\u0026#39;, \u0026#39;Little Italy\u0026#39;, \u0026#39;Long Island City\u0026#39;, \u0026#39;Lower East Side\u0026#39;, \u0026#39;Lower Manhattan\u0026#39;, \u0026#39;Meatpacking District\u0026#39;, \u0026#39;Midtown\u0026#39;, \u0026#39;Midtown East\u0026#39;], dtype=object) In [19]: len(bundle[\u0026#39;label_encoders\u0026#39;][\u0026#39;end_neighborhood\u0026#39;].classes_) Out[19]: 65 In [42]: bu.print_bundle(bundle) Out[42]: {\u0026#39;bundle_name\u0026#39;: \u0026#39;tree-foo-bundle-pensive-swirles\u0026#39;, \u0026#39;clf\u0026#39;: RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\u0026#39;gini\u0026#39;, max_depth=5, max_features=\u0026#39;auto\u0026#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False), \u0026#39;clf_info\u0026#39;: {\u0026#39;feature_importances\u0026#39;: [(\u0026#39;start_postal_code\u0026#39;, 0.4118465753431595), (\u0026#39;start_sublocality\u0026#39;, 0.2228924462201325), (\u0026#39;start_neighborhood\u0026#39;, 0.28008403752725497), (\u0026#39;start_day\u0026#39;, 0.006151427471547376), (\u0026#39;start_hour\u0026#39;, 0.03971509075090292), (\u0026#39;age\u0026#39;, 0.0180403831546044), (\u0026#39;gender\u0026#39;, 0.008144851450140815), (\u0026#39;usertype\u0026#39;, 0.013125188082257624)]}, \u0026#39;evaluation\u0026#39;: {\u0026#39;test_metrics\u0026#39;: {\u0026#39;confusion_matrix\u0026#39;: 64, \u0026#39;f1_scores\u0026#39;: {\u0026#39;macro\u0026#39;: 0.041605116043552354, \u0026#39;micro\u0026#39;: 0.1599860211928701, \u0026#39;weighted\u0026#39;: 0.06355337311627869}, \u0026#39;rank_k_proba_scores\u0026#39;: {1: 0.1599860211928701, 2: 0.2536814721966162, 3: 0.3240640528912417, 4: 0.3873466833318772, 5: 0.443358194451626, 10: 0.629839760791229}}, \u0026#39;validation_metrics\u0026#39;: {\u0026#39;confusion_matrix\u0026#39;: 64, \u0026#39;f1_scores\u0026#39;: {\u0026#39;macro\u0026#39;: 0.04327900735885162, \u0026#39;micro\u0026#39;: 0.16284068269032595, \u0026#39;weighted\u0026#39;: 0.06549596580599053}, \u0026#39;rank_k_proba_scores\u0026#39;: {1: 0.16284068269032595, 2: 0.2563720053782964, 3: 0.3247784397051751, 4: 0.3873943724175835, 5: 0.4436898254016547, 10: 0.6304384096730821}}}, \u0026#39;features\u0026#39;: {\u0026#39;dtypes\u0026#39;: {\u0026#39;age\u0026#39;: float, \u0026#39;end_neighborhood\u0026#39;: str, \u0026#39;start_neighborhood\u0026#39;: str, \u0026#39;start_postal_code\u0026#39;: str, \u0026#39;start_sublocality\u0026#39;: str, \u0026#39;usertype\u0026#39;: str}, \u0026#39;input\u0026#39;: [\u0026#39;start_postal_code\u0026#39;, \u0026#39;start_sublocality\u0026#39;, \u0026#39;start_neighborhood\u0026#39;, \u0026#39;start_day\u0026#39;, \u0026#39;start_hour\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;usertype\u0026#39;], \u0026#39;output_label\u0026#39;: \u0026#39;end_neighborhood\u0026#39;}, \u0026#39;label_encoders\u0026#39;: {\u0026#39;age\u0026#39;: LabelEncoder(), \u0026#39;end_neighborhood\u0026#39;: LabelEncoder(), \u0026#39;start_neighborhood\u0026#39;: LabelEncoder(), \u0026#39;start_postal_code\u0026#39;: LabelEncoder(), \u0026#39;start_sublocality\u0026#39;: LabelEncoder(), \u0026#39;usertype\u0026#39;: LabelEncoder()}, \u0026#39;model_id\u0026#39;: \u0026#39;tree-foo\u0026#39;, \u0026#39;test_metadata\u0026#39;: {\u0026#39;testset_fn\u0026#39;: \u0026#39;/opt/ml/input/data/testing/201602-citibike-tripdata.csv\u0026#39;}, \u0026#39;timestamp\u0026#39;: \u0026#39;2018-12-04T210259ZUTC\u0026#39;, \u0026#39;train_metadata\u0026#39;: {\u0026#39;hyperparameters\u0026#39;: {u\u0026#39;max_depth\u0026#39;: u\u0026#39;5\u0026#39;, u\u0026#39;n_estimators\u0026#39;: u\u0026#39;20\u0026#39;}, \u0026#39;stations_df_fn\u0026#39;: \u0026#39;/opt/ml/input/data/training/support/stations-2018-12-04-c.csv\u0026#39;, \u0026#39;trainset_fn\u0026#39;: \u0026#39;/opt/ml/input/data/training/201601-citibike-tripdata.csv\u0026#39;}} I like how the AWS SageMaker setup allows for custom Docker image based models, only requiring a particular csv format as a data input for predictions and also requiring the Docker image implement a train command for running training jobs. I ended up adding a setup.py to my git repo to version what code a particular Dockerfile would use I made many iterations in trying to get the model on SageMaker up and running, so I liked the experience overall. Bad data strikes again After quickly updating docking station data and retraining, I ended up with a model which was returning only one value as an output I took a deeper dive into my docking station data and found yet again that I had a lot of blank geolocation neighborhood and postal code data. This is the same problem I had to deal with in the past as well. I ended up finding while debugging, that the empty data was essentially pinning the majority of the training data as neighborhood : 'nan', and so the predictions, per this confusion matrix, were basically all the same output. ipdb\u0026gt; pp skm.confusion_matrix(y_validation, y_predictions, classes) array([[ 0, 0, 0, 0, 0, 0, 0, 174], [ 0, 0, 0, 0, 0, 0, 0, 116], [ 0, 0, 0, 0, 0, 0, 0, 130], [ 0, 0, 0, 0, 0, 0, 0, 357], [ 0, 0, 0, 0, 0, 0, 0, 364], [ 0, 0, 0, 0, 0, 0, 0, 255], [ 0, 0, 0, 0, 0, 0, 0, 862], [ 0, 0, 0, 0, 0, 0, 0, 97977]]) ipdb\u0026gt; Of course one data problem always leads to another data problem. This time around, when I started updating my docking station geolocation data, I found that my payment information may have changed and so I was getting the following ipdb\u0026gt; pp geocoding_result {u\u0026#39;error_message\u0026#39;: u\u0026#39;You have exceeded your daily request quota for this API. If you did not set a custom daily request quota, verify your project has an active billing account: http://g.co/dev/maps-no-account\u0026#39;, u\u0026#39;results\u0026#39;: [], u\u0026#39;status\u0026#39;: u\u0026#39;OVER_QUERY_LIMIT\u0026#39;} And of course I also ended up stepping on my own foot as well. I found I had created an unfortinate git commit where I accidentally undid the url-encoding and intersections with \u0026amp; are I think treated as query string parameters , , which ended up being a reason why some of my data was coming back as just the geolocation for a single street (route) and not an actual intersection. Future Improvements I am hoping to come back to this and continue to iterate the approach. In particular, I would like to continue to explore model degradation over time. And in discussing with a few colleagues, seasonality would also be a really good feature to consider. Time bucketing was explored to a limited extent, but the day of the week nor the month of the year was not explored. There may also be many other datasets which can be joined with this one to bolster the information available, including information about the weather or perhaps other demographic attributes available. A more thorough comparison of algorithms should also be considered. ","permalink":"https://michal.piekarczyk.xyz/project/2016-12-18-citibike-project/","summary":"Can your Bike Share Destination be Predicted","title":"Citibike Learn Project"},{"content":"This may apply to Athena and or prestodb in general\narray empty? Per stackoverflow learned the name for this in the docs is cardinality select cardinality(array[]) = 0; This cannot be applied to the output of a json_extract(json_parse(data), '$.blah.flah.clah') since cardinality() takes ARRAY and not JSON. However, that JSON can be cast. For example, if '$.blah.flah.clah' is like [{\u0026quot;hi\u0026quot;: \u0026quot;there\u0026quot;}, {\u0026quot;so\u0026quot;: \u0026quot;then\u0026quot;}], then this cardinality(cast(json_extract(json_parse(what), \u0026#39;$.blah.flah.clah\u0026#39;) as array(map(varchar, varchar)))) will produce the length of those arrays.\n","permalink":"https://michal.piekarczyk.xyz/handy/aws/athena/","summary":"This may apply to Athena and or prestodb in general\narray empty? Per stackoverflow learned the name for this in the docs is cardinality select cardinality(array[]) = 0; This cannot be applied to the output of a json_extract(json_parse(data), '$.blah.flah.clah') since cardinality() takes ARRAY and not JSON. However, that JSON can be cast. For example, if '$.blah.flah.clah' is like [{\u0026quot;hi\u0026quot;: \u0026quot;there\u0026quot;}, {\u0026quot;so\u0026quot;: \u0026quot;then\u0026quot;}], then this cardinality(cast(json_extract(json_parse(what), \u0026#39;$.blah.flah.clah\u0026#39;) as array(map(varchar, varchar)))) will produce the length of those arrays.","title":""},{"content":"Make a zip file for a lambda layer From a well written reference here : adjust the python version as needed #!/bin/bash echo -e \u0026#34;blah-lib==2.0\\n\\ umm-lib==0.45\u0026#34; \u0026gt; requirements.txt export LIB_DIR=\u0026#34;python\u0026#34; rm -rf ${LIB_DIR} \u0026amp;\u0026amp; mkdir -p ${LIB_DIR} docker run --rm -v $(pwd):/foo -w /foo lambci/lambda:build-python3.8 \\ pip install -r requirements.txt -t ${LIB_DIR} zip -r layer.zip python And I like to use vim layer.zip to look at the contents Get lambda configuration details by boto Super handy client = boto3.client(\u0026#39;lambda\u0026#39;) out = client.get_function_configuration(FunctionName=\u0026#39;myAwesomeLambda\u0026#39;, # Qualifier=\u0026#39;99\u0026#39;, # optional version. ) In [30]: list(out.keys()) Out[30]: [\u0026#39;ResponseMetadata\u0026#39;, \u0026#39;FunctionName\u0026#39;, \u0026#39;FunctionArn\u0026#39;, \u0026#39;Runtime\u0026#39;, \u0026#39;Role\u0026#39;, \u0026#39;Handler\u0026#39;, \u0026#39;CodeSize\u0026#39;, \u0026#39;Description\u0026#39;, \u0026#39;Timeout\u0026#39;, \u0026#39;MemorySize\u0026#39;, \u0026#39;LastModified\u0026#39;, \u0026#39;CodeSha256\u0026#39;, \u0026#39;Version\u0026#39;, \u0026#39;VpcConfig\u0026#39;, \u0026#39;Environment\u0026#39;, \u0026#39;TracingConfig\u0026#39;, \u0026#39;RevisionId\u0026#39;, \u0026#39;Layers\u0026#39;, \u0026#39;State\u0026#39;, \u0026#39;LastUpdateStatus\u0026#39;, \u0026#39;PackageType\u0026#39;] ","permalink":"https://michal.piekarczyk.xyz/handy/aws/lambda/","summary":"Make a zip file for a lambda layer From a well written reference here : adjust the python version as needed #!/bin/bash echo -e \u0026#34;blah-lib==2.0\\n\\ umm-lib==0.45\u0026#34; \u0026gt; requirements.txt export LIB_DIR=\u0026#34;python\u0026#34; rm -rf ${LIB_DIR} \u0026amp;\u0026amp; mkdir -p ${LIB_DIR} docker run --rm -v $(pwd):/foo -w /foo lambci/lambda:build-python3.8 \\ pip install -r requirements.txt -t ${LIB_DIR} zip -r layer.zip python And I like to use vim layer.zip to look at the contents Get lambda configuration details by boto Super handy client = boto3.","title":""},{"content":"State Machine life cycle Adapting this from my stackoverflow answer to a question on updating state machines in step functions step_function_stack.yaml AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: \u0026#39;AWS::Serverless-2016-10-31\u0026#39; Description: \u0026gt;- A description of the State Machine goes here. Resources: MyStateMachineName: Type: AWS::StepFunctions::StateMachine Properties: RoleArn: \u0026#34;arn:aws:iam::{{aws_account_id}}:role/service-role/StepFunctions-MyStepFunctionRole\u0026#34; StateMachineName: \u0026#34;MyStateMachineName\u0026#34; StateMachineType: \u0026#34;EXPRESS\u0026#34; DefinitionString: Fn::Sub: | {{full_json_definition}} manage_step_functions.py import boto3 import os import time from jinja2 import Environment def do_render(full_json_definition): with open(\u0026#39;step_function_stack.yaml\u0026#39;) as fd: template = fd.read() yaml = Environment().from_string(template).render( full_json_definition=full_json_definition, aws_account_id=os.getenv(\u0026#39;AWS_ACCOUNT_ID\u0026#39;)) return yaml def update_step_function(stack_name, full_json_definition,): yaml = do_render(full_json_definition) client = boto3.client(\u0026#39;cloudformation\u0026#39;) response = client.update_stack( StackName=stack_name, TemplateBody=yaml, Capabilities=[ \u0026#39;CAPABILITY_AUTO_EXPAND\u0026#39;, ]) return response def create_step_function(stack_name, full_json_definition,): yaml = do_render(full_json_definition) client = boto3.client(\u0026#39;cloudformation\u0026#39;) response = client.update_stack( StackName=stack_name, TemplateBody=yaml, Capabilities=[ \u0026#39;CAPABILITY_AUTO_EXPAND\u0026#39;, ]) return response def get_lambdas_stack_latest_events(stack_name): # Get the first 100 most recent events. client = boto3.client(\u0026#39;cloudformation\u0026#39;) return client.describe_stack_events( StackName=stack_name) def wait_on_update(stack_name): events = None while events is None or events[\u0026#39;StackEvents\u0026#39;][0][\u0026#39;ResourceStatus\u0026#39;] not in [\u0026#39;UPDATE_COMPLETE\u0026#39;, \u0026#39;UPDATE_ROLLBACK_COMPLETE\u0026#39;, \u0026#39;DELETE_COMPLETE\u0026#39;, \u0026#39;CREATE_COMPLETE\u0026#39;]: print(events[\u0026#39;StackEvents\u0026#39;][0][\u0026#39;ResourceStatus\u0026#39;] if events else ...) events = get_lambdas_stack_latest_events(stack_name) time.sleep(1) return events step_function_definition.json { \u0026#34;Comment\u0026#34;: \u0026#34;This is a Hello World State Machine from https://docs.aws.amazon.com/step-functions/latest/dg/getting-started.html#create-state-machine\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;Hello\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;Hello\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Pass\u0026#34;, \u0026#34;Result\u0026#34;: \u0026#34;Hello\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;World\u0026#34; }, \u0026#34;World\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Pass\u0026#34;, \u0026#34;Result\u0026#34;: \u0026#34;World\u0026#34;, \u0026#34;End\u0026#34;: true } } } Create that step function # From a python shell for example # First just set any privileged variables through environmental variables so they are not checked into code # export AWS_ACCOUNT_ID=999999999 # edit step_function_definition.json then read it with open(\u0026#39;step_function_definition.json\u0026#39;) as fd: step_function_definition = fd.read() import manage_step_functions as msf stack_name = \u0026#39;MyGloriousStepFuncStack\u0026#39; msf.create_step_function(stack_name, step_function_definition) # If you are ready to update your State Machine, # you can edit step_function_definition.json or you might create a new file for reference, # step_function_definition-2021-01-29.json # (Because at time of this writing Step Functions dont have versions like Lambda for instance) with open(\u0026#39;step_function_definition-2021-01-29.json\u0026#39;) as fd: step_function_definition = fd.read() msf.update_step_function(stack_name, step_function_definition) terraform step func API Gateway v1 only works w/ StartExecution and not StartSyncExecution , but here is the basic terraform way to do that. This approach assumes a variables.tf file that defines sfn_orchestrater_arn as your step function arn. resource \u0026#34;aws_api_gateway_resource\u0026#34; \u0026#34;potato\u0026#34; { rest_api_id = aws_api_gateway_rest_api.foo-api.id parent_id = aws_api_gateway_rest_api.foo-api.root_resource_id path_part = \u0026#34;potato\u0026#34; } resource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;potato-method\u0026#34; { rest_api_id = aws_api_gateway_rest_api.foo-api.id resource_id = aws_api_gateway_resource.potato.id http_method = \u0026#34;POST\u0026#34; authorization = \u0026#34;AWS_IAM\u0026#34; } resource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;potato-foo-integration\u0026#34; { rest_api_id = aws_api_gateway_rest_api.foo-api.id resource_id = aws_api_gateway_resource.potato.id http_method = aws_api_gateway_method.potato-method.http_method type = \u0026#34;AWS\u0026#34; integration_http_method = \u0026#34;POST\u0026#34; credentials = \u0026#34;arn:aws:iam::${var.aws_account_id}:role/MyRoleFoo\u0026#34; uri = \u0026#34;arn:aws:apigateway:${var.aws_region}:states:action/StartExecution\u0026#34; passthrough_behavior = \u0026#34;NEVER\u0026#34; request_templates = { \u0026#34;application/x-amz-json-1.0\u0026#34; = \u0026lt;\u0026lt;EOF { \u0026#34;input\u0026#34;: \u0026#34;$util.escapeJavaScript($input.json(\u0026#39;$\u0026#39;))\u0026#34;, \u0026#34;stateMachineArn\u0026#34;: \u0026#34;${var.sfn_orchestrater_arn}\u0026#34; } EOF } } resource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;potato_response_200\u0026#34; { rest_api_id = aws_api_gateway_rest_api.foo-api.id resource_id = aws_api_gateway_resource.potato.id http_method = aws_api_gateway_method.potato-method.http_method status_code = \u0026#34;200\u0026#34; } resource \u0026#34;aws_api_gateway_integration_response\u0026#34; \u0026#34;potato_integration_response_200\u0026#34; { rest_api_id = aws_api_gateway_rest_api.foo-api.id resource_id = aws_api_gateway_resource.potato.id http_method = aws_api_gateway_method.potato-method.http_method status_code = aws_api_gateway_method_response.potato_response_200.status_code } resource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;potato_response_400\u0026#34; { rest_api_id = aws_api_gateway_rest_api.foo-api.id resource_id = aws_api_gateway_resource.potato.id http_method = aws_api_gateway_method.potato-method.http_method status_code = \u0026#34;400\u0026#34; } ","permalink":"https://michal.piekarczyk.xyz/handy/aws/step_functions/","summary":"State Machine life cycle Adapting this from my stackoverflow answer to a question on updating state machines in step functions step_function_stack.yaml AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: \u0026#39;AWS::Serverless-2016-10-31\u0026#39; Description: \u0026gt;- A description of the State Machine goes here. Resources: MyStateMachineName: Type: AWS::StepFunctions::StateMachine Properties: RoleArn: \u0026#34;arn:aws:iam::{{aws_account_id}}:role/service-role/StepFunctions-MyStepFunctionRole\u0026#34; StateMachineName: \u0026#34;MyStateMachineName\u0026#34; StateMachineType: \u0026#34;EXPRESS\u0026#34; DefinitionString: Fn::Sub: | {{full_json_definition}} manage_step_functions.py import boto3 import os import time from jinja2 import Environment def do_render(full_json_definition): with open(\u0026#39;step_function_stack.yaml\u0026#39;) as fd: template = fd.read() yaml = Environment().","title":""},{"content":" I Stumbled on this gem of a stack overflow answer ( in here ) , which goes into some great detail but one of the hacks I learned is you can modify IFS to change the following behavior. (For context, I have some files with spaces in the names and they end in RR.jpg . $ for file in $(ls *RR.jpg) ; do echo $file ; done 2021-05-19 09.01.51RR.jpg 2021-05-19 09.02.00RR.jpg 2021-05-19 09.02.07RR.jpg $ echo $IFS $ OIFS=\u0026#34;$IFS\u0026#34; $ IFS=$\u0026#39;\\n\u0026#39; $ for file in $(ls *RR.jpg) ; do echo $file ; done 2021-05-19 09.01.51RR.jpg 2021-05-19 09.02.00RR.jpg 2021-05-19 09.02.07RR.jpg $ IFS=\u0026#34;$OIFS\u0026#34; $ for file in $(ls *RR.jpg) ; do echo $file ; done 2021-05-19 09.01.51RR.jpg 2021-05-19 09.02.00RR.jpg 2021-05-19 09.02.07RR.jpg ","permalink":"https://michal.piekarczyk.xyz/handy/bash/spaces-in-filenames-traversal/","summary":"I Stumbled on this gem of a stack overflow answer ( in here ) , which goes into some great detail but one of the hacks I learned is you can modify IFS to change the following behavior. (For context, I have some files with spaces in the names and they end in RR.jpg . $ for file in $(ls *RR.jpg) ; do echo $file ; done 2021-05-19 09.01.51RR.jpg 2021-05-19 09.","title":""},{"content":"atom vec [] and while loop\u0026hellip; (let [results-atom (atom [])] ; ; (println \u0026#34;elements: \u0026#34; (count @results-atom)) ; (swap! results-atom conj \u0026#34;hi\u0026#34;) ;results-atom (while (\u0026lt; (count @results-atom) 3) (do (println \u0026#34;doing\u0026#34;) ; insert (swap! results-atom conj \u0026#34;hi\u0026#34;) (println \u0026#34;elements: \u0026#34; (count @results-atom)) )) ; done (println \u0026#34;Done. Now have elements: \u0026#34; (count @results-atom)) ) =\u0026gt; doing elements: 1 doing elements: 2 doing elements: 3 Done. Now have elements: 3 nil ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/atom-vec-and-while-loop/","summary":"atom vec [] and while loop\u0026hellip; (let [results-atom (atom [])] ; ; (println \u0026#34;elements: \u0026#34; (count @results-atom)) ; (swap! results-atom conj \u0026#34;hi\u0026#34;) ;results-atom (while (\u0026lt; (count @results-atom) 3) (do (println \u0026#34;doing\u0026#34;) ; insert (swap! results-atom conj \u0026#34;hi\u0026#34;) (println \u0026#34;elements: \u0026#34; (count @results-atom)) )) ; done (println \u0026#34;Done. Now have elements: \u0026#34; (count @results-atom)) ) =\u0026gt; doing elements: 1 doing elements: 2 doing elements: 3 Done. Now have elements: 3 nil ","title":""},{"content":"basic callbacks\u0026hellip; (comment \u0026#34;basic callback\u0026#34; (let [callback-fn #(+ 5 %) ] (callback-fn 10)) ) \u0026hellip; user=\u0026gt; (let [callback-fn #(+ 5 %) #_=\u0026gt; ] #_=\u0026gt; (callback-fn 10)) 15 callback w future (comment \u0026#34;future w callback func\u0026#34; (defn use-callback-when-done [callback-fn] (future (callback-fn (+ 4 5)))) (def output (use-callback-when-done #(println \u0026#34;printings.. \u0026#34; % \u0026#34; .end\u0026#34;))) ) =\u0026gt; user=\u0026gt; (def output (use-callback-when-done #(println \u0026#34;printings.. \u0026#34; % \u0026#34; .end\u0026#34;))) #\u0026#39;user/output printings.. 9 .end user=\u0026gt; callback and core async\u0026hellip; (comment \u0026#34;use a go put onto a channel callback...\u0026#34; (defn use-callback-when-done [callback-fn] (future (callback-fn (+ 4 5)))) (require [\u0026#39;clojure.core.async :refer [\u0026#39;\u0026gt;! \u0026#39;\u0026lt;! \u0026#39;\u0026gt;!! \u0026#39;\u0026lt;!! \u0026#39;go \u0026#39;chan \u0026#39;alts! \u0026#39;alts!! \u0026#39;timeout]]) (let [ my-results-chan (chan) callback-fn #(go (\u0026gt;! my-results-chan %)) output (use-callback-when-done callback-fn) ] ; (fn [x] (\u0026lt;!! my-results-chan)) (println \u0026#34;looking at the channel: \u0026#34; (\u0026lt;!! my-results-chan)) (println \u0026#34;done..\u0026#34;) ) ) ==\u0026gt; user=\u0026gt; (let [ #_=\u0026gt; my-results-chan (chan) #_=\u0026gt; callback-fn #(go (\u0026gt;! my-results-chan %)) #_=\u0026gt; output (use-callback-when-done callback-fn) #_=\u0026gt; ] #_=\u0026gt; ; (fn [x] (\u0026lt;!! my-results-chan)) #_=\u0026gt; (println \u0026#34;looking at the channel: \u0026#34; (\u0026lt;!! my-results-chan)) #_=\u0026gt; (println \u0026#34;done..\u0026#34;) #_=\u0026gt; ) looking at the channel: 9 done.. nil user=\u0026gt; ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/callbacks-and-async/","summary":"basic callbacks\u0026hellip; (comment \u0026#34;basic callback\u0026#34; (let [callback-fn #(+ 5 %) ] (callback-fn 10)) ) \u0026hellip; user=\u0026gt; (let [callback-fn #(+ 5 %) #_=\u0026gt; ] #_=\u0026gt; (callback-fn 10)) 15 callback w future (comment \u0026#34;future w callback func\u0026#34; (defn use-callback-when-done [callback-fn] (future (callback-fn (+ 4 5)))) (def output (use-callback-when-done #(println \u0026#34;printings.. \u0026#34; % \u0026#34; .end\u0026#34;))) ) =\u0026gt; user=\u0026gt; (def output (use-callback-when-done #(println \u0026#34;printings.. \u0026#34; % \u0026#34; .end\u0026#34;))) #\u0026#39;user/output printings.. 9 .end user=\u0026gt; callback and core async\u0026hellip; (comment \u0026#34;use a go put onto a channel callback.","title":""},{"content":"(Porting over my notes from gists .. here ),..\nquick note on imports i originally inferred this (ns blah-namespace (:use clojure.core.blah ) ; this would take all the terms in blah and put them into the current namespace (:refer-clojure :exclude [blarg flarg]) ; but this is supposed to be a way to avoid term clash ; so blarg and flarg will not be used. (not that they are in clojure.core however) ) Then I read https://www.braveclojure.com/organization/#Anchor-3 , and read use is a shortcut for require followed by a refer. hash-maps Many ways to make a hash-map,\n; These are both fine boot.user=\u0026gt; {:name \u0026#34;Steve\u0026#34; :age 24 :salary 7886 :company \u0026#34;Acme\u0026#34;} {:name \u0026#34;Steve\u0026#34;, :age 24, :salary 7886, :company \u0026#34;Acme\u0026#34;} boot.user=\u0026gt; {:name \u0026#34;Steve\u0026#34;, :age 24, :salary 7886, :company \u0026#34;Acme\u0026#34;} {:name \u0026#34;Steve\u0026#34;, :age 24, :salary 7886, :company \u0026#34;Acme\u0026#34;} how to get a value from the hash-map\u0026hellip;\nboot.user=\u0026gt; (steve \u0026#34;age\u0026#34;) ; no nil boot.user=\u0026gt; (steve 24) ; no. not even sure what this is trying to do nil boot.user=\u0026gt; (steve :company) ; yes \u0026#34;Acme\u0026#34; boot.user=\u0026gt; (steve :name) ; yes \u0026#34;Steve\u0026#34; what is insane is that this is also legal\u0026hellip; and I see this one used more often:\nboot.user=\u0026gt; (:name steve) \u0026#34;Steve\u0026#34; and get too..\nboot.user=\u0026gt; (get steve :company) \u0026#34;Acme\u0026#34; accessing nested hashes\u0026hellip;\napp.core=\u0026gt; (get-in {:foo {:and \u0026#34;yes\u0026#34; :here \u0026#34;no\u0026#34;}} [:foo :here]) \u0026#34;no\u0026#34; Array things similarly can index arrays like hash-maps\u0026hellip;\nboot.user=\u0026gt; (def a1 [1 2 3 4]) #\u0026#39;boot.user/a1 boot.user=\u0026gt; a1 [1 2 3 4] boot.user=\u0026gt; (first a1) 1 Boot-repl According to https://github.com/boot-clj/boot , you can set environmental variables \u0026hellip;\nboot.user=\u0026gt; (set-env! #_=\u0026gt; :resource-paths #{\u0026#34;src\u0026#34;} #_=\u0026gt; :dependencies \u0026#39;[[me.raynes/conch \u0026#34;0.8.0\u0026#34;]]) but this is not working for me or at least not for the REPL i\u0026rsquo;m in,\nboot.user=\u0026gt; (:development env) nil boot.user=\u0026gt; (set-env! #_=\u0026gt; :development true) nil boot.user=\u0026gt; boot.user=\u0026gt; (:development env) nil And to look at one or more of what is in env\nboot.user=\u0026gt; (use \u0026#39;[environ.core :as env]) nil ; now you can look at anything with boot.user=\u0026gt; (env :term) \u0026#34;xterm-256color\u0026#34; and clojure re-writes envrironmental variables, changing CAPITAL_LETTERS into capital-letters, so if on shell one did export CAPITAL_LETTERS=bingo Then environ.core could get this with (env :capital-letters) lambda functions in clojure ( aka anonymous functions) the #(...)\nboot.user=\u0026gt; (#(+ %1 5) 2) 7 boot.user=\u0026gt; (#(+ %1 5) 2 3) clojure.lang.ArityException: Wrong number of args (2) passed to: user/eval5691/fn--5692 boot.user=\u0026gt; (#(+ %1 5 %2) 2 3) 10 actually, another nicer looking lambda form. I like this more..\nboot.user=\u0026gt; (map (fn [x] (x :name)) [{:name \u0026#34;Jo\u0026#34;},{:name \u0026#34;Jane\u0026#34;},{:hmm \u0026#34;Yea\u0026#34;}]) (\u0026#34;Jo\u0026#34; \u0026#34;Jane\u0026#34; nil) boot.user=\u0026gt; anonymous func approach to collecting keys from hash\u0026hellip;\napp.core=\u0026gt; ((fn [{:keys [status error]}] (println status error)) {:status 400 :error false}) 400 false nil Another note on conj and hashes\u0026hellip; conj hashes, app.core=\u0026gt; (def blah {\u0026#34;acc\u0026#34; \u0026#34;123\u0026#34;}) #\u0026#39;app.core/blah app.core=\u0026gt; blah {\u0026#34;acc\u0026#34; \u0026#34;123\u0026#34;} app.core=\u0026gt; (conj blah {\u0026#34;secret\u0026#34; \u0026#34;455\u0026#34;}) {\u0026#34;acc\u0026#34; \u0026#34;123\u0026#34;, \u0026#34;secret\u0026#34; \u0026#34;455\u0026#34;} app.core=\u0026gt; (conj blah {\u0026#34;secret\u0026#34; \u0026#34;455\u0026#34; \u0026#34;client_id\u0026#34; \u0026#34;5667\u0026#34;}) {\u0026#34;acc\u0026#34; \u0026#34;123\u0026#34;, \u0026#34;secret\u0026#34; \u0026#34;455\u0026#34;, \u0026#34;client_id\u0026#34; \u0026#34;5667\u0026#34;} a weird way of calling a func using apply, and some strange dereferencing thing , Best quote about apply i read is \u0026hellip;\napply is used to apply an operator to its operands.\nso apply takes a func as its first arg and then throws whatever is next , 1 or more args, to func.\nI Guess that\u0026rsquo;s a lie, because the thing is though, you want to give func a list thing.\nboot.user=\u0026gt; (max 1 2 3 4 ) 4 boot.user=\u0026gt; (apply #(+ %1 %3) \u0026#39;[4 5 6 ]) 10 but all of these forms seem to work which is weird\nboot.user=\u0026gt; (apply #(+ %1 %3) [4 5 6] ) 10 boot.user=\u0026gt; (apply #(+ %1 %3) `[4 5 6] ) 10 boot.user=\u0026gt; (apply #(+ %1 %3) \u0026#39;[4 5 6] ) 10 importing from other files\u0026hellip; for a file \u0026quot;src/lesson_two/dynamo.clj\u0026quot; , use will let the namespace in it referred to as db\nboot.user=\u0026gt; (load-file \u0026#34;src/lesson_two/dynamo.clj\u0026#34;) #\u0026#39;lesson-two.dynamo/get-prime boot.user=\u0026gt; (use \u0026#39;[lesson-two.dynamo :as db]) nil Reload mechanism\u0026hellip; reload after making changes if using use\nfrom http://stackoverflow.com/questions/7658981/how-to-reload-a-clojure-file-in-repl#20337925 (use \u0026#39;your.namespace :reload) When using a lein repl, the above was not working for some reason, but this pattern worked for me\u0026hellip;\npull up a REPL..\napp.core=\u0026gt; (require \u0026#39;[app.core :as mycore]) app.core=\u0026gt; (mycore/blah \u0026#34;hi\u0026#34;) edit code \u0026hellip;\nthen in the REPL\u0026hellip;\napp.core=\u0026gt; (require \u0026#39;[app.core] :reload) nil app.core=\u0026gt; (mycore/blah \u0026#34;hi\u0026#34;) and mycore/blah is available after the reload with any changes taking effect.\nbut also this was also good , reload-all \u0026hellip; not sure about the difference definitively\n(require \u0026#39;app.core :reload-all) Also, for the namespace the repl itself is in, this worked to reload that..\n(use \u0026#39;app.core :reload-all) interesting way of checking types in a hash map throw error if wrong type\nboot.user=\u0026gt; (def steve {:name \u0026#34;Steve\u0026#34;, :age 24, :salary 7886, :company \u0026#34;Acme\u0026#34;}) #\u0026#39;boot.user/steve boot.user=\u0026gt; boot.user=\u0026gt; steve {:name \u0026#34;Steve\u0026#34;, :age 24, :salary 7886, :company \u0026#34;Acme\u0026#34;} boot.user=\u0026gt; boot.user=\u0026gt; (-\u0026gt; steve :age num) 24 boot.user=\u0026gt; (-\u0026gt; steve :company num) java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number boot.user=\u0026gt; (-\u0026gt; steve :company str) \u0026#34;Acme\u0026#34; string splitting requires an import\nboot.user=\u0026gt; (use \u0026#39;[clojure.string :only (split triml)]) nil boot.user=\u0026gt; (split \u0026#34;blarmey ; klarg ; jlarf ; mlarg\u0026#34; #\u0026#34;;\u0026#34;) [\u0026#34;blarmey \u0026#34; \u0026#34; klarg \u0026#34; \u0026#34; jlarf \u0026#34; \u0026#34; mlarg\u0026#34;] and trimming white space and beginning, end of a string,\nboot.user=\u0026gt; (clojure.string/trim \u0026#34; a sdfd \u0026#34;) \u0026#34;a sdfd\u0026#34; Exception handling casting exception\nboot.user=\u0026gt; steve {:name \u0026#34;Steve\u0026#34;, :age 24, :salary 7886, :company \u0026#34;Acme\u0026#34;} boot.user=\u0026gt; (try (-\u0026gt; steve :company num) (catch java.lang.ClassCastException ex \u0026#34;nope\u0026#34;)) \u0026#34;nope\u0026#34; useful technique for capturing problems with the NullPointerException. Sometimes I will get one without explanation\u0026hellip;\nfrom, https://stackoverflow.com/questions/10529063/getting-clojure-stacktrace\nusing \u0026hellip; clojure.repl.pst\nuser=\u0026gt; (try (/ 1 0) (catch Exception e (pst e))) ArithmeticException Divide by zero clojure.lang.Numbers.divide (Numbers.java:156) clojure.lang.Numbers.divide (Numbers.java:3691) user/eval28 (NO_SOURCE_FILE:8) clojure.lang.Compiler.eval (Compiler.java:6511) clojure.lang.Compiler.eval (Compiler.java:6477) clojure.core/eval (core.clj:2797) clojure.main/repl/read-eval-print--6569 (main.clj:245) clojure.main/repl/fn--6574 (main.clj:266) clojure.main/repl (main.clj:266) clojure.main/repl-opt (main.clj:332) clojure.main/main (main.clj:427) clojure.lang.Var.invoke (Var.java:423) Another Exception handling and or tracing technique, using org.clojure/tools.trace user=\u0026gt; (dotrace [list?] #_=\u0026gt; (do #_=\u0026gt; (list? []) #_=\u0026gt; (list? \u0026#39;(1 2 3)) #_=\u0026gt; (list?) #_=\u0026gt; (list? (defn f [] (do ()))) #_=\u0026gt; (list? \u0026#34;a\u0026#34;)) #_=\u0026gt; ) IllegalStateException Can\u0026#39;t dynamically bind non-dynamic var: clojure.core/list? clojure.lang.Var.pushThreadBindings (Var.java:353) , user=\u0026gt; (dotrace [list?] #_=\u0026gt; (do #_=\u0026gt; (list? []) #_=\u0026gt; (list? \u0026#39;(1 2 3)) #_=\u0026gt; (list?) #_=\u0026gt; (list? (defn f [] (do ()))) #_=\u0026gt; (list? \u0026#34;a\u0026#34;))) TRACE t1216: (list? []) TRACE t1216: =\u0026gt; false TRACE t1217: (list? (1 2 3)) TRACE t1217: =\u0026gt; true TRACE t1218: (list?) ArityException Wrong number of args (0) passed to: core$list-QMARK- clojure.lang.AFn.throwArity (AFn.java:437) The arrow -\u0026gt; this example says it all.\n(def c 5) ;; =\u0026gt; #\u0026#39;user/c (-\u0026gt; c (+ 3) (/ 2) (- 1)) ;; =\u0026gt; 3 ;; and if you are curious why (use \u0026#39;clojure.walk) ;; =\u0026gt; nil (macroexpand-all \u0026#39;(-\u0026gt; c (+ 3) (/ 2) (- 1))) ;; =\u0026gt; (- (/ (+ c 3) 2) 1) The -\u0026gt;\u0026gt; is also used. Have not yet differentiated between the two.\nvectors and lists with a vector, like [\u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot;], you can index, like\nboot.user=\u0026gt; ([\u0026#34;a\u0026#34; \u0026#34;b\u0026#34; \u0026#34;c\u0026#34;] 1) \u0026#34;b\u0026#34; but apparently, vectors are difficult to modify.\nWhereas , a list '(1 2 3) can have easier appending, but it is harder to index. nth on a list will be a O(n) operation as opposed to O(1) on a vector.\n(nth (list 1 2 3 4 5) 3) reference: http://stackoverflow.com/a/11505188/472876\nAlso..\n\u0026ldquo;Lists logically grow at the head, while vectors logically grow on the tail. You can see this in action when using the conj function. It will grow the collection according to the type of collection given to it. While you can grow collections on either side, it is performant to do so in this way.\u0026rdquo;\ngroup by , wow this is cool just like what you expect from a standard group-by, and works out of the box, http://stackoverflow.com/a/9089403/472876\n(group-by #(select-keys % [:a :b]) vector-of-hashes) and a group-by followed by an aggregate, good example, http://stackoverflow.com/questions/36139680/clojure-aggregate-and-count-in-maps http://stackoverflow.com/a/36140333/472876\nthis particular example has a hash map in DATA like [ { \u0026#34;a\u0026#34;: \u0026#34;X\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;c\u0026#34;: 188 }, { \u0026#34;a\u0026#34;: \u0026#34;Y\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;c\u0026#34;: 165 }, { \u0026#34;a\u0026#34;: \u0026#34;Y\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;c\u0026#34;: 313 }, { \u0026#34;a\u0026#34;: \u0026#34;Y\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;P\u0026#34;, \u0026#34;c\u0026#34;: 188 } ] and uses a basic count, but any kind of reduce style func can be applied. (into {} (map ; f (fn [ [k vs] ] ;[k `unique count`] [k (count (into #{} (map #(get % \u0026#34;b\u0026#34;) vs)))]) ; coll (group-by #(get % \u0026#34;a\u0026#34;) DATA))) frequencies is like the pythonic collections.Counter Get counts nicely. nice way to extract partial hash map , based on keys using something that reminds me of perl for some reason,\nboot.user=\u0026gt; (def h {:time-length 15N, :core-category \u0026#34;work\u0026#34;, :project-identifier \u0026#34;proj-1\u0026#34;, :sub-category \u0026#34;code\u0026#34;} ) boot.user=\u0026gt; ( #(select-keys % [:core-category :project-identifier]) h) {:core-category \u0026#34;work\u0026#34;, :project-identifier \u0026#34;proj-1\u0026#34;} was missing this \u0026hellip; take a few items at a time with partition, Strange detail about clojure repl , namespaces and filenames, looks like namespaces with hypens, expect clojure files with underscores, My file is one/date-utils.clj and I tried to use it like so, but I got this message. boot.user=\u0026gt; (use \u0026#39;[one.date-utils :as mydateutils]) java.io.FileNotFoundException: Could not locate one/date_utils__init.class or one/date_utils.clj on classpath. Please check that namespaces with dashes use underscores in the Clojure file name. boot.user=\u0026gt; Then i changed the file to one/date_utils.clj and problem solved. boot.user=\u0026gt; (use \u0026#39;[one.date-utils :as mydateutils]) nil concat vs using concat with apply As the docs https://www.conj.io/store/v1/org.clojure/clojure/1.8.0/clj/clojure.core/concat make note of, using just concat does not always have the intuitive behavior, boot.user=\u0026gt; (concat [[5 6 7] [2 1 2]]) ([5 6 7] [2 1 2]) boot.user=\u0026gt; (apply concat [[5 6 7] [2 1 2]]) (5 6 7 2 1 2) boot.user=\u0026gt; Basically concat takes sequences, and not a sequence of sequences And (apply f args) has the power to take a sequence args and hand its individual components to f as bare parameters. The first example was initially unintuitive, because I was trying to concat the output of a map, (def full-list (concat (map (fn [x] (blah x)) input-vector))) that was not working, and so i needed to update w/ an apply, (def full-list (apply concat (map (fn [x] (blah x)) input-vector))) Similarly to concat, conj can be used with apply in the same way\u0026hellip; app.core=\u0026gt; (def vec [0 1 2 3 4]) #\u0026#39;app.core/vec app.core=\u0026gt; (apply conj vec [99 88]) [0 1 2 3 4 99 88] So to explain the above, the normal form for conj is actually (conj vec 99 88), but if we have those args as a vector [99 88] then we can use apply to make the args vector be presented to conj as args. doseq and for from \u0026hellip; https://stackoverflow.com/a/4725502/472876 The difference is that for builds a lazy sequence and returns it while doseq is for executing side-effects and returns nil. user=\u0026gt; (for [x [1 2 3]] (+ x 5)) (6 7 8) user=\u0026gt; (doseq [x [1 2 3]] (+ x 5)) nil user=\u0026gt; (doseq [x [1 2 3]] (println x)) 1 2 3 nil If you want to build a new sequence based on other sequences, use for. If you want to do side-effects (printing, writing to a database, launching a nuclear warhead, etc) based on elements from some sequences, use doseq. Feels like comp is one of the identities of clojure in that it helps keep you using simple functions The order of operations is just like in maths \u0026hellip; (fÂºgÂºh)(x) := f(g(h(x))) play.core=\u0026gt; ((comp #(- 4 %) #(* % 9) #(+ % 95)) 5) -896 Another way to combine , merge maps.. This feels like a very particular solution playsync.core=\u0026gt; (def m1 {:foo 1 :boo 0}) #\u0026#39;playsync.core/m1 playsync.core=\u0026gt; (def m2 {:foo 2 :yoo 0}) #\u0026#39;playsync.core/m2 playsync.core=\u0026gt; playsync.core=\u0026gt; (merge-with + m1 m2) {:foo 3, :boo 0, :yoo 0} ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/clojure_tips/","summary":"(Porting over my notes from gists .. here ),..\nquick note on imports i originally inferred this (ns blah-namespace (:use clojure.core.blah ) ; this would take all the terms in blah and put them into the current namespace (:refer-clojure :exclude [blarg flarg]) ; but this is supposed to be a way to avoid term clash ; so blarg and flarg will not be used. (not that they are in clojure.core however) ) Then I read https://www.","title":""},{"content":"keys keywords from map (let [ {:keys [status headers body error]} {:status 0 :headers 1 :body 3 :error 5 :extra 88} ] (println status body) ) your.app=\u0026gt; (let [ #_=\u0026gt; {:keys [status headers body error]} {:status 0 :headers 1 :body 3 :error 5 :extra 88} #_=\u0026gt; ] #_=\u0026gt; (println status body) #_=\u0026gt; ) 0 3 nil your.app=\u0026gt; also keyword args arity (comment \u0026#34;\u0026#34; (defn foof [a b \u0026amp; {:keys [op-fn] :or {op-fn +}}] (op-fn a b)) (foof 4 5 :op-fn *) (foof 4 5 :op-fn #(str %1 \u0026#34;..\u0026#34; %2)) ) ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/cool-arity-thing/","summary":"keys keywords from map (let [ {:keys [status headers body error]} {:status 0 :headers 1 :body 3 :error 5 :extra 88} ] (println status body) ) your.app=\u0026gt; (let [ #_=\u0026gt; {:keys [status headers body error]} {:status 0 :headers 1 :body 3 :error 5 :extra 88} #_=\u0026gt; ] #_=\u0026gt; (println status body) #_=\u0026gt; ) 0 3 nil your.app=\u0026gt; also keyword args arity (comment \u0026#34;\u0026#34; (defn foof [a b \u0026amp; {:keys [op-fn] :or {op-fn +}}] (op-fn a b)) (foof 4 5 :op-fn *) (foof 4 5 :op-fn #(str %1 \u0026#34;.","title":""},{"content":"print data structures so as to preserve quotation (quotes) wow .. took too long to come across this nugget https://stackoverflow.com/questions/21136766/clojure-printing-functions-pr-vs-print pr/prn is to print/println for human readability. user=\u0026gt; (def d1 {:foo {:nil true :and \u0026#34;yay\u0026#34;}}) #\u0026#39;user/d1 user=\u0026gt; (prn \u0026#34;ok... \u0026#34; d1) \u0026#34;ok... \u0026#34; {:foo {:nil true, :and \u0026#34;yay\u0026#34;}} nil ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/debug-and-troubleshooting/","summary":"print data structures so as to preserve quotation (quotes) wow .. took too long to come across this nugget https://stackoverflow.com/questions/21136766/clojure-printing-functions-pr-vs-print pr/prn is to print/println for human readability. user=\u0026gt; (def d1 {:foo {:nil true :and \u0026#34;yay\u0026#34;}}) #\u0026#39;user/d1 user=\u0026gt; (prn \u0026#34;ok... \u0026#34; d1) \u0026#34;ok... \u0026#34; {:foo {:nil true, :and \u0026#34;yay\u0026#34;}} nil ","title":""},{"content":"Dependencies and the repl It appears adding new dependencies into the build.boot file, and then running boot local repl again, downloads required dependencies and makes them useable for in the project.\ndirectory structure for a project\nmy-project-root/ VERSION build.boot src/ blah/ foo.clj blarth.clj ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/devops-and-environment/","summary":"Dependencies and the repl It appears adding new dependencies into the build.boot file, and then running boot local repl again, downloads required dependencies and makes them useable for in the project.\ndirectory structure for a project\nmy-project-root/ VERSION build.boot src/ blah/ foo.clj blarth.clj ","title":""},{"content":"list tables, first get the faraday class thing ready.\nboot.user=\u0026gt; (use \u0026#39;[taoensso.faraday :as dynamo]) with a client-config hash defined in a separate clj file, here one/dynamo.clj is my file.\n(def client-config (if (:development env) {:access-key \u0026#34;OMGDEVELOPMENT\u0026#34; :secret-key \u0026#34;I_SHOULD_KEEP_THIS_SECRET!\u0026#34; ; Point the configuration at the DynamoDB Local :endpoint \u0026#34;http://localhost:8000\u0026#34;} {:endpoint \u0026#34;http://dynamodb.us-east-1.amazonaws.com\u0026#34;} ) ) and use list-tables from the module/class thing,\nboot.user=\u0026gt; (use \u0026#39;[one.dynamo :as db]) ; `one/dynamo.clj` boot.user=\u0026gt; (dynamo/list-tables db/client-config) (:primes :projects :times) create table to get this env part to work, I didnt see any way to set the env vars in clojure, so I just set them on my shell,\nexport development=true # and this could have been anything that reduces to boolean true actually. and the dynamodb local credentials dont matter it turns out,\n; borrowing this from source at https://github.com/jamesleonis/serverless-in-clojure ; , https://medium.com/@jamesleonis/clojure-in-aws-serverless-dynamodb-cd5ed29027a5#.u29ighn8s ; (def client-config (if (:development env) {:access-key \u0026#34;OMGDEVELOPMENT\u0026#34; :secret-key \u0026#34;I_SHOULD_KEEP_THIS_SECRET!\u0026#34; ; Point the configuration at the DynamoDB Local :endpoint \u0026#34;http://localhost:8000\u0026#34;} {:endpoint \u0026#34;http://dynamodb.us-east-1.amazonaws.com\u0026#34;} ; this is the else part. ) ) (def table-name :my_table) (dynamo/create-table client-config table-name [:index :n] {:throughput {:read 5 :write 5} :block? true}) continuing to follow along from https://github.com/jamesleonis/serverless-in-clojure ,\nboot.user=\u0026gt; (use \u0026#39;[lesson-two.dynamo :as db] ) ; using some of the predefined functions from here, boot.user=\u0026gt; (db/list-primes) [] boot.user=\u0026gt; (db/put-prime 0 2) nil boot.user=\u0026gt; (db/list-primes) [2] ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/dynamodb/","summary":"list tables, first get the faraday class thing ready.\nboot.user=\u0026gt; (use \u0026#39;[taoensso.faraday :as dynamo]) with a client-config hash defined in a separate clj file, here one/dynamo.clj is my file.\n(def client-config (if (:development env) {:access-key \u0026#34;OMGDEVELOPMENT\u0026#34; :secret-key \u0026#34;I_SHOULD_KEEP_THIS_SECRET!\u0026#34; ; Point the configuration at the DynamoDB Local :endpoint \u0026#34;http://localhost:8000\u0026#34;} {:endpoint \u0026#34;http://dynamodb.us-east-1.amazonaws.com\u0026#34;} ) ) and use list-tables from the module/class thing,\nboot.user=\u0026gt; (use \u0026#39;[one.dynamo :as db]) ; `one/dynamo.clj` boot.user=\u0026gt; (dynamo/list-tables db/client-config) (:primes :projects :times) create table to get this env part to work, I didnt see any way to set the env vars in clojure, so I just set them on my shell,","title":""},{"content":"read\n(use \u0026#39;clojure.java.io) (with-open [rdr (reader \u0026#34;/tmp/test.txt\u0026#34;)] (doseq [line (line-seq rdr)] (println line))) write (use \u0026#39;clojure.java.io) (with-open [wrtr (writer \u0026#34;/tmp/test.txt\u0026#34;)] (.write wrtr \u0026#34;Line to be written\u0026#34;)) ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/file-io/","summary":"read\n(use \u0026#39;clojure.java.io) (with-open [rdr (reader \u0026#34;/tmp/test.txt\u0026#34;)] (doseq [line (line-seq rdr)] (println line))) write (use \u0026#39;clojure.java.io) (with-open [wrtr (writer \u0026#34;/tmp/test.txt\u0026#34;)] (.write wrtr \u0026#34;Line to be written\u0026#34;)) ","title":""},{"content":"sleeping\u0026hellip; (Thread/sleep 4000) simple multithreading , from the Brave clojure book (future (Thread/sleep 4000) (println \u0026#34;I\u0026#39;ll print after 4 seconds\u0026#34;)) (println \u0026#34;I\u0026#39;ll print immediately\u0026#34;) hmm this is weird. so dereferencing the future blocks? (defn fight-crime [] (let [] (println \u0026#34;hi\u0026#34;) (Thread/sleep 2000) (println \u0026#34;ho\u0026#34;) (Thread/sleep 1000) (println \u0026#34;yo\u0026#34;) 5 )) (let [result (future (fight-crime))] (println \u0026#34;@: \u0026#34; @result) (println \u0026#34;snore. \u0026#34; ) (println \u0026#34;@: \u0026#34; @result) (Thread/sleep 1000) (println \u0026#34;@: \u0026#34; @result) ) Ah ok, but you can stop waiting.. so here we wait 10ms and then return \u0026ldquo;hmmf\u0026rdquo; if future isnt done yet. (let [result (future (fight-crime))] (println \u0026#34;@: \u0026#34; (deref result 10 \u0026#34;hmmf\u0026#34;)) (println \u0026#34;snore. \u0026#34; ) (println \u0026#34;@: \u0026#34; (deref result 10 \u0026#34;hmmf\u0026#34;)) (Thread/sleep 5000) (println \u0026#34;@: \u0026#34; (deref result 10 \u0026#34;hmmf\u0026#34;)) ) delays. cool (def jackson-5-delay (delay (let [message \u0026#34;Just call my name and I\u0026#39;ll be there\u0026#34;] (println \u0026#34;First deref:\u0026#34; message) message))) and a colorful example with future, force, delay , from http://www.braveclojure.com/concurrency/ (def gimli-headshots [\u0026#34;serious.jpg\u0026#34; \u0026#34;fun.jpg\u0026#34; \u0026#34;playful.jpg\u0026#34;]) (defn email-user [email-address] (println \u0026#34;Sending headshot notification to\u0026#34; email-address)) (defn upload-document \u0026#34;Needs to be implemented\u0026#34; [headshot] true) (let [notify (delay (email-user \u0026#34;and-my-axe@gmail.com\u0026#34;))] (doseq [headshot gimli-headshots] (future (upload-document headshot) (force notify)))) Blocking thread in action\u0026hellip; ; make a future which blocks on a promise. Then deliver promise in main thread and see what happens. (def a-promise (promise)) (def futr (future (def result (+ @a-promise 10)) (Thread/sleep 5000) (println \u0026#34;result \u0026#34; result) result)) ; (println \u0026#34;nothing yet: \u0026#34; (deref futr 10 \u0026#34;nothing.\u0026#34;)) ; deliver.. (deliver a-promise 99) (println \u0026#34;right after deliverin \u0026#34; (deref futr 10 \u0026#34;still nothin.\u0026#34;)) (Thread/sleep 5500) (println \u0026#34;had some time to think... \u0026#34; (deref futr)) delivering to promise multiple times? hmm, how about the other way around\u0026hellip; make some workers do something and have them all try deliver same promise. What will happen? (def another-promise (promise)) (def d1 (delay (Thread/sleep 6000) (deliver another-promise 6000) )) ; (def d2 (delay (Thread/sleep 4000) (deliver another-promise 4000) )) ; (def d3 (delay (Thread/sleep 1000) (deliver another-promise 1000) )) ; nothing there right? (realized? another-promise) ; now run them all... (doseq [a-worker [d1 d2 d3]] (future (force a-worker)) ) (println \u0026#34;Check promise: \u0026#34; (deref another-promise 10 \u0026#34;nothin.\u0026#34;)) (println \u0026#34;Check promise: \u0026#34; @another-promise) hmm, for the above, strange that I tried but was not able to deref the delays. Error was ClassCastException clojure.lang.Delay cannot be cast to java.util.concurrent.Future clojure.core/deref-future (core.clj:2206) exercise in http://www.braveclojure.com/concurrency/ given the yak butter search data\u0026hellip; (def yak-butter-international {:store \u0026#34;Yak Butter International\u0026#34; :price 90 :smoothness 90}) (def butter-than-nothing {:store \u0026#34;Butter Than Nothing\u0026#34; :price 150 :smoothness 83}) ;; This is the butter that meets our requirements (def baby-got-yak {:store \u0026#34;Baby Got Yak\u0026#34; :price 94 :smoothness 99}) (defn mock-api-call [result] (Thread/sleep 1000) result) (defn satisfactory? \u0026#34;If the butter meets our criteria, return the butter, else return false\u0026#34; [butter] (and (\u0026lt;= (:price butter) 100) (\u0026gt;= (:smoothness butter) 97) butter)) The original non concurrent code was \u0026hellip; (time (some (comp satisfactory? mock-api-call) [yak-butter-international butter-than-nothing baby-got-yak])) ; =\u0026gt; \u0026#34;Elapsed time: 3002.132 msecs\u0026#34; ; =\u0026gt; {:store \u0026#34;Baby Got Yak\u0026#34;, :smoothness 99, :price 94} and a concurrent version\u0026hellip; (defn blah-func [] (def best-butter-promise (promise)) (time (doseq [some-butter [yak-butter-international butter-than-nothing baby-got-yak]] (future (if ((comp satisfactory? mock-api-call) some-butter) (deliver best-butter-promise some-butter) ))) ) (time (println \u0026#34;result is: \u0026#34; @best-butter-promise)) ) (blah-func) =\u0026gt; cloj-multiproc-play.core=\u0026gt; (blah-func) \u0026#34;Elapsed time: 0.615436 msecs\u0026#34; result is: {:store Baby Got Yak, :price 94, :smoothness 99} \u0026#34;Elapsed time: 1001.655823 msecs\u0026#34; nil cloj-multiproc-play.core=\u0026gt; one more example , racing Bing vs Google (def search-result-promise (promise)) (def bingurl \u0026#34;https://www.bing.com/search?q=foobar\u0026#34;) (def googleurl \u0026#34;https://www.google.com/?gws_rd=ssl#q=foobar\u0026#34;) (doseq [url [bingurl googleurl]] (future (deliver search-result-promise (slurp url) )) ) (def html1 (slurp \u0026#34;https://www.google.com/?gws_rd=ssl#q=foobar\u0026#34;)) hot dog machine from http://www.braveclojure.com/core-async/ my version of a hot dog machine that only dispenses max number of hot dogs\u0026hellip; (defn hot-dog-machine-v2 [how-many-dogs-init] (let [in (chan) out (chan)] (go (loop [num-dogs how-many-dogs-init] ; if no hot dogs left, then done. (if (= num-dogs 0) true ; Otherwise (do (println \u0026#34;Going to \u0026lt;! block on \u0026lt;in\u0026gt; now.\u0026#34;) (\u0026lt;! in) (println \u0026#34;Now we have \u0026#34; num-dogs \u0026#34; dogs.\u0026#34;) (println \u0026#34;Going to \u0026gt;! block on \u0026lt;out\u0026gt; now.\u0026#34;) (\u0026gt;! out \u0026#34;hot dog\u0026#34;) )) (recur (dec num-dogs)) )) [in out] )) ; get channels.. (def machine-channels (hot-dog-machine-v2 5)) (def in (first machine-channels)) (def out (last machine-channels)) ; ... dispensing them (println \u0026#34;one\u0026#34;) (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) (\u0026lt;!! out) (println \u0026#34;two\u0026#34;) (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) (\u0026lt;!! out) (println \u0026#34;three\u0026#34;) (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) (\u0026lt;!! out) (println \u0026#34;four\u0026#34;) (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) (\u0026lt;!! out) (println \u0026#34;five\u0026#34;) (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) (\u0026lt;!! out) .... ; that was fine except we went into negative hot dogs... playsync.core=\u0026gt; (println \u0026#34;five\u0026#34;) five nil playsync.core=\u0026gt; (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) trueNow we have 1 dogs. Going to \u0026gt;! block on \u0026lt;out\u0026gt; now. playsync.core=\u0026gt; (\u0026lt;!! out) Going to \u0026lt;! block on \u0026lt;in\u0026gt; now. \u0026#34;hot dog\u0026#34; playsync.core=\u0026gt; playsync.core=\u0026gt; (\u0026gt;!! in \u0026#34;pocket lint\u0026#34;) Now we have -1 dogs. true Going to \u0026gt;! block on \u0026lt;out\u0026gt; now. playsync.core=\u0026gt; (\u0026lt;!! out) \u0026#34;hot dog\u0026#34;Going to \u0026lt;! block on \u0026lt;in\u0026gt; now. .. update (let [results-chan (chan) results-vector (atom [])] ; For several things in stuff-vec , start some async call that will throw results, ; into the results-chan (doseq [x stuff-vec] (do-something-async results-chan x)) ; in this case, we take from the results-chan once they are ready, ; and update (using conj) the atom results-vector (doseq [_ stuff-vec] (swap! results-vector conj (\u0026lt;!! results-chan))) ) swap syntax for updating atoms\u0026hellip; ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/futures-promises/","summary":"sleeping\u0026hellip; (Thread/sleep 4000) simple multithreading , from the Brave clojure book (future (Thread/sleep 4000) (println \u0026#34;I\u0026#39;ll print after 4 seconds\u0026#34;)) (println \u0026#34;I\u0026#39;ll print immediately\u0026#34;) hmm this is weird. so dereferencing the future blocks? (defn fight-crime [] (let [] (println \u0026#34;hi\u0026#34;) (Thread/sleep 2000) (println \u0026#34;ho\u0026#34;) (Thread/sleep 1000) (println \u0026#34;yo\u0026#34;) 5 )) (let [result (future (fight-crime))] (println \u0026#34;@: \u0026#34; @result) (println \u0026#34;snore. \u0026#34; ) (println \u0026#34;@: \u0026#34; @result) (Thread/sleep 1000) (println \u0026#34;@: \u0026#34; @result) ) Ah ok, but you can stop waiting.","title":""},{"content":"test http-kit timeout.. look at result for a timeout 1ms.. (require [\u0026#39;org.httpkit.client :as \u0026#39;http]) (let [options {:timeout 1} url \u0026#34;http://yahoo.com\u0026#34; ] (def vout @(http/get url options))) ==\u0026gt; user=\u0026gt; (keys vout) (:opts :error) user=\u0026gt; vout {:opts {:timeout 1, :method :get, :url \u0026#34;http://yahoo.com\u0026#34;}, :error #error { :cause \u0026#34;read timeout: 1ms\u0026#34; :via [{:type org.httpkit.client.TimeoutException :message \u0026#34;read timeout: 1ms\u0026#34; :at [org.httpkit.client.HttpClient clearTimeout \u0026#34;HttpClient.java\u0026#34; 82]}] :trace [[org.httpkit.client.HttpClient clearTimeout \u0026#34;HttpClient.java\u0026#34; 82] [org.httpkit.client.HttpClient run \u0026#34;HttpClient.java\u0026#34; 433] [java.lang.Thread run \u0026#34;Thread.java\u0026#34; 748]]}} user=\u0026gt; (type (:error vout)) org.httpkit.client.TimeoutException Answer the question: does http-kit func use the callback if there is an {:error ...} ? \u0026hellip; (let [options {:timeout 1} url \u0026#34;http://yahoo.com\u0026#34; callback-fn #(println \u0026#34;callback yay! ____\u0026#34; % \u0026#34;_____\u0026#34;) ] ;(def vout @(http/get url options)) ;(def vout @(http/get url options callback-fn)) (def vout (http/get url options callback-fn)) ) =\u0026gt; well looks like the Callback func is still called on an error. callback yay! ____ {:opts {:timeout 1, :method :get, :url http://yahoo.com}, :error #error { :cause read timeout: 1ms :via [{:type org.httpkit.client.TimeoutException :message read timeout: 1ms :at [org.httpkit.client.HttpClient clearTimeout HttpClient.java 82]}] :trace [[org.httpkit.client.HttpClient clearTimeout HttpClient.java 82] [org.httpkit.client.HttpClient run HttpClient.java 433] [java.lang.Thread run Thread.java 748]]}} _____ user=\u0026gt; ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/http-kit-and-timeouts/","summary":"test http-kit timeout.. look at result for a timeout 1ms.. (require [\u0026#39;org.httpkit.client :as \u0026#39;http]) (let [options {:timeout 1} url \u0026#34;http://yahoo.com\u0026#34; ] (def vout @(http/get url options))) ==\u0026gt; user=\u0026gt; (keys vout) (:opts :error) user=\u0026gt; vout {:opts {:timeout 1, :method :get, :url \u0026#34;http://yahoo.com\u0026#34;}, :error #error { :cause \u0026#34;read timeout: 1ms\u0026#34; :via [{:type org.httpkit.client.TimeoutException :message \u0026#34;read timeout: 1ms\u0026#34; :at [org.httpkit.client.HttpClient clearTimeout \u0026#34;HttpClient.java\u0026#34; 82]}] :trace [[org.httpkit.client.HttpClient clearTimeout \u0026#34;HttpClient.java\u0026#34; 82] [org.httpkit.client.HttpClient run \u0026#34;HttpClient.java\u0026#34; 433] [java.lang.Thread run \u0026#34;Thread.","title":""},{"content":"class cast exception clojure.lang.LazySeq cannot be cast to clojure.lang.IFn =\u0026gt; some code is expecting a function, but is getting a LazySeq. ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/interesting-exceptions/","summary":"class cast exception clojure.lang.LazySeq cannot be cast to clojure.lang.IFn =\u0026gt; some code is expecting a function, but is getting a LazySeq. ","title":""},{"content":"Amazonica and s3 This was not super obvious, because this example uses a java.io.ByteArrayInputStream with the the :input-stream parameter of the put-object function But in my mind this feels more like an output stream since we\u0026rsquo;re writing. but maybe this is because we\u0026rsquo;re reading from the payload . (require [\u0026#39;amazonica.aws.s3 :as \u0026#39;ss3]) (defn put-s3-obj [bucket-name s3key content] (let [payload (.getBytes content \u0026#34;UTF-8\u0026#34;) input-stream (java.io.ByteArrayInputStream. payload)] (ss3/put-object :bucket-name bucket-name :key s3key :input-stream input-stream ; :metadata {:server-side-encryption \u0026#34;AES256\u0026#34;} ;? ;:file content ))) ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/io-and-streams/","summary":"Amazonica and s3 This was not super obvious, because this example uses a java.io.ByteArrayInputStream with the the :input-stream parameter of the put-object function But in my mind this feels more like an output stream since we\u0026rsquo;re writing. but maybe this is because we\u0026rsquo;re reading from the payload . (require [\u0026#39;amazonica.aws.s3 :as \u0026#39;ss3]) (defn put-s3-obj [bucket-name s3key content] (let [payload (.getBytes content \u0026#34;UTF-8\u0026#34;) input-stream (java.io.ByteArrayInputStream. payload)] (ss3/put-object :bucket-name bucket-name :key s3key :input-stream input-stream ; :metadata {:server-side-encryption \u0026#34;AES256\u0026#34;} ;?","title":""},{"content":"logging, https://github.com/futurice/timbre ; require... [taoensso.timbre :as log] ; i have ended up using it like this, in a let, with fake variables, (let [ var1 (myfunc \u0026#34;blah\u0026#34;) fake1 (log/info (str \u0026#34;var1: \u0026#34; var1))] () ; do stuff) ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/library-and-reference/","summary":"logging, https://github.com/futurice/timbre ; require... [taoensso.timbre :as log] ; i have ended up using it like this, in a let, with fake variables, (let [ var1 (myfunc \u0026#34;blah\u0026#34;) fake1 (log/info (str \u0026#34;var1: \u0026#34; var1))] () ; do stuff) ","title":""},{"content":"I am used to python\u0026rsquo;s zip zip([1, 2, 3], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) # [(1, \u0026#39;a\u0026#39;), (2, \u0026#39;b\u0026#39;), (3, \u0026#39;c\u0026#39;)] Interleaving and partitioning can do the same thing From stackoverflow , below, This is so clever \u0026hellip; (partition 2 (interleave \u0026#39;(1 2 3) \u0026#39;(4 5 6))) ; =\u0026gt; ((1 4) (2 5) (3 6)) ; or more generally (defn zip [\u0026amp; colls] (partition (count colls) (apply interleave colls))) (zip \u0026#39;( 1 2 3) \u0026#39;(4 5 6)) ;=\u0026gt; ((1 4) (2 5) (3 6)) (zip \u0026#39;( 1 2 3) \u0026#39;(4 5 6) \u0026#39;(2 4 8)) ;=\u0026gt; ((1 4 2) (2 5 4) (3 6 8)) This was also a cool solution From here user=\u0026gt; (map vector [1 2 3] [4 5 6]) ([1 4] [2 5] [3 6]) user=\u0026gt; ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/like-zip-in-clojure/","summary":"I am used to python\u0026rsquo;s zip zip([1, 2, 3], [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) # [(1, \u0026#39;a\u0026#39;), (2, \u0026#39;b\u0026#39;), (3, \u0026#39;c\u0026#39;)] Interleaving and partitioning can do the same thing From stackoverflow , below, This is so clever \u0026hellip; (partition 2 (interleave \u0026#39;(1 2 3) \u0026#39;(4 5 6))) ; =\u0026gt; ((1 4) (2 5) (3 6)) ; or more generally (defn zip [\u0026amp; colls] (partition (count colls) (apply interleave colls))) (zip \u0026#39;( 1 2 3) \u0026#39;(4 5 6)) ;=\u0026gt; ((1 4) (2 5) (3 6)) (zip \u0026#39;( 1 2 3) \u0026#39;(4 5 6) \u0026#39;(2 4 8)) ;=\u0026gt; ((1 4 2) (2 5 4) (3 6 8)) This was also a cool solution From here user=\u0026gt; (map vector [1 2 3] [4 5 6]) ([1 4] [2 5] [3 6]) user=\u0026gt; ","title":""},{"content":"? If on a repl, but wanting to simulate a namespace ns in a project file using discussion in https://www.braveclojure.com/organization/#Anchor-3 \u0026hellip; (in-ns \u0026#39;foo.my-test) hmm I thought that would give me access to the names in that namespace, but in my project, that didnt work\u0026hellip; My namespace in .. has (ns foo.my-test (:require [org.httpkit.client :as http] [org.httpkit.fake])) and when i tried \u0026hellip; user=\u0026gt; (in-ns \u0026#39;foo.my-test) #object[clojure.lang.Namespace 0x37b2f7ef \u0026#34;foo.my-test\u0026#34;] foo.my-test=\u0026gt; (org.httpkit.fake/with-fake-http [\u0026#34;http://google.com/\u0026#34; \u0026#34;faked\u0026#34; #_=\u0026gt; \u0026#34;http://flickr.com/\u0026#34; 500] #_=\u0026gt; (:body @(http/get \u0026#34;http://google.com/\u0026#34;)) ; \u0026#34;faked\u0026#34; #_=\u0026gt; (:status @(http/get \u0026#34;http://flickr.com/\u0026#34;))) ; 500 ClassNotFoundException org.httpkit.fake java.net.URLClassLoader.findClass (URLClassLoader.java:381) Also initially confused with \u0026hellip; Starting new repl and cannot use require with raw library strings\u0026hellip; user=\u0026gt; (require [org.httpkit.client :as http]) CompilerException java.lang.ClassNotFoundException: org.httpkit.client, compiling:(/private/var/folders/mj/7bwn1wld4pscycn91fpjn1h40000gn/T/form-init5251815666209634293.clj:1:1) but use works. user=\u0026gt; (use \u0026#39;[org.httpkit.client :as http]) WARNING: get already refers to: #\u0026#39;clojure.core/get in namespace: user, being replaced by: #\u0026#39;org.httpkit.client/get nil Hmm even so, I am using :as http, so why is use replacing the clojure.core/get in this namespace? but using a fresh repl, this worked: user=\u0026gt; (require [\u0026#39;org.httpkit.client :as \u0026#39;http]) nil user=\u0026gt; user=\u0026gt; (def v1 @(http/get \u0026#34;https://www.braveclojure.com/organization/\u0026#34;)) #\u0026#39;user/v1 user=\u0026gt; (keys v1) (:opts :body :headers :status) So here, 'org.httpkit.client is a symbol and not a variable like org.httpkit.client . ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/more-on-namespaces/","summary":"? If on a repl, but wanting to simulate a namespace ns in a project file using discussion in https://www.braveclojure.com/organization/#Anchor-3 \u0026hellip; (in-ns \u0026#39;foo.my-test) hmm I thought that would give me access to the names in that namespace, but in my project, that didnt work\u0026hellip; My namespace in .. has (ns foo.my-test (:require [org.httpkit.client :as http] [org.httpkit.fake])) and when i tried \u0026hellip; user=\u0026gt; (in-ns \u0026#39;foo.my-test) #object[clojure.lang.Namespace 0x37b2f7ef \u0026#34;foo.my-test\u0026#34;] foo.my-test=\u0026gt; (org.httpkit.fake/with-fake-http [\u0026#34;http://google.com/\u0026#34; \u0026#34;faked\u0026#34; #_=\u0026gt; \u0026#34;http://flickr.","title":""},{"content":"from clojure for machine learnings.. (defn plot-points \u0026#34;plots sample points of a solution s\u0026#34; [s] (let [X (concat (:hidden s) (:observed s)) Y (concat (:hidden-values s) (:observed-values s))] (view ; NOTE save instead of view can save to a file. (add-points (xy-plot X Y) (:observed s) (:observed-values s))))) ; namespace... ; [incanter \u0026#34;1.5.4\u0026#34;] (ns my-namespace (:use [incanter.charts :only [xy-plot add-points]] [incanter.core :only [view]]) (:require [clojure.core.matrix.operators :as M] [clatrix.core :as cl])) (ns my-namespace (:use clojure.core.matrix) (:require [clatrix.core :as cl])) ; from csv , to matrix.. (with-open [reader (io/reader \u0026#34;in-file.csv\u0026#34;)] (doall (csv/read-csv reader))) ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/oh-plotting-cool/","summary":"from clojure for machine learnings.. (defn plot-points \u0026#34;plots sample points of a solution s\u0026#34; [s] (let [X (concat (:hidden s) (:observed s)) Y (concat (:hidden-values s) (:observed-values s))] (view ; NOTE save instead of view can save to a file. (add-points (xy-plot X Y) (:observed s) (:observed-values s))))) ; namespace... ; [incanter \u0026#34;1.5.4\u0026#34;] (ns my-namespace (:use [incanter.charts :only [xy-plot add-points]] [incanter.core :only [view]]) (:require [clojure.core.matrix.operators :as M] [clatrix.core :as cl])) (ns my-namespace (:use clojure.","title":""},{"content":"port my notes from here https://gist.github.com/namoopsoo/fa903799b958ffc9f279cd293e83e9d9 and here https://gist.github.com/namoopsoo/df08c674b4e3e4794e97601682242c51\nand here https://gist.github.com/namoopsoo/607f29e923ceaba890588e69293413cf\n","permalink":"https://michal.piekarczyk.xyz/handy/clojure/todo/","summary":"port my notes from here https://gist.github.com/namoopsoo/fa903799b958ffc9f279cd293e83e9d9 and here https://gist.github.com/namoopsoo/df08c674b4e3e4794e97601682242c51\nand here https://gist.github.com/namoopsoo/607f29e923ceaba890588e69293413cf","title":""},{"content":"tools.trace debugging exceptions and stack trace https://github.com/clojure/tools.trace dependency: [org.clojure/tools.trace \u0026quot;0.7.9\u0026quot;] user=\u0026gt; (use \u0026#39;clojure.tools.trace) ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/tools.trace-debugging-and-stack-trace/","summary":"tools.trace debugging exceptions and stack trace https://github.com/clojure/tools.trace dependency: [org.clojure/tools.trace \u0026quot;0.7.9\u0026quot;] user=\u0026gt; (use \u0026#39;clojure.tools.trace) ","title":""},{"content":"hmm\u0026hellip; using is so the library is built in, but you still have to start useing it.. boot.user=\u0026gt; (is (= 4 (+ 2 2))) java.lang.RuntimeException: Unable to resolve symbol: is in this context clojure.lang.Compiler$CompilerException: java.lang.RuntimeException: Unable to resolve symbol: is in this context, compiling:(/var/folders/7_/sbz867_n7bdcdtdry2mdz1z00000gn/T/boot.user2780891586981282255.clj:1:1) boot.user=\u0026gt; boot.user=\u0026gt; boot.user=\u0026gt; (use \u0026#39;clojure.test) nil boot.user=\u0026gt; (is (= 4 (+ 2 2))) true lein test Running all tests in a file lein test module/blah/test_file.py Running specific deftest in module_hmm/blah/test_file.py called test-foo lein test :only module-hmm.blah.test-file/test-foo ","permalink":"https://michal.piekarczyk.xyz/handy/clojure/unit-testing/","summary":"hmm\u0026hellip; using is so the library is built in, but you still have to start useing it.. boot.user=\u0026gt; (is (= 4 (+ 2 2))) java.lang.RuntimeException: Unable to resolve symbol: is in this context clojure.lang.Compiler$CompilerException: java.lang.RuntimeException: Unable to resolve symbol: is in this context, compiling:(/var/folders/7_/sbz867_n7bdcdtdry2mdz1z00000gn/T/boot.user2780891586981282255.clj:1:1) boot.user=\u0026gt; boot.user=\u0026gt; boot.user=\u0026gt; (use \u0026#39;clojure.test) nil boot.user=\u0026gt; (is (= 4 (+ 2 2))) true lein test Running all tests in a file lein test module/blah/test_file.py Running specific deftest in module_hmm/blah/test_file.","title":""},{"content":"Passing large dataframes with dbutils.notebook.run ! At one point when migrating databricks notebooks to be useable purely with dbutils.notebook.run, the question came up, hey dbutils.notebook.run is a great way of calling notebooks explicitly, avoiding global variables that make code difficult to lint and debug, but what about spark dataframes?\nI had come across this https://docs.databricks.com/notebooks/notebook-workflows.html#pass-structured-data nice bit of documentation about using the spark global temp view to handle name references to nicely shuttle around dataframes by reference, given that a caller notebook and a callee notebook share a JVM and theoretically this is instantaneous.\nHowever the example code was a bit lacking and I ended up writing some nice helper functions to make the passing of dataframes, alongside other parameters, a little bit easier and more intuitive!\nOne of the issues I had with the toy example was that it used static names to pass dataframes, like my_data. This was clearly just an example, but I wanted a higher gurantee in avoiding weird collisions, so I used uuid to help randomize the names.\nBut also I wanted to be able to nicely debug my new view names, so I wanted to mix the random uuid names with informative names too. It is not straightforward to programmatically capture the name of a variable as a string (you can go down a rabbit hole trying to figure this out haha) so I just settled on creating a simple function prepare_arguments which takes keyword arguments and uses them as the string names of dataframes, when creating random temp view names.\nI also wanted the flexibility of just mixing and matching the plain parmeters you pass in, along with dataframes, without making a big deal about it.\nHere is where I ended up below :)\nimport json import pandas as pd from uuid import uuid4 from pyspark.sql import SparkSession def prepare_arguments(**kwargs): \u0026#34;\u0026#34;\u0026#34;Create the dbutils.notebook.run payload and put dataframes into global_temp.\u0026#34;\u0026#34;\u0026#34; input_dataframes = {k: v for (k, v) in kwargs.items() if isinstance(v, pd.DataFrame)} the_rest = {k: v for (k, v) in kwargs.items() if k not in input_dataframes} dataframes_dict = prepare_dataframe_references(**input_dataframes) return {**the_rest, \u0026#34;input_dataframes\u0026#34;: json.dumps(dataframes_dict)} def handle_output(raw_output): output = json.loads(raw_output) dataframes_dict = output.pop(\u0026#34;output_dataframe_references\u0026#34;, {}) output_dataframes = dereference_dataframes(dataframes_dict) the_rest = {k: v for (k, v) in output.items() if k not in output_dataframes} return {**output_dataframes, **the_rest} def dereference_dataframes(dataframes_dict): spark = SparkSession.builder.appName(\u0026#34;project\u0026#34;).getOrCreate() return { name: spark.table(\u0026#34;global_temp.\u0026#34; + view_name) for (name, view_name) in dataframes_dict.items() } def prepare_dataframe_references(**kwargs): \u0026#34;\u0026#34;\u0026#34;Puts dataframes into the global_temp schema and returns the view names. Args: kwargs: key value pairs of names and dataframes e.g. \u0026#34;some_df\u0026#34;: \u0026lt;DataFrame\u0026gt;, \u0026#34;another_df\u0026#34;: \u0026lt;DataFrame\u0026gt;, If any value is not a DataFrame, throws an exception. Returns: Dict mapping the same input names to view names. e.g. { \u0026#34;some_df\u0026#34;: \u0026#34;some_df_fae8f78\u0026#34;, \u0026#34;another_df\u0026#34;: \u0026#34;another_df_0a54d6fe\u0026#34;, } \u0026#34;\u0026#34;\u0026#34; input_dataframes = [ {\u0026#34;name\u0026#34;: k, \u0026#34;df\u0026#34;: v, \u0026#34;view_name\u0026#34;: f\u0026#34;{k}_{str(uuid4())[:8]}\u0026#34;} for (k, v) in kwargs.items() if isinstance(v, pd.DataFrame) ] the_rest = { k: v for (k, v) in kwargs.items() if k not in [x[\u0026#34;name\u0026#34;] for x in input_dataframes] } print(\u0026#34;the_rest\u0026#34;, the_rest) if the_rest: print(\u0026#34;also got non dataframe arguments, oops\u0026#34;, the_rest) raise Exception(\u0026#34;Oops, got some non dataframe arguments.\u0026#34;) for x in input_dataframes: x[\u0026#34;df\u0026#34;].createOrReplaceGlobalTempView(x[\u0026#34;view_name\u0026#34;]) return {x[\u0026#34;name\u0026#34;]: x[\u0026#34;view_name\u0026#34;] for x in input_dataframes} ","permalink":"https://michal.piekarczyk.xyz/handy/databricks-spark/dbutils-notebook-run-and-global-temp-view/","summary":"Passing large dataframes with dbutils.notebook.run ! At one point when migrating databricks notebooks to be useable purely with dbutils.notebook.run, the question came up, hey dbutils.notebook.run is a great way of calling notebooks explicitly, avoiding global variables that make code difficult to lint and debug, but what about spark dataframes?\nI had come across this https://docs.databricks.com/notebooks/notebook-workflows.html#pass-structured-data nice bit of documentation about using the spark global temp view to handle name references to nicely shuttle around dataframes by reference, given that a caller notebook and a callee notebook share a JVM and theoretically this is instantaneous.","title":""},{"content":"comparing really large spark dataframes I had this usecase where I wanted to be able to check if very large multi-million row and multi-thousand column dataframes were equal, but the advice online about using df1.subtract(df2) just was not cutting it because it was just too slow. It seems to me the df1.subtract(df2) approach more or less is a O(n^2) approach where it is necessary to compare each row in df1 with each row in df2. Instead I was wondering, hey if there are known index columns in these dataframes, maybe we can cheat a little and join them first and then do the comparison after joining them.\nWhat I came up with below so far does not handle the case yet of index keys that are not common. I would like to add that at some point. And for now this also requires the schemas of the dataframes to be the same.\ndef join_compare(df1, df2, index_cols, head_n_rows=None, cache=True): # TODO fix to be outer join I think is needed? For now assumes all index cols match fully. assert_schema_equality_ignore_nullable(df1.schema, df2.schema) if head_n_rows is not None: df1 = df1.limit(head_n_rows) feature_cols = list(set(df1.columns) - set(index_cols)) which_type_names = { x.name: x.dataType.typeName() for x in df1.schema.fields if x.name not in index_cols } double_cols = [k for (k, v) in which_type_names.items() if v == \u0026#34;double\u0026#34;] string_cols = [k for (k, v) in which_type_names.items() if v == \u0026#34;string\u0026#34;] integer_cols = [k for (k, v) in which_type_names.items() if v == \u0026#34;integer\u0026#34;] # To find the actual doubles that are \u0026lt; 1, calculate col means. stats_dict = dict( df1.select([f.mean(k).alias(k) for k in double_cols]) .toPandas() .to_dict(orient=\u0026#34;records\u0026#34;)[0] ) double_cols_for_reals = [k for (k, v) in stats_dict.items() if v \u0026lt;= 1] integer_types_double_on_paper = [k for (k, v) in stats_dict.items() if v \u0026gt; 1] print( \u0026#34;all doubles:\u0026#34;, len(double_cols), \u0026#34;, doubles for real:\u0026#34;, len(double_cols_for_reals), ) diff_dfs = {} double_sensitivity = 0.1 integer_condition = reduce( or_, ( [ (f.round(f.abs(f.col(f\u0026#34;x.{col}\u0026#34;) - f.col(f\u0026#34;y.{col}\u0026#34;))) \u0026gt; 0) for col in (integer_types_double_on_paper + integer_cols) ] ), ) string_condition = reduce( or_, ([(f.col(f\u0026#34;x.{col}\u0026#34;) != f.col(f\u0026#34;y.{col)\u0026#34;)) for col in string_cols]) ) doubles_condition = reduce( or_, ( [ (f.abs(f.col(f\u0026#34;x.{col}\u0026#34;) - f.col(f\u0026#34;y.{col)\u0026#34;)) \u0026gt; double_sensitivity) for col in double_cols_for_reals ] ), ) rounded_cols = reduce( lambda x, y: x + y, [ ( f.round(f.col(f\u0026#34;x.{col)\u0026#34;), 2).alias(f\u0026#34;x_{col}\u0026#34;), f.round(f.col(f\u0026#34;y.{col)\u0026#34;), 2).alias(f\u0026#34;y_{col}\u0026#34;), ) for col in double_cols_for_reals ], ) select_integer_cols = reduce( lambda x, y: x + y, [ (f.col(f\u0026#34;x.{col}\u0026#34;).alias(f\u0026#34;y_ {col)\u0026#34;), f.col(f\u0026#34;y.{col}\u0026#34;).alias(f\u0026#34;y_{col}\u0026#34;)) for col in (integer_types_double_on_paper + integer_cols) ], ) select_string_cols = reduce( lambda x, y: x + y, [ (f.col(f\u0026#34;x.{col}\u0026#34;).alias(f\u0026#34;x_{col}\u0026#34;), f.col(f\u0026#34;y.{col}\u0026#34;).alias(f\u0026#34;y_{col}\u0026#34;)) for col in string_cols ], ) print(\u0026#34;starting doubles_diff\u0026#34;) diff_dfs[\u0026#34;doubles_diffdf\u0026#34;] = ( df1.alias(\u0026#34;x\u0026#34;).join(df2.alias(\u0026#34;y\u0026#34;), index_cols, ).where(doubles_condition).select(*index_cols, *rounded_cols) ) print(\u0026#34;starting integer diff\u0026#34;) diff_dfs[\u0026#34;integer_diffdf\u0026#34;] = ( df1.alias(\u0026#34;x\u0026#34;).join(df2.alias(\u0026#34;y\u0026#34;), index_cols, ).where(integer_condition).select( *index_cols, *select_integer_cols) ) print(\u0026#34;starting string diff\u0026#34;) diff_dfs[\u0026#34;string_diffdf\u0026#34;] = ( df1.alias(\u0026#34;x\u0026#34;).join(df2.alias(\u0026#34;y\u0026#34;), index_cols, ).where(string_condition).select(*index_cols, *select_string_cols) ) if cache: print(\u0026#34;caching\u0026#34;) for k in diff_dfs.keys(): diff_dfs[k] = diff_dfs[k].cache() return diff_dfs ","permalink":"https://michal.piekarczyk.xyz/handy/databricks-spark/join-merge-parquet/","summary":"comparing really large spark dataframes I had this usecase where I wanted to be able to check if very large multi-million row and multi-thousand column dataframes were equal, but the advice online about using df1.subtract(df2) just was not cutting it because it was just too slow. It seems to me the df1.subtract(df2) approach more or less is a O(n^2) approach where it is necessary to compare each row in df1 with each row in df2.","title":""},{"content":"My Rules of text dbutils.widgets (0) Reading a widget that does not exist results in\n\u0026#34;com.databricks.dbutilsvl.InputWidgetNotDefined\u0026#34;` (1)\n\u0026#34;dbutils.widgets.text (name, value)\u0026#34; will set the value of a widget only if it does not already exist. If it already exists, this does nothing\n(2) You cannot change the value of a widget, but you can remove it and then set it again with the same name, with\n\u0026#34;dbutils.widgets.text (name, value)\u0026#34; . However, if a widget was set in cell1, then cell2 cannot both remove and reset the widget. This will surprisingly have no effect!\nFor example you can do this\n# Cell 1 dbutils.widgets. text (\u0026#34;root\u0026#34; , \u0026#34;potato\u0026#34;) dbutils.widgets. get ( \u0026#34;root\u0026#34;) # =\u0026gt; \u0026#34;potato\u0026#34; # Cell 2 dbutils.widgets.remove ( \u0026#34;root\u0026#34; ) dbutils.widgets. get (\u0026#34;root\u0026#34;) # =\u0026gt; InputWidgetNotDefined # Cell 3 dbutils.widgets.text ( \u0026#34;root\u0026#34;, \u0026#34;yam\u0026#34; ) dbutils.widgets. get (\u0026#34;root\u0026#34; ) # =\u0026gt; \u0026#34;yam\u0026#34; But this will have no effect,\n# Cell 1 dbutils.widgets.text (\u0026#34;root\u0026#34;, \u0026#34;potato\u0026#34;) dbutils.widgets.get (\u0026#34;root\u0026#34;) # =\u0026gt; \u0026#34;potato\u0026#34; # Cell 2 dbutils.widgets. remove (\u0026#34;root\u0026#34;) dbutils.widgets.text(\u0026#34;root\u0026#34;, \u0026#34;yam\u0026#34;) dbutils.widgets.get(\u0026#34;root\u0026#34;) # =\u0026gt; \u0026#34;potato\u0026#34; (3) In spite of the above, if a single cell sets a widget, it can then remove it and reset it, any number of times\n# Cell 1 dbutils.widgets.text (\u0026#34;root\u0026#34;, \u0026#34;potato\u0026#34;) print (dbutils.widgets.get(\u0026#34;root\u0026#34;)) # =\u0026gt; \u0026#34;potato\u0026#34; dbutils.widgets.remove (\u0026#34;root\u0026#34;=) dbutils.widgets.text(\u0026#34;root\u0026#34;, \u0026#34;yam\u0026#34;) print (dbutils.widgets.get(\u0026#34;root\u0026#34;)) # =\u0026gt;\u0026#34;yam\u0026#34; dbutils.widgets.remove (\u0026#34;root\u0026#34;) dbutils.widgets.text(\u0026#34;root\u0026#34;, \u0026#34;potato\u0026#34;) print (dbutils.widgets.get(\u0026#34;root\u0026#34;)) # =\u0026gt; \u0026#34;potato\u0026#34; dbutils.widgets.remove (\u0026#34;root\u0026#34;) (4) If you set a widget in \u0026ldquo;notebookA\u0026rdquo; and then \u0026ldquo;%run notebookB\u0026rdquo;, \u0026ldquo;notebookB\u0026rdquo; will be able to read the widget that was set by \u0026ldquo;notebookA\u0026rdquo;.\n(5) Trying to remove a widget twice results in a \u0026quot;com.databricks.dbutils_v1.InputwidgetNotDefined\u0026quot; error.\n(6) If \u0026ldquo;notebookA\u0026rdquo; defines a widget and then calls \u0026ldquo;%run notebookB\u0026rdquo; and \u0026ldquo;notebookB\u0026rdquo; tries to remove the widget, it will not be able to. Basically seems like a notebook cannot remove a widget that was defined in another notebook, but it can still read it, as long as \u0026ldquo;%run\u0026rdquo; is used.\n","permalink":"https://michal.piekarczyk.xyz/handy/databricks-spark/rules-of-widgets/","summary":"My Rules of text dbutils.widgets (0) Reading a widget that does not exist results in\n\u0026#34;com.databricks.dbutilsvl.InputWidgetNotDefined\u0026#34;` (1)\n\u0026#34;dbutils.widgets.text (name, value)\u0026#34; will set the value of a widget only if it does not already exist. If it already exists, this does nothing\n(2) You cannot change the value of a widget, but you can remove it and then set it again with the same name, with\n\u0026#34;dbutils.widgets.text (name, value)\u0026#34; . However, if a widget was set in cell1, then cell2 cannot both remove and reset the widget.","title":""},{"content":"Do a build login from shell, $(aws --profile my-local-aws-profile ecr get-login --no-include-email --region us-east-1) build, docker build -t name-of-image -f path/to/Dockerfile path/to/docker/context Run your container # run using an image name, # note that -v takes an absolute path... docker run -i -t -v $(pwd)/local/path:/docker/path \u0026lt;name-of-image\u0026gt;:\u0026lt;tag\u0026gt; # or with a specific image id... say \u0026#34;ad6576e\u0026#34; docker run -d=false -i -t ad6576e If you need your container to have your aws creds Nice hack is to map the \u0026ldquo;root\u0026rdquo; user of your container .aws directory\ndocker run -i -t -v ~/.aws:/root/.aws -v $(pwd)/local/path:/docker/path \u0026lt;name-of-image\u0026gt;:\u0026lt;tag\u0026gt; ( cool idea from a colleague ^ ) ","permalink":"https://michal.piekarczyk.xyz/handy/docker/hmm/","summary":"Do a build login from shell, $(aws --profile my-local-aws-profile ecr get-login --no-include-email --region us-east-1) build, docker build -t name-of-image -f path/to/Dockerfile path/to/docker/context Run your container # run using an image name, # note that -v takes an absolute path... docker run -i -t -v $(pwd)/local/path:/docker/path \u0026lt;name-of-image\u0026gt;:\u0026lt;tag\u0026gt; # or with a specific image id... say \u0026#34;ad6576e\u0026#34; docker run -d=false -i -t ad6576e If you need your container to have your aws creds Nice hack is to map the \u0026ldquo;root\u0026rdquo; user of your container .","title":""},{"content":"F test statistic to evaluate the features One F test produces a ratio (called an F-value) comparing the variation between two populations\u0026rsquo; sample means and the variation within the samples. With a greater variation between the population samples, we are more likely to reject the null hypothesis that the samples are of the same source distribution. With a higher F-value, the lower the p-value associated for the distribution of this test. [1] . Also good example at the [2] Here below, I had a DataFrame , df with some features, f1, f2, f3, f4 and target , y , Based on my results, f3 is great, f4 barely better than random numbers. f_regression is meant for real number y And f_classif should only be used for a classification problem where y is a class. [3] from sklearn.feature_selection import f_regression, mutual_info_regression def evaluate_feature(df, feature, target): X = np.array(df[feature].tolist()) num_rows = X.shape[0] X = np.reshape(X, (num_rows, 1)) y = df[target].tolist() f_value, _ = f_regression(X, y) print(feature, f_value) num_rows = df.shape[0] # Random X = np.random.rand(num_rows, 1) f_value, _ = f_regression(X, y) print(\u0026#39;random, \u0026#39;, f_value) # Random X = np.random.rand(num_rows, 1) f_value, _ = f_regression(X, y) print(\u0026#39;random, \u0026#39;, f_value) # Random X = np.random.rand(num_rows, 1) f_value, _ = f_regression(X, y) print(\u0026#39;random, \u0026#39;, f_value) for feature in [\u0026#39;y\u0026#39;, \u0026#39;f1\u0026#39;, \u0026#39;f2\u0026#39;, \u0026#39;f3\u0026#39;, \u0026#39;f4\u0026#39;, ]: evaluate_feature(df, feature, \u0026#39;y\u0026#39;) random, [0.42851302] random, [0.60725371] random, [0.56094036] y [3.50677485e+16] f1 [52.90786486] f2 [900.76441029] f3 [4145.1618757] f4 [1.22335227] Refs [1] https://www.statology.org/what-does-a-high-f-value-mean/ [2] https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html [3] https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif\n","permalink":"https://michal.piekarczyk.xyz/handy/eda/f_test_statistic/","summary":"F test statistic to evaluate the features One F test produces a ratio (called an F-value) comparing the variation between two populations\u0026rsquo; sample means and the variation within the samples. With a greater variation between the population samples, we are more likely to reject the null hypothesis that the samples are of the same source distribution. With a higher F-value, the lower the p-value associated for the distribution of this test.","title":""},{"content":"Appreciate this post on helping to choose between a few available tests in determining if there are meaningful relationships between feature data. In particular,\nANOVA compares two variables, where one is categorical (binning is helpful here) and one is continuous. Chi-square is useful for two categorical comparing two cateorical varables, on the other hand. And Pearson Correlation can be used between two continiuous variables But the caveat is that this test assumes both variables are normally distributed And outliers should be chopped off with some preprocessing. ","permalink":"https://michal.piekarczyk.xyz/handy/eda/variable_independence_techniques/","summary":"Appreciate this post on helping to choose between a few available tests in determining if there are meaningful relationships between feature data. In particular,\nANOVA compares two variables, where one is categorical (binning is helpful here) and one is continuous. Chi-square is useful for two categorical comparing two cateorical varables, on the other hand. And Pearson Correlation can be used between two continiuous variables But the caveat is that this test assumes both variables are normally distributed And outliers should be chopped off with some preprocessing.","title":""},{"content":"get the parents of a \u0026lt;blah-branch\u0026gt; git rev-list --parents \u0026lt;blah-commit\u0026gt; beaaaafffff1111111111111111111 fe0000aaaad111111111111111 One of them will typically be the hash of \u0026lt;blah-branch\u0026gt; itself\n","permalink":"https://michal.piekarczyk.xyz/handy/git/","summary":"get the parents of a \u0026lt;blah-branch\u0026gt; git rev-list --parents \u0026lt;blah-commit\u0026gt; beaaaafffff1111111111111111111 fe0000aaaad111111111111111 One of them will typically be the hash of \u0026lt;blah-branch\u0026gt; itself","title":""},{"content":"Super cool model baseline technique From scikitlearn, this is a super cool way of getting a baseline. In the past I had done this majority class assignment manually. But it is super cool this is kind of built in.\nfrom sklearn.dummy import DummyClassifier X, y = get_some_data() X_train, X_test, y_train, y_test = do_some_splitting(X, y) dummy_clf = DummyClassifier(strategy=\u0026#34;most_frequent\u0026#34;) dummy_clf.fit(X_train, y_train) dummy_clf.score(X_test, y_test) ","permalink":"https://michal.piekarczyk.xyz/handy/model/dummy_baseline/","summary":"Super cool model baseline technique From scikitlearn, this is a super cool way of getting a baseline. In the past I had done this majority class assignment manually. But it is super cool this is kind of built in.\nfrom sklearn.dummy import DummyClassifier X, y = get_some_data() X_train, X_test, y_train, y_test = do_some_splitting(X, y) dummy_clf = DummyClassifier(strategy=\u0026#34;most_frequent\u0026#34;) dummy_clf.fit(X_train, y_train) dummy_clf.score(X_test, y_test) ","title":""},{"content":"Mainly notes from reaading the Natural Language Processing with Transformers book Really nice book! I have the urge to write down for myself some snippets so I can more easily refer to them later.\nRead a dataset to pandas import pandas as pd from datasets import load_dataset emotions = load_dataset(\u0026#34;emotion\u0026#34;) # emotions[\u0026#34;train\u0026#34;] # this is still a datasets.arrow_dataset.Dataset emotions.set_format(type=\u0026#34;pandas\u0026#34;) df = emotions[\u0026#34;train\u0026#34;][:] # but adding that \u0026#34;[:]\u0026#34; slice grants a DataFrame ! df.head() # Go back to initial format emotions.reset_format() Cool Mini Tokenization example from transformers import AutoTokenizer model_ckpt = \u0026#34;distilbert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) text = \u0026#34;Tokenizing text is a core task of NLP.\u0026#34; encoded_text = tokenizer(text) print(encoded_text) {\u0026#39;input_ids\u0026#39;: [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} # oh cool and the tokenizer lets you convert back , tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids) print(tokens) [\u0026#39;[CLS]\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;##izing\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;core\u0026#39;, \u0026#39;task\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;nl\u0026#39;, \u0026#39;##p\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;[SEP]\u0026#39;] And finally\nprint(tokenizer.convert_tokens_to_string(tokens)) [CLS] tokenizing text is a core task of nlp. [SEP] ","permalink":"https://michal.piekarczyk.xyz/handy/model/hugging-face/","summary":"Mainly notes from reaading the Natural Language Processing with Transformers book Really nice book! I have the urge to write down for myself some snippets so I can more easily refer to them later.\nRead a dataset to pandas import pandas as pd from datasets import load_dataset emotions = load_dataset(\u0026#34;emotion\u0026#34;) # emotions[\u0026#34;train\u0026#34;] # this is still a datasets.arrow_dataset.Dataset emotions.set_format(type=\u0026#34;pandas\u0026#34;) df = emotions[\u0026#34;train\u0026#34;][:] # but adding that \u0026#34;[:]\u0026#34; slice grants a DataFrame !","title":""},{"content":"Loss functions vs Metric functions I like the phrasing in this SO answer, that loss functions are optimized directly when training but that metrics are optimized indirectly. I was trying to figure out last year why functions commonly used as metrics (F1 and AUC) are not listed in the tensor flow keras loss functions . I did however earlier try using F1 as a loss function when trying to understand my particular problem. (At least one error I ran into hints that it is not that simplle because you need to write extra code for computing the gradient.) But even if you can produce the code to compute a gradient for your custom loss function, maybe some metrics are more expensive to run SGD than others. (Also F1 is clearly less sensitive than a function that uses probabilities or logits directly) .\n","permalink":"https://michal.piekarczyk.xyz/handy/model/loss/","summary":"Loss functions vs Metric functions I like the phrasing in this SO answer, that loss functions are optimized directly when training but that metrics are optimized indirectly. I was trying to figure out last year why functions commonly used as metrics (F1 and AUC) are not listed in the tensor flow keras loss functions . I did however earlier try using F1 as a loss function when trying to understand my particular problem.","title":""},{"content":"TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob \u0026gt;= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=[\u0026#39;actual\u0026#39;], colnames=[\u0026#39;predictions\u0026#39;]) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook.com/ .\nSo first, when using ConfusionMatrixDisplay out of the box, I get\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix with plt.style.context(\u0026#39;dark_background\u0026#39;): cm = confusion_matrix(y_valid, y_preds, normalize=\u0026#34;true\u0026#34;) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot() plt.show() And I can see why that was not used in the book haha because the modified version, below, looks much better indeed,\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix def plot_confusion_matrix(y_preds, y_true, labels): with plt.style.context(\u0026#34;dark_background\u0026#34;): cm = confusion_matrix(y_true, y_preds, normalize=\u0026#34;true\u0026#34;) fix, ax = plt.subplots(figsize=(6, 6)) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(cmap=\u0026#34;Blues\u0026#34;, values_format=\u0026#34;.2f\u0026#34;, ax=ax, colorbar=False) plt.title(\u0026#34;Normalized confusion matrix\u0026#34;) plt.show() y_preds = lr_clf.predict(X_valid) plot_confusion_matrix(y_preds, y_valid, labels) f1 def calc_f1(confusion): TN = confusion.loc[0, 0] FP = confusion.loc[0, 1] FN = confusion.loc[1, 0] TP = confusion.loc[1, 1] precision = TP/(FP + TP) recall = TP/(FN + TP) return 2*(precision**2)/(precision + recall) predictions = [] # list of probabilities , e.g. array([0.05567192, 0.03781519, 0.05437384, 0.01572161, ...]) cutoffs = np.arange(0.01, 0.5, 0.01) f1_vec = [] for c in cutoffs: confusion = pd.crosstab(index=testdf.iloc[:, 0], columns= (predictions \u0026gt; c), rownames=[\u0026#39;actual\u0026#39;], colnames=[\u0026#39;predictions\u0026#39;]) try: f1 = calc_f1(confusion) except TypeError: f1 = np.nan f1_vec.append(f1) # fig = plt.figure() plt.plot(cutoffs, np.array(f1_vec)) plt.xlabel(\u0026#39;cutoff\u0026#39;) plt.ylabel(\u0026#39;f1\u0026#39;) plt.show() ks for a cutoff def get_flargs(confusion): cols = confusion.columns.tolist() if False not in cols: TN = 0 FN = 0 else: TN = confusion.loc[0, False] # loc[0, 0] this works in newer pandas , not 0.18 FN = confusion.loc[1, False] if True not in cols: FP = 0 TP = 0 else: FP = confusion.loc[0, True] TP = confusion.loc[1, True] return (TP, FP, TN, FN) def calc_f1(TP, FP, TN, FN): if (FP + TP) == 0 or (FN + TP) == 0: return np.nan precision = 1.0*TP/(FP + TP) recall = 1.0*TP/(FN + TP) return {2*(precision*recall)/(precision + recall)} def ks_for_cutoff(TP, FP, TN, FN): # It is the maximum difference between TPR (aka recall) and FPR () tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) return tpr - fpr def thisthings(y_true, y_prob): cutoffs = np.arange(0.01, 1.0, 0.01) f1_vec = [] ks_vec = [] tpr_vec = [] fpr_vec = [] tnr_vec = [] for c in cutoffs: y_pred = (y_prob \u0026gt; c) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=[\u0026#39;actual\u0026#39;], colnames=[\u0026#39;predictions\u0026#39;]) # print (c, confusion.shape, confusion.columns.tolist()) (TP, FP, TN, FN) = get_flargs(confusion) try: tpr = 1.0*TP/(FN + TP) # aka recall except: tpr = np.nan try: fpr = 1.0*FP/(FP + TN) except: fpr = np.nan tpr_vec.append(tpr) fpr_vec.append(fpr) # f1 = calc_f1(confusion) f1 = sklearn.metrics.f1_score(y_true, y_pred) f1_vec.append(f1) ks = ks_for_cutoff(TP, FP, TN, FN) ks_vec.append(ks) return [cutoffs, f1_vec, ks_vec, tpr_vec, fpr_vec, tnr_vec] [cutoffs, f1_vec, ks_vec, tpr_vec, fpr_vec, tnr_vec] = thisthings(y_true, y_prob) plt.plot(cutoffs[:20], np.array(ks_vec)[:20], label=\u0026#39;ks\u0026#39;) plt.plot(cutoffs[:20], np.array(fpr_vec)[:20], label=\u0026#39;fpr\u0026#39;) plt.plot(cutoffs[:20], np.array(tpr_vec)[:20], label=\u0026#39;tpr\u0026#39;) plt.xlabel(\u0026#39;cutoff\u0026#39;) plt.legend() plt.show() Weighted Precision Had to reverse engineer this from the source code here , https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html \u0026hellip; But for a 2 class classification problem weighted precision is basically weighing correctness of positive predictions and correctness of negative predictions, proportional to the number of positive and negative labels in the data. weighted_precision = [(TN/(FN + TN)) * ((size (label = N))/( size (total)))] + [(TP/(FP + TP)) * ((size(label = P))/(size (total)))] References https://www.datavedas.com/model-evaluation-in-python/ ","permalink":"https://michal.piekarczyk.xyz/handy/model/metrics/","summary":"TPR, FPR tpr = 1.0*TP/(FN + TP) # aka recall fpr = 1.0*FP/(FP + TN) # Confusion matrix Given a testdf where first column contains actual labels, 0, 1, and predictions is a list of probabilities, y_pred = (y_prob \u0026gt;= 0.08) confusion = pd.crosstab(index=y_true, columns=y_pred, rownames=[\u0026#39;actual\u0026#39;], colnames=[\u0026#39;predictions\u0026#39;]) predictions False True actual 0 509 132 1 32 22 Also there is a super nice helper in scikitlearn Below, using some pre-baked results from running some of the chapter 2 code from https://transformersbook.","title":""},{"content":" One particularly killer feature about the ColumnTransformer is that you can apply a specific preprocessor for a subset of the columns, and then set remainder=\u0026quot;passthrough\u0026quot; for the others import numpy as np from sklearn.preprocessing import (MinMaxScaler, OneHotEncoder, LabelEncoder) from sklearn.preprocessing import Binarizer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn import metrics from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score, train_test_split def make_data(): X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=42, shuffle=False, weights=(0.25,), ) # return X, y X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42, ) return X_train, X_test, y_train, y_test # In [90]: Counter(y) # Out[90]: Counter({0: 258, 1: 742}) def forest_one(): preprocessor = ColumnTransformer([ (\u0026#34;scaler\u0026#34;, MinMaxScaler(), [0, 1]), ], remainder=\u0026#34;passthrough\u0026#34;) pipeline = Pipeline([ # (\u0026#34;scale\u0026#34;, MinMaxScaler()), (\u0026#34;preproc\u0026#34;, preprocessor), ( \u0026#34;clf\u0026#34;, RandomForestClassifier( max_depth=2, random_state=0, n_estimators=100, # class_weight=\u0026#34;balanced_subsample\u0026#34;, # \u0026#34;balanced\u0026#34;, \u0026#34;balanced_subsample\u0026#34; or {0: 0.1, 1: 0.9 } weights per class ) ), ]) return pipeline def forest_balanced(): pipeline = Pipeline([ (\u0026#34;scale\u0026#34;, MinMaxScaler() ), ( \u0026#34;clf\u0026#34;, RandomForestClassifier( max_depth=2, random_state=0, n_estimators=100, class_weight=\u0026#34;balanced_subsample\u0026#34;, # \u0026#34;balanced\u0026#34;, \u0026#34;balanced_subsample\u0026#34; or {0: 0.1, 1: 0.9 } weights per class ) ), ]) return pipeline def e2e(X_train, y_train, pipeline): scorers = [\u0026#34;f1_micro\u0026#34;, \u0026#34;roc_auc\u0026#34;] scores = [ [scorer, cross_val_score(pipeline, X_train, y_train, cv=3, scoring=scorer) ] for scorer in scorers ] pipeline.fit(X_train, y_train) return pipeline, scores def holdout_test(X_test, y_test, pipeline): y_preds = pipeline.predict(X_test) f1 = metrics.f1_score(y_test, y_preds, average=\u0026#34;micro\u0026#34;) y_preds = pipeline.predict_proba(X_test)[:, 1] fpr, tpr, thresholds = metrics.roc_curve(y_test, y_preds, pos_label=1) auc = metrics.auc(fpr, tpr) return {\u0026#34;f1\u0026#34;: f1, \u0026#34;auc\u0026#34;: auc} \u0026#34;\u0026#34;\u0026#34; # X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]]) scaler = MinMaxScaler() print(scaler.fit(X)) print(scaler.data_min_, scaler.data_max_) print(scaler.transform(X)) # X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; X_train, X_test, y_train, y_test = qp.make_data() p1 = qp.forest_one() p2 = qp.forest_balanced() _, scores1 = qp.e2e(X, y, p1) print(\u0026#34;p1\u0026#34;, scores1) qp.holdout_test(X_test, y_test, p1) _, scores2 = qp.e2e(X, y, p2) print(\u0026#34;p2\u0026#34;, scores2) qp.holdout_test(X_test, y_test, p2) \u0026#34;\u0026#34;\u0026#34; ","permalink":"https://michal.piekarczyk.xyz/handy/model/pipeline/","summary":"One particularly killer feature about the ColumnTransformer is that you can apply a specific preprocessor for a subset of the columns, and then set remainder=\u0026quot;passthrough\u0026quot; for the others import numpy as np from sklearn.preprocessing import (MinMaxScaler, OneHotEncoder, LabelEncoder) from sklearn.preprocessing import Binarizer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn import metrics from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score, train_test_split def make_data(): X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=42, shuffle=False, weights=(0.","title":""},{"content":" from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X, y = load_iris(return_X_y=True) clf = LogisticRegression( random_state=0, penalty=\u0026#34;l2\u0026#34;, class_weight=\u0026#34;balanced\u0026#34;, # or dict {0: 0.1, 1: 0.9} ).fit(X, y, # sample_weight= # array , n_samples, for each row. ) clf.predict(X[:2, :]) clf.predict_proba(X[:2, :]) clf.score(X, y) clf.decision_function(X) from sklearn import metrics from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False) clf = RandomForestClassifier( max_depth=2, random_state=0, n_estimators=100, class_weight= # \u0026#34;balanced\u0026#34;, \u0026#34;balanced_subsample\u0026#34; or {0: 0.1, 1: 0.9 } weights per class ) clf.fit(X, y, # sample_weight= # array , n_samples, for each row. ) print(clf.predict([[0, 0, 0, 0]])) In [16]: pd.DataFrame(X).corr() Out[16]: 0 1 2 3 0 1.000000 0.065124 0.026765 0.028988 1 0.065124 1.000000 0.031176 -0.026317 2 0.026765 0.031176 1.000000 -0.006788 3 0.028988 -0.026317 -0.006788 1.000000 In [17]: clf.feature_importances_ Out[17]: array([0.14205973, 0.76664038, 0.0282433 , 0.06305659]) print(clf.predict_log_proba([[0, 0, 0, 0]])) print(clf.predict_proba([[0, 0, 0, 0]])) print(clf.predict([[0, 0, 0, 0]])) In [18]: print(clf.predict_log_proba([[0, 0, 0, 0]])) ...: [[-1.72562562 -0.19608985]] In [19]: print(clf.predict_proba([[0, 0, 0, 0]])) ...: [[0.17806162 0.82193838]] In [20]: from math import log In [21]: log(0.82193838) Out[21]: -0.19608985023951067 In [22]: print(clf.predict([[0, 0, 0, 0]])) [1] y_true = y y_pred = clf.pred(X) metrics.accuracy_score(y_true, y_pred) Out[29]: 0.925 metrics.confusion_matrix(y_true, y_pred) Out[30]: array([[434, 70], [ 5, 491]]) fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=1) metrics.auc(fpr, tpr) # Out[32]: 0.9255152329749103 metrics.log_loss(y_true, y_pred,) Out[33]: 2.590464201438415 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Cross Validation https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold \u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import ( KFold, StratifiedKFold, # preserves percentage of samples per class. ) \u0026gt;\u0026gt;\u0026gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) \u0026gt;\u0026gt;\u0026gt; y = np.array([1, 2, 3, 4]) \u0026gt;\u0026gt;\u0026gt; kf = KFold(n_splits=2) \u0026gt;\u0026gt;\u0026gt; kf.get_n_splits(X) 2 \u0026gt;\u0026gt;\u0026gt; print(kf) KFold(n_splits=2, random_state=None, shuffle=False) \u0026gt;\u0026gt;\u0026gt; for train_index, test_index in kf.split(X): ... print(\u0026#34;TRAIN:\u0026#34;, train_index, \u0026#34;TEST:\u0026#34;, test_index) ... X_train, X_test = X[train_index], X[test_index] ... y_train, y_test = y[train_index], y[test_index] TRAIN: [2 3] TEST: [0 1] TRAIN: [0 1] TEST: [2 3] from sklearn import utils utils.class_weight.compute_class_weight() utils.class_weight.compute_sample_weight() Other handy references https://ml-cheatsheet.readthedocs.io/\n","permalink":"https://michal.piekarczyk.xyz/handy/model/quick/","summary":"from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X, y = load_iris(return_X_y=True) clf = LogisticRegression( random_state=0, penalty=\u0026#34;l2\u0026#34;, class_weight=\u0026#34;balanced\u0026#34;, # or dict {0: 0.1, 1: 0.9} ).fit(X, y, # sample_weight= # array , n_samples, for each row. ) clf.predict(X[:2, :]) clf.predict_proba(X[:2, :]) clf.score(X, y) clf.decision_function(X) from sklearn import metrics from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False) clf = RandomForestClassifier( max_depth=2, random_state=0, n_estimators=100, class_weight= # \u0026#34;balanced\u0026#34;, \u0026#34;balanced_subsample\u0026#34; or {0: 0.","title":""},{"content":"Read loc = \u0026#34;dbfs:/databricks-datasets/wine-quality/winequality-red.csv\u0026#34; blah_df = spark.read.csv(loc, sep=\u0026#34;;\u0026#34;, header=True) Map an existing function import spark.sql.functions as F loc = \u0026#34;dbfs:/databricks-datasets/wine-quality/winequality-red.csv\u0026#34; df = spark.read.csv(loc, sep=\u0026#34;;\u0026#34;, header=True) df = df.withColumn(\u0026#34;sugar_rounded\u0026#34;, F.round(df[\u0026#34;residual sugar\u0026#34;])) df.select(\u0026#34;residual sugar\u0026#34;, \u0026#34;sugar_rounded\u0026#34;).show(5) +--------------+-------------+ |residual sugar|sugar_rounded| +--------------+-------------+ | 1.9 | 2.0| | 2.6 | 3.0| +--------------+-------------+ Also can split a col to a json array Here imagine there is a column , \u0026ldquo;_c0\u0026rdquo; which has tab separated data,\ndf = df.withColumn(\u0026#34;col_split\u0026#34;, F.split(F.col(\u0026#34;_c0\u0026#34;), \u0026#34;\\t\u0026#34;)) And casting\ndf = df.withColumn(\u0026#34;foo\u0026#34;, df[\u0026#34;foo\u0026#34;].cast(\u0026#34;double\u0026#34;)) unique ids! df = df.withColumn(\u0026#34;id\u0026#34;, F.monotonically_increasing_id()) df.write.parquet(\u0026#34;foo.parquet\u0026#34;) User Defined Functions A user defined function needs to be defined with a return type For instance, say there\u0026rsquo;s a dataframe df with a name column, that have spaces between first and last names say, and you can split them up like so, only grabbing the first 2 , for example, by also using F.lit to specify a literal value being passed to the func as well. import pyspark.sql.functions as F from pyspark.types import ArrayType, StringType def split_name(name): return name.split(\u0026#34; \u0026#34;)[:2] udfSplitter = F.udf(split_name, ArrayType(StringType())) df = ... df = df.withColumn(\u0026#34;separated_names\u0026#34;, udfSplitter(df.name, F.lit(2))) Quick Spark ml lib Logistic Regression Pipeline Given a dataframe with features you would like to use/transform in a LogisticRegression, similarly to sklearn taking an input without feature names, the spark flavor does the same, taking a single column for the input features.\nfrom pyspark.ml.classification import LogisticRegression from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler from pyspark.ml import Pipeline def predict_all_of_the_things(df): vector_assembler = VectorAssembler(inputCols=[ \u0026#34;f1\u0026#34;, \u0026#34;f2\u0026#34;, \u0026#34;f3\u0026#34;, ], outputCol=\u0026#34;features\u0026#34;) lr = LogisticRegression( featuresCol=\u0026#34;features\u0026#34;, labelCol=\u0026#34;y_my_label\u0026#34;, maxIter=10, regParam=0.1, elasticNetParam=1, threshold=0.5, ) pipeline = Pipeline(stages=[vector_assembler, lr]) e2e = pipeline.fit(df) outdf = e2e.transform(df) print(outdf.head(10)) return outdf.select([\u0026#34;user_id\u0026#34;, \u0026#34;rawPrediction\u0026#34;, \u0026#34;probability\u0026#34;, \u0026#34;prediction\u0026#34;]) Pipeline with train/test handling also from pyspark.ml.classification import LogisticRegression from pyspark.ml.linalg import Vectors from pyspark.ml.feature import VectorAssembler from pyspark.ml.feature import OneHotEncoderEstimator from pyspark.ml.feature import StringIndexer from pyspark.ml import Pipeline indexer = StringIndexer(...) onehot = OneHotEncoderEstimator(...) assemble = VectorAssembler(...) regression = LogisticRegression(...) pipeline = Pipeline(stages=[indexer, onehot, assemble, regression]) blah_df = spark.read.csv(...) train_df, test_df = blah_df.randomSplit([0.8, 0.2], seed=42) # And now fit the pipeline only on the train part pipeline = pipeline.fit(train_df) # And predictions.. predictions = pipeline.transform(test_df) And the various stages of the pipeline are indexable, for example, to get the intercept and coefficient of the regression step,\nprint(pipeline.stages[3].intercept, pipeline.stages[3].coefficients) Will produce the intercept 3.9 and coefficients, DenseVector([...]) for the regression stage of the pipeline.\nspark StringIndexer is like scikitlearn\u0026rsquo;s LabelEncoder Given a dataframe flugts and a categorical col blah , we can do a fit , transform , kind of like in scikitlearn.\nfrom pyspark.ml.feature import StringIndexer flugts = StringIndexer( inputCol=\u0026#34;blah\u0026#34;, outputCol=\u0026#34;blah_index\u0026#34; ).fit( flugts ).transform( flugts ) Decision tree classifier from pyspark.ml.classification import DecisionTreeClassifier model = DecisionTreeClassifier.fit(foo_train) prediction = model.transform(foo_test) This will produce two new columns, in prediction, \u0026ldquo;prediction\u0026rdquo; and \u0026ldquo;probability\u0026rdquo; quick confusion matrix , if you also for instance, had the \u0026ldquo;label\u0026rdquo; column, prediction.groupBy(\u0026#34;label\u0026#34;, \u0026#34;prediction\u0026#34;).count().show() Logistic Regression from pyspark.ml.classification import LogisticRegression Linear Regression from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator regression = LinearRegression(labelCol=\u0026#34;the_label_col\u0026#34;) regression = regression.fit(train_df) predictions = regression.transform(test_df) regression.intercept regression.coefficients # \u0026lt;== weights for the regression # using whatever the default evaluator is ... ( rmse I think) RegressionEvaluator(labelCol=\u0026#34;the_label_col\u0026#34;).evaluate(predictions) # And also if \u0026#34;predictions_col\u0026#34; is where predictions are , evaluator = RegressionEvaluator(labelCol=\u0026#34;the_label_col\u0026#34;).setPredictionCol(\u0026#34;predictions_col\u0026#34;) evaluator.evaluate(predictions, {evaluator.metricName: \u0026#34;mae\u0026#34;}) # \u0026#34;mean absolute error\u0026#34; evaluator.evaluate(predictions, {evaluator.metricName: \u0026#34;r2\u0026#34;}) And Linear Regression with regularization Lambda term =0 ==\u0026gt; no regularization Lambda term =inf ==\u0026gt; complete regularization , all coefficients are zero. Ridge ridge = LinearRegression( labelCol=\u0026#34;my_label\u0026#34;, elasticNetParam=0, regParam=0.1 ) ridge.fit(train_df) Lasso lasso = LinearRegression( labelCol=\u0026#34;my_label\u0026#34;, elasticNetParam=1, regParam=0.1 ) lasso.fit(train_df) Train test split A Dataframe has this built in func,\ntrain, test = mydf.randomSplit([0.8, 0.2], seed=42) But it does not produce separate X/y train/test variables the way that is typical in scikitlearn. Maybe that is a helper func that is available.\nGetting fancier with evaluation Given a prediction dataframe with columns, label and prediction , which have been calculated at a particular threshold, we can evaluate as follows,\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator evaluator = MulticlassClassificationEvaluator() evaluator.evaluate(prediction, {evaluator.metricName: \u0026#34;weightedPrecision\u0026#34;}) evaluator.evaluate(prediction, {evaluator.metricName: \u0026#34;weightedRecall\u0026#34;}) evaluator.evaluate(prediction, {evaluator.metricName: \u0026#34;accuracy\u0026#34;}) evaluator.evaluate(prediction, {evaluator.metricName: \u0026#34;f1\u0026#34;}) from pyspark.ml.evaluation import BinaryClassificationEvaluator binary_evaluator = BinaryClassificationEvaluator() auc = binary_evaluator.evaluate( prediction, {binary_evaluator.metricName: \u0026#34;areaUnderROC\u0026#34;} ) Text Simple regex substitution from pyspark.sql.functions import regexp_replace REGEX = \u0026#39;[,\\\\-]\u0026#39; df = df.withColumn(\u0026#39;text\u0026#39;, regexp_replace(books.text, REGEX, \u0026#39; \u0026#39;)) Tokenization Create a new column with an array of words from free form text.\nfrom pyspark.ml.feature import Tokenizer df = Tokenizer(inputCol=\u0026#34;text\u0026#34;, outputCol=\u0026#34;tokens\u0026#34;).transform(df) Remove stop words\nfrom pyspark.ml.feature import StopWordsRemover stopwords = StopWordsRemover() stopwords.getStopWords() [\u0026#39;i\u0026#39;, \u0026#39;me\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;myself\u0026#39;, \u0026#39;we\u0026#39;, \u0026#39;our\u0026#39;, \u0026#39;ours\u0026#39;, \u0026#39;ourselves\u0026#39;, \u0026#39;you\u0026#39;, \u0026#39;your\u0026#39;, \u0026#39;yours\u0026#39;,\u0026#39;yourself\u0026#39;, \u0026#39;yourselves\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;himself\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;her\u0026#39;, \u0026#39;hers\u0026#39;, \u0026#39;herself\u0026#39;,\u0026#39;it\u0026#39;, \u0026#39;its\u0026#39;, \u0026#39;itself\u0026#39;, \u0026#39;they\u0026#39;, \u0026#39;them\u0026#39;, \u0026#39;their\u0026#39;, \u0026#39;theirs\u0026#39;, \u0026#39;themselves\u0026#39;, \u0026#39;what\u0026#39;, \u0026#39;which\u0026#39;,\u0026#39;who\u0026#39;, \u0026#39;whom\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;that\u0026#39;, \u0026#39;these\u0026#39;, \u0026#39;those\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;are\u0026#39;, \u0026#39;was\u0026#39;, \u0026#39;were\u0026#39;, \u0026#39;be\u0026#39;,\u0026#39;been\u0026#39;, \u0026#39;being\u0026#39;, \u0026#39;have\u0026#39;, \u0026#39;has\u0026#39;, \u0026#39;had\u0026#39;, \u0026#39;having\u0026#39;, \u0026#39;do\u0026#39;, \u0026#39;does\u0026#39;, \u0026#39;did\u0026#39;, \u0026#39;doing\u0026#39;, ...] # Specify the input and output column names stopwords = stopwords.setInputCol(\u0026#39;tokens\u0026#39;).setOutputCol(\u0026#39;words\u0026#39;) df = stopwords.transform(df) Term frequency transformer HashingTF , will use a hash algo MurmurHash 3 (not sure why not a more well known hash func) , to map to an integer from 1 to a default of 262,144 . (Oh that\u0026rsquo;s probably part of the difference, using an integer as opposed to a long 256 bit output hash then) . And the output will include the frequency of the hashed output.\nfrom pyspark.ml.feature import HashingTF hasher = HashingTF(inputCol=\u0026#34;words\u0026#34;, outputCol=\u0026#34;hash\u0026#34;, numFeatures=32) df = hasher.transform(df) And we can do page-rank like proportional inverted indexing too\nfrom pyspark.ml.feature import IDF df = IDF(inputCol=\u0026#34;hash\u0026#34;, outputCol=\u0026#34;features\u0026#34;).fit(df).transform(df) pipeline for some of these NLP steps Below, assume we have an input dataframe with some kind of raw_text column that has free form text. Then the below pipeline can tokenize that text, remove stop words, and create a term frequency inverted index,\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF from pyspark.ml.regression import LogisticRegression from pyspark.ml.feature import Pipeline tokenizer = Tokenizer( inputCol=\u0026#34;raw_text\u0026#34;, outputCol=\u0026#34;tokens\u0026#34; ) remover = StopWordsRemover( inputCol=\u0026#34;tokens\u0026#34;, outputCol=\u0026#34;terms\u0026#34; ) hasher = HashingTF( inputCol=\u0026#34;terms\u0026#34;, outputCol=\u0026#34;hash\u0026#34; ) idf = IDF( inputCol=\u0026#34;hash\u0026#34;, outputCol=\u0026#34;features\u0026#34; ) logistic = LogisticRegression() pipeline = Pipeline( stages=[ tokenizer, remover, hasher, idf, logistic, ] ) One Hot Encoding Spark uses a sparse representation of one-hot-encoded features\nfrom pyspark.ml.feature import OneHotEncoderEstimator onehot = OneHotEncoderEstimator( inputCols=[\u0026#34;type_blah\u0026#34;], outputCols=[\u0026#34;type_one_hot\u0026#34;] ) onehot.fit(df) onehot.categorySizes # \u0026lt;== gives how many categories processed. df = onehot.transform(df) A SparseVector takes the length of the vector as the first arg and a key-val dict for the sparse values\nfrom pyspark.mllib.linalg import DenseVector, SparseVector DenseVector([1, 0, 0, 0, 0, 7, 0, 0]) # each value is kept SparseVector(8, {0: 1.0, 5: 7.0}) Bucketing from pyspark.ml.feature import Bucketizer bucketizer = Bucketizer( splits=[20, 30, 40, 50], inputCol=\u0026#34;age\u0026#34;, outputCol=\u0026#34;age_bin\u0026#34; ) df = bucketizer.transform(df) Similar to categorical encoding benefiting from one hot encoding, bucketing will also benefit from one hot encoding\nCross Validation Given a model and an evaluator, where the model can also be a pipeline,\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder model = LinearRegression(labelCol=\u0026#34;y_label\u0026#34;) evaluator = RegressionEvaluator(labelCol=\u0026#34;y_label\u0026#34;) grid = ParamGridBuilder() \\ .addGrid(model.elasticNetParam, [0, 0.5, 1.]) \\ .addGrid(model.regParam, [0.01, 0.1, 1, 10]) \\ .addGrid(model.fitIntercept, [True, False]) \\ .build() print(\u0026#34;number of models to be built from the grid =\u0026gt;\u0026#34;, len(grid)) cv = CrossValidator( estimator=model, estimatorParamMaps=grid, evaluator=evaluator, numFolds=5, seed=42, ) cv.fit(train_df) # the average metric whatever it is, for each combo in the grid. cv.avgMetrics # can use the best model like this, cv.bestModel.transform(test_df) # Can also use the best model implicitly... cv.transform(test_df) # And look at a metric hence for that model, print(\u0026#34;rmse\u0026#34;, evaluator.evaluate(cv.bestModel.transform(test_df), {evaluator.metricName: \u0026#34;rmse\u0026#34;})) # You can get some quick documentation like this wow. Neat trick. cv.bestModel.explainParam(\u0026#34;elasticNetParam\u0026#34;) # Can look at the params like this too for param, val in cv.bestModel.extractParamMap().items: print((param.name, val), f\u0026#34;({param.doc})\u0026#34;) for a RandomForestClassifier this will print for instance \u0026hellip; something like\npredictionCol prediction featureSubsetStrategy onethird maxMemoryInMB 256 rawPredictionCol rawPrediction cacheNodeIds False probabilityCol probability impurity gini featuresCol features maxDepth 20 labelCol label subsamplingRate 1.0 maxBins 32 checkpointInterval 10 minInstancesPerNode 1 minInfoGain 0.0 numTrees 20 seed 1720035589386331064 random forest from pyspark.ml.classification import RandomForestClassifier, GBTClassifier from pyspark.ml.evaluation import BinaryClassificationEvaluator forest = RandomForestClassifier() forest.featureImportances # produces a SparseVector , gbt = GBTClassifier() gbt.getNumTrees # number of trees There is also an amazing debug output available with , gbt.toDebugString\nIn [10]: print(gbt.toDebugString.split(\u0026#34;Tree\u0026#34;)[0]) GBTClassificationModel (uid=GBTClassifier_c601194e39a1) with 20 trees In [12]: print(gbt.toDebugString.split(\u0026#34;Tree\u0026#34;)[1]) 0 (weight 1.0): If (feature 1 \u0026lt;= 9.6) If (feature 2 \u0026lt;= 118.5) If (feature 0 \u0026lt;= 2.5) If (feature 1 \u0026lt;= 7.075) If (feature 2 \u0026lt;= 109.5) Predict: -0.5702479338842975 Else (feature 2 \u0026gt; 109.5) Predict: -0.17391304347826086 Else (feature 1 \u0026gt; 7.075) If (feature 2 \u0026lt;= 92.5) Predict: -0.3117782909930716 Else (feature 2 \u0026gt; 92.5) Predict: -0.1232876712328767 Else (feature 0 \u0026gt; 2.5) If (feature 0 \u0026lt;= 10.5) If (feature 2 \u0026lt;= 92.5) Predict: -0.6527027027027027 Else (feature 2 \u0026gt; 92.5) Predict: -0.48745046235138706 Else (feature 0 \u0026gt; 10.5) If (feature 1 \u0026lt;= 7.075) Predict: -0.47368421052631576 Else (feature 1 \u0026gt; 7.075) Predict: -0.19090909090909092 Else (feature 2 \u0026gt; 118.5) If (feature 0 \u0026lt;= 5.5) If (feature 1 \u0026lt;= 7.74) If (feature 2 \u0026lt;= 197.5) Predict: -0.3770491803278688 Else (feature 2 \u0026gt; 197.5) Predict: -0.0916030534351145 Else (feature 1 \u0026gt; 7.74) If (feature 0 \u0026lt;= 4.5) Predict: -0.10258418167580266 Else (feature 0 \u0026gt; 4.5) Predict: 0.10580204778156997 Else (feature 0 \u0026gt; 5.5) If (feature 0 \u0026lt;= 10.5) If (feature 0 \u0026lt;= 8.5) Predict: -0.27740863787375414 Else (feature 0 \u0026gt; 8.5) Predict: -0.5332348596750369 Else (feature 0 \u0026gt; 10.5) If (feature 1 \u0026lt;= 8.66) Predict: -0.014492753623188406 Else (feature 1 \u0026gt; 8.66) Predict: 0.23333333333333334 Else (feature 1 \u0026gt; 9.6) If (feature 0 \u0026lt;= 6.5) If (feature 2 \u0026lt;= 124.5) If (feature 1 \u0026lt;= 16.509999999999998) If (feature 0 \u0026lt;= 1.5) Predict: 0.11760883690708251 Else (feature 0 \u0026gt; 1.5) Predict: -0.023830031581969568 Else (feature 1 \u0026gt; 16.509999999999998) If (feature 2 \u0026lt;= 50.5) Predict: -0.23404255319148937 Else (feature 2 \u0026gt; 50.5) Predict: 0.20102827763496145 Else (feature 2 \u0026gt; 124.5) If (feature 1 \u0026lt;= 15.675) If (feature 0 \u0026lt;= 1.5) Predict: 0.2877813504823151 Else (feature 0 \u0026gt; 1.5) Predict: 0.19178515007898894 Else (feature 1 \u0026gt; 15.675) If (feature 2 \u0026lt;= 288.0) Predict: 0.475375296286542 Else (feature 2 \u0026gt; 288.0) Predict: 0.18562874251497005 Else (feature 0 \u0026gt; 6.5) If (feature 0 \u0026lt;= 10.5) If (feature 0 \u0026lt;= 8.5) If (feature 2 \u0026lt;= 85.5) Predict: -0.27121464226289516 Else (feature 2 \u0026gt; 85.5) Predict: 0.0723354000590493 Else (feature 0 \u0026gt; 8.5) If (feature 1 \u0026lt;= 13.16) Predict: -0.4181152790484904 Else (feature 1 \u0026gt; 13.16) Predict: -0.2569395017793594 Else (feature 0 \u0026gt; 10.5) If (feature 1 \u0026lt;= 15.125) If (feature 2 \u0026lt;= 60.5) Predict: 0.3333333333333333 Else (feature 2 \u0026gt; 60.5) Predict: 0.15768056968463887 Else (feature 1 \u0026gt; 15.125) If (feature 2 \u0026lt;= 76.5) Predict: 0.12863070539419086 Else (feature 2 \u0026gt; 76.5) Predict: 0.37316017316017314 Special databricks stuff Check out what local file system access is available , by display(dbutils.fs.ls(\u0026#34;dbfs:/\u0026#34;)) how about ADLS/blob storage on ADLS Azure ?? display(dbutils.fs.ls(f\u0026#34;abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\u0026#34;)) And the above require special configuration addition too\u0026hellip;\nspark.databricks.pyspark.trustedFilesystems org.apache.hadoop.fs.LocalFileSystem,com.databricks.adl.AdlFileSystem,com.databricks.s3a.S3AFileSystem,shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem,shaded.datrabricks.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem ML FLow If you are not on the special \u0026ldquo;ML\u0026rdquo; instnce, you can install mlflow on a cluster like ..\ndbutils.library.installPyPI(\u0026#34;mlflow\u0026#34;, \u0026#34;1.0.0\u0026#34;) dbutils.library.restartPython() import pylab import matplotlib.pyplot as plt import mlflow.sklearn with mlflow.start_run(run_name=\u0026#34;Basic RF Experiment\u0026#34;) as run: rf = RandomForestRegressor() rf.fit(X_train, y_train) predictions = rf.predict(X_test) # log model mlflow.sklearn.log_model(rf, \u0026#34;random-forest-model\u0026#34;) mse = mean_squared_error(y_test, predictions) # log metrics mlflow.log_metric(\u0026#34;mse\u0026#34;, mse) runID = run.info.run_uuid experimentID = run.info.experiment_id print(f\u0026#34;Inside mlflow run with run id {runID} and experiment id {experimentID}\u0026#34;) fig, ax = plt.subplots() sns.residplot(predictions, y_test, lowess=True) plt.xlabel(\u0026#34;Preds\u0026#34;) plt.ylabel(\u0026#34;Residuals\u0026#34;) pylab.savefig(\u0026#34;foo_file.png\u0026#34;) # saving locally mlflow.log_artifacts(\u0026#34;foo_file\u0026#34;, \u0026#34;residuals.png\u0026#34;) # and also as an artifact Faster column renaming For instance if you want to rename multiple columns , instead of , using a for loop like\nimport spark.sql.functions as F cols = df.columns for c in cols: df = df.withColumn(c + \u0026#34;_blahblah\u0026#34;, F.col(c)) df = df.select(*[c + \u0026#34;_blahblah\u0026#34; for c in cols]) Slightly cleaner first maybe to use withColumnRenamed cols = df.columns for c in cols: df = df.withColumnRenamed(c, c + \u0026#34;_blahblah\u0026#34;) And I wonder if the above can be faster if it is chained, df.withColumnRenamed(c1, c2).withColumnRenamed(c2, c3) . But not sure But other than that, a list comprehension with .alias() , might be faster too. Have not yet checked.. df = df.select(*[F.col(c).alias(c + \u0026#34;_blahblah\u0026#34;) for c in df.columns]) Comparing if two large dataframes are the close import spark.sql.functions as F from functools import reduce from operators import or_ # Writing some of this from memory, so I think have to fix some parts later... double_cols = [col for col in df.columns if df.getSchema()[col].dataType == \u0026#34;double\u0026#34;] # If the mean is \u0026gt; 1 then can round w/ precision=0 col_means = df.agg({col: \u0026#34;Mean\u0026#34; for col in double_cols}) double_actually_int_cols = [col for col, mean in col_means.items() if mean \u0026gt; 1] double_actually_double_cols = [col for col, mean in col_means.items() if mean \u0026lt;= 1] conditions = [ F.abs( (F.col(f\u0026#34;x.{col}\u0026#34;)) - (F.col(f\u0026#34;y.{col}\u0026#34;)) ) \u0026gt; 0.01 for col in double_actually_double_cols ] + [ F.abs( (F.col(f\u0026#34;x.{col}\u0026#34;)) - (F.col(f\u0026#34;y.{col}\u0026#34;)) ) \u0026gt; 1 for col in double_actually_int_cols ] condition = reduce(or_, conditions) index_cols = [\u0026#34;id\u0026#34;] # Here for instance, just selecting the index cols matching the conditions diffdf = df.alias(\u0026#34;x\u0026#34;).join(df.alias(\u0026#34;y\u0026#34;), on=index_cols).where( condition ).select(index_cols) References A lot of this was inspired by this great DataCamp course .\n","permalink":"https://michal.piekarczyk.xyz/handy/model/spark/","summary":"Read loc = \u0026#34;dbfs:/databricks-datasets/wine-quality/winequality-red.csv\u0026#34; blah_df = spark.read.csv(loc, sep=\u0026#34;;\u0026#34;, header=True) Map an existing function import spark.sql.functions as F loc = \u0026#34;dbfs:/databricks-datasets/wine-quality/winequality-red.csv\u0026#34; df = spark.read.csv(loc, sep=\u0026#34;;\u0026#34;, header=True) df = df.withColumn(\u0026#34;sugar_rounded\u0026#34;, F.round(df[\u0026#34;residual sugar\u0026#34;])) df.select(\u0026#34;residual sugar\u0026#34;, \u0026#34;sugar_rounded\u0026#34;).show(5) +--------------+-------------+ |residual sugar|sugar_rounded| +--------------+-------------+ | 1.9 | 2.0| | 2.6 | 3.0| +--------------+-------------+ Also can split a col to a json array Here imagine there is a column , \u0026ldquo;_c0\u0026rdquo; which has tab separated data,\ndf = df.","title":""},{"content":"mean/std technique For xgboost/random forest type models per this article , the proposed idea is to use the preditions of all the trees as the prediction space or a kind of an uncertainty interval. I wonder if we can say predictions that a model is more certain about have a tighter distribution of predictions. And conversely that a model is unsure about its predictions if the distribution of predictions is wide. I have a feeling that the LSS approach to XGBoost here tries to automate something like that. References hmm mean/std technique intervals ","permalink":"https://michal.piekarczyk.xyz/handy/model/uncertainty/","summary":"mean/std technique For xgboost/random forest type models per this article , the proposed idea is to use the preditions of all the trees as the prediction space or a kind of an uncertainty interval. I wonder if we can say predictions that a model is more certain about have a tighter distribution of predictions. And conversely that a model is unsure about its predictions if the distribution of predictions is wide.","title":""},{"content":"Initial stab on interpreting Variance inflation factor (VIF) So far my skim on https://en.wikipedia.org/wiki/Variance_inflation_factor and https://en.wikipedia.org/wiki/Multicollinearity tells me that high Variance Inflation Factor (VIF) indicates high multicolinearity w/ one or more other independent variables. And thatâ€™s bad because\n(a) when building a linear model (at least using ordinary least squares (OLS) , not yet sure if this is still true if you use regularization ) , the coefficients calculated for the independent variables can change â€œerraticallyâ€ given slightly different data .\n(b) So (a) tells me that those coefficients maybe are less useful in using those coefficients to interpret importance for instance.\n(c) That smells like a risk of overfitting to me.\n(d) Sounds like also building those coefficients is more deterministic for adjustments/data updates\nTherefore eliminating variables w/ high VIF reduces overfitting risk and improves overall interpretability / stability\n(Oh yea and a cutoff of 5 to 10 is mentioned to be common )\n","permalink":"https://michal.piekarczyk.xyz/handy/model/variance_inflation_factor/","summary":"Initial stab on interpreting Variance inflation factor (VIF) So far my skim on https://en.wikipedia.org/wiki/Variance_inflation_factor and https://en.wikipedia.org/wiki/Multicollinearity tells me that high Variance Inflation Factor (VIF) indicates high multicolinearity w/ one or more other independent variables. And thatâ€™s bad because\n(a) when building a linear model (at least using ordinary least squares (OLS) , not yet sure if this is still true if you use regularization ) , the coefficients calculated for the independent variables can change â€œerraticallyâ€ given slightly different data .","title":""},{"content":"Create New ssh key With ssh-keygen, w/ a pass phrase too.\nLet ssh-agent manage the ssh key passphrase With ssh-add ~/.ssh/path/to/key\nAnd Save to macbook keychain Save that passphrase with\nssh-add -K ~/.ssh/path/to/private/key But apparently according to this stackoverflow answer, with Monterey, ssh-add uses #\nssh-add --apple-use-keychain ~/.ssh/path/to/private/key because --apple-use-keychain is the new -K.\nAnd similarly --apple-load-keychain is the new -A , to load a key into your ssh-agent after logging in.\nssh-add --apple-load-keychain ~/.ssh/path/to/private/key ","permalink":"https://michal.piekarczyk.xyz/handy/new_laptop_setup/ssh/","summary":"Create New ssh key With ssh-keygen, w/ a pass phrase too.\nLet ssh-agent manage the ssh key passphrase With ssh-add ~/.ssh/path/to/key\nAnd Save to macbook keychain Save that passphrase with\nssh-add -K ~/.ssh/path/to/private/key But apparently according to this stackoverflow answer, with Monterey, ssh-add uses #\nssh-add --apple-use-keychain ~/.ssh/path/to/private/key because --apple-use-keychain is the new -K.\nAnd similarly --apple-load-keychain is the new -A , to load a key into your ssh-agent after logging in.","title":""},{"content":"Nice that now python has this built in method for creating virtual environments per docs\n# like this python3 -m venv /path/to/new/virtual/environment python -m venv ~/.python_venvs/skpy39 source ~/.python_venvs/skpy39/bin/activate pip install scikit-learn scikit-learn pandas ipdb ipython matplotlib tqdm colormap easydev ","permalink":"https://michal.piekarczyk.xyz/handy/new_laptop_setup/virtual_envs/","summary":"Nice that now python has this built in method for creating virtual environments per docs\n# like this python3 -m venv /path/to/new/virtual/environment python -m venv ~/.python_venvs/skpy39 source ~/.python_venvs/skpy39/bin/activate pip install scikit-learn scikit-learn pandas ipdb ipython matplotlib tqdm colormap easydev ","title":""},{"content":"histogram overlays # Nice technique from https://srome.github.io/Covariate-Shift,-i.e.-Why-Prediction-Quality-Can-Degrade-In-Production-and-How-To-Fix-It/ # ... put two histograms on same plot ... def produce_overlayed_hists_for_col_dfs(col, dfs): fig = plt.figure(figsize=(12,12)) ax = fig.add_subplot(121) ax.hist(dfs[0][1][col], color=\u0026#39;r\u0026#39;, alpha=0.2, bins=50) ax.hist(dfs[1][1][col], color=\u0026#39;b\u0026#39;, alpha=0.2, bins=50) ax.set(title=f\u0026#39;{dfs[0][0]} (red) vs {dfs[1][0]} (blue)\u0026#39;, ylabel=col) Basic goal looks like the below.\nsparse diagonal x axis ticks import matplotlib.pyplot as plt import pandas as pd import datetime def make_xtick_labels(x, step=5): \u0026#39;\u0026#39;\u0026#39;Given x, step the labels every \u0026lt;step\u0026gt; Aka, take every \u0026lt;step\u0026gt;th x label \u0026#39;\u0026#39;\u0026#39; x_ticks = [i for i in range(len(x)) if i % step == 0] x_labels = [x[i] for i in x_ticks] return x_ticks, x_labels Did not add an example x , y yet, but showing an example where x contains dates and y is numeric. x = ? y = ? fig = plt.figure(figsize=(12,4)) ax = fig.add_subplot(111) ax.plot(y) x_ticks, x_labels = make_xtick_labels(x, step=20) ax.set_xticks(x_ticks) ax.set_xticklabels(x_labels, rotation=-45) fig.show() Multiple time plots and fill nulls with zeroes! Need to fill the nulls, otherwise the behavior can be weird. Here, have a df with timestamp and label , that is sparse, (meaning there are missing rows) import matplotlib.pyplot as plt import pandas as pd import datetime import random def random_df(size=500): X = [random.random() for _ in range(size)] vec = [] for (i, x) in enumerate(X): vec.extend([{ \u0026#34;label\u0026#34;: (\u0026#34;one\u0026#34; if x \u0026lt;= 0.33 else (\u0026#34;two\u0026#34; if 0.33 \u0026lt; x \u0026lt;= 0.66 else \u0026#34;three\u0026#34;)), \u0026#34;timestamp\u0026#34;: datetime.date(2021, 1, 1) + datetime.timedelta(days=1*i) } for _ in range(random.randint(0, 50)) ]) return pd.DataFrame.from_records(vec) def fill_empties(statsdf): statsdf = statsdf.copy() for x in statsdf[\u0026#34;date\u0026#34;].unique().tolist(): for label in statsdf.label.unique().tolist(): if statsdf[(statsdf.date == x) \u0026amp; (statsdf.label == label)].empty: statsdf = pd.concat([statsdf, pd.DataFrame.from_records([{\u0026#34;date\u0026#34;: x, \u0026#34;label\u0026#34;: label, \u0026#34;count\u0026#34;: 0}])], ignore_index=True ) statsdf = statsdf.sort_values(by=[\u0026#34;date\u0026#34;, \u0026#34;label\u0026#34;]) return statsdf def plot_trends(df, out_loc): statsdf = df.groupby(by=[\u0026#39;date\u0026#39;, \u0026#39;label\u0026#39;]).size().reset_index().rename(columns={0: \u0026#34;count\u0026#34;}) statsdf = fill_empties(statsdf) fig = plt.figure(figsize=(12,4)) ax = fig.add_subplot(111) x = statsdf.date.unique().tolist() x_ticks, x_labels = make_xtick_labels(x, step=3) for label in statsdf.label.unique().tolist(): x = statsdf[statsdf.label == label][\u0026#39;date\u0026#39;].tolist() y = statsdf[statsdf.label == label][\u0026#39;count\u0026#39;].tolist() ax.plot(x, y, label=label) ax.set_xticks(x_ticks) ax.set_xticklabels(x_labels, rotation=-45) ax.legend() print(\u0026#39;saving to \u0026#39;, out_loc) pylab.savefig(out_loc) pylab.close() # df = random_df(100) df[\u0026#34;date\u0026#34;] = df[\u0026#34;timestamp\u0026#34;].map(lambda x:x.strftime(\u0026#34;%m-%d\u0026#34;)) workdir = \u0026#34;some_folder\u0026#34; out_loc = f\u0026#34;{workdir}/trends.png\u0026#34; plot_trends(df, out_loc) Heatmaps are nice plt.figure(figsize=(10,10)) plt.imshow(bitmap) plt.colorbar() plt.grid(False) plt.show() using np.histogram and quantiles to spot check bimodal distributions I had this use case where I wanted to collect walltime from a service, from a dataset where a bimodal distribution was basically a given. I wanted thea mean of the second distribution. Instead of trying to use clustering analysis like dbscan which would have probably worked, I just started collecting the time series np.histogram and quantile data and I was able to visually inspect / prove that the median is a good enough statistic in this case, without too much extra data preprocessing required! sampling data from athena every 7 days , here are two examples below. supporting codes\u0026hellip; (I didnt add code for daa.run_it but basically that just pulls data into a dataframe with a column backend_processing_time that is being used here. And make_query just makes a query for a particular date to pull that data. So nothing really special about those. They can be replaced with any particular method of gathering data.) import datetime from tqdm import tqdm d1 = datetime.date(2019, 1, 1) d2 = datetime.date(2020, 7, 1) dd = ddu.range_dates(d1, d2, 7) # outvec = [] for dt in tqdm(dd): query = make_query(dt) athenadf = daa.run_it(query, query_name=\u0026#39;Unsaved\u0026#39;) hist = np.histogram(athenadf.backend_processing_time.tolist(), bins=10, range=None) mean = np.mean(athenadf.backend_processing_time.tolist()) quantiles = get_quantiles(athenadf.backend_processing_time.tolist()) outvec.append({\u0026#39;hist\u0026#39;: hist, \u0026#39;quantiles\u0026#39;: quantiles, \u0026#39;date\u0026#39;: dt.strftime(\u0026#39;%Y-%m-%d\u0026#39;), \u0026#39;mean\u0026#39;: mean}) import numpy as np import matplotlib.pyplot as plt def get_quantiles(unsorted): data = sorted(unsorted) minimum = data[0] Q1 = np.percentile(data, 25, interpolation = \u0026#39;midpoint\u0026#39;) median = np.median(data) Q3 = np.percentile(data, 75, interpolation = \u0026#39;midpoint\u0026#39;) maximum = data[-1] return [minimum, Q1, median, Q3, maximum] def show_da_stats(bundle): H, bins = bundle[\u0026#39;hist\u0026#39;] quantiles = bundle[\u0026#39;quantiles\u0026#39;] # plt.plot(x[1][:-1], x[0], drawstyle=\u0026#39;steps\u0026#39;) #print(H, bins) #print(quantiles) plt.scatter(quantiles, [1, 1, 1, 1, 1]) plt.axvline(quantiles[1], label=\u0026#39;q:25%\u0026#39;) plt.axvline(quantiles[2], label=\u0026#39;q:50%\u0026#39;) plt.axvline(quantiles[3], label=\u0026#39;q:75%\u0026#39;) plt.title(f\u0026#34;walltime histogram at {bundle[\u0026#39;date\u0026#39;]}\u0026#34;) plt.plot(bins, np.insert(H, 0, H[0]), drawstyle=\u0026#39;steps\u0026#39;, color=\u0026#39;green\u0026#39;) plt.grid(True) plt.legend() plt.show() bundle = outvec[0] show_da_stats(bundle) Nice how you can save figures from ipython if you need to import pylab import matplotlib.pyplot as plt plt.hist([1,2,3,4,1,2,3,4,1,2,1,2,2], bins=50) plt.title(\u0026#39;Histogram blah\u0026#39;) out_loc = \u0026#39;/your/location.blah.png\u0026#39; print(\u0026#39;saving to \u0026#39;, out_loc) pylab.savefig(out_loc) pylab.close() Running this in a jupyter notebook (NOTE: this data is from one of the Keras Hello World datasets) , per below import matplotlib.pyplot as plt image = [[0, 0, 0, 0, 0, 0, 0, 0, 33, 96, 175, 156, 64, 14, 54, 137, 204, 194, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 73, 186, 177, 183, 175, 188, 232, 255, 223, 219, 194, 179, 186, 213, 146, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 35, 163, 140, 150, 152, 150, 146, 175, 175, 173, 171, 156, 152, 148, 129, 156, 140, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 150, 142, 140, 152, 160, 156, 146, 142, 127, 135, 133, 140, 140, 137, 133, 125, 169, 75, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 54, 167, 146, 129, 142, 137, 137, 131, 148, 148, 133, 131, 131, 131, 125, 140, 140, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 110, 188, 133, 146, 152, 133, 125, 127, 119, 129, 133, 119, 140, 131, 150, 14, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 221, 158, 137, 135, 123, 110, 110, 114, 108, 112, 117, 127, 142, 77, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0, 25, 158, 137, 125, 119, 119, 110, 117, 117, 110, 119, 127, 144, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 123, 156, 129, 112, 110, 102, 112, 100, 121, 117, 129, 114, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 125, 169, 127, 119, 106, 108, 104, 94, 121, 114, 129, 91, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 98, 171, 129, 112, 104, 114, 106, 102, 112, 104, 133, 64, 0, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 66, 173, 135, 129, 98, 100, 119, 102, 108, 98, 135, 60, 0, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 56, 171, 135, 127, 100, 108, 117, 85, 106, 110, 135, 66, 0, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 52, 150, 129, 110, 100, 91, 102, 94, 83, 104, 123, 66, 0, 4, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 66, 167, 140, 148, 148, 127, 137, 152, 146, 146, 148, 96, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 45, 123, 94, 104, 96, 119, 121, 106, 98, 112, 87, 114, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 106, 89, 58, 50, 37, 50, 66, 56, 50, 75, 75, 137, 22, 0, 2, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 29, 148, 114, 106, 125, 89, 100, 133, 117, 131, 131, 131, 125, 112, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 100, 106, 114, 91, 137, 62, 102, 131, 89, 135, 112, 131, 108, 135, 37, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 146, 100, 108, 98, 144, 62, 106, 131, 87, 133, 104, 160, 117, 121, 68, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 33, 121, 108, 96, 100, 140, 71, 106, 127, 85, 140, 104, 150, 140, 114, 89, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 62, 119, 112, 102, 110, 137, 75, 106, 144, 81, 144, 108, 117, 154, 117, 104, 18, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 66, 121, 102, 112, 117, 131, 73, 104, 156, 77, 137, 135, 83, 179, 129, 121, 35, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 85, 127, 81, 125, 133, 119, 79, 100, 169, 83, 129, 175, 60, 163, 135, 146, 39, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 106, 129, 62, 140, 144, 108, 85, 83, 158, 85, 129, 175, 48, 146, 133, 135, 64, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 117, 119, 79, 140, 152, 102, 89, 110, 137, 96, 150, 196, 83, 144, 135, 133, 77, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 154, 121, 87, 140, 154, 112, 94, 52, 142, 100, 83, 152, 85, 160, 133, 100, 12, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 35, 4, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] plt.figure() plt.imshow(train_images[3]) plt.colorbar() plt.grid(False) plt.show() And wow that displays\u0026hellip; And the matplot grid , wow this is cool too Example code from this tutorial According to help(plt.subplot) , plt.subplot(5, 5, i) below is an instruction to place the ith thing, within a 5x5 grid, so basically the count starts at 0 from the upper left corner and spreads the grid as if it were a tape, from 0 to 5*5 - 1 plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i]) # , cmap=plt.cm.binary plt.xlabel(class_names[train_labels[i]]) plt.show() Obtaining image data from tensorflow import keras fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() image = train_mages[3] Plot colors With tips from here # print(plt.style.available) # [\u0026#39;Solarize_Light2\u0026#39;, \u0026#39;_classic_test_patch\u0026#39;, \u0026#39;bmh\u0026#39;, \u0026#39;classic\u0026#39;, \u0026#39;dark_background\u0026#39;, \u0026#39;fast\u0026#39;, \u0026#39;fivethirtyeight\u0026#39;, \u0026#39;ggplot\u0026#39;, \u0026#39;grayscale\u0026#39;, \u0026#39;seaborn\u0026#39;, \u0026#39;seaborn-bright\u0026#39;, \u0026#39;seaborn-colorblind\u0026#39;, \u0026#39;seaborn-dark\u0026#39;, \u0026#39;seaborn-dark-palette\u0026#39;, \u0026#39;seaborn-darkgrid\u0026#39;, \u0026#39;seaborn-deep\u0026#39;, \u0026#39;seaborn-muted\u0026#39;, \u0026#39;seaborn-notebook\u0026#39;, \u0026#39;seaborn-paper\u0026#39;, \u0026#39;seaborn-pastel\u0026#39;, \u0026#39;seaborn-poster\u0026#39;, \u0026#39;seaborn-talk\u0026#39;, \u0026#39;seaborn-ticks\u0026#39;, \u0026#39;seaborn-white\u0026#39;, \u0026#39;seaborn-whitegrid\u0026#39;, \u0026#39;tableau-colorblind10\u0026#39;] fig = plt.figure(figsize=(6, 6)) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) ax = fig.add_subplot(111, ) ax.hist([1, 2, 1, 2, 2, 3, 4, 5, 6], bins=2, ) ax.set_facecolor(\u0026#39;xkcd:salmon\u0026#39;) #plt.show(transparent=False) #help(ax) # set_subplotspec For the background this also helped.. ( per here ) # print(plt.style.available) # [\u0026#39;Solarize_Light2\u0026#39;, \u0026#39;_classic_test_patch\u0026#39;, \u0026#39;bmh\u0026#39;, \u0026#39;classic\u0026#39;, \u0026#39;dark_background\u0026#39;, \u0026#39;fast\u0026#39;, \u0026#39;fivethirtyeight\u0026#39;, \u0026#39;ggplot\u0026#39;, \u0026#39;grayscale\u0026#39;, \u0026#39;seaborn\u0026#39;, \u0026#39;seaborn-bright\u0026#39;, \u0026#39;seaborn-colorblind\u0026#39;, \u0026#39;seaborn-dark\u0026#39;, \u0026#39;seaborn-dark-palette\u0026#39;, \u0026#39;seaborn-darkgrid\u0026#39;, \u0026#39;seaborn-deep\u0026#39;, \u0026#39;seaborn-muted\u0026#39;, \u0026#39;seaborn-notebook\u0026#39;, \u0026#39;seaborn-paper\u0026#39;, \u0026#39;seaborn-pastel\u0026#39;, \u0026#39;seaborn-poster\u0026#39;, \u0026#39;seaborn-talk\u0026#39;, \u0026#39;seaborn-ticks\u0026#39;, \u0026#39;seaborn-white\u0026#39;, \u0026#39;seaborn-whitegrid\u0026#39;, \u0026#39;tableau-colorblind10\u0026#39;] with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(111, ) ax.hist([1, 2, 1, 2, 2, 3, 4, 5, 6], bins=2, ) more colors How to display a png from a file from IPython.display import Image, display loc = \u0026#39;somefile.png\u0026#39; display(Image(filename=loc)) Prevent some parts of figure from getting cut off This bbox_inches='tight' option really helps with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): plt.plot(np.random.randint(0, 100, size=100), np.random.randint(0, 100, size=100)) plt.title(\u0026#39;blah title\u0026#39;) out_loc = \u0026#39;blah.png\u0026#39; print(\u0026#39;saving to \u0026#39;, out_loc) pylab.savefig(out_loc, bbox_inches=\u0026#39;tight\u0026#39;) pylab.close() broken bar chart intended for gantt and I suspect useful as a waterfall for walltimes Borrowing this beautiful example from Geeks for Geeks # Declaring a figure \u0026#34;gnt\u0026#34; fig, gnt = plt.subplots() # Setting Y-axis limits gnt.set_ylim(0, 50) # Setting X-axis limits gnt.set_xlim(0, 160) # Setting labels for x-axis and y-axis gnt.set_xlabel(\u0026#39;seconds since start\u0026#39;) gnt.set_ylabel(\u0026#39;Processor\u0026#39;) # Setting ticks on y-axis gnt.set_yticks([15, 25, 35]) # Labelling tickes of y-axis gnt.set_yticklabels([\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;]) # Setting graph attribute gnt.grid(True) # Declaring a bar in schedule gnt.broken_barh([(40, 50)], (30, 9), facecolors =(\u0026#39;tab:orange\u0026#39;)) # Declaring multiple bars in at same level and same width gnt.broken_barh([(110, 10), (150, 10)], (10, 9), facecolors =\u0026#39;tab:blue\u0026#39;) gnt.broken_barh([(10, 50), (100, 20), (130, 10)], (20, 9), facecolors =(\u0026#39;tab:red\u0026#39;)) loc = \u0026#39;gantt.png\u0026#39; plt.savefig(loc) ","permalink":"https://michal.piekarczyk.xyz/handy/plot/","summary":"histogram overlays # Nice technique from https://srome.github.io/Covariate-Shift,-i.e.-Why-Prediction-Quality-Can-Degrade-In-Production-and-How-To-Fix-It/ # ... put two histograms on same plot ... def produce_overlayed_hists_for_col_dfs(col, dfs): fig = plt.figure(figsize=(12,12)) ax = fig.add_subplot(121) ax.hist(dfs[0][1][col], color=\u0026#39;r\u0026#39;, alpha=0.2, bins=50) ax.hist(dfs[1][1][col], color=\u0026#39;b\u0026#39;, alpha=0.2, bins=50) ax.set(title=f\u0026#39;{dfs[0][0]} (red) vs {dfs[1][0]} (blue)\u0026#39;, ylabel=col) Basic goal looks like the below.\nsparse diagonal x axis ticks import matplotlib.pyplot as plt import pandas as pd import datetime def make_xtick_labels(x, step=5): \u0026#39;\u0026#39;\u0026#39;Given x, step the labels every \u0026lt;step\u0026gt; Aka, take every \u0026lt;step\u0026gt;th x label \u0026#39;\u0026#39;\u0026#39; x_ticks = [i for i in range(len(x)) if i % step == 0] x_labels = [x[i] for i in x_ticks] return x_ticks, x_labels Did not add an example x , y yet, but showing an example where x contains dates and y is numeric.","title":""},{"content":"concurrent.futures Recently at work I needed to add retry logic to some code that was using the concurrent python library.\nI had done some research and I ended up also answering this stack overflow question too in the process.\nI am finding concurrent.futures to be pretty nice! Of course joblib is nice too.\nAnyway, re-posting my answer below as well.\nimport concurrent.futures import time import urllib from random import randint from collections import defaultdict URLS = [\u0026#39;http://www.foxnews.com/\u0026#39;, \u0026#39;http://www.cnn.com/\u0026#39;, \u0026#39;http://europe.wsj.com/\u0026#39;, \u0026#39;http://www.bbc.co.uk/\u0026#39;, \u0026#39;http://some-made-up-domain.com/\u0026#39;] URLS = [f\u0026#34;http://fake{i}.com\u0026#34; for i in range(20)] # Retrieve a single page and report the URL and contents def load_url(url, timeout): if \u0026#34;fake\u0026#34; in url: time.sleep(1) x = randint(1, 10) if x \u0026lt;= 5: return {\u0026#34;timeout\u0026#34;: True, \u0026#34;error\u0026#34;: \u0026#34;SimulatedTimeout\u0026#34;, \u0026#34;url\u0026#34;: url} elif x in [6, 7]: return {\u0026#34;error\u0026#34;: \u0026#34;SomeOtherError\u0026#34;, \u0026#34;url\u0026#34;: url} else: return {\u0026#34;data\u0026#34;: \u0026#34;\u0026lt;html\u0026gt;\u0026#34; + str(randint(1, 999999)) + \u0026#34;\u0026lt;/html\u0026gt;\u0026#34;, \u0026#34;url\u0026#34;: url} try: with urllib.request.urlopen(url, timeout=timeout) as conn: data = conn.read() return {\u0026#34;data\u0026#34;: data, \u0026#34;url\u0026#34;: url} # except urllib.error.URLError as e: except Exception as e: if \u0026#34;TimeoutError\u0026#34; in repr(e): return {\u0026#34;timeout\u0026#34;: True, \u0026#34;error\u0026#34;: repr(e), \u0026#34;url\u0026#34;: url} else: return {\u0026#34;error\u0026#34;: repr(e), \u0026#34;url\u0026#34;: url} todo = [{\u0026#34;url\u0026#34;: url} for url in URLS] final_results = [] retry_counts = defaultdict(int) max_retries = 5 with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: while True: future_list = [executor.submit(load_url, item[\u0026#34;url\u0026#34;], 60) for item in todo] todo = [] for future in concurrent.futures.as_completed(future_list): result = future.result() if result.get(\u0026#34;data\u0026#34;): final_results.append({**result, \u0026#34;retries\u0026#34;: retry_counts[result[\u0026#34;url\u0026#34;]]}) elif result.get(\u0026#34;error\u0026#34;) and not result.get(\u0026#34;timeout\u0026#34;): final_results.append({**result, \u0026#34;retries\u0026#34;: retry_counts[result[\u0026#34;url\u0026#34;]]}) elif result.get(\u0026#34;timeout\u0026#34;) and retry_counts[result[\u0026#34;url\u0026#34;]] \u0026lt; max_retries: retry_counts[result[\u0026#34;url\u0026#34;]] += 1 todo.append({\u0026#34;url\u0026#34;: result[\u0026#34;url\u0026#34;]}) else: final_results.append({**result, \u0026#34;reached_max_retries\u0026#34;: True, \u0026#34;retries\u0026#34;: retry_counts[result[\u0026#34;url\u0026#34;]]}) if len(final_results) == len(URLS): print(\u0026#34;Done!\u0026#34;) break else: print(f\u0026#34;we are now {len(final_results)} out of {len(URLS)}\u0026#34;) with the output\nwe are now 10 out of 20 we are now 11 out of 20 we are now 16 out of 20 we are now 17 out of 20 we are now 18 out of 20 Done! In [45]: pd.DataFrame.from_records(final_results) Out[45]: error url retries data timeout reached_max_retries 0 SomeOtherError http://fake0.com 0 NaN NaN NaN 1 NaN http://fake2.com 0 \u0026lt;html\u0026gt;124983\u0026lt;/html\u0026gt; NaN NaN 2 SomeOtherError http://fake3.com 0 NaN NaN NaN 3 NaN http://fake7.com 0 \u0026lt;html\u0026gt;459880\u0026lt;/html\u0026gt; NaN NaN 4 SomeOtherError http://fake10.com 0 NaN NaN NaN 5 NaN http://fake13.com 0 \u0026lt;html\u0026gt;598498\u0026lt;/html\u0026gt; NaN NaN 6 NaN http://fake15.com 0 \u0026lt;html\u0026gt;477976\u0026lt;/html\u0026gt; NaN NaN 7 NaN http://fake16.com 0 \u0026lt;html\u0026gt;748633\u0026lt;/html\u0026gt; NaN NaN 8 SomeOtherError http://fake17.com 0 NaN NaN NaN 9 NaN http://fake19.com 0 \u0026lt;html\u0026gt;104853\u0026lt;/html\u0026gt; NaN NaN 10 NaN http://fake9.com 1 \u0026lt;html\u0026gt;677035\u0026lt;/html\u0026gt; NaN NaN 11 NaN http://fake8.com 2 \u0026lt;html\u0026gt;249557\u0026lt;/html\u0026gt; NaN NaN 12 NaN http://fake5.com 2 \u0026lt;html\u0026gt;516063\u0026lt;/html\u0026gt; NaN NaN 13 SomeOtherError http://fake6.com 2 NaN NaN NaN 14 SomeOtherError http://fake11.com 2 NaN NaN NaN 15 NaN http://fake12.com 2 \u0026lt;html\u0026gt;66441\u0026lt;/html\u0026gt; NaN NaN 16 NaN http://fake1.com 3 \u0026lt;html\u0026gt;604868\u0026lt;/html\u0026gt; NaN NaN 17 SomeOtherError http://fake18.com 4 NaN NaN NaN 18 SimulatedTimeout http://fake4.com 5 NaN True True 19 SimulatedTimeout http://fake14.com 5 NaN True True ","permalink":"https://michal.piekarczyk.xyz/handy/python/concurrent_futures/","summary":"concurrent.futures Recently at work I needed to add retry logic to some code that was using the concurrent python library.\nI had done some research and I ended up also answering this stack overflow question too in the process.\nI am finding concurrent.futures to be pretty nice! Of course joblib is nice too.\nAnyway, re-posting my answer below as well.\nimport concurrent.futures import time import urllib from random import randint from collections import defaultdict URLS = [\u0026#39;http://www.","title":""},{"content":"environmental variable local injection using https://pypi.org/project/python-dotenv/ pip install -U python-dotenv Given a file like .env.test \u0026hellip;\nFOO=hi from dotenv import load_dotenv, find_dotenv load_dotenv(find_dotenv(\u0026#34;.env.test\u0026#34;, raise_error_if_not_found=True)) import os os.getenv(\u0026#39;FOO\u0026#39;) # =\u0026gt; \u0026#39;hi\u0026#39; ","permalink":"https://michal.piekarczyk.xyz/handy/python/dotenv/","summary":"environmental variable local injection using https://pypi.org/project/python-dotenv/ pip install -U python-dotenv Given a file like .env.test \u0026hellip;\nFOO=hi from dotenv import load_dotenv, find_dotenv load_dotenv(find_dotenv(\u0026#34;.env.test\u0026#34;, raise_error_if_not_found=True)) import os os.getenv(\u0026#39;FOO\u0026#39;) # =\u0026gt; \u0026#39;hi\u0026#39; ","title":""},{"content":"import numpy as np from bokeh.plotting import figure, show, output_file def doplot(x, y, **figure_kwargs): N = x.shape[0] radii = np.array([0.1,]*N) # print \u0026#39;DEBUG, \u0026#39;, radii[:4], \u0026#39;, \u0026#39;, N colors = [ \u0026#34;#%02x%02x%02x\u0026#34; % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y) ] TOOLS=\u0026#34;hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\u0026#34; p = figure(tools=TOOLS, **figure_kwargs) p.scatter(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None) output_file(\u0026#34;color_scatter.html\u0026#34;) show(p) # open a browser def make_data(N=100, trials=1000, minmax=(0, 1)): a, b = minmax data = [[sum(vec), fano(vec)] for vec in [a + (b - a)*np.random.random_sample(N) for i in range(trials)]] vec1, vec2 = zip(*data) return np.array(vec1), np.array(vec2) figure_kwargs = {\u0026#39;x_axis_label\u0026#39;: \u0026#39;sum(X)\u0026#39;, \u0026#39;y_axis_label\u0026#39;: \u0026#39;fano(X)\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;sum(X) vs fano(X)\u0026#39;} doplot(*make_data(minmax=(0,1)), **figure_kwargs) ","permalink":"https://michal.piekarczyk.xyz/handy/python/fano/","summary":"import numpy as np from bokeh.plotting import figure, show, output_file def doplot(x, y, **figure_kwargs): N = x.shape[0] radii = np.array([0.1,]*N) # print \u0026#39;DEBUG, \u0026#39;, radii[:4], \u0026#39;, \u0026#39;, N colors = [ \u0026#34;#%02x%02x%02x\u0026#34; % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y) ] TOOLS=\u0026#34;hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\u0026#34; p = figure(tools=TOOLS, **figure_kwargs) p.scatter(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None) output_file(\u0026#34;color_scatter.html\u0026#34;) show(p) # open a browser def make_data(N=100, trials=1000, minmax=(0, 1)): a, b = minmax data = [[sum(vec), fano(vec)] for vec in [a + (b - a)*np.","title":""},{"content":"import sys import time import datetime file = sys.stderr def log(logfile, tag): now = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M EST\u0026#39;) with open(logfile, \u0026#39;a\u0026#39;) as fd: fd.write(f\u0026#39;{now} {tag}\\n\u0026#39;) def do(minutes, logfile, tag): log(logfile, f\u0026#39;{tag} start\u0026#39;) seconds = minutes*60 for i in range(seconds): file.flush() #s = str(i%60).zfill(2) file.write(f\u0026#39;\\r{i//60}:{str(i%60).zfill(2)}\u0026#39;) time.sleep(1) log(logfile, f\u0026#39;{tag} end\u0026#39;) And choose any logfile location and any tag ..\n","permalink":"https://michal.piekarczyk.xyz/handy/python/poormans_pomodoro/","summary":"import sys import time import datetime file = sys.stderr def log(logfile, tag): now = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M EST\u0026#39;) with open(logfile, \u0026#39;a\u0026#39;) as fd: fd.write(f\u0026#39;{now} {tag}\\n\u0026#39;) def do(minutes, logfile, tag): log(logfile, f\u0026#39;{tag} start\u0026#39;) seconds = minutes*60 for i in range(seconds): file.flush() #s = str(i%60).zfill(2) file.write(f\u0026#39;\\r{i//60}:{str(i%60).zfill(2)}\u0026#39;) time.sleep(1) log(logfile, f\u0026#39;{tag} end\u0026#39;) And choose any logfile location and any tag ..","title":""},{"content":"line profiler Big fan of the line_profiler ( formerly here ) pip install line_profiler expensive.py import time @profile def foo(): for x in range(10): bar() flarg() @profile def bar(): time.sleep(.1) @profile def flarg(): time.sleep(.1) foo() profile (pandars38) ãƒ„ kernprof -lv expensive.py Wrote profile results to expensive.py.lprof Timer unit: 1e-06 s Total time: 2.06251 s File: expensive.py Function: foo at line 3 Line # Hits Time Per Hit % Time Line Contents ============================================================== 3 @profile 4 def foo(): 5 11 54.0 4.9 0.0 for x in range(10): 6 10 1027267.0 102726.7 49.8 bar() 7 10 1035191.0 103519.1 50.2 flarg() Total time: 1.02698 s File: expensive.py Function: bar at line 9 Line # Hits Time Per Hit % Time Line Contents ============================================================== 9 @profile 10 def bar(): 11 10 1026983.0 102698.3 100.0 time.sleep(.1) Total time: 1.0349 s File: expensive.py Function: flarg at line 13 Line # Hits Time Per Hit % Time Line Contents ============================================================== 13 @profile 14 def flarg(): 15 10 1034899.0 103489.9 100.0 time.sleep(.1) ","permalink":"https://michal.piekarczyk.xyz/handy/python/profiling/","summary":"line profiler Big fan of the line_profiler ( formerly here ) pip install line_profiler expensive.py import time @profile def foo(): for x in range(10): bar() flarg() @profile def bar(): time.sleep(.1) @profile def flarg(): time.sleep(.1) foo() profile (pandars38) ãƒ„ kernprof -lv expensive.py Wrote profile results to expensive.py.lprof Timer unit: 1e-06 s Total time: 2.06251 s File: expensive.py Function: foo at line 3 Line # Hits Time Per Hit % Time Line Contents ============================================================== 3 @profile 4 def foo(): 5 11 54.","title":""},{"content":"What the what Notes from after converting a project using the 2to3, of additional gotchas\nTOC StringIO Pickling Uuid xrange wow the silent division bug! func.func_name calling lambdas w/ boto3 and using BytesIO Bytes and json lambda , [ERROR] Runtime.MarshalError: Unable to marshal response: b'gAN9cQAoWA4 dict merging Meat StringIO Doing this fixes things typically.. Change import StringIO to try: from StringIO import StringIO except: from io import StringIO And update any StringIO.StringIO() to just StringIO() cPickle and pickle Because theres no more cPickle I changed cPickle to pickle and started getting this 226 with open(fn) as fd: --\u0026gt; 227 dtypes_dict = pickle.load(fd) 228 return dtypes_dict 229 TypeError: a bytes-like object is required, not \u0026#39;str\u0026#39; because pickled objects encoded with the string like protocol need to be re-encoded I think. But I was able to actually read the python2 ASCII pickle by doing this. Worked for me with open(fn,\u0026#39;rb\u0026#39;) as fd: dtypes_dict = pickle.load(fd) Treating somedict.keys() as a list In [32]: dtypes_dict.keys()[:5] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-32-41046874d947\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 dtypes_dict.keys()[:5] TypeError: \u0026#39;dict_keys\u0026#39; object is not subscriptable I think just need change to this\u0026hellip; list(dtypes_dict.keys())[:5] uuid Got this. 43 def make_nonce(): ---\u0026gt; 44 return uuid.uuid4().get_hex() 45 46 def make_date_s3_prefix(timestamp): AttributeError: \u0026#39;UUID\u0026#39; object has no attribute \u0026#39;get_hex\u0026#39; Changed to \u0026hellip; In [12]: uu.hex Out[12]: \u0026#39;19487abb29fb4e8197df6f000c31b358\u0026#39; xrange no more xrange. it\u0026rsquo;s now just range note per here Partition code bug This func didnt crash in python 3 but the result was quite different. def get_partitions(vec, slice_size): assert slice_size \u0026gt; 0 assert isinstance(vec, list) num_slices = int(math.ceil(len(vec)/slice_size)) size_remainder = len(vec) - num_slices*slice_size slices = [vec[k*slice_size:k*slice_size+slice_size] for k in range(num_slices)] if size_remainder: slices.append(vec[-(size_remainder):]) return slices python 2: as expected ids = [2220706, 2220705, 2220703, 2220700, 2220696, 2220690, 2220688, 2220687, 2220682, 2220676, 2220674, 2220671] # len(ids) # 12 get_partitions(ids, 5) # =\u0026gt; [[2220706, 2220705, 2220703, 2220700, 2220696], [2220690, 2220688, 2220687, 2220682, 2220676], [2220674, 2220671]] python 3, wo what the heck get_partitions(ids, 5) # =\u0026gt; [[2220706, 2220705, 2220703, 2220700, 2220696], [2220690, 2220688, 2220687, 2220682, 2220676], [2220674, 2220671], [2220700, 2220696, 2220690, 2220688, 2220687, 2220682, 2220676, 2220674, 2220671]] The file where this function exists was missing the standard from __future__ import division, absolute_import, print_function, unicode_literals line, so that\u0026rsquo;s why this happened in the first place. The fix to make this work for both python2 and python3 was to rewrite the / division as // explicitly \u0026hellip; def get_partitions(vec, slice_size): assert slice_size \u0026gt; 0 assert isinstance(vec, list) num_slices = len(vec)//slice_size size_remainder = len(vec) - num_slices*slice_size slices = [vec[k*slice_size:k*slice_size+slice_size] for k in range(num_slices)] if size_remainder: slices.append(vec[-(size_remainder):]) return slices No more Func name per https://docs.python.org/3/whatsnew/3.0.html#operators-and-special-methods godamn getattr(some_func, 'func_name') to retrieve the name of a func. no longer works in python 3 that is now some_func.__name__ :grimacing: similarly some_func.func_code was renamed to some_func.__code__ Notes on reading python2 pickle in python3 Given a pandas DataFrame written like this, cPickle.dumps(df) I was able to read it in python 3 like this with open(\u0026#39;blah.pkl\u0026#39;, \u0026#39;rb\u0026#39;) as fd: df = pickle.load(fd, encoding=\u0026#39;latin1\u0026#39;) # And if having read it from s3 to a bytes object, this worked too df = pickle.loads(pkl, encoding=\u0026#39;latin1\u0026#39;) Noticing boto3 uses bytes now now str Before it was possible to do this import boto3 import json from StringIO import StringIO client = boto3.client(\u0026#39;lambda\u0026#39;) json_payload = json.dumps(payload) s = StringIO(json_payload) version = \u0026#39;4\u0026#39; response = client.invoke( FunctionName=\u0026#39;myBlahBlahLambda\u0026#39;, InvocationType=\u0026#39;RequestResponse\u0026#39;, LogType=\u0026#39;Tail\u0026#39;, Payload=s, Qualifier=version) out_dict = json.loads(response.get(\u0026#39;Payload\u0026#39;).read()) return out_dict Now that complains with TypeError: Unicode-objects must be encoded before hashing But it works to use this instead\u0026hellip; import boto3 import json from io import BytesIO client = boto3.client(\u0026#39;lambda\u0026#39;) json_payload = json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;) # \u0026lt;-- encode s = BytesIO(json_payload) version = \u0026#39;4\u0026#39; response = client.invoke( FunctionName=\u0026#39;myBlahBlahLambda\u0026#39;, InvocationType=\u0026#39;RequestResponse\u0026#39;, LogType=\u0026#39;Tail\u0026#39;, Payload=s, Qualifier=version) out_dict = json.loads(response.get(\u0026#39;Payload\u0026#39;).read()) return out_dict Bytes and json Relevant to data obtained with requests and base64.b64encode for example. These now produce bytes as opposed to str. \u0026#34;TypeError: Object of type bytes is not JSON serializable\u0026#34;, this comes up when trying to use json.dumps . Previously strings now bytes in there, so typically need to b\u0026rsquo;blah\u0026rsquo;.decode(\u0026lsquo;utf-8\u0026rsquo;) lambda cannot return bytes/json [ERROR] Runtime.MarshalError: Unable to marshal response: b\u0026#39;gAN9cQAoWA4 happening when have bytes in the response.. Dict merging interestingly the dict() vs {} behavior is different.. In [34]: dict(**{\u0026#39;hi\u0026#39;: \u0026#39;there\u0026#39;}, **{\u0026#39;hello\u0026#39;: \u0026#39;there\u0026#39;}, **{\u0026#39;hello\u0026#39;: \u0026#39;sailor\u0026#39;}) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-34-3bc078749ddb\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 dict(**{\u0026#39;hi\u0026#39;: \u0026#39;there\u0026#39;}, **{\u0026#39;hello\u0026#39;: \u0026#39;there\u0026#39;}, **{\u0026#39;hello\u0026#39;: \u0026#39;sailor\u0026#39;}) TypeError: type object got multiple values for keyword argument \u0026#39;hello\u0026#39; In [36]: dict(list({\u0026#39;hi\u0026#39;: \u0026#39;there\u0026#39;}.items())+ list({\u0026#39;hello\u0026#39;: \u0026#39;there\u0026#39;}.items())+ list({\u0026#39;he ...: llo\u0026#39;: \u0026#39;sailor\u0026#39;}.items())) Out[36]: {\u0026#39;hi\u0026#39;: \u0026#39;there\u0026#39;, \u0026#39;hello\u0026#39;: \u0026#39;sailor\u0026#39;} In [37]: {**{\u0026#39;hi\u0026#39;: \u0026#39;there\u0026#39;}, **{\u0026#39;hello\u0026#39;: \u0026#39;there\u0026#39;}, **{\u0026#39;hello\u0026#39;: \u0026#39;sailor\u0026#39;}} Out[37]: {\u0026#39;hi\u0026#39;: \u0026#39;there\u0026#39;, \u0026#39;hello\u0026#39;: \u0026#39;sailor\u0026#39;} urlparse From import urlparse To from urllib.parse import urlparse ","permalink":"https://michal.piekarczyk.xyz/handy/python/python2-to-3/","summary":"What the what Notes from after converting a project using the 2to3, of additional gotchas\nTOC StringIO Pickling Uuid xrange wow the silent division bug! func.func_name calling lambdas w/ boto3 and using BytesIO Bytes and json lambda , [ERROR] Runtime.MarshalError: Unable to marshal response: b'gAN9cQAoWA4 dict merging Meat StringIO Doing this fixes things typically.. Change import StringIO to try: from StringIO import StringIO except: from io import StringIO And update any StringIO.","title":""},{"content":"Generate a CTE from a local csv file import pandas as pd replace_nan = lambda x: x.replace(\u0026#39;nan\u0026#39;, \u0026#39;null\u0026#39;) def df_to_values(df, columns=None, replace_nans=True): if columns is None: columns = df.columns.tolist() newdata = str(list(df[columns].to_records( index=False)) )[1:-1] if replace_nans: newdata = replace_nan(newdata) return newdata def cte_from_csv(localfile, colgroups, cte_names, head=False): df = pd.read_csv(localfile) if head: df = df.head() return \u0026#39;with \u0026#39; + \u0026#39;, \u0026#39;.join([ f\u0026#39;\u0026#39;\u0026#39; {cte_names[i]}({\u0026#39;, \u0026#39;.join(colgroups[i])}) as ( VALUES {df_to_values(df, columns=colgroups[i], replace_nans=True)} ) \u0026#39;\u0026#39;\u0026#39; for i, _ in enumerate(colgroups) ]) temp.csv one,two,three 1,2.,3.3 ,2.3,3.5 11,.22,.003 Example loc = \u0026#34;temp.csv\u0026#34; print( cte_from_csv(loc, [[\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]], [\u0026#39;foo\u0026#39;],)) =\u0026gt; with foo(one, two, three) as ( VALUES (1., 2., 3.3), (null, 2.3, 3.5), (11., 0.22, 0.003) ) Dollar encode def doll_df_to_values(df, cols, cols_to_dollar_encode=None): data = [tuple(x) for x in df[cols].to_records(index=False)] vec = [\u0026#39;(\u0026#39; + \u0026#39;, \u0026#39;.join([enc(x[i], cols[i] in cols_to_dollar_encode) for (i, _) in enumerate(x)]) + \u0026#39;)\u0026#39; for x in data] return \u0026#39;, \u0026#39;.join(vec) def enc(x, dollar_enc): if dollar_enc: return f\u0026#39;$${x}$$\u0026#39; elif isinstance(x, str): return f\u0026#34;\u0026#39;{x}\u0026#39;\u0026#34; elif np.isnan(x): return \u0026#39;null\u0026#39; else: return f\u0026#34;{x}\u0026#34; ","permalink":"https://michal.piekarczyk.xyz/handy/python/python_sql_helpers/","summary":"Generate a CTE from a local csv file import pandas as pd replace_nan = lambda x: x.replace(\u0026#39;nan\u0026#39;, \u0026#39;null\u0026#39;) def df_to_values(df, columns=None, replace_nans=True): if columns is None: columns = df.columns.tolist() newdata = str(list(df[columns].to_records( index=False)) )[1:-1] if replace_nans: newdata = replace_nan(newdata) return newdata def cte_from_csv(localfile, colgroups, cte_names, head=False): df = pd.read_csv(localfile) if head: df = df.head() return \u0026#39;with \u0026#39; + \u0026#39;, \u0026#39;.join([ f\u0026#39;\u0026#39;\u0026#39; {cte_names[i]}({\u0026#39;, \u0026#39;.join(colgroups[i])}) as ( VALUES {df_to_values(df, columns=colgroups[i], replace_nans=True)} ) \u0026#39;\u0026#39;\u0026#39; for i, _ in enumerate(colgroups) ]) temp.","title":""},{"content":"Search and return json paths def substring_exists_lower(substring, string): # f = lambda key, term: term in key.lower() return substring.lower() in string.lower() def path_join(path, key): return f\u0026#39;{path}{\u0026#34;.\u0026#34; if path else \u0026#34;\u0026#34;}{key}\u0026#39; def find_term(path, term, node, found, only_leaves=False): # must be dict or list if not ((isinstance(node, dict)) or (isinstance(node, list))): return # look in this node if isinstance(node, dict): for key in node.keys(): if substring_exists_lower(term, key): if only_leaves: if not ((isinstance(node[key], dict)) or (isinstance(node[key], list))): found.add(path_join(path, key)) else: found.add(path_join(path, key)) for key in node.keys(): #if isinstance(node[key], dict): find_term(path_join(path, key), term, node[key], found, only_leaves) if isinstance(node, list): for i, x in enumerate(node): find_term(f\u0026#39;{path}[{i}]\u0026#39;, term, node[i], found, only_leaves) Example found = set() find_term(\u0026#39;\u0026#39;, \u0026#39;name\u0026#39;, {}, found) ","permalink":"https://michal.piekarczyk.xyz/handy/python/search_muh_jsons/","summary":"Search and return json paths def substring_exists_lower(substring, string): # f = lambda key, term: term in key.lower() return substring.lower() in string.lower() def path_join(path, key): return f\u0026#39;{path}{\u0026#34;.\u0026#34; if path else \u0026#34;\u0026#34;}{key}\u0026#39; def find_term(path, term, node, found, only_leaves=False): # must be dict or list if not ((isinstance(node, dict)) or (isinstance(node, list))): return # look in this node if isinstance(node, dict): for key in node.keys(): if substring_exists_lower(term, key): if only_leaves: if not ((isinstance(node[key], dict)) or (isinstance(node[key], list))): found.","title":""},{"content":"Fetch all events for an issue like this issue = \u0026#34;12345678\u0026#34; events = get_all_issue_events(issue) First set your SENTRY_AUTH_TOKEN as env var With definitions.. import requests import time def get_all_issue_events(organization_slug, project_slug, issue_id): url = \u0026#39;https://app.getsentry.com/api/0/issues/%s/events/\u0026#39; % issue_id all_data = get_all_data(url) event_ids = [x[\u0026#34;id\u0026#34;] for x in all_data] detail_vec = [get_event_data(organization_slug, project_slug, event_id) for event_id in event_ids] return all_data, detail_vec def get_all_data(url): token = os.environ.get(\u0026#39;SENTRY_AUTH_TOKEN\u0026#39;) headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer %s\u0026#34; % token, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} next_results = \u0026#39;true\u0026#39; next_url = url all_data = [] while next_results == \u0026#39;true\u0026#39;: # Do fetch pass response = requests.get(next_url, headers=headers) if response.status_code == 200: data = response.json() if isinstance(data, list): all_data += data elif isinstance(data, dict): all_data.append(data) next_results = response.links.get(\u0026#39;next\u0026#39;, {}).get(\u0026#39;results\u0026#39;) next_url = response.links.get(\u0026#39;next\u0026#39;, {}).get(\u0026#39;url\u0026#39;) time.sleep(0.4) else: next_results = \u0026#39;false\u0026#39; return all_data def get_event_data(organization_slug, project_slug, event_id): url = f\u0026#34;https://sentry.io/api/0/projects/{organization_slug}/{project_slug}/events/{event_id}/\u0026#34; token = os.environ.get(\u0026#39;SENTRY_AUTH_TOKEN\u0026#39;) headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer %s\u0026#34; % token, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} response = requests.get(url, headers=headers) return response.json() ","permalink":"https://michal.piekarczyk.xyz/handy/python/sentry-io-api/","summary":"Fetch all events for an issue like this issue = \u0026#34;12345678\u0026#34; events = get_all_issue_events(issue) First set your SENTRY_AUTH_TOKEN as env var With definitions.. import requests import time def get_all_issue_events(organization_slug, project_slug, issue_id): url = \u0026#39;https://app.getsentry.com/api/0/issues/%s/events/\u0026#39; % issue_id all_data = get_all_data(url) event_ids = [x[\u0026#34;id\u0026#34;] for x in all_data] detail_vec = [get_event_data(organization_slug, project_slug, event_id) for event_id in event_ids] return all_data, detail_vec def get_all_data(url): token = os.environ.get(\u0026#39;SENTRY_AUTH_TOKEN\u0026#39;) headers = {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer %s\u0026#34; % token, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} next_results = \u0026#39;true\u0026#39; next_url = url all_data = [] while next_results == \u0026#39;true\u0026#39;: # Do fetch pass response = requests.","title":""},{"content":"enter and exit According to stack overflow you can.. In [97]: class Blah(object): ...: def __enter__(self): ...: print(\u0026#34;hello\u0026#34;) ...: return self ...: def __exit__(self, exc_type, exc_val, exc_tb): ...: print(\u0026#34;bye!\u0026#34;) ...: my = \u0026#34;stuff\u0026#34; ...: In [98]: with Blah(): ...: print(\u0026#34;doing stuff\u0026#34;) ...: hello doing stuff bye! The specific example that follows this is the object which is returned by import psycopg2 conn = psycopg2.connect() Help on connection object: class connection(builtins.object) | connection(dsn, ...) -\u0026gt; new connection object | | :Groups: | * `DBAPI-2.0 errors`: Error, Warning, InterfaceError, | DatabaseError, InternalError, OperationalError, | ProgrammingError, IntegrityError, DataError, NotSupportedError | | Methods defined here: | | __enter__(...) | __enter__ -\u0026gt; self | | __exit__(...) | __exit__ -- commit if no exception, else roll back | ","permalink":"https://michal.piekarczyk.xyz/handy/python/with_context_manager/","summary":"enter and exit According to stack overflow you can.. In [97]: class Blah(object): ...: def __enter__(self): ...: print(\u0026#34;hello\u0026#34;) ...: return self ...: def __exit__(self, exc_type, exc_val, exc_tb): ...: print(\u0026#34;bye!\u0026#34;) ...: my = \u0026#34;stuff\u0026#34; ...: In [98]: with Blah(): ...: print(\u0026#34;doing stuff\u0026#34;) ...: hello doing stuff bye! The specific example that follows this is the object which is returned by import psycopg2 conn = psycopg2.connect() Help on connection object: class connection(builtins.object) | connection(dsn, .","title":""},{"content":"Glom does not let you write or at least I couldnt figure out how. Tinkering with a spec based writer\u0026hellip;\nfrom glom import glom, PathAccessError def nested_assign(target, spec, value): parts = spec.split(\u0026#34;.\u0026#34;) last = parts[-1] while parts[:-1]: top = parts.pop(0) target = target[top] target[last] = value def _plant(target, spec): \u0026#34;\u0026#34;\u0026#34;This is the equivalent of mkdir -p blah/flarg/blarg/klarf \u0026#34;\u0026#34;\u0026#34; parts = spec.split(\u0026#34;.\u0026#34;) try: for i, part in enumerate(parts): glom(target, \u0026#34;.\u0026#34;.join(parts[:i + 1])) except PathAccessError as e: print(repr(e)) print(\u0026#34;stopped at \u0026#34;, i, part) print(\u0026#34;going to add remaining\u0026#34;, parts[i:]) print(\u0026#34;..\u0026#34;, list(range(i, len(parts)))) for j in range(i + 1, len(parts)): this_spec = \u0026#34;.\u0026#34;.join(parts[:j]) print(\u0026#34;this_spec\u0026#34;, this_spec) nested_assign(target, this_spec, {}) def transplant(source_dict, mappings): skeleton = {} for m in mappings: _plant(skeleton, m[\u0026#34;original\u0026#34;]) value = glom(source_dict, m[\u0026#34;new\u0026#34;]) nested_assign(skeleton, m[\u0026#34;original\u0026#34;], value) return skeleton Note I saw someone on stack overflow had a similar question but the approach looks a bit complex. May look at this later. ","permalink":"https://michal.piekarczyk.xyz/handy/python/write_glom/","summary":"Glom does not let you write or at least I couldnt figure out how. Tinkering with a spec based writer\u0026hellip;\nfrom glom import glom, PathAccessError def nested_assign(target, spec, value): parts = spec.split(\u0026#34;.\u0026#34;) last = parts[-1] while parts[:-1]: top = parts.pop(0) target = target[top] target[last] = value def _plant(target, spec): \u0026#34;\u0026#34;\u0026#34;This is the equivalent of mkdir -p blah/flarg/blarg/klarf \u0026#34;\u0026#34;\u0026#34; parts = spec.split(\u0026#34;.\u0026#34;) try: for i, part in enumerate(parts): glom(target, \u0026#34;.\u0026#34;.join(parts[:i + 1])) except PathAccessError as e: print(repr(e)) print(\u0026#34;stopped at \u0026#34;, i, part) print(\u0026#34;going to add remaining\u0026#34;, parts[i:]) print(\u0026#34;.","title":""},{"content":"for tar.gz Assuming we have a func read_from_s3 that reads from s3\u0026hellip;\nimport tarfile targz = read_from_s3(bucket, s3fn) tar = tarfile.open(fileobj=io.BytesIO(targz), mode=\u0026#34;r:gz\u0026#34;) blahstream = tar.extractfile(\u0026#39;blah-filename\u0026#39;) for zip files Nice doc here from zipfile import ZipFile with ZipFile(\u0026#39;foo.zip\u0026#39;) as zip_archive: foo = zip_archive.read(\u0026#39;some/file.txt\u0026#39;) ","permalink":"https://michal.piekarczyk.xyz/handy/python/zip/","summary":"for tar.gz Assuming we have a func read_from_s3 that reads from s3\u0026hellip;\nimport tarfile targz = read_from_s3(bucket, s3fn) tar = tarfile.open(fileobj=io.BytesIO(targz), mode=\u0026#34;r:gz\u0026#34;) blahstream = tar.extractfile(\u0026#39;blah-filename\u0026#39;) for zip files Nice doc here from zipfile import ZipFile with ZipFile(\u0026#39;foo.zip\u0026#39;) as zip_archive: foo = zip_archive.read(\u0026#39;some/file.txt\u0026#39;) ","title":""},{"content":"Macvim brew install macvim\nThen cp -R /usr/local/Cellar/macvim/8.2-171_1/MacVim.app ~/Applications And add alias mvim=/Users/${username}/Applications/MacVim.app/Contents/bin/mvim to ~/.bash_profile If above not possible, then download MacVIM from macvim github ( which was forked from here originally I think )\nother vim stuff ctrlp, from https://github.com/ctrlpvim/ctrlp.vim\nmkdir -p ~/.vim/pack/plugins/start git clone --depth=1 https://github.com/ctrlpvim/ctrlp.vim.git ~/.vim/pack/plugins/start/ctrlp theme, solarized8 sometimes is good (also slate too )\nmkdir -p ~/.vim/pack/themes/opt/ cd ~/.vim/pack/themes/opt/ git clone git@github.com:lifepillar/vim-solarized8.git Vim notes Vim doesnt know about \u0026ldquo;terraform\u0026rdquo; files like .tf and the hashivim github helps with that. vim-terraform actually has a magical :TerraformFmt command that inspects the syntax of your .tf file too. ","permalink":"https://michal.piekarczyk.xyz/handy/rc/readme/","summary":"Macvim brew install macvim\nThen cp -R /usr/local/Cellar/macvim/8.2-171_1/MacVim.app ~/Applications And add alias mvim=/Users/${username}/Applications/MacVim.app/Contents/bin/mvim to ~/.bash_profile If above not possible, then download MacVIM from macvim github ( which was forked from here originally I think )\nother vim stuff ctrlp, from https://github.com/ctrlpvim/ctrlp.vim\nmkdir -p ~/.vim/pack/plugins/start git clone --depth=1 https://github.com/ctrlpvim/ctrlp.vim.git ~/.vim/pack/plugins/start/ctrlp theme, solarized8 sometimes is good (also slate too )\nmkdir -p ~/.vim/pack/themes/opt/ cd ~/.vim/pack/themes/opt/ git clone git@github.com:lifepillar/vim-solarized8.git Vim notes Vim doesnt know about \u0026ldquo;terraform\u0026rdquo; files like .","title":""},{"content":"Add reverse lookup to zsh bindkey \u0026#39;^R\u0026#39; history-incremental-search-backward ( read on Stack Exchange \u0026gt; Unix\u0026amp;Linux \u0026gt; \u0026ldquo;How to enable reverse search in zsh?\u0026rdquo;\n","permalink":"https://michal.piekarczyk.xyz/handy/rc/zsh/","summary":"Add reverse lookup to zsh bindkey \u0026#39;^R\u0026#39; history-incremental-search-backward ( read on Stack Exchange \u0026gt; Unix\u0026amp;Linux \u0026gt; \u0026ldquo;How to enable reverse search in zsh?\u0026rdquo;","title":""},{"content":"Edit files in place This -i '' was necessary on MacOs to avoid creating a greatings.txt.bak file as a backup $ sed -i \u0026#39;\u0026#39; \u0026#39;s/hello/bonjour/\u0026#39; greetings.txt xargs into vim Per this helpful answer , you can xargs into vim on macos x with xargs -o \u0026hellip; find . -name \u0026#39;blahfile*py\u0026#39; |head -1 |xargs -o vim xargs to md5 This is nice too, quickly md5 files.\n$ fd spark-wee en/post/2021-01-23-spark-weekend.md posts/2021-01-23-spark-weekend.md $ $ fd spark-wee|xargs -o md5 MD5 (./en/post/2021-01-23-spark-weekend.md) = 5534f81599860e239340b41ffa5aee09 MD5 (./posts/2021-01-23-spark-weekend.md) = 5534f81599860e239340b41ffa5aee09 ","permalink":"https://michal.piekarczyk.xyz/handy/shell/","summary":"Edit files in place This -i '' was necessary on MacOs to avoid creating a greatings.txt.bak file as a backup $ sed -i \u0026#39;\u0026#39; \u0026#39;s/hello/bonjour/\u0026#39; greetings.txt xargs into vim Per this helpful answer , you can xargs into vim on macos x with xargs -o \u0026hellip; find . -name \u0026#39;blahfile*py\u0026#39; |head -1 |xargs -o vim xargs to md5 This is nice too, quickly md5 files.\n$ fd spark-wee en/post/2021-01-23-spark-weekend.md posts/2021-01-23-spark-weekend.md $ $ fd spark-wee|xargs -o md5 MD5 (.","title":""},{"content":"import datetime def date_from_date_str(date_str): return datetime.datetime.strptime(date_str, \u0026#39;%Y-%m-%d\u0026#39;).date() def make_start_end_clauses(start, end): \u0026#39;\u0026#39;\u0026#39;Make sql to take advantage of Athena date partitioning. Example WHERE ((year = 2017 AND month = 10 AND day \u0026gt;=30) OR (year = 2017 AND month = 11 AND day = 1))\u0026#39;\u0026#39;\u0026#39; assert start \u0026lt;= end month_tuples = make_start_end_month_tuples(start, end) clauses = [] if len(month_tuples) == 1: clause_raw = (\u0026#39;(year = {} AND month = {} AND day BETWEEN {} AND {})\u0026#39;) clause = clause_raw.format(start.year, start.month, start.day, end.day) clauses.append(clause) else: # First month. start_clause = \u0026#39;(year = {} AND month = {} AND day \u0026gt;= {})\u0026#39;.format( start.year, start.month, start.day, ) clauses.append(start_clause) # Middle months, if any. months_in_between = month_tuples[1:-1] for year, month in months_in_between: clauses.append( \u0026#39;(year = {} AND month = {})\u0026#39;.format(year, month)) # Last month. end_clause = \u0026#39;(year = {} AND month = {} AND day \u0026lt;= {})\u0026#39;.format( end.year, end.month, end.day, ) clauses.append(end_clause) all_clause = \u0026#39; OR \u0026#39;.join(clauses) final_clause = \u0026#39; ( {} ) \u0026#39;.format(all_clause) return final_clause def make_date_clause(start_str=None, end_str=None, start_date=None, end_date=None): \u0026#39;\u0026#39;\u0026#39;If given a start, return sql from start until today. Otherwise, return sql which only goes back 7 days. \u0026#39;\u0026#39;\u0026#39; if start_str is None and end_str is None and start_date is None and end_date is None: # do 7 days... today = utc_today() date_clause = date_clause_from_days_ago(today, days_ago=7) return date_clause if start_str is not None and end_str is not None: start_date = date_from_date_str(start_str) end_date = date_from_date_str(end_str) date_clause = make_start_end_clauses(start=start_date, end=end_date) return date_clause if start_date is not None and end_date is not None: date_clause = make_start_end_clauses(start=start_date, end=end_date) return date_clause def make_start_end_month_tuples(start, end): assert start \u0026lt;= end tuples = set([(start.year, start.month)]) next_date = start while next_date \u0026lt;= end: tuples |= set([(next_date.year, next_date.month)]) next_date += datetime.timedelta(days=1) return sorted(list(tuples)) Examples import datetime make_start_end_clauses(datetime.date(2021,4,29), datetime.date(2021, 5, 5)) # \u0026#39; ( (year = 2021 AND month = 4 AND day \u0026gt;= 29) OR (year = 2021 AND month = 5 AND day \u0026lt;= 5) ) \u0026#39; make_start_end_clauses(datetime.date(2021,4,29), datetime.date(2022, 5, 5)) # \u0026#39; ( (year = 2021 AND month = 4 AND day \u0026gt;= 29) OR (year = 2021 AND month = 5) OR (year = 2021 AND month = 6) OR (year = 2021 AND month = 7) OR (year = 2021 AND month = 8) OR (year = 2021 AND month = 9) OR (year = 2021 AND month = 10) OR (year = 2021 AND month = 11) OR (year = 2021 AND month = 12) OR (year = 2022 AND month = 1) OR (year = 2022 AND month = 2) OR (year = 2022 AND month = 3) OR (year = 2022 AND month = 4) OR (year = 2022 AND month = 5 AND day \u0026lt;= 5) ) \u0026#39; ","permalink":"https://michal.piekarczyk.xyz/handy/sql/date_range_builder/","summary":"import datetime def date_from_date_str(date_str): return datetime.datetime.strptime(date_str, \u0026#39;%Y-%m-%d\u0026#39;).date() def make_start_end_clauses(start, end): \u0026#39;\u0026#39;\u0026#39;Make sql to take advantage of Athena date partitioning. Example WHERE ((year = 2017 AND month = 10 AND day \u0026gt;=30) OR (year = 2017 AND month = 11 AND day = 1))\u0026#39;\u0026#39;\u0026#39; assert start \u0026lt;= end month_tuples = make_start_end_month_tuples(start, end) clauses = [] if len(month_tuples) == 1: clause_raw = (\u0026#39;(year = {} AND month = {} AND day BETWEEN {} AND {})\u0026#39;) clause = clause_raw.","title":""},{"content":"List indexes From here SELECT tablename, indexname, indexdef FROM pg_indexes WHERE schemaname = \u0026#39;public\u0026#39; ORDER BY tablename, indexname; Disk Usage per table from the postgresql wiki except one minor change \u0026hellip; for ('user_blah', 'user_blar', 'schema1', 'schema2') schemas only \u0026hellip; SELECT *, pg_size_pretty(total_bytes) AS total , pg_size_pretty(index_bytes) AS INDEX , pg_size_pretty(toast_bytes) AS toast , pg_size_pretty(table_bytes) AS TABLE FROM ( SELECT *, total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes FROM ( SELECT c.oid,nspname AS table_schema, relname AS TABLE_NAME , c.reltuples AS row_estimate , pg_total_relation_size(c.oid) AS total_bytes , pg_indexes_size(c.oid) AS index_bytes , pg_total_relation_size(reltoastrelid) AS toast_bytes FROM pg_class c LEFT JOIN pg_namespace n ON n.oid = c.relnamespace WHERE relkind = \u0026#39;r\u0026#39; and nspname in (\u0026#39;user_blah\u0026#39;, \u0026#39;user_blar\u0026#39;, \u0026#39;schema1\u0026#39;, \u0026#39;schema2\u0026#39;) ) a ) a detect blocked queries? This didnt exactly work for me as expected, but colleague had mentioned this \u0026hellip; SELECT COALESCE(blockingl.relation::regclass::text,blockingl.locktype) as locked_item, now() - blockeda.query_start AS waiting_duration, blockeda.pid AS blocked_pid, blockeda.query as blocked_query, blockedl.mode as blocked_mode, blockinga.pid AS blocking_pid, blockinga.query as blocking_query, blockingl.mode as blocking_mode FROM pg_catalog.pg_locks blockedl JOIN pg_stat_activity blockeda ON blockedl.pid = blockeda.pid JOIN pg_catalog.pg_locks blockingl ON( ( (blockingl.transactionid=blockedl.transactionid) OR (blockingl.relation=blockedl.relation AND blockingl.locktype=blockedl.locktype) ) AND blockedl.pid != blockingl.pid) JOIN pg_stat_activity blockinga ON blockingl.pid = blockinga.pid AND blockinga.datid = blockeda.datid WHERE NOT blockedl.granted AND blockinga.datname = current_database() hmmmm how about this select blockingl.relation, blockingl.pid, blockingl.mode, blockingl.granted, pgclass.relname, stat.usename, stat.application_name, stat.wait_event_type, stat.wait_event, stat.state, stat.query from pg_catalog.pg_locks blockingl join pg_class pgclass on blockingl.relation = pgclass.oid join pg_stat_activity stat on stat.pid = blockingl.pid check role membership select rr.*, pam.* from pg_catalog.pg_roles rr join pg_catalog.pg_auth_members pam on rr.\u0026#34;oid\u0026#34; = pam.roleid in line table using a CTE \u0026hellip; with datar(col1,col2) as ( values (1,2), (2,3), (4,5) ) select col1, col2 from datar COALESCE uses early stopping I had a pretty expensive COALESCE(col1, col2, ..., col50) with 50 arguments recently. And in testing whether my non-null value was first or last made a big difference! round does not work on double precision Wow interesting. I was comparing features generated in two tables to do some QA. Table 1 was the original or \u0026ldquo;target gold\u0026rdquo; table and table 2 was a table generated by production code. Using a simple table1.col1 = table2.col1 comparison, a particular float column was coming back as 0.092 , for about 100k rows. Hand inspecting, this looked like a difference of precision, but when I applied round(table1.col1, 3) = round(table2.col1, 3) instead, I got UndefinedFunction: function round(double precision, integer) does not exist And surely enough after reading the docs , oddly enough round with precision is only defined for numeric and not float8 (aka double precision). After casting to numeric my result of round(table1.col1::numeric, 3) = round(table2.col1::numeric, 3) was 0.990. Can dig deeper about the implementation later!\nTo infinity and beyond select 999999\u0026gt; \u0026#39;infinity\u0026#39;::float unnesting , the opposite of crosstab (aka pivoting) CREATE TABLE foo (id int, a text, b text, c text); INSERT INTO foo VALUES (1, \u0026#39;ant\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;chimp\u0026#39;), (2, \u0026#39;grape\u0026#39;, \u0026#39;mint\u0026#39;, \u0026#39;basil\u0026#39;), (3, \u0026#39;blur\u0026#39;, \u0026#39;cart\u0026#39;, \u0026#39;jump\u0026#39;); select * from foo id a b c 1 ant cat chimp 2 grape mint basil 3 blur cart jump SELECT id, unnest(array[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]) AS colname, unnest(array[a, b, c]) AS thing FROM foo ORDER BY id; id colname thing 1 a ant 1 b cat 1 c chimp 2 a grape 2 b mint 2 c basil 3 a blur 3 b cart 3 c jump copy paste a table with create table as Copy-pastaing a table with CREATE TABLE AS is sort of obvious, but laying it out doesn\u0026rsquo;t hurt CREATE TABLE blah_backup AS SELECT * FROM source_table cool stack overflow response here too Create user CREATE USER blah_user WITH PASSWORD \u0026#39;swear_words\u0026#39;; GRANT CONNECT ON DATABASE mycooldb TO blah_user; GRANT USAGE ON SCHEMA myschema TO blah_user; GRANT SELECT ON myschema.quick_table TO blah_user; Change password ALTER USER user_name WITH PASSWORD \u0026#39;new_password\u0026#39;; check grants.. ( user w/ appropriate permissions ) SELECT table_catalog, table_schema, table_name, privilege_type, grantee FROM information_schema.table_privileges list users select * from pg_user set lock_timeout to avoid waiting endlessly on table alter # example from article , which bit me personally.. SET lock_timeout TO \u0026#39;2s\u0026#39;; ALTER TABLE items ADD COLUMN last_update timestamptz; source: here arbitrarily select unrelated data for presentation purposes Not sure if this is the best way to do this, but I wanted to be able select three different max values in one shot with arf as (select 1 as aa, max(id) as max_arf from mytable where key = \u0026#39;arf\u0026#39;), bar as (select 1 as aa, max(id) as max_bar from mytable where key = \u0026#39;bar\u0026#39;), car as (select 1 as aa, max(id) as max_car from mytable where key = \u0026#39;car\u0026#39;) select arf.*, bar.*, car.* from arf join bar on arf.aa = bar.aa join car on arf.aa = car.aa aa max_arf aa max_bar aa max_car 1 4,585,834 1 4,046,591 1 4,585,835 Random sample from a table Read this cool solution here SELECT * FROM myTable WHERE attribute = \u0026#39;myValue\u0026#39; ORDER BY random() LIMIT 1000; in place jsonb extraction / unnesting If we have a table with a jsonb type column , called value , and we want to \u0026ldquo;un-nest\u0026rdquo; or normalize some data that happens to be one level below, in the jsonb\u0026hellip; ids_sql = \u0026#39;(1, 2, 3)\u0026#39; blah_key = \u0026#39;green_type\u0026#39; # some additional constraint other than the id target_cols = [\u0026#39;norm_col_1\u0026#39;, \u0026#39;norm_col_2\u0026#39;, \u0026#39;norm_col_3\u0026#39;] # Map between foovec = [[\u0026#39;norm_col_1\u0026#39;, \u0026#39;nested_col_1\u0026#39;], [\u0026#39;norm_col_2\u0026#39;, \u0026#39;nested_col_2\u0026#39;], [\u0026#39;norm_col_3\u0026#39;, \u0026#39;nested_col_3\u0026#39;]] select_cols = \u0026#39;, \u0026#39;.join([f\u0026#39;\u0026#39;\u0026#39;value-\u0026gt;\u0026gt;\u0026#39;{x[1]}\u0026#39; as \u0026#34;{x[0]}\u0026#34;\u0026#39;\u0026#39;\u0026#39; for x in foovec]) col_str = \u0026#39;, \u0026#39;.join([f\u0026#39;\u0026#34;{x}\u0026#34;\u0026#39; for x in target_cols]) targetcol_str = \u0026#39;, \u0026#39;.join([f\u0026#39;blah.\u0026#34;{x}\u0026#34;\u0026#39; for x in target_cols]) UPDATE {schema}.mytable as dp SET ({col_str}) = ({targetcol_str}) FROM (select id, key, {select_cols} -- json-\u0026gt;expressions-\u0026gt;to-\u0026gt;unpack-\u0026gt;data! from {schema}.mytable where {ids_sql} and key = \u0026#39;{blah_key}\u0026#39; ) as blah(id, key, {col_str}) WHERE (blah.id = dp.id AND blah.key = dp.key) order of joins matters looks like I had two gigantic tables (10million+ each) , t1, t2, this first query was taking forever\u0026hellip; with ids(foo) as ( select * from generate_series(1, 5000) as foo ) select * from t1 join t2 on t1.id = t2.id where t2.id in (select foo from ids) but this one was quick , I think because the join was done after constraining the ids not before with ids(foo) as ( select * from generate_series(1, 5000) as foo ) select * from t1 join t2 on t1.id = t2.id where t1.id in (select foo from ids) hashtag binning width_bucket very nice func for binning some data and then running a sum() aggregation afterwards for instance, the array passed to width_bucket is an array of the lower bounds with deltas(delta, countt) as ( values (-1, 3), (0, 4), (19, 9), (50, 2), (2, 8), (189, 3), (2000, 98), (2001, 3), (null::int, 2), (null, 9) ), binned as ( select width_bucket (deltas.delta::float, array[ 0, 50, 100, 150,200, 2000]::float[] ) as bin, deltas.delta, deltas.countt from deltas ) select delta, countt, bin -- sum(countt) from binned delta countt bin -1 3 0 0 4 1 19 9 1 50 2 2 2 8 1 189 3 4 2,000 98 6 2,001 3 6 [NULL] 2 [NULL] [NULL] 9 [NULL] with deltas(delta, countt) as ( values (10, 3), (11, 4), (19, 9), (50, 2), (2, 8), (189, 3), (77, 98), (178, 3)), binned as ( select width_bucket (deltas.delta::float, array[ 0::float, 50::float, 100::float, 150::float,200::float, 2000::float] ) as bin, deltas.delta, deltas.countt from deltas ) select bin, sum(countt) from binned group by bin order by bin bin sum 1 24 2 100 4 6 auto make that markdown table header line def make_table_header_line(header): dashes = [\u0026#39;--\u0026#39; for x in header.split(\u0026#39;|\u0026#39;)] return \u0026#39;|\u0026#39;.join(dashes) list constraints from stack overflow \u0026hellip; SELECT con.* FROM pg_catalog.pg_constraint con INNER JOIN pg_catalog.pg_class rel ON rel.oid = con.conrelid INNER JOIN pg_catalog.pg_namespace nsp ON nsp.oid = connamespace WHERE nsp.nspname = \u0026#39;\u0026lt;schema name\u0026gt;\u0026#39; AND rel.relname = \u0026#39;\u0026lt;table name\u0026gt;\u0026#39;; Oh wow\u0026hellip; also this section from the ALTER TABLE ADD CONSTRAINT postgresql doc was super useful/well written.. Scanning a large table to verify a new foreign key or check constraint can take a long time, and other updates to the table are locked out until the ALTER TABLE ADD CONSTRAINT command is committed. The main purpose of the NOT VALID constraint option is to reduce the impact of adding a constraint on concurrent updates. With NOT VALID, the ADD CONSTRAINT command does not scan the table and can be committed immediately. After that, a VALIDATE CONSTRAINT command can be issued to verify that existing rows satisfy the constraint. The validation step does not need to lock out concurrent updates, since it knows that other transactions will be enforcing the constraint for rows that they insert or update; only pre-existing rows need to be checked. Hence, validation acquires only a SHARE UPDATE EXCLUSIVE lock on the table being altered. (If the constraint is a foreign key then a ROW SHARE lock is also required on the table referenced by the constraint.) In addition to improving concurrency, it can be useful to use NOT VALID and VALIDATE CONSTRAINT in cases where the table is known to contain pre-existing violations. Once the constraint is in place, no new violations can be inserted, and the existing problems can be corrected at leisure until VALIDATE CONSTRAINT finally succeeds.\n-- step one. alter table the_table_name ADD constraint the_constraint_name unique (col1, col2) NOT VALID -- step two.. alter table the_table_name VALIDATE CONSTRAINT the_constraint_name Having Interesting conditional syntax aroung group by , without using a CTE\u0026hellip; select id, key , count(*) countt from my_table group by id, key having count(*) \u0026gt; 1 =\u0026gt; id key countt 1 foo 2 2 bar 3 invalid input syntax for integer Having come across this a few times, writing down here.. InvalidTextRepresentation: invalid input syntax for integer: \u0026#34;\u0026#34; This typically means a blank string is being coaxed as an integer somewhere. The solution is basically to find these and make sure they\u0026rsquo;re nulls on the db. And can also partition with an id column too for example, to do a little at a time to experiment. update mytable set blahcol = null where blahcol = \u0026#39;\u0026#39; and ( id between 1 and 100000) A warning about CTEs Per this article , postgresql will run full CTE and store it, which is why I feel the only practical way of using CTEs in large expressions is along with partitioning . In other words, although most postgresql queries will be able to run only the first 200 rows like a \u0026ldquo;generator\u0026rdquo; and return that to you really quickly, when you have a CTE in there, it has to run the whole thing first. Triggers Per here SELECT trigger_schema,event_object_table,trigger_schema,trigger_name,event_manipulation,action_statement,action_timing FROM information_schema.triggers ORDER BY event_object_table,event_manipulation Using window function to dedupe some data Given some rows like user_id laptop purchase_date 1 sony 1 1 nokia 2 1 bell 3 2 3m 2 2 nokia 8 If we want to dedupe by this simplified integer purchase_date with laptops(user_id, laptop, purchase_date) as ( values (1, \u0026#39;sony\u0026#39;, 1), (1, \u0026#39;nokia\u0026#39;, 2), (1, \u0026#39;bell\u0026#39;, 3), (2, \u0026#39;3m\u0026#39;, 2), (2, \u0026#39;nokia\u0026#39;, 8) ) select l.*, row_number() over w as rnum from laptops as l window w as (partition by l.user_id order by purchase_date asc ) user_id laptop purchase_date rnum 1 sony 1 1 1 nokia 2 2 1 bell 3 3 2 3m 2 1 2 nokia 8 2 And then keep only the rnum = 1 \u0026hellip;\nwith laptops(user_id, laptop, purchase_date) as ( values (1, \u0026#39;sony\u0026#39;, 1), (1, \u0026#39;nokia\u0026#39;, 2), (1, \u0026#39;bell\u0026#39;, 3), (2, \u0026#39;3m\u0026#39;, 2), (2, \u0026#39;nokia\u0026#39;, 8) ), select aa.* from ( select l.*, row_number() over w as rnum from laptops as l window w as (partition by l.user_id order by purchase_date asc ) ) as aa where aa.rnum = 1 user_id laptop purchase_date rnum 1 sony 1 1 2 3m 2 1 length of an array is cardinality select cardinality(ARRAY[[1,2],[3,4]]) 4 logistic CREATE or replace FUNCTION myschema.logistic(val float) RETURNS float AS $$ BEGIN RETURN (1/(1 + ( exp(1)^(-(val))))) ; END; $$ LANGUAGE PLPGSQL; left join two tables with an extra condition and keep the null join rows This was bothering me for a bit but I think I have found the solution multiple times now. Writing this out for later In this example, there are authors and authors can have one or multiple articles. Self Join articles , to get multiple articles, but only where the left id is less than the right id. This is a contrived example, but my use case typically is not a self join but a join with different tables, but the idea still holds. with articles(author, article_id, article_title) as (values (\u0026#39;joe\u0026#39;, 1, \u0026#39;birds\u0026#39;), (\u0026#39;joe\u0026#39;, 2, \u0026#39;more birds\u0026#39;), (\u0026#39;sally\u0026#39;, 3, \u0026#39;eagles\u0026#39;), (\u0026#39;sally\u0026#39;, 4, \u0026#39;seagulls\u0026#39;), (\u0026#39;jan\u0026#39;, 5, \u0026#39;the philosophical of flying\u0026#39;)) select a.author, a.article_id as left_id, a.article_title as left_title, b.article_id as right_id, b.article_title as right_title from articles as a left join articles as b on (a.author = b.author and a.article_id \u0026lt; b.article_id) author left_id left_title right_id right_title joe 1 birds 2 more birds joe 2 more birds [NULL] [NULL] sally 3 eagles 4 seagulls sally 4 seagulls [NULL] [NULL] jan 5 the philosophical of flying [NULL] [NULL] ","permalink":"https://michal.piekarczyk.xyz/handy/sql/hmm/","summary":"List indexes From here SELECT tablename, indexname, indexdef FROM pg_indexes WHERE schemaname = \u0026#39;public\u0026#39; ORDER BY tablename, indexname; Disk Usage per table from the postgresql wiki except one minor change \u0026hellip; for ('user_blah', 'user_blar', 'schema1', 'schema2') schemas only \u0026hellip; SELECT *, pg_size_pretty(total_bytes) AS total , pg_size_pretty(index_bytes) AS INDEX , pg_size_pretty(toast_bytes) AS toast , pg_size_pretty(table_bytes) AS TABLE FROM ( SELECT *, total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes FROM ( SELECT c.oid,nspname AS table_schema, relname AS TABLE_NAME , c.","title":""},{"content":"Interesting\nThe operator ~~ is equivalent to LIKE, and ~~* corresponds to ILIKE. There are also !~~ and !~~* operators that represent NOT LIKE and NOT ILIKE. All of these operators are PostgreSQL-specific.\nper 7.3 doc. not sure if outdated\n","permalink":"https://michal.piekarczyk.xyz/handy/sql/ilike/","summary":"Interesting\nThe operator ~~ is equivalent to LIKE, and ~~* corresponds to ILIKE. There are also !~~ and !~~* operators that represent NOT LIKE and NOT ILIKE. All of these operators are PostgreSQL-specific.\nper 7.3 doc. not sure if outdated","title":""},{"content":"Good to know.\nsql = \u0026#39;\u0026#39;\u0026#39; select id, true is true true_is_true, true = true true_eq_true, false is false false_is_false, false = false false_eq_false, true is false true_is_false, true = false true_eq_false, false is true false_is_true, false = true false_eq_true, null = true null_eq_true, null is true null_is_true, null = false null_eq_false, null is false null_is_false, true is null true_is_null, true = null true_eq_null, false is null false_is_null, false = null false_eq_null, null is null null_is_null, null = null null_eq_null from blahblah limit 1 \u0026#39;\u0026#39;\u0026#39; db.read(sql, stage=\u0026#39;foo\u0026#39;) Out[125]: [{\u0026#39;id\u0026#39;: 4270416, \u0026#39;true_is_true\u0026#39;: True, \u0026#39;true_eq_true\u0026#39;: True, \u0026#39;false_is_false\u0026#39;: True, \u0026#39;false_eq_false\u0026#39;: True, \u0026#39;true_is_false\u0026#39;: False, \u0026#39;true_eq_false\u0026#39;: False, \u0026#39;false_is_true\u0026#39;: False, \u0026#39;false_eq_true\u0026#39;: False, \u0026#39;null_eq_true\u0026#39;: None, \u0026#39;null_is_true\u0026#39;: False, \u0026#39;null_eq_false\u0026#39;: None, \u0026#39;null_is_false\u0026#39;: False, \u0026#39;true_is_null\u0026#39;: False, \u0026#39;true_eq_null\u0026#39;: None, \u0026#39;false_is_null\u0026#39;: False, \u0026#39;false_eq_null\u0026#39;: None, \u0026#39;null_is_null\u0026#39;: True, \u0026#39;null_eq_null\u0026#39;: None}] ","permalink":"https://michal.piekarczyk.xyz/handy/sql/is_equal/","summary":"Good to know.\nsql = \u0026#39;\u0026#39;\u0026#39; select id, true is true true_is_true, true = true true_eq_true, false is false false_is_false, false = false false_eq_false, true is false true_is_false, true = false true_eq_false, false is true false_is_true, false = true false_eq_true, null = true null_eq_true, null is true null_is_true, null = false null_eq_false, null is false null_is_false, true is null true_is_null, true = null true_eq_null, false is null false_is_null, false = null false_eq_null, null is null null_is_null, null = null null_eq_null from blahblah limit 1 \u0026#39;\u0026#39;\u0026#39; db.","title":""},{"content":"subtract intervals from dates although in postgresql you can freely add/subtract dates/timestamps and intervals,\nSELECT \u0026#39;2001-01-01\u0026#39;::timestamp + \u0026#39;1 year\u0026#39;::interval; in mysql land you need to do use date_sub and date_add\ndate_sub(\u0026#39;2019-06-30\u0026#39; , interval 90 days) Subtract dates Also in postgresql you can just subtract dates,\n\u0026#39;2021-01-01\u0026#39;::date - \u0026#39;2021-05-01\u0026#39;::date And in mysql to do this you can do\nDATEDIFF(\u0026#39;2021-01-01\u0026#39;, \u0026#39;2021-05-01\u0026#39;) ","permalink":"https://michal.piekarczyk.xyz/handy/sql/time/","summary":"subtract intervals from dates although in postgresql you can freely add/subtract dates/timestamps and intervals,\nSELECT \u0026#39;2001-01-01\u0026#39;::timestamp + \u0026#39;1 year\u0026#39;::interval; in mysql land you need to do use date_sub and date_add\ndate_sub(\u0026#39;2019-06-30\u0026#39; , interval 90 days) Subtract dates Also in postgresql you can just subtract dates,\n\u0026#39;2021-01-01\u0026#39;::date - \u0026#39;2021-05-01\u0026#39;::date And in mysql to do this you can do\nDATEDIFF(\u0026#39;2021-01-01\u0026#39;, \u0026#39;2021-05-01\u0026#39;) ","title":""},{"content":"Create a new user username = \u0026#39;new_user_foo\u0026#39; passw = input() sql = \u0026#34;CREATE USER {} WITH PASSWORD \u0026#39;{}\u0026#39; \u0026#34;.format(username, passw) Make some quick grants tables = [\u0026#39;table1\u0026#39;, \u0026#39;table_foo\u0026#39;, ] username = \u0026#39;xx\u0026#39; grant_queries = [q.format(username) for q in [\u0026#34;GRANT CONNECT ON DATABASE mydb TO {}\u0026#34;, \u0026#34;GRANT USAGE ON SCHEMA public TO {}\u0026#34;,] + [\u0026#34;GRANT SELECT ON {} TO \u0026#34;.format(t) + \u0026#34; {} \u0026#34; for t in tables]] check exissting users select * from pg_user update user password ; change ALTER USER user_name WITH PASSWORD \u0026#39;new_password\u0026#39;; can use input() here too actually Check Existing Grants / permissions The user running this query might not be able to see all the rows SELECT table_catalog, table_schema, table_name, privilege_type, grantee FROM information_schema.table_privileges where grantee = \u0026#39;foo_user\u0026#39; Check what roles blah_user is a part of WITH RECURSIVE cte AS ( SELECT oid FROM pg_catalog.pg_roles WHERE rolname = \u0026#39;blah_user\u0026#39; UNION ALL SELECT m.roleid FROM cte JOIN pg_catalog.pg_auth_members m ON m.member = cte.oid ) SELECT oid, oid::regrole::text AS rolename FROM cte; -- oid \u0026amp; name Find owner of table -- select tablename, tableowner from pg_catalog.pg_tables where schemaname = \u0026#39;fooschema\u0026#39; and tablename = \u0026#39;footable\u0026#39; tablename tableowner footable foouser ","permalink":"https://michal.piekarczyk.xyz/handy/sql/user_management/","summary":"Create a new user username = \u0026#39;new_user_foo\u0026#39; passw = input() sql = \u0026#34;CREATE USER {} WITH PASSWORD \u0026#39;{}\u0026#39; \u0026#34;.format(username, passw) Make some quick grants tables = [\u0026#39;table1\u0026#39;, \u0026#39;table_foo\u0026#39;, ] username = \u0026#39;xx\u0026#39; grant_queries = [q.format(username) for q in [\u0026#34;GRANT CONNECT ON DATABASE mydb TO {}\u0026#34;, \u0026#34;GRANT USAGE ON SCHEMA public TO {}\u0026#34;,] + [\u0026#34;GRANT SELECT ON {} TO \u0026#34;.format(t) + \u0026#34; {} \u0026#34; for t in tables]] check exissting users select * from pg_user update user password ; change ALTER USER user_name WITH PASSWORD \u0026#39;new_password\u0026#39;; can use input() here too actually Check Existing Grants / permissions The user running this query might not be able to see all the rows SELECT table_catalog, table_schema, table_name, privilege_type, grantee FROM information_schema.","title":""},{"content":"lpad is the func to make sure a month is always two digits as an example. select concat( extract (year from foo.timestamp)::text, lpad (extract (month from foo.timestamp)::text, 2, \u0026#39;0\u0026#39;) ) as yearmonth, count(1) from foo where group by yearmonth order by yearmonth asc yearmonth count 202005 5208 202006 8584 202007 7780 202008 5382 202009 3635 202010 2791 202011 1284 202012 2704 202101 2416 202102 1964 202103 2554 202104 2935 202105 2909 202106 160 ","permalink":"https://michal.piekarczyk.xyz/handy/sql/zero_fill_dates/","summary":"lpad is the func to make sure a month is always two digits as an example. select concat( extract (year from foo.timestamp)::text, lpad (extract (month from foo.timestamp)::text, 2, \u0026#39;0\u0026#39;) ) as yearmonth, count(1) from foo where group by yearmonth order by yearmonth asc yearmonth count 202005 5208 202006 8584 202007 7780 202008 5382 202009 3635 202010 2791 202011 1284 202012 2704 202101 2416 202102 1964 202103 2554 202104 2935 202105 2909 202106 160 ","title":""},{"content":"Checking sha256 For older versions of sshd awk \u0026#39;{print $2}\u0026#39; /etc/sshd/ssh_host_rsa_key.pub | base64 -d | sha256sum -b | awk \u0026#39;{print $1}\u0026#39; | xxd -r -p | base64 Newer sshd ssh-keygen -l -f key.pub -E (sha256|md5) ","permalink":"https://michal.piekarczyk.xyz/handy/ssh/","summary":"Checking sha256 For older versions of sshd awk \u0026#39;{print $2}\u0026#39; /etc/sshd/ssh_host_rsa_key.pub | base64 -d | sha256sum -b | awk \u0026#39;{print $1}\u0026#39; | xxd -r -p | base64 Newer sshd ssh-keygen -l -f key.pub -E (sha256|md5) ","title":""},{"content":"ag the silver searcher here\nusually the_silver_searcher on homebrew\npreserve color when paging! (This is amazing!)\nag \u0026#34;search term\u0026#34; --pager \u0026#34;less -R\u0026#34; Faster and more colorful find! Wow fd , brew install fd , from https://github.com/sharkdp/fd\nPreserve color when paging like this\nfd \u0026#34;foo\u0026#34; --color always |less -R jq command line json parsing here markdown to pdf With pandoc (I used brew install pandoc ). And thanks stackoverflow , pandoc MANUAL.md -o example13.pdf documentation from markdown to latex pandoc -f markdown -t latex hello.txt Even --toc, --table-of-contents , --toc-depth=NUMBER , for automatic table of contents.\nSpecifically for double spaced output pdf this stackoverflow tip was amazing, to create a file like options.sty\n\\usepackage{setspace} \\doublespacing \\usepackage[vmargin=1in,hmargin=1in]{geometry} \\usepackage{lineno} \\linenumbers) And then use pandoc -H options.sty blah.md -o blah.pdf . Amazing!\npdf for kindle !! Equally as amazing is the k2pdf software which restructures pdf files to comfortable kindle viewing. This is absolutely mind blowing work! ","permalink":"https://michal.piekarczyk.xyz/handy/tools/","summary":"ag the silver searcher here\nusually the_silver_searcher on homebrew\npreserve color when paging! (This is amazing!)\nag \u0026#34;search term\u0026#34; --pager \u0026#34;less -R\u0026#34; Faster and more colorful find! Wow fd , brew install fd , from https://github.com/sharkdp/fd\nPreserve color when paging like this\nfd \u0026#34;foo\u0026#34; --color always |less -R jq command line json parsing here markdown to pdf With pandoc (I used brew install pandoc ). And thanks stackoverflow , pandoc MANUAL.","title":""},{"content":"packet capture filters Havent been able to get this one to work yet but this is as claimed here, https://semfionetworks.com/wp-content/uploads/2021/04/wireshark_802.11_filters_-_reference_sheet.pdf wlan_mgt.ssid == â€œyour_SSIDâ€ and this one hmm did not work for me wlan.addr == xx:xx:xx:xx:xx:xx as opposed this, eth.addr == xx:xx:xx:xx:xx:xx which did work for me.\nMaybe I can\u0026rsquo;t see the low level 802.11 control packets/frames somehow. ","permalink":"https://michal.piekarczyk.xyz/handy/wireshark/","summary":"packet capture filters Havent been able to get this one to work yet but this is as claimed here, https://semfionetworks.com/wp-content/uploads/2021/04/wireshark_802.11_filters_-_reference_sheet.pdf wlan_mgt.ssid == â€œyour_SSIDâ€ and this one hmm did not work for me wlan.addr == xx:xx:xx:xx:xx:xx as opposed this, eth.addr == xx:xx:xx:xx:xx:xx which did work for me.\nMaybe I can\u0026rsquo;t see the low level 802.11 control packets/frames somehow. ","title":""},{"content":"First i needed to get ruby from homebrew Using gem install bundler jekyll didn\u0026rsquo;t work for me because I was getting a permission error,\nERROR: While executing gem ... (Gem::FilePermissionError) You don't have write permissions for the /Library/Ruby/Gems/2.3.0 directory. I did brew install ruby and brew install rbenv ruby-build\nAlso saw this message\n(venv3) $ brew install ruby ==\u0026gt; Installing dependencies for ruby: libyaml, openssl@1.1 and readline ==\u0026gt; Installing ruby dependency: libyaml ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/libyaml-0.2.2.mojave.bottle.tar.gz ######################################################################## 100.0% ==\u0026gt; Pouring libyaml-0.2.2.mojave.bottle.tar.gz ðŸº /usr/local/cellar/libyaml/0.2.2: 9 files, 291.6kb ==\u0026gt; installing ruby dependency: openssl@1.1 ==\u0026gt; downloading https://homebrew.bintray.com/bottles/openssl@1.1-1.1.1d.mojave.bottle.tar.gz ==\u0026gt; downloading from https://akamai.bintray.com/10/104ef018b7bb8fcc49f57e5a60359a28a02d480d85a959e6141394b0571cbb28?__gda__=exp=1571 ######################################################################## 100.0% ==\u0026gt; pouring openssl@1.1-1.1.1d.mojave.bottle.tar.gz ==\u0026gt; caveats a ca file has been bootstrapped using certificates from the system keychain. to add additional certificates, place .pem files in /usr/local/etc/openssl@1.1/certs and run /usr/local/opt/openssl@1.1/bin/c_rehash openssl@1.1 is keg-only, which means it was not symlinked into /usr/local, because openssl/libressl is provided by macos so don't link an incompatible version. if you need to have openssl@1.1 first in your path run: echo 'export path=\u0026quot;/usr/local/opt/openssl@1.1/bin:$path\u0026quot;' \u0026gt;\u0026gt; ~/.bash_profile for compilers to find openssl@1.1 you may need to set: export ldflags=\u0026quot;-l/usr/local/opt/openssl@1.1/lib\u0026quot; export cppflags=\u0026quot;-i/usr/local/opt/openssl@1.1/include\u0026quot; for pkg-config to find openssl@1.1 you may need to set: export pkg_config_path=\u0026quot;/usr/local/opt/openssl@1.1/lib/pkgconfig\u0026quot; ==\u0026gt; summary ðŸº /usr/local/cellar/openssl@1.1/1.1.1d: 7,983 files, 17.9mb ==\u0026gt; installing ruby dependency: readline ==\u0026gt; downloading https://homebrew.bintray.com/bottles/readline-8.0.1.mojave.bottle.tar.gz ==\u0026gt; downloading from https://akamai.bintray.com/3c/3c754391e9d243835811d128771ca0f1a565024100fd2c2871534353d46aaf0e?__gda__=exp=1571 ######################################################################## 100.0% ==\u0026gt; pouring readline-8.0.1.mojave.bottle.tar.gz ==\u0026gt; caveats readline is keg-only, which means it was not symlinked into /usr/local, because macos provides the bsd libedit library, which shadows libreadline. in order to prevent conflicts when programs look for libreadline we are defaulting this gnu readline installation to keg-only. for compilers to find readline you may need to set: export ldflags=\u0026quot;-l/usr/local/opt/readline/lib\u0026quot; export cppflags=\u0026quot;-i/usr/local/opt/readline/include\u0026quot; for pkg-config to find readline you may need to set: export pkg_config_path=\u0026quot;/usr/local/opt/readline/lib/pkgconfig\u0026quot; ==\u0026gt; summary ðŸº /usr/local/cellar/readline/8.0.1: 48 files, 1.5mb ==\u0026gt; installing ruby ==\u0026gt; downloading https://homebrew.bintray.com/bottles/ruby-2.6.5.mojave.bottle.tar.gz ==\u0026gt; downloading from https://akamai.bintray.com/e3/e30782df0575e8df1bc3c23da701644919099cc31f6b05b163cde847d116414c?__gda__=exp=1571 ######################################################################## 100.0% ==\u0026gt; pouring ruby-2.6.5.mojave.bottle.tar.gz ==\u0026gt; caveats by default, binaries installed by gem will be placed into: /usr/local/lib/ruby/gems/2.6.0/bin you may want to add this to your path. ruby is keg-only, which means it was not symlinked into /usr/local, because macos already provides this software and installing another version in parallel can cause all kinds of trouble. if you need to have ruby first in your path run: echo 'export path=\u0026quot;/usr/local/opt/ruby/bin:$path\u0026quot;' \u0026gt;\u0026gt; ~/.bash_profile for compilers to find ruby you may need to set: export ldflags=\u0026quot;-l/usr/local/opt/ruby/lib\u0026quot; export cppflags=\u0026quot;-i/usr/local/opt/ruby/include\u0026quot; for pkg-config to find ruby you may need to set: export pkg_config_path=\u0026quot;/usr/local/opt/ruby/lib/pkgconfig\u0026quot; ==\u0026gt; summary ðŸº /usr/local/cellar/ruby/2.6.5: 19,390 files, 31.1mb And rbenv\n(venv3) $ brew install rbenv ruby-build ==\u0026gt; Installing dependencies for rbenv: ruby-build ==\u0026gt; Installing rbenv dependency: ruby-build ==\u0026gt; Downloading https://github.com/rbenv/ruby-build/archive/v20191004.tar.gz ==\u0026gt; Downloading from https://codeload.github.com/rbenv/ruby-build/tar.gz/v20191004 ######################################################################## 100.0% ==\u0026gt; ./install.sh ðŸº /usr/local/Cellar/ruby-build/20191004: 459 files, 229.8KB, built in 7 seconds ==\u0026gt; Installing rbenv ==\u0026gt; Downloading https://homebrew.bintray.com/bottles/rbenv-1.1.2.mojave.bottle.tar.gz ######################################################################## 100.0% ==\u0026gt; Pouring rbenv-1.1.2.mojave.bottle.tar.gz ðŸº /usr/local/Cellar/rbenv/1.1.2: 36 files, 65KB (venv3) $ which ruby /usr/bin/ruby (venv3) $ . /Users/blahmichal/.bash_profile I did some of those things I put the new brew /usr/local into my .bash_profile updating my PATH ..\nAnd I retried the jekyll instllation\u0026hellip;\n$ which ruby /usr/local/opt/ruby/bin/ruby $ $ which gem /usr/local/opt/ruby/bin/gem $ $ gem install bundler jekyll Fetching bundler-2.0.2.gem bundler's executable \u0026quot;bundle\u0026quot; conflicts with /usr/local/lib/ruby/gems/2.6.0/bin/bundle Overwrite the executable? [yN] y bundler's executable \u0026quot;bundler\u0026quot; conflicts with /usr/local/lib/ruby/gems/2.6.0/bin/bundler Overwrite the executable? [yN] y Successfully installed bundler-2.0.2 Parsing documentation for bundler-2.0.2 Installing ri documentation for bundler-2.0.2 Done installing documentation for bundler after 4 seconds Fetching http_parser.rb-0.6.0.gem Fetching em-websocket-0.5.1.gem Fetching public_suffix-4.0.1.gem Fetching addressable-2.7.0.gem Fetching colorator-1.1.0.gem Fetching eventmachine-1.2.7.gem Fetching concurrent-ruby-1.1.5.gem Fetching i18n-1.7.0.gem Fetching ffi-1.11.1.gem Fetching sassc-2.2.1.gem Fetching jekyll-sass-converter-2.0.1.gem Fetching rb-fsevent-0.10.3.gem Fetching rb-inotify-0.10.0.gem Fetching listen-3.2.0.gem Fetching jekyll-watch-2.2.1.gem Fetching kramdown-2.1.0.gem Fetching kramdown-parser-gfm-1.1.0.gem Fetching liquid-4.0.3.gem Fetching mercenary-0.3.6.gem Fetching forwardable-extended-2.6.0.gem Fetching pathutil-0.16.2.gem Fetching rouge-3.12.0.gem Fetching safe_yaml-1.0.5.gem Fetching unicode-display_width-1.6.0.gem Fetching jekyll-4.0.0.gem Fetching terminal-table-1.8.0.gem Successfully installed public_suffix-4.0.1 Successfully installed addressable-2.7.0 Successfully installed colorator-1.1.0 Building native extensions. This could take a while... Successfully installed http_parser.rb-0.6.0 Building native extensions. This could take a while... Successfully installed eventmachine-1.2.7 Successfully installed em-websocket-0.5.1 Successfully installed concurrent-ruby-1.1.5 HEADS UP! i18n 1.1 changed fallbacks to exclude default locale. But that may break your application. Please check your Rails app for 'config.i18n.fallbacks = true'. If you're using I18n (\u0026gt;= 1.1.0) and Rails (\u0026lt; 5.2.2), this should be 'config.i18n.fallbacks = [I18n.default_locale]'. If not, fallbacks will be broken in your app by I18n 1.1.x. For more info see: https://github.com/svenfuchs/i18n/releases/tag/v1.1.0 Successfully installed i18n-1.7.0 Building native extensions. This could take a while... Successfully installed ffi-1.11.1 Building native extensions. This could take a while... Successfully installed sassc-2.2.1 Successfully installed jekyll-sass-converter-2.0.1 Successfully installed rb-fsevent-0.10.3 Successfully installed rb-inotify-0.10.0 Successfully installed listen-3.2.0 Successfully installed jekyll-watch-2.2.1 Successfully installed kramdown-2.1.0 Successfully installed kramdown-parser-gfm-1.1.0 Successfully installed liquid-4.0.3 Successfully installed mercenary-0.3.6 Successfully installed forwardable-extended-2.6.0 Successfully installed pathutil-0.16.2 Successfully installed rouge-3.12.0 Successfully installed safe_yaml-1.0.5 Successfully installed unicode-display_width-1.6.0 Successfully installed terminal-table-1.8.0 ------------------------------------------------------------------------------------- Jekyll 4.0 comes with some major changes, notably: * Our `link` tag now comes with the `relative_url` filter incorporated into it. You should no longer prepend `{{ site.baseurl }}` to `{ % link foo.md % }` For further details: https://github.com/jekyll/jekyll/pull/6727 * Our `post_url` tag now comes with the `relative_url` filter incorporated into it. You shouldn't prepend `{{ site.baseurl }}` to `{ % post_url 2019-03-27-hello % }` For further details: https://github.com/jekyll/jekyll/pull/7589 * Support for deprecated configuration options has been removed. We will no longer output a warning and gracefully assign their values to the newer counterparts internally. ------------------------------------------------------------------------------------- Successfully installed jekyll-4.0.0 Parsing documentation for public_suffix-4.0.1 Installing ri documentation for public_suffix-4.0.1 Parsing documentation for addressable-2.7.0 Installing ri documentation for addressable-2.7.0 Parsing documentation for colorator-1.1.0 Installing ri documentation for colorator-1.1.0 Parsing documentation for http_parser.rb-0.6.0 unknown encoding name \u0026quot;chunked\\r\\n\\r\\n25\u0026quot; for ext/ruby_http_parser/vendor/http-parser-java/tools/parse_tests.rb, skipping Installing ri documentation for http_parser.rb-0.6.0 Parsing documentation for eventmachine-1.2.7 Installing ri documentation for eventmachine-1.2.7 Parsing documentation for em-websocket-0.5.1 Installing ri documentation for em-websocket-0.5.1 Parsing documentation for concurrent-ruby-1.1.5 Installing ri documentation for concurrent-ruby-1.1.5 Parsing documentation for i18n-1.7.0 Installing ri documentation for i18n-1.7.0 Parsing documentation for ffi-1.11.1 Installing ri documentation for ffi-1.11.1 Parsing documentation for sassc-2.2.1 Installing ri documentation for sassc-2.2.1 Parsing documentation for jekyll-sass-converter-2.0.1 Installing ri documentation for jekyll-sass-converter-2.0.1 Parsing documentation for rb-fsevent-0.10.3 Installing ri documentation for rb-fsevent-0.10.3 Parsing documentation for rb-inotify-0.10.0 Installing ri documentation for rb-inotify-0.10.0 Parsing documentation for listen-3.2.0 Installing ri documentation for listen-3.2.0 Parsing documentation for jekyll-watch-2.2.1 Installing ri documentation for jekyll-watch-2.2.1 Parsing documentation for kramdown-2.1.0 Installing ri documentation for kramdown-2.1.0 Parsing documentation for kramdown-parser-gfm-1.1.0 Installing ri documentation for kramdown-parser-gfm-1.1.0 Parsing documentation for liquid-4.0.3 Installing ri documentation for liquid-4.0.3 Parsing documentation for mercenary-0.3.6 Installing ri documentation for mercenary-0.3.6 Parsing documentation for forwardable-extended-2.6.0 Installing ri documentation for forwardable-extended-2.6.0 Parsing documentation for pathutil-0.16.2 Installing ri documentation for pathutil-0.16.2 Parsing documentation for rouge-3.12.0 Installing ri documentation for rouge-3.12.0 Parsing documentation for safe_yaml-1.0.5 Installing ri documentation for safe_yaml-1.0.5 Parsing documentation for unicode-display_width-1.6.0 Installing ri documentation for unicode-display_width-1.6.0 Parsing documentation for terminal-table-1.8.0 Installing ri documentation for terminal-table-1.8.0 Parsing documentation for jekyll-4.0.0 Installing ri documentation for jekyll-4.0.0 Done installing documentation for public_suffix, addressable, colorator, http_parser.rb, eventmachine, em-websocket, concurrent-ruby, i18n, ffi, sassc, jekyll-sass-converter, rb-fsevent, rb-inotify, listen, jekyll-watch, kramdown, kramdown-parser-gfm, liquid, mercenary, forwardable-extended, pathutil, rouge, safe_yaml, unicode-display_width, terminal-table, jekyll after 52 seconds 27 gems installed So I guess that looks good.\nRan the new command.. $ jekyll new namoopsoo-github Running bundle install in /blahdir/namoopsoo-github... Bundler: Fetching gem metadata from https://rubygems.org/........... Bundler: Fetching gem metadata from https://rubygems.org/. Bundler: Resolving dependencies... Bundler: Using public_suffix 4.0.1 Bundler: Using addressable 2.7.0 Bundler: Using bundler 1.17.2 Bundler: Using colorator 1.1.0 Bundler: Using concurrent-ruby 1.1.5 Bundler: Using eventmachine 1.2.7 Bundler: Using http_parser.rb 0.6.0 Bundler: Using em-websocket 0.5.1 Bundler: Using ffi 1.11.1 Bundler: Using forwardable-extended 2.6.0 Bundler: Using i18n 1.7.0 Bundler: Using sassc 2.2.1 Bundler: Using jekyll-sass-converter 2.0.1 Bundler: Using rb-fsevent 0.10.3 Bundler: Using rb-inotify 0.10.0 Bundler: Using listen 3.2.0 Bundler: Using jekyll-watch 2.2.1 Bundler: Using kramdown 2.1.0 Bundler: Using kramdown-parser-gfm 1.1.0 Bundler: Using liquid 4.0.3 Bundler: Using mercenary 0.3.6 Bundler: Using pathutil 0.16.2 Bundler: Using rouge 3.12.0 Bundler: Using safe_yaml 1.0.5 Bundler: Using unicode-display_width 1.6.0 Bundler: Using terminal-table 1.8.0 Bundler: Using jekyll 4.0.0 Bundler: Fetching jekyll-feed 0.12.1 Bundler: Installing jekyll-feed 0.12.1 Bundler: Fetching jekyll-seo-tag 2.6.1 Bundler: Installing jekyll-seo-tag 2.6.1 Bundler: Fetching minima 2.5.1 Bundler: Installing minima 2.5.1 Bundler: Bundle complete! 6 Gemfile dependencies, 30 gems now installed. Bundler: Use `bundle info [gemname]` to see where a bundled gem is installed. New jekyll site installed in /blahdir/namoopsoo-github. $ ls namoopsoo-github But trying to host failed\u0026hellip;\n$ bundle exec jekyll serve Traceback (most recent call last): 2: from /usr/local/opt/ruby/bin/bundle:23:in `\u0026lt;main\u0026gt;' 1: from /usr/local/Cellar/ruby/2.6.5/lib/ruby/2.6.0/rubygems.rb:303:in `activate_bin_path' /usr/local/Cellar/ruby/2.6.5/lib/ruby/2.6.0/rubygems.rb:284:in `find_spec_for_exe': Could not find 'bundler' (1.17.2) required by your /blahdir/namoopsoo-github/Gemfile.lock. (Gem::GemNotFoundException) To update to the latest version installed on your system, run `bundle update --bundler`. To install the missing version, run `gem install bundler:1.17.2` $ which bundle /usr/local/opt/ruby/bin/bundle $ Okay I did what it said..\n$ which bundle /usr/local/opt/ruby/bin/bundle $ bundle update --bundler Using public_suffix 4.0.1 Using addressable 2.7.0 Using bundler 2.0.2 Using colorator 1.1.0 Using concurrent-ruby 1.1.5 Using eventmachine 1.2.7 Using http_parser.rb 0.6.0 Using em-websocket 0.5.1 Using ffi 1.11.1 Using forwardable-extended 2.6.0 Using i18n 1.7.0 Using sassc 2.2.1 Using jekyll-sass-converter 2.0.1 Using rb-fsevent 0.10.3 Using rb-inotify 0.10.0 Using listen 3.2.0 Using jekyll-watch 2.2.1 Using kramdown 2.1.0 Using kramdown-parser-gfm 1.1.0 Using liquid 4.0.3 Using mercenary 0.3.6 Using pathutil 0.16.2 Using rouge 3.12.0 Using safe_yaml 1.0.5 Using unicode-display_width 1.6.0 Using terminal-table 1.8.0 Using jekyll 4.0.0 Using jekyll-feed 0.12.1 Using jekyll-seo-tag 2.6.1 Using minima 2.5.1 Bundle updated! $ gem install bundler:1.17.2 Fetching bundler-1.17.2.gem Successfully installed bundler-1.17.2 Parsing documentation for bundler-1.17.2 Installing ri documentation for bundler-1.17.2 Done installing documentation for bundler after 3 seconds 1 gem installed $ Try to serve again\u0026hellip; and nice this time worked\n$ bundle exec jekyll serve Configuration file: /blahdir/namoopsoo-github/_config.yml Source: /blahdir/namoopsoo-github Destination: /blahdir/namoopsoo-github/_site Incremental build: disabled. Enable with --incremental Generating... Jekyll Feed: Generating feed for posts done in 0.404 seconds. Auto-regeneration: enabled for '/blahdir/namoopsoo-github' Server address: http://127.0.0.1:4000/ Server running... press ctrl-c to stop. ","permalink":"https://michal.piekarczyk.xyz/post/2019-10-19--jekyll/","summary":"First i needed to get ruby from homebrew Using gem install bundler jekyll didn\u0026rsquo;t work for me because I was getting a permission error,\nERROR: While executing gem ... (Gem::FilePermissionError) You don't have write permissions for the /Library/Ruby/Gems/2.3.0 directory. I did brew install ruby and brew install rbenv ruby-build\nAlso saw this message\n(venv3) $ brew install ruby ==\u0026gt; Installing dependencies for ruby: libyaml, openssl@1.1 and readline ==\u0026gt; Installing ruby dependency: libyaml ==\u0026gt; Downloading https://homebrew.","title":""},{"content":"Build issues again haha I am doing jekyll build but this time from a Makefile , containing\nbuild: jekyll build with make build And I\u0026rsquo;m getting $ make build jekyll build Traceback (most recent call last): 10: from /usr/local/lib/ruby/gems/2.6.0/bin/jekyll:23:in `\u0026lt;main\u0026gt;\u0026#39; 9: from /usr/local/lib/ruby/gems/2.6.0/bin/jekyll:23:in `load\u0026#39; 8: from /usr/local/lib/ruby/gems/2.6.0/gems/jekyll-4.0.0/exe/jekyll:11:in `\u0026lt;top (required)\u0026gt;\u0026#39; 7: from /usr/local/lib/ruby/gems/2.6.0/gems/jekyll-4.0.0/lib/jekyll/plugin_manager.rb:52:in `require_from_bundler\u0026#39; 6: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler.rb:107:in `setup\u0026#39; 5: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/runtime.rb:26:in `setup\u0026#39; 4: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/runtime.rb:26:in `map\u0026#39; 3: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/spec_set.rb:148:in `each\u0026#39; 2: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/spec_set.rb:148:in `each\u0026#39; 1: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/runtime.rb:31:in `block in setup\u0026#39; /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/runtime.rb:319:in `check_for_activated_spec!\u0026#39;: You have already activated public_suffix 4.0.5, but your Gemfile requires public_suffix 4.0.1. Prepending `bundle exec` to your command may solve this. (Gem::LoadError) make: *** [build] Error 1 Ok cool. Whatever that means haha. So going full Memento here, looking at my earlier notes , I see last time I used gem update --system to solve the problem. So doing that again this time , and saw this output.. $ gem update --system Updating rubygems-update Fetching rubygems-update-3.1.4.gem Successfully installed rubygems-update-3.1.4 Parsing documentation for rubygems-update-3.1.4 Installing ri documentation for rubygems-update-3.1.4 Installing darkfish documentation for rubygems-update-3.1.4 Done installing documentation for rubygems-update after 59 seconds Parsing documentation for rubygems-update-3.1.4 Done installing documentation for rubygems-update after 0 seconds Installing RubyGems 3.1.4 Successfully built RubyGem Name: bundler Version: 2.1.4 File: bundler-2.1.4.gem Bundler 2.1.4 installed RubyGems 3.1.4 installed Regenerating binstubs Parsing documentation for rubygems-3.1.4 Installing ri documentation for rubygems-3.1.4 === 3.1.4 / 2020-06-03 Minor enhancements: * Deprecate rubyforge_project attribute only during build time. Pull request #3609 by Josef Å imÃ¡nek. * Update links. Pull request #3610 by Josef Å imÃ¡nek. * Run CI at 3.1 branch head as well. Pull request #3677 by Josef Å imÃ¡nek. * Remove failing ubuntu-rvm CI flow. Pull request #3611 by Josef Å imÃ¡nek. ------------------------------------------------------------------------------ RubyGems installed the following executables: /usr/local/Cellar/ruby/2.6.5/bin/gem /usr/local/Cellar/ruby/2.6.5/bin/bundle Ruby Interactive (ri) documentation was installed. ri is kind of like man pages for Ruby libraries. You may access it like this: ri Classname ri Classname.class_method ri Classname#instance_method If you do not wish to install this documentation in the future, use the --no-document flag, or set it as the default in your ~/.gemrc file. See \u0026#39;gem help env\u0026#39; for details. RubyGems system software updated You have new mail in /var/mail/michal $ ok letss try again make build Darn getting same result. Ok it says to use bundle exec jekyll build instead. Going to try that\nOk now it did stuff\u0026hellip;\n$ make build bundle exec jekyll build Configuration file: /xxxx/repo/namoopsoo.github.io/_config.yml Source: /xxxx/repo/namoopsoo.github.io Destination: /xxxx/namoopsoo.github.io/_site Incremental build: disabled. Enable with --incremental Generating... Error: could not read file /xxxx/namoopsoo.github.io/_posts/2019-05-13--keras-hello-world-fashion_files/2019-05-13--keras-hello-world-fashion_18_0.png: invalid byte sequence in UTF-8 ... ... Jekyll Feed: Generating feed for posts done in 1.715 seconds. Auto-regeneration: disabled. Use --watch to enable. $ Now it doesnt like some pngs. Maybe it worked though. Will try. Also I forgot that as long as I don\u0026rsquo;t want to care what this looks like locally, I can just push commits to github and then everything just builds there and nothing needs to be done locally. Anyway but for local preview, I wonder if I can just find a jekyll docker to make this easier so as not to worry about these weird ruby configurations. Oh wow docker Turns out per reading here , there is indeed a jekyll Docker image already. Let me just test run that wow. docker pull jekyll/jekyll ... Digest: sha256:bb45414c3fefa80a75c5001f30baf1dff48ae31dc961b8b51003b93b60675334 Status: Downloaded newer image for jekyll/jekyll:latest But then when running from the github , export JEKYLL_VERSION=3.8 docker run --rm \\ --volume=\u0026#34;$PWD:/srv/jekyll\u0026#34; \\ -it jekyll/jekyll:$JEKYLL_VERSION # jekyll build \u0026lt;= left out this part so I can look inside... But I got this Unable to find image \u0026#39;jekyll/jekyll:3.8\u0026#39; locally 3.8: Pulling from jekyll/jekyll ... Digest: sha256:9521c8aae4739fcbc7137ead19f91841b833d671542f13e91ca40280e88d6e34 Status: Downloaded newer image for jekyll/jekyll:3.8 So I suppose latesst was not 3.8. Anyway wow, a few minutes later this run command is still well, running Fetching gem metadata from https://rubygems.org/......... Fetching public_suffix 4.0.1 Installing public_suffix 4.0.1 Fetching addressable 2.7.0 Installing addressable 2.7.0 Using bundler 2.0.2 Fetching colorator 1.1.0 Installing colorator 1.1.0 Fetching concurrent-ruby 1.1.5 Installing concurrent-ruby 1.1.5 Fetching eventmachine 1.2.7 Installing eventmachine 1.2.7 with native extensions Fetching http_parser.rb 0.6.0 Installing http_parser.rb 0.6.0 with native extensions Fetching em-websocket 0.5.1 Installing em-websocket 0.5.1 Fetching ffi 1.11.1 Installing ffi 1.11.1 with native extensions Fetching forwardable-extended 2.6.0 Installing forwardable-extended 2.6.0 Fetching i18n 1.7.0 Installing i18n 1.7.0 Fetching sassc 2.2.1 Installing sassc 2.2.1 with native extensions ok continued.. that last part took a while, but the later stuff was quick. Fetching jekyll-sass-converter 2.0.1 Installing jekyll-sass-converter 2.0.1 Fetching rb-fsevent 0.10.3 Installing rb-fsevent 0.10.3 Fetching rb-inotify 0.10.0 Installing rb-inotify 0.10.0 Fetching listen 3.2.0 Installing listen 3.2.0 Fetching jekyll-watch 2.2.1 Installing jekyll-watch 2.2.1 Fetching kramdown 2.1.0 Installing kramdown 2.1.0 Fetching kramdown-parser-gfm 1.1.0 Installing kramdown-parser-gfm 1.1.0 Fetching liquid 4.0.3 Installing liquid 4.0.3 Fetching mercenary 0.3.6 Installing mercenary 0.3.6 Fetching pathutil 0.16.2 Installing pathutil 0.16.2 Fetching rouge 3.12.0 Installing rouge 3.12.0 Fetching safe_yaml 1.0.5 Installing safe_yaml 1.0.5 Fetching unicode-display_width 1.6.0 Installing unicode-display_width 1.6.0 Fetching terminal-table 1.8.0 Installing terminal-table 1.8.0 Fetching jekyll 4.0.0 Installing jekyll 4.0.0 Fetching jekyll-feed 0.12.1 Installing jekyll-feed 0.12.1 Fetching jekyll-seo-tag 2.6.1 Installing jekyll-seo-tag 2.6.1 Fetching jekyll-sitemap 1.3.1 Installing jekyll-sitemap 1.3.1 Fetching minima 2.5.1 Installing minima 2.5.1 Bundle complete! 8 Gemfile dependencies, 31 gems now installed. Bundled gems are installed into `/usr/local/bundle` ruby 2.6.3p62 (2019-04-16 revision 67580) [x86_64-linux-musl] jekyll 4.0.0 -- Jekyll is a blog-aware, static site generator in Ruby Usage: jekyll \u0026lt;subcommand\u0026gt; [options] Options: -s, --source [DIR] Source directory (defaults to ./) -d, --destination [DIR] Destination directory (defaults to ./_site) --safe Safe mode (defaults to false) -p, --plugins PLUGINS_DIR1[,PLUGINS_DIR2[,...]] Plugins directory (defaults to ./_plugins) --layouts DIR Layouts directory (defaults to ./_layouts) --profile Generate a Liquid rendering profile -h, --help Show this message -v, --version Print the name and version -t, --trace Show the full backtrace when an error occurs Subcommands: compose docs import build, b Build your site clean Clean the site (removes site output and metadata file) without building. doctor, hyde Search site and print specific deprecation warnings help Show the help message, optionally for a given subcommand. new Creates a new Jekyll site scaffold in PATH new-theme Creates a new Jekyll theme scaffold serve, server, s Serve your site locally Oops, guess I ran this poorly\u0026hellip; wait hmm I have the interactive flags, not sure why it quit. weird. Going to try again \u0026hellip; and w/o the --rm so I dont have to redo it\u0026hellip; export JEKYLL_VERSION=3.8 docker run \\ --volume=\u0026#34;$PWD:/srv/jekyll\u0026#34; \\ -i -t jekyll/jekyll:$JEKYLL_VERSION Start 16:18 UTC .. 16:23 UTC \u0026hellip; but still not playing nice. not sure why quitting. But the container should at least be running now hopefully.. So let me at least try build. Hmm but docker ps shows nothing weird. COnfused why it\u0026rsquo;s exiting\u0026hellip; $ docker ps --last 5 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9ac4abb74269 jekyll/jekyll:3.8 \u0026#34;/usr/jekyll/bin/entâ€¦\u0026#34; 7 minutes ago Exited (0) 2 minutes ago compassionate_mccarthy Anyway trying with jekyll build did work but of course it exited again, so this is not really useful if it takes 5 minutes for stuff to happen and I can\u0026rsquo;t keep the container around.\nI do see in the Dockerfile here that 4000 is permanently exposed so I dont even have to specify that with the docker run command so that\u0026rsquo;s cool.\nHmm in the README it is written to use gem \u0026quot;jekyll\u0026quot;, \u0026quot;~\u0026gt; 3.8\u0026quot; in your Gemfile. My Gemfile has \u0026quot;jekyll\u0026quot;, \u0026quot;~\u0026gt; 4.0.0\u0026quot; actually. I wonder if \u0026hellip;\nAha and in the output it does actually show\nFetching jekyll 4.0.0 Installing jekyll 4.0.0 So it is almost like this mismatch may have caused this to be taking extra time.\nSo probably the Gemfile should be consistent with this docker image and then it will not take 5 bonus minutes ?\nMaybe this phenomenon is described here \u0026ldquo;my gems arent caching\u0026rdquo;\u0026hellip;. Ok so I\u0026rsquo;m diverging indeed ok.\nOk trying again w/ gem \u0026quot;jekyll\u0026quot;, \u0026quot;~\u0026gt; 3.8\u0026quot; in my Gemfile this time.\n$ docker run --volume=\u0026#34;$PWD:/srv/jekyll\u0026#34; -i -t jekyll/jekyll:$JEKYLL_VERSION jekyll build Fetching gem metadata from https://rubygems.org/.......... Fetching gem metadata from https://rubygems.org/. You have requested: jekyll ~\u0026gt; 3.8 The bundle currently has jekyll locked at 4.0.0. Try running `bundle update jekyll` If you are updating multiple gems in your Gemfile at once, try passing them all to `bundle update` Hmm but where is this state maintained? ","permalink":"https://michal.piekarczyk.xyz/post/2020-10-20--build-issues-again/","summary":"Build issues again haha I am doing jekyll build but this time from a Makefile , containing\nbuild: jekyll build with make build And I\u0026rsquo;m getting $ make build jekyll build Traceback (most recent call last): 10: from /usr/local/lib/ruby/gems/2.6.0/bin/jekyll:23:in `\u0026lt;main\u0026gt;\u0026#39; 9: from /usr/local/lib/ruby/gems/2.6.0/bin/jekyll:23:in `load\u0026#39; 8: from /usr/local/lib/ruby/gems/2.6.0/gems/jekyll-4.0.0/exe/jekyll:11:in `\u0026lt;top (required)\u0026gt;\u0026#39; 7: from /usr/local/lib/ruby/gems/2.6.0/gems/jekyll-4.0.0/lib/jekyll/plugin_manager.rb:52:in `require_from_bundler\u0026#39; 6: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler.rb:107:in `setup\u0026#39; 5: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/runtime.rb:26:in `setup\u0026#39; 4: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/runtime.rb:26:in `map\u0026#39; 3: from /usr/local/lib/ruby/gems/2.6.0/gems/bundler-2.0.2/lib/bundler/spec_set.rb:148:in `each\u0026#39; 2: from /usr/local/lib/ruby/gems/2.","title":""},{"content":"Binomial Distribution Plot Binomial distribution for p, and a sample of n\nfrom scipy.stats import binom import numpy as np import matplotlib.pyplot as plt import pylab workdir = \u0026#39;_posts/2020-11-14-georgia-recount-stats_files\u0026#39; fig, ax = plt.subplots(1, 1) n, p = 5, 0.4 mean, var, skew, kurt = binom.stats(n, p, moments=\u0026#39;mvsk\u0026#39;) x = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p)) ax.plot(x, binom.pmf(x, n, p), \u0026#39;bo\u0026#39;, ms=8, label=\u0026#39;binom pmf\u0026#39;) ax.vlines(x, 0, binom.pmf(x, n, p), colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) out_loc = f\u0026#39;{workdir}/foo.png\u0026#39; pylab.savefig(out_loc) pylab.close() ","permalink":"https://michal.piekarczyk.xyz/post/2020-11-14-georgia-recount-stats/","summary":"Binomial Distribution Plot Binomial distribution for p, and a sample of n\nfrom scipy.stats import binom import numpy as np import matplotlib.pyplot as plt import pylab workdir = \u0026#39;_posts/2020-11-14-georgia-recount-stats_files\u0026#39; fig, ax = plt.subplots(1, 1) n, p = 5, 0.4 mean, var, skew, kurt = binom.stats(n, p, moments=\u0026#39;mvsk\u0026#39;) x = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p)) ax.plot(x, binom.pmf(x, n, p), \u0026#39;bo\u0026#39;, ms=8, label=\u0026#39;binom pmf\u0026#39;) ax.vlines(x, 0, binom.pmf(x, n, p), colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.","title":""},{"content":"Here, looking at the recent recount in Georgia as an example around determining, what would a good number of votes to randomly sample to help validate an election result.\nAccording to the latest results of this writing there is\nB, T = 2472182, 2458010 p_observed = B*1./(B + T) # 0.501437 For now just starting off below visualizing the binomial distribution around the p_observed for different numbers of samples to look at what likelihoods are assigned to the different outcomes specified by that given binomial distribution.\nfrom scipy.stats import binom import numpy as np import matplotlib.pyplot as plt import pylab B, T = 2472182, 2458010 p_observed = B*1./(B + T) def do_plot(n, p): fig, ax = plt.subplots(1, 1) fig.patch.set_facecolor(\u0026#39;xkcd:mint green\u0026#39;) mean, var, skew, kurt = binom.stats(n, p, moments=\u0026#39;mvsk\u0026#39;) # print(\u0026#39;mean, \u0026#39;, mean, \u0026#39;var\u0026#39;, var) print(\u0026#39;mean, \u0026#39;, mean/n, #\u0026#39;var\u0026#39;, var/n ) x = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p)) y = binom.pmf(x, n, p) print(\u0026#39;sum y,\u0026#39;, y.sum()) # with plt.style.context(\u0026#39;fivethirtyeight\u0026#39;): x2 = x/n ax.plot(x2, y, \u0026#39;bo\u0026#39;, ms=8, label=\u0026#39;binom pmf\u0026#39;) ax.vlines(x2, 0, y, colors=\u0026#39;b\u0026#39;, lw=5, alpha=0.5) plt.show() return list(zip(x2, y)) p=p_observed print(f\u0026#39;n={n}, p={p}\u0026#39;) a = do_plot(n=10, p=p) sum(x[1] for x in a if x[0] \u0026lt; 0.5) n=10, p=0.5014372665405323 mean, 0.5014372665405323 sum y, 0.9880537609732434 0.37247237316599874 p=p_observed print(f\u0026#39;n={n}, p={p}\u0026#39;) a = do_plot(n=100, p=p) sum(x[1] for x in a if x[0] \u0026lt; 0.5) n=10, p=0.5014372665405323 mean, 0.5014372665405323 sum y, 0.9789685520962167 0.4390661773494172 p=p_observed print(f\u0026#39;n={n}, p={p}\u0026#39;) a = do_plot(n=1000, p=p) sum(x[1] for x in a if x[0] \u0026lt; 0.5) n=10, p=0.5014372665405323 mean, 0.5014372665405323 sum y, 0.9790245952379818 0.44151098858308324 p=p_observed print(f\u0026#39;n={n}, p={p}\u0026#39;) do_plot(n=10000, p=p) n=10, p=0.5014372665405323 mean, 0.5014372665405323 sum y, 0.9801955037573005 p=p_observed print(f\u0026#39;n={n}, p={p}\u0026#39;) do_plot(n=100000, p=p) n=10, p=0.5014372665405323 mean, 0.5014372665405323 sum y, 0.9800587882139997 Seeing above, that\n","permalink":"https://michal.piekarczyk.xyz/post/2020-11-14-georgia-recount/","summary":"Here, looking at the recent recount in Georgia as an example around determining, what would a good number of votes to randomly sample to help validate an election result.\nAccording to the latest results of this writing there is\nB, T = 2472182, 2458010 p_observed = B*1./(B + T) # 0.501437 For now just starting off below visualizing the binomial distribution around the p_observed for different numbers of samples to look at what likelihoods are assigned to the different outcomes specified by that given binomial distribution.","title":""},{"content":"Trying to figure out whether Calories specified typically include grams from Fiber Managed to find some documentation , here on \u0026ldquo;proximates\u0026rdquo; , \u0026ldquo;Carbohydrate content, referred to as â€œcarbohydrate by differenceâ€ in the tables, is expressed as the difference between 100 and the sum of the percentages of water, protein, total lipid (fat), ash, and alcohol (when present). Values for carbohydrate by difference include total dietary fiber content. â€œSugars, total NLEAâ€ refers to the sum of the values for individual monosaccharides (galactose, glucose, and fructose) and disaccharides (sucrose, lactose, and maltose), which are those sugars analyzed for nutrition labelling. Because the analyses of total dietary fiber, total sugars, and starch content are conducted separately and reflect the analytical variability inherent in the measurement process, the sum of these carbohydrate fractions may not equal the carbohydrate-by-difference value or may even exceed it.\u0026rdquo;\nAs for energy, \u0026ldquo;Food energy is expressed in kcal and is no longer expressed in kJ as of October 2020. The data represent physiologically available energy, which is the value remaining after digestive and urinary losses are deducted from gross energy (Merrill and Watt, 1973).Energy values are calculated when fat and protein values are available for a food.\u0026rdquo;\n\u0026ldquo;Most energy values are calculated using the Atwater general factors of 4, 9, and 4 for protein, fat, and carbohydrates, respectively. These general calculations are represented in FoodData Central as â€œMetabolizable Energy (Atwater General Factor)â€ and is identified in download files and API with nutrient ID: 2047. \u0026quot;\nI think at least based on looking at macadamia nuts data which is basically also from NAL USDA, I think the answer is yes, the calculated \u0026ldquo;total calories\u0026rdquo; is from total carbs, which includes fiber grams. ","permalink":"https://michal.piekarczyk.xyz/post/2021-03-27-nutrition-calcs/","summary":"Trying to figure out whether Calories specified typically include grams from Fiber Managed to find some documentation , here on \u0026ldquo;proximates\u0026rdquo; , \u0026ldquo;Carbohydrate content, referred to as â€œcarbohydrate by differenceâ€ in the tables, is expressed as the difference between 100 and the sum of the percentages of water, protein, total lipid (fat), ash, and alcohol (when present). Values for carbohydrate by difference include total dietary fiber content. â€œSugars, total NLEAâ€ refers to the sum of the values for individual monosaccharides (galactose, glucose, and fructose) and disaccharides (sucrose, lactose, and maltose), which are those sugars analyzed for nutrition labelling.","title":""},{"content":"TLDR This time, the brew update \u0026amp;\u0026amp; brew upgrade approach resolved my openssl dyld: Library not loaded woes. And brew apparently no longer has the switch command which had been the cornerstone of a popular stackoverflow answer for this problem.\nTrying to run this new ffmpeg usage, but\u0026hellip; $ ffmpeg -i 2021*.jpg -sameq -r 25 outmovie.mp4 dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/ffmpeg Reason: image not found Abort trap: 6 $ brew update Error: homebrew-core is a shallow clone. homebrew-cask is a shallow clone. To `brew update`, first run: git -C /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core fetch --unshallow git -C /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask fetch --unshallow These commands may take a few minutes to run due to the large size of the repositories. This restriction has been made on GitHub\u0026#39;s request because updating shallow clones is an extremely expensive operation due to the tree layout and traffic of Homebrew/homebrew-core and Homebrew/homebrew-cask. We don\u0026#39;t do this for you automatically to avoid repeatedly performing an expensive unshallow operation in CI systems (which should instead be fixed to not use shallow clones). Sorry for the inconvenience! $ openssl version OpenSSL 1.0.2n 7 Dec 2017 $ ls -al /usr/local/Cellar/openssl ls: /usr/local/Cellar/openssl: No such file or directory $ ls -al /usr/local/Cellar/openssl\\@1.1/ total 0 drwxr-xr-x 3 michal staff 96 22 nov 20:38 . drwxr-xr-x 111 michal admin 3552 1 mai 16:11 .. drwxr-xr-x 14 michal staff 448 16 oct 2020 1.1.1h Well this subject is super busy on stack overflow. But the top answer did not work for me hmm $ brew switch openssl 1.1.1h Error: Unknown command: switch $ And yea according to here , switch command is no more\nDoing brew upgrade\n==\u0026gt; Installing ruby dependency: openssl@1.1 ==\u0026gt; Pouring openssl@1.1--1.1.1k.mojave.bottle.tar.gz ==\u0026gt; Regenerating CA certificate bundle from keychain, this may take a while... ðŸº /usr/local/Cellar/openssl@1.1/1.1.1k: 8,071 files, 18.4MB ... ... ==\u0026gt; Upgrading openssl@1.1 1.1.1k -\u0026gt; 1.1.1k Removing: /usr/local/Cellar/openssl@1.1/1.1.1h... (8,067 files, 18.4MB) Ok anyway that took like 20 minutes but now running my ffmpeg command again, now no more openssl error.. ","permalink":"https://michal.piekarczyk.xyz/post/2021-05-23-that-openssl-load-error/","summary":"TLDR This time, the brew update \u0026amp;\u0026amp; brew upgrade approach resolved my openssl dyld: Library not loaded woes. And brew apparently no longer has the switch command which had been the cornerstone of a popular stackoverflow answer for this problem.\nTrying to run this new ffmpeg usage, but\u0026hellip; $ ffmpeg -i 2021*.jpg -sameq -r 25 outmovie.mp4 dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/ffmpeg Reason: image not found Abort trap: 6 $ brew update Error: homebrew-core is a shallow clone.","title":""},{"content":"Trying out this Blue Bottle Opascope espresso w/ this hand grinder w/ my Kamira espresso maker. But the Kamira is not pulling the espresso for some reason.\nComparing the Lavazza grounds to the Opascope grounds I can\u0026rsquo;t tell there\u0026rsquo;s a difference. Is it too fine? Not fine enough?\nThis is the hand grinder.\nHere is what 2 out of 4 of the Blue Bottle pulls ended up like\nAnd after that I did a Lavazza pull and it looked pretty normal..\nTried to use same grind level but also tamping this time Going to call this grind level 1. So \u0026ldquo;tamping\u0026rdquo; means using force to \u0026ldquo;compact\u0026rdquo; the grinds into the filter. This is not typically done w/ a Kamira filter. But tried it this time.. And result was no pull and a swimming pool at the end Grind level 2 , no more tamping ended up with half a pull.. Also tried grind level 3\u0026hellip; This time basically no pull. And slightly watery filter afterwards also.\nNext? Not sure what\u0026rsquo;s next. Maybe going to try to fix the pressure on the Kamira.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-11-15-blue-bottle-espresso-troubleshooting/","summary":"Trying out this Blue Bottle Opascope espresso w/ this hand grinder w/ my Kamira espresso maker. But the Kamira is not pulling the espresso for some reason.\nComparing the Lavazza grounds to the Opascope grounds I can\u0026rsquo;t tell there\u0026rsquo;s a difference. Is it too fine? Not fine enough?\nThis is the hand grinder.\nHere is what 2 out of 4 of the Blue Bottle pulls ended up like\nAnd after that I did a Lavazza pull and it looked pretty normal.","title":"blue bottle espresso troubleshooting"},{"content":"We had a surprise Halloween pipe leak but luckily I got to our local hardware store just 15 minutes before they closed. And they had the pipe I needed. We had guests over that night, so this went much better than the worst case scenario!\n","permalink":"https://michal.piekarczyk.xyz/post/2020-10-31-kitchen-sink-pipe-leak/","summary":"We had a surprise Halloween pipe leak but luckily I got to our local hardware store just 15 minutes before they closed. And they had the pipe I needed. We had guests over that night, so this went much better than the worst case scenario!","title":"Halloween kitchen sink pipe leak"},{"content":"Spoiler alert, don\u0026rsquo;t do it\nI wrote some high level and low level markdown summaries for my aviation kaggle data sciency project. I placed them in the github repo of the project itself. Today I have been thinking about how to best build the Github Pages repo to showcase those markdowns that live in that other repository.\nLooking for inspiration, I saw this author has chosen to have a typical github.io repo and also a separate portfolio repo. On the other hand this author\u0026rsquo;s approach was to maintain any source python notebook and markdown files within the main GithubPages repo itself.\nFor my other two projects, bam and babam, I ended up manually copying my README.md and what have you. I felt dirty doing it that way with all of the copy-pasta!\nHaving recently read Richard Hamming\u0026rsquo;s 1986 seminar on dopeness, the part that stuck with me the most was his notion of generalizability. I am paraphrasing, but he emphasized doing things with a multiplier effect. Ok now actually quoting, \u0026ldquo;Instead of attacking isolated problems, I made the resolution that I would never again solve an isolated problem except as characteristic of a class.\u0026rdquo; What a cry for open source this is. Anyway so living with intention, I am going to relocate my notes into the main Github Pages repo. What a transition!\nA repo should still keep its README for sure, so I suppose that level of duplication is just unavoidable.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-05-31--github-pages/","summary":"Spoiler alert, don\u0026rsquo;t do it\nI wrote some high level and low level markdown summaries for my aviation kaggle data sciency project. I placed them in the github repo of the project itself. Today I have been thinking about how to best build the Github Pages repo to showcase those markdowns that live in that other repository.\nLooking for inspiration, I saw this author has chosen to have a typical github.","title":"How to use Github pages with multiple repositories"},{"content":"Made some Xero shoes.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-11-07-make-some-xero-shoes/","summary":"Made some Xero shoes.","title":"make some xero shoes"},{"content":"Meritocracy Trap Summary I Read this after hearing an interview with Daniel Markovits initially. (2020-08-23 to 2020-12-12)\nThe Book in Three Sentences Meritocracy replaced direct inheritance style aristocracy after the post World War II baby boomers overwhelmed colleges\u0026ndash;forcing them to create the SAT to deal with admissions\u0026ndash;and has been snowballing a self reinforcing selection process that has been undoing the middle class ever since. The elite make bank slogging gnarly hours more than by stereotypical capital gains loop holes and \u0026ldquo;the rest\u0026rdquo; work fewer hours, involuntarily left out of the economy. The contribution of the elite is more self serving and the States would benefit from unraveling and debunking the idea that the elite are \u0026ldquo;adding incremental value\u0026rdquo;.\nImpressions I better understand why America is suffering right now and why the so called \u0026ldquo;elite\u0026rdquo; class is being asked to take responsibility. Mid level jobs have been getting displaced for decades now. The author points to examples in law, finance, medicine and technology. \u0026ldquo;Elites\u0026rdquo; can \u0026ldquo;invest\u0026rdquo; their money in their kids education \u0026ldquo;tax free\u0026rdquo; (the author points out this is a tax loop hole that lets the wealthy bypass estate taxes) and the roughly $130,000 cap on payroll taxes (paid by employers) incentivizes employers to hire fewer highly skilled individuals as opposed to a larger number of mid skilled individuals. And the author argues that removing this $130,000 cap would help reverse this perverse incentive and include more folks in the economy.\nI did not realize that only 25% of people in the US go to college.\nWhy the benefits of the elite are over-sold The author raises the point that the elite are not adding \u0026ldquo;incremental value\u0026rdquo;, where they not present at a particular company, with some re-arranging, the output of that company would be comparable.\n\u0026ldquo;Today, the meritocratic elite, not individually but as a class, is in precisely this position. Superordinate labor is essential to production given the current state of technology, which causes the labor market to fetishize elite skills. This entails that total output is much greater when elites work than when the remaining less skilled workers attempt to deploy current technologies without the elite.\u0026rdquo;\nThe argument is that The elite add value on paper but without them society if given the chance can potentially reorganize and still achieve an optimal outcome . This makes sense to me on many levels most in the ranking algorithms used to predict for merit including the SAT. They can and are gamed by \u0026ldquo;teaching to the test\u0026rdquo;. If you have the resources you will get the best mark but theres no reason to believe this has transfer to the real world. So if the metric is flawed to begin with the the whole promise is flawed. The same happens in the work place all the time. Doctors have had very little training in nutrition which turns out today to be the reason Diabetes and heart disease are leading deaths in the US. Even in my field Data Science theres a large stigma against non phd folks. But having a phd you don\u0026rsquo;t solve real world problems. Real world data is messy. Which is why like the author would agree i think on the job training is invaluable.\n\u0026ldquo;And the elite executives who have monopolized the management function congratulate themselves on their vast and productive powers of command and again expect to be paid commensurately. Superordinate workers of all stripes therefore insist that the inequality that their wages produce is meritocratic.\u0026rdquo;\nYes. Management takes the prize for the biggest false value add that has still not been noticed. In technical fields, in software, top down waterfall design h, today masquerading as \u0026ldquo;agile development\u0026rdquo;, wastes more time than saves, somehow appears to add value but we would all be much better off without it.\n\u0026ldquo;The precise balance between gain and loss of course remains speculative. But the best evidence suggests that the eliteâ€™s true product may be near zero.\u0026rdquo;\nI think the better yardstick of success should be general well being in your country. Like how good is the median income? And America fails in health, wealth, education inequality and all other objective measures. It just perpetuates and justifies itself through its own made up metrics that are disconnected from reality. And it creates perverse incentives. Doctors treat wealthier patients because they can help them pay off their crazy student debt and they are penalized for trying to treat medicare patients with less money and less time to spend with patients. Similarly Apple watch now has an EKG cool but that benefits those who can afford it. Maybe only Google can be congratulated for democratizing information. Facebook is a gateway to rent seekers buying your data for selling you junk you don\u0026rsquo;t need or political shenanigans to swing your vote.\n\u0026ldquo;Like the warriors, the eliteâ€™s true product must be offset by the costs of meritocratic inequality, and especially by meritocracyâ€™s suppression (through inducing innovation that fetishizes skill) of mid-skilled, middle-class production.\u0026rdquo;\nYes this is the cost that society is burdened with. Financial crises adding instability to peoples Retirements. Or elite companies like amazon concentrating wealth for themselves while redirecting people to purchase not US based US made services and products. Or Elite lawyers/ lobbyists crafting laws that only benefit their constituents on paper but really serve corporations or simply gerrymandering to disenfranchise voters. Or elite insurance company executives figuring out how to line their own pockets without actually providing care to those who need it if it affects their bottom line. Or the elite at facebook who have allowed the likes of Cambridge Analytica to influence our elections.\nSome of my other thoughts I was hoping to learn more about the day in the life of these highly paid lawyers and bankers I work in technology and even after reading this book I cannot yet visualize what does the 100 hour work week of a lawyer or banker look like. Personally I think most people over estimate the usefulness of the time they spend (evidence goes to all the books being written on the topic of distraction lately such as Indistractable ). I can do my best to achieve six to eight low distraction hours on a good day, with an additional two or three more hours coming in as pure administrative overhead or communication cost. But on most days, collaboration cost is much higher and I will spend my time helping other people solve their problems. I wonder whether the these kinds of discounts also affect the legal and financial fields. I know I have read and listened to friends anecdotes that in health care the beautacratic overhead is a high discount on time, taking away from time otherwise spent performing \u0026ldquo;health care\u0026rdquo;.\nMore Quotes \u0026ldquo;A simple example illustrates the special burden that the payroll tax imposes specifically on middle-class labor. If a bank deploys midcentury financial technologies to issue home mortgages using twenty mid-skilled loan officers who each earn $100,000 per year, this costs the bank and the workers, taken together, $306,000 in payroll taxes. By contrast, if the bank were to switch to the current mode of production and displace the mid- skilled loan officers with a single Wall Street trader who earns $2 million, this would cost the bank and the trader only about $90,000. Where two technologies of production are economically equivalent, but one requires twenty mid-skilled workers while the other requires one super-skilled worker,\u0026rdquo;\nNice example but i feel like id like to understand what that one person is doing thats letting them do the work of twenty. Unless theyre writing software to automate the work i dont get it. oh unless were just talking about Not doing the same work but just comparing two ways a bank can makd money. One way is by issuing mortgages and another is by investing.\n\u0026ldquo;Progressive reformers must take aim at each of the two mechanisms that produced meritocratic inequality.\u0026rdquo;\nTwo mechanisms? Let me guess . Education ( move training back to on the job and deemphasize this elite style education based on disconnected metrics like SAT LSAT MCAT etc). And two Parenting ( hmm well not sure how you can dismantle elite parenting) . Employers ( yea again training, you hire more loosely maybe by laws, and people will end up being great anyway because if given a chance most people will not squander). State Laws and or standards boards ( i feel like the bar to entry into fields like medicine and law have become too monetary. Licenses and exams cost thousands. Where is that money going. Make that free) . Free education ( but if you make it free , cut those loans , more people can participate ). management elite ( Management consulting finds itself as some magical experts but i am unclear whether they add value. But not sure what that has to do with helping inequality yet. ) investors ( almost forgot. Yea these power grabs of vulture capitalism and leveraged buyouts , don\u0026rsquo;t really help companies only some select few. Lets back away from that so called shareholder value myth).\n\u0026ldquo;Progressives cannot answer because they remain under meritocracyâ€™s thumb. They are captives who embrace their captor, through a sort of ideological Stockholm syndrome. As a result, progressives exacerbate problems that they do not even see. When they focus on identity politics and poverty relief, progressives dismiss middle- class discontent as special pleading.\u0026rdquo;\nAlso universal basic income . With automation hanging over the heads of many, this is still better than a beauracratic system where everyone has to prove their poverty. No penalties. But yea coddling folks can be counterproductive. I like the laws Germany was passing for no elite daycare. I like universal pre-k. Nyc has something like this now. i think if we cant rely on government to help since it is too slow and also very non cooperative politicians these days. Instead maybe we need a social tax of sorts. Collectively stop Feeding the elite companies cash and make it easier for people to shop local. Cut them off embargo them. Cancel Christmas shopping. But short of purchases, Education needs a huge cost restructuring. Maybe that does need to be free. Online education has slowly gotten democratized.\n\u0026ldquo;This allowed unskilled workers, doing simple repetitive tasks coordinated by industrial engineers, to make goods whose production previously required the integrated efforts of a skilled artisan. Along the way, the innovations displaced older artisanal methods and the highly skilled workers who once deployed them.\u0026rdquo;\nAnd today similar shifts may happen in technical, legal fields, which have An artisan component. Example with web site development today wordpress is really high in rankings (technology automation of web Skills which are routine). Similarly data centers now automate away infrastructure which used to be done physically. Even devops is being automated with infrastructure As code\n\u0026ldquo;process called corporate â€œreengineering,â€ which aspired to â€œbreak an organization down into its components parts and then put some of them together again to create a new machine.â€ The remaining parts, left out of the new machine, typically consisted of middle managers.\u0026rdquo;\nWow is this where silos got created. People can design structures all they want but more and more i think imposed structure creates constraints/teams/silos that are hard to undo. Instead, interdisciplinary teams built around tackling specific problems would increase cross pollination and reduce communication coordination cost . Maybe the so called Connway silos can work sometimes but Im observing a lot of resistance at my company when teams have a hard time banding together on new problems they would have done better to just make new break out teams. Feels like Shape Up jason fried territory too\n\u0026ldquo;The match between elite education and finance was made. The sleepy, mid-skilled, middle-class model of the sector gave way to rapid growth, constant innovation, and a super-elite (immensely skilled and extravagantly paid) workforce.\u0026rdquo;\nFunny how the author i think shows the world has glossed over how these securitized mortgages and these over priced junk assets brought the lending economy to a halt in 2008. Somehow these elites get a free pass in finance? The collateralization of small risky business loans is already a new looming danger just a decade later. Will student loans crisis multiplied by a covid economy yield another pat on the back for clever finance\n\u0026ldquo;Overall, about a quarter of financeâ€™s exceptional growth came immediately and directly from the inequality- driven rise in household credit, and in particular the explosion of residential mortgagesâ€”though consumer credit, including credit card debt, also contributed substantially to this facet of finance. A further half of financeâ€™s growth came from economic inequalityâ€™s other side, through the increased output of the securities industry. The securities boom was overwhelmingly propelled by the growth of asset management servicesâ€”with especially rapid growth in private equity firms, venture capital firms, and hedge fundsâ€”which by nature serve the wealthy\u0026rdquo;\nOk a convenient summary. 0.25 mortgages 0.25 credit cards and 0.5 to securitization and hedge funds.\n\u0026ldquo;A generation of newly minted PhDs in physics and engineering found themselves without academic jobs. The new supply of super-skilled workers went looking for demand. At first, energy and communications companies, including most notably Exxon and Bell Labs, absorbed the new super-skilled workforce. But by 1980, Wall Street recognized that physicists and engineers could profitably develop and deploy new financial technologies and came callingâ€”often literally. A physicist who entered finance early and eventually became a managing director at Goldman Sachs\u0026rdquo;\nOh wow interesting story. Crazy that no one would have predicted studying nuclear physics and pivot to financial engineering haha.\n","permalink":"https://michal.piekarczyk.xyz/post/2020-12-12-meritocracytrap-book-summary/","summary":"Meritocracy Trap Summary I Read this after hearing an interview with Daniel Markovits initially. (2020-08-23 to 2020-12-12)\nThe Book in Three Sentences Meritocracy replaced direct inheritance style aristocracy after the post World War II baby boomers overwhelmed colleges\u0026ndash;forcing them to create the SAT to deal with admissions\u0026ndash;and has been snowballing a self reinforcing selection process that has been undoing the middle class ever since. The elite make bank slogging gnarly hours more than by stereotypical capital gains loop holes and \u0026ldquo;the rest\u0026rdquo; work fewer hours, involuntarily left out of the economy.","title":"Meritocracy Trap Book Summary"},{"content":"Michal Piekarczyk Professional Experience Data Scientist at Katapult, New York, NY (Sep 2014 - Present) Data Scientist since 2017 and 2nd software engineering hire since 2014\nCollaborating with a colleague, rebuilt, feature engineered, iterated, tuned and productionized our underwriting model with XGboost + ScikitLearn + SageMaker to help our main business function when on short notice our main data provider deprecated their data products. Created a Data Drift Monitor to more quickly catch the need to refit our model. Decoupled and parallelized our underwriting data pipeline into a separate AWS Lambda API Gateway stack to help our team deploy faster and with more confidence. Dockerized our production underwriting stack and split from the main company git repo to give us the flexibility to deploy both scikit learn and XGBoost models with AWS SageMaker. This update has now been validated over many months of iterations and model deployments. Optimized and re-engineered a colleague\u0026rsquo;s SQL based logistic regression model, to let us make better underwriting decisions on returning customers. Built a python + Docker + AWS Batch SQL pipeline, optimizing from a 6+ hour under an hour runtime so it can run daily. And added the ability to update and version to make iteration easier. Sped up our live underwriting model data pipeline by cutting less important features to save 1.5 sec, while maintaining model accuracy, to help keep our biggest retailer, Also re-wrote our feature engineering code almost from scratch to cut another 2 sec. Parallelized our costly data provider pulls in Clojure, Docker, MongoDB and AWS ECS to cut 2 more seconds. Overhauled our anti-fraud code, responding to new data requirements, to help onboard our biggest retailer. Introduced velocity checks to save thousands of dollars in redundant data costs. Removed our data blind spots by logging to AWS Athena in critical areas, helping us with countless data investigations and business questions. VP and Software Lead at Cortix Systems Inc, Alexandria, VA (Feb 2011 - Jul 2014) Lead system and software engineer responsible for taking initial search engine for data company concept into a Python code base (with a demo helping us secure a $720,000 contract) and iterating it through several subsequent stages of customer validation.\nArchitected the API for how our system consumes new data sources. Led the design, implementation, parallelization, and testing of NLP processing algorithms weighing aspects about data and user decisions, to make predictions about new data introduced into the system. Iterated through several versions of our query interpretation system, with partial text matching and allowing us to support both free-form and unambiguous query language. Formalized our system\u0026rsquo;s resource model and web API with RESTful HTTP. Projects A look into physiological time series data (Kaggle) (Portfolio/Github) Iterated Tensor Flow LSTM based models to predict physiological states. Built datasets in chunks with numpy and h5py for online learning, reliable memory consumption and parallel prediction Used matplotlib to plot validation learning curves for a better understanding of progress in model iteration. Bike share destination prediction (Portfolio/Github) Iterated a scikit learn classification problem over many data normalization, feature engineering and model evaluation steps. Feature engineered additional geolocation data with Google\u0026rsquo;s Geocoding API. Built a redis backend to track the performance of different models and hyper parameters for easier ranking. Dockerized and deployed the model with AWS SageMaker, Lambda and API Gateway, Docker, XGBoost and Python. Clojure 168 hours time parser (Portfolio/Github) Built an AWS Lambda API Gateway and DynamoDB backed micro service to log and summarize time data. Skills Python (pandas, numpy, matplotlib, scikitlearn, jupyter) Machine Learning (XGboost, TensorFlow) PostgreSQL, Docker Clojure, JavaScript Education NYU Polytechnic School of Engineering (Sep 2003 - May 2008) Honors College Brooklyn, NY\nBachelor of Science magna cum laude in Computer Engineering Master of Science in Electrical Engineering University of Sheffield (Spring 2005) Sheffield, UK Global Engineering Education Exchange (Study Abroad Program)\n","permalink":"https://michal.piekarczyk.xyz/resume/","summary":"Michal Piekarczyk Professional Experience Data Scientist at Katapult, New York, NY (Sep 2014 - Present) Data Scientist since 2017 and 2nd software engineering hire since 2014\nCollaborating with a colleague, rebuilt, feature engineered, iterated, tuned and productionized our underwriting model with XGboost + ScikitLearn + SageMaker to help our main business function when on short notice our main data provider deprecated their data products. Created a Data Drift Monitor to more quickly catch the need to refit our model.","title":"Michal Piekarczyk"},{"content":" ","permalink":"https://michal.piekarczyk.xyz/post/2020-11-01-our-lock-finally-bent-my-key-haha/","summary":" ","title":"our lock finally bent my key haha"},{"content":"I have been trying to solve the steak puzzle for several attempts now. That is, how do you cook a good cast iron to oven transfer? The problem so far has been that the recommended time for the oven for a 450 finish, in this article , 4-5 minutes is the recommendation for a medium steak and 2-4 minutes for a rare.\nAlthough I did not quite follow the 450, I used 400 F, I followed the temperature lore from here last night, using a meat thermometer instead of time, because I suspect oven temperatures in one oven is not the same as that of another. There are possibly incorrect temperature readings.\nSo with a temperature reading fluctuating between 110 and 100\nwith just 5 minutes after, just doing the stove top cast iron for now, here is what the inside looked like.\nSo I know doing these pre-cuts is sacrilegious, but I just wanted to see.\nAt this point, and oh I was trying this new Le Creuset grill pan that my partner in crime acquisitioned, I was just trying to see if the new grill pan can cook the steak to temperature without using the oven. I forgot to mention this steak was huge. It was like 2 to 3 inches in thickness mostly.\nAnyway, the weird part next is that after several attempts in the oven at 400 F, after about 15 minutes in the oven, best reading I got was 128F,\nAnd oddly enough after even more time in the oven, my readings fell down back to 120 F.\nAt this point we (I) just ate the steak. The steak at this point (now maybe 30 minutes in the oven?) was clearly well done.\nI guess I only took one photo because it was still tasty with a lot of Utah salt.\nBut nonetheless I am confused by the discrepancy of 130F being a temp according to that Food \u0026amp; Wine article as being a \u0026ldquo;medium rare\u0026rdquo; classification.\nBut bone-the-less, at this point I suspect either the thermometer is faulty after being exposed to super high heat the other day (300+ even though it is only rated to 220) or the steak actually cools extremely fast after I take it out of the oven to do the measurement? Anyway my partner in crime here suggested I make a pie for which the temp for me at least consistently reads 170 for done-ness and in comparing the thermometer outcome in a known scenario I will take the excuse to make a pie thank you!\n","permalink":"https://michal.piekarczyk.xyz/post/2020-05-31-steak-puzzle/","summary":"I have been trying to solve the steak puzzle for several attempts now. That is, how do you cook a good cast iron to oven transfer? The problem so far has been that the recommended time for the oven for a 450 finish, in this article , 4-5 minutes is the recommendation for a medium steak and 2-4 minutes for a rare.\nAlthough I did not quite follow the 450, I used 400 F, I followed the temperature lore from here last night, using a meat thermometer instead of time, because I suspect oven temperatures in one oven is not the same as that of another.","title":"Steak Puzzle"},{"content":"Vitamin D questions There are a couple mini researach questions I have around Vitamin D. How different are humans with respect to Vitamin D, from other life forms or just other mammals. Also what does the synthesis pathway look like for Vitamin D (or more than one pathway if dietary is a separate pathway from skin). And how exactly does vitamin D enable immune function?\nThe names. Per 1 , \u0026ldquo;plasma concentration of 25-hydroxyvitamin D\u0026rdquo; is one \u0026ldquo;vitamin D\u0026rdquo; biomarker. Fish Per 2, \u0026ldquo;Fish incorporates vitamin D2 and D3 through the diet.\u0026rdquo; Roles of \u0026ldquo;Vitamin D\u0026rdquo; Also per 2] , \u0026ldquo;In mammals, vitamin D forms are involved in mineral metabolism, cell growth, tissue differentiation, and antibacterial immune response.\u0026rdquo; Synthesis pathway for \u0026ldquo;Vitamin D\u0026rdquo; From 3, \u0026ldquo;The synthesis of vitamin D begins in the skin with activation of a precursor molecule by ultraviolet light from the sun. After that, the process moves to the liver for further chemical modification and then on to the kidneys for a final tweak.\u0026rdquo; And \u0026ldquo;Humans and other animals can synthesize this molecule right from cholesterol, something we always have plenty of\u0026rdquo; 3 . Dietary vitamin D Hmm ok so according to 3, \u0026ldquo;Many animals, including humans, can escape this paradox by simply ingesting the activated vitamin D precursor. Dietary vitamin D still requires the activation steps in the liver and kidney, but at least it lets us stay out of the sunlight. Such supplementation has allowed humans to live in frigid climates and it is no coincidence that Arctic diets tend toward fish and whale blubber, two very rich sources of vitamin D.\u0026rdquo; So I\u0026rsquo;m reading here that if you eat the vitamin D precursor, you can skip out on the sun. Dumb evolution Haha And from 3 , \u0026ldquo;it would make much more sense to house the enzymes for the synthesis of vitamin D within fat cells, which are often tucked right underneath the skin anyway, where the UV light is needed for the first step.\u0026rdquo; Indeed there is a list of these weird suboptimalities in life out there. Or do we just not understand what\u0026rsquo;s going on? References 1 here\n2 here\n3 here ","permalink":"https://michal.piekarczyk.xyz/post/2020-11-07-vitamin-d/","summary":"Vitamin D questions There are a couple mini researach questions I have around Vitamin D. How different are humans with respect to Vitamin D, from other life forms or just other mammals. Also what does the synthesis pathway look like for Vitamin D (or more than one pathway if dietary is a separate pathway from skin). And how exactly does vitamin D enable immune function?\nThe names. Per 1 , \u0026ldquo;plasma concentration of 25-hydroxyvitamin D\u0026rdquo; is one \u0026ldquo;vitamin D\u0026rdquo; biomarker.","title":"vitamin D"}]