<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Some xgboost notes so far | michal.piekarczyk.xyz</title>
<meta name=keywords content><meta name=description content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Some xgboost notes so far"><meta property="og:description" content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-06-21T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-26T18:33:54-05:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Some xgboost notes so far"><meta name=twitter:description content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Some xgboost notes so far","item":"https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Some xgboost notes so far","name":"Some xgboost notes so far","description":"Let\u0026rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.\nI have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.\nThis is mainly around navigating XGBoost.\nThere are two XGBoost APIs With the sklearn API you can\nimport xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn.","keywords":[],"articleBody":"Let’s summarize I want to just summarize some learnings from some of my recent notebooks yea.\nI have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.\nThis is mainly around navigating XGBoost.\nThere are two XGBoost APIs With the sklearn API you can\nimport xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, log_loss X, y = np.random.random(size=(1000, 3)), np.random.choice([1, 0], size=(1000,), replace=True) X_train, X_test, y_train, y_test = train_test_split(X, y) model = XGBClassifier().fit(X_train, y_train) y_pred_prob = model.predict_proba(X_test) y_pred = np.argmax(y_pred_prob, axis=1) acc = accuracy_score(y_test, y_pred) print(acc) And you can also just\nparams = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'} dtrain = xgb.DMatrix(X_train, labels=y_train) dtest = xgb.DMatrix(X_test) bst = xgb.train(params, dtrain, num_round=2) y_pred = bst.predict(dtest) The sklearn API I believe uses 100 rounds/iterations/epochs (I have seen these used interchangeably) by default, and the xg.train method let’s you specify that. I am not sure how to override that w/ the sklearn wrapper.\nThe sklearn API starts you off with a bunch of default parameters that appear to be different than the raw parameters.\nLearning continuation is not a good method for batch learning I tried to split up my data into chunks and use this approach to batch learn..\ndef batch_train(X, y) parts = [range(1000), range(1000, 2000), range(2000, 3000)] prev_model_loc = None model = XGBClassifier() for i, part in enumerate(parts): model.fit(X[part], y[part], xgb_model=prev_model_loc) prev_model_loc = f'model.xg' model.save_model(prev_model_loc) return model With about half a million rows, training was doable in one go and it took maybe 7 minutes. But when I tried to “batch” this into 10k sections, this lasted for 8 hours . Luckily I was sleeping and kept my laptop on with Amphetamine, but indeed that was crazy.\nThis answer around pickling/unpickling seems to even say incremental learning with the sklearn API is not possible. So indeed I feel like I want to try it with the functional API.\nI am thinking the lesson is the data caching approach mentioned here for example, is the way to go, but I have not been able to find how to use it with the sklearn api.\nI tried that in my notebook here but my results were very different. I think this was something to do with a very different set of default parameters.\nOr something tells me using num_rounds=2 could have been the culprit. I would like to retry this with more rounds! (In next section I did try num_rounds=100 but that did not help )\nSomehow the caching feature is not mentioned in this blogpost .\nUsing the functional Xgboost api with caching seems to be hit or miss I did try the functional xgboost api w/ num_rounds=100 in this notebook , although it feels like something’s wrong. The verbose xgboost output looks like no learning is happening. Going to have to try to pick that apart. According to the parameters documentation , as far as the “tree construction algorithm” goes, “Experimental support for external memory is available for approx and gpu_hist.” for the tree_method parameters.\nLater on here I can see learning does happen as long as that “cache” feature is not used. Indeed very odd.\nParallelism One anecdote around parallelism. In these two notebooks, 2020-07-03-aws and 2020-07-04-aws, I used the same data and same xgboost parameters except in the first go I used the functional API and in the second go I used the sklearn API.\nAmazingly, the accuracy and logloss on the test set was exactly the same to several decimal places in the two cases (I passed seed=42 in both cases as a parameter but I didn’t expect such a high level of determinism!).\nThe walltime difference was 4min 18s vs 49min 6s ! (What a difference multithreading makes!)\nThe first case used 2 threads. I didn’t actually set the nthread parameter, but I read in the docs it defaults to the max. The sklearn doc seems to show n_jobs is the equivalent parameter here, but it does not appear to describe the default.\nNicely, the magic func %%time shows the parallelization as\nCPU times: user 8min 24s, sys: 1.24 s, total: 8min 26s Wall time: 4min 18s vs\nCPU times: user 49min 10s, sys: 1.15 s, total: 49min 11s Wall time: 49min 6s Side note In the multiclass problem I have here, this is really making me think of the difference between winner take all political elections vs proportional representation elections, as binary vs multi-label problems. ","wordCount":"763","inLanguage":"en","datePublished":"2020-06-21T00:00:00Z","dateModified":"2023-02-26T18:33:54-05:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Some xgboost notes so far</h1><div class=post-meta>&lt;span title='2020-06-21 00:00:00 +0000 UTC'>June 21, 2020&lt;/span>&amp;nbsp;·&amp;nbsp;4 min&amp;nbsp;·&amp;nbsp;763 words&amp;nbsp;·&amp;nbsp;Michal Piekarczyk</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li></li></ul></li></ul></nav></div></details></div><div class=post-content><h4 id=lets-summarize>Let&rsquo;s summarize<a hidden class=anchor aria-hidden=true href=#lets-summarize>#</a></h4><p>I want to just summarize some learnings <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-quick-mvp-xgboost--snapshot-2020-06-10T0239Z.md>from</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-10-again.md>some</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-12--snapshot-2020-06-14T2258Z.md>of</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-14.md>my</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-16.md>recent</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-19.md>notebooks</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-20.md>yea</a>.</p><p>I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.</p><p>This is mainly around navigating XGBoost.</p><h4 id=there-are-two-xgboost-apis>There are two XGBoost APIs<a hidden class=anchor aria-hidden=true href=#there-are-two-xgboost-apis>#</a></h4><p>With the sklearn API you can</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> xgboost <span style=color:#66d9ef>as</span> xgb
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> xgboost <span style=color:#f92672>import</span> XGBClassifier
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> accuracy_score, log_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>random(size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>3</span>)), np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1000</span>,), replace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> XGBClassifier()<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>y_pred_prob <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict_proba(X_test)
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(y_pred_prob, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>acc <span style=color:#f92672>=</span> accuracy_score(y_test, y_pred)
</span></span><span style=display:flex><span>print(acc)
</span></span></code></pre></div><p>And you can also just</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>params <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#39;eta&#39;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;objective&#39;</span>: <span style=color:#e6db74>&#39;binary:logistic&#39;</span>}
</span></span><span style=display:flex><span>dtrain <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(X_train, labels<span style=color:#f92672>=</span>y_train)
</span></span><span style=display:flex><span>dtest <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(X_test)
</span></span><span style=display:flex><span>bst <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>train(params, dtrain, num_round<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> bst<span style=color:#f92672>.</span>predict(dtest)
</span></span></code></pre></div><p>The sklearn API I believe uses <code>100</code> rounds/iterations/epochs (I have seen these used interchangeably) by default, and the <code>xg.train</code> method let&rsquo;s you specify that. I am not sure how to override that w/ the sklearn wrapper.</p><p>The sklearn API starts you off with a bunch of default parameters that appear to be different than the raw parameters.</p><h4 id=learning-continuation-is-not-a-good-method-for-batch-learning>Learning continuation is not a good method for batch learning<a hidden class=anchor aria-hidden=true href=#learning-continuation-is-not-a-good-method-for-batch-learning>#</a></h4><p>I tried to split up my data into chunks and use this approach to batch learn..</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>batch_train</span>(X, y)
</span></span><span style=display:flex><span>    parts <span style=color:#f92672>=</span> [range(<span style=color:#ae81ff>1000</span>), range(<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>2000</span>), range(<span style=color:#ae81ff>2000</span>, <span style=color:#ae81ff>3000</span>)]
</span></span><span style=display:flex><span>    prev_model_loc <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> XGBClassifier()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, part <span style=color:#f92672>in</span> enumerate(parts):
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>fit(X[part], y[part], xgb_model<span style=color:#f92672>=</span>prev_model_loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        prev_model_loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;model.xg&#39;</span>
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>save_model(prev_model_loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model
</span></span></code></pre></div><p>With about half a million rows, training was doable in one go and it took maybe <code>7 minutes</code>. But when I tried to &ldquo;batch&rdquo; this into <code>10k</code> sections, this lasted for <code>8 hours</code> . Luckily I was sleeping and kept my laptop on with Amphetamine, but indeed that was crazy.</p><p><a href=https://stackoverflow.com/a/44922590>This answer</a> around pickling/unpickling seems to even say incremental learning with the sklearn API is not possible. So indeed I feel like I want to try it with the functional API.</p><p>I am thinking the lesson is the data caching approach mentioned <a href=https://stackoverflow.com/questions/43972009/how-to-load-a-big-train-csv-for-xgboost>here</a> for example, is the way to go, but I have not been able to find how to use it with the sklearn api.</p><p>I tried that in my <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-12.md#i-ended-up-trying-out-the-external-memory-approach>notebook here</a> but my results were very different. I think this was something to do with a very different set of default parameters.</p><p>Or something tells me using <code>num_rounds=2</code> could have been the culprit. I would like to retry this with more rounds! (In next <a href=#using-the-functional-xgboost-api-with-caching-seems-to-be-hit-or-miss>section</a> I did try <code>num_rounds=100</code> but that did not help )</p><p>Somehow the caching feature is not mentioned in <a href=https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d>this blogpost</a> .</p><h5 id=using-the-functional-xgboost-api-with-caching-seems-to-be-hit-or-miss>Using the functional Xgboost api with caching seems to be hit or miss<a hidden class=anchor aria-hidden=true href=#using-the-functional-xgboost-api-with-caching-seems-to-be-hit-or-miss>#</a></h5><p>I did try the functional xgboost api w/ <code>num_rounds=100</code> in <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-21.md>this notebook</a> , although it feels like something&rsquo;s wrong. The verbose xgboost output looks like no learning is happening. Going to have to try to pick that apart. According to the <a href=https://xgboost.readthedocs.io/en/latest/parameter.html>parameters documentation</a> , as far as the <em>&ldquo;tree construction algorithm&rdquo;</em> goes, <em>&ldquo;Experimental support for external memory is available for approx and gpu_hist.&rdquo;</em> for the <code>tree_method</code> parameters.</p><p>Later on <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-03-aws.md>here</a> I can see learning does happen as long as that &ldquo;cache&rdquo; feature is not used. Indeed very odd.</p><h4 id=parallelism>Parallelism<a hidden class=anchor aria-hidden=true href=#parallelism>#</a></h4><p>One anecdote around parallelism. In these two notebooks, <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-03-aws.md>2020-07-03-aws</a> and <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-04-aws.md>2020-07-04-aws</a>, I used the same data and same xgboost parameters except in the first go I used the functional API and in the second go I used the sklearn API.</p><p>Amazingly, the accuracy and logloss on the test set was exactly the same to several decimal places in the two cases (I passed <code>seed=42</code> in both cases as a parameter but I didn&rsquo;t expect such a high level of determinism!).</p><p>The walltime difference was <code>4min 18s</code> vs <code>49min 6s</code> ! (What a difference multithreading makes!)</p><p>The first case used <code>2 threads</code>. I didn&rsquo;t actually set the <code>nthread</code> parameter, but I read in <a href=https://xgboost.readthedocs.io/en/latest/parameter.html>the docs</a> it defaults to the max. The <a href=https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier>sklearn doc</a> seems to show <code>n_jobs</code> is the equivalent parameter here, but it does not appear to describe the default.</p><p>Nicely, the magic func <code>%%time</code> shows the parallelization as</p><pre tabindex=0><code>CPU times: user 8min 24s, sys: 1.24 s, total: 8min 26s
Wall time: 4min 18s
</code></pre><p>vs</p><pre tabindex=0><code>CPU times: user 49min 10s, sys: 1.15 s, total: 49min 11s
Wall time: 49min 6s
</code></pre><h4 id=side-note>Side note<a hidden class=anchor aria-hidden=true href=#side-note>#</a></h4><ul><li>In the multiclass problem I have here, this is really making me think of the difference between <a href=https://ballotpedia.org/Winner-take-all>winner take all political elections</a> vs <a href=https://ballotpedia.org/Proportional_representation>proportional representation</a> elections, as binary vs multi-label problems.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2020-07-13-multi-multi-class/><span class=title>« Prev</span><br><span>Notes on multi-multi-class classifiers</span>
</a><a class=next href=https://michal.piekarczyk.xyz/post/2020-06-06-heart-datar/><span class=title>Next »</span><br><span>Does the Wahoo TIKR measure intervals that can be used for Heart Rate Variability measurements?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>