<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Some xgboost notes so far | michal.piekarczyk.xyz</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."><meta name=generator content="Hugo 0.110.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><link rel=stylesheet href=/css/custom_code_style.css><meta property="og:title" content="Some xgboost notes so far"><meta property="og:description" content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-06-21T00:00:00+00:00"><meta property="article:modified_time" content="2020-06-21T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta itemprop=name content="Some xgboost notes so far"><meta itemprop=description content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."><meta itemprop=datePublished content="2020-06-21T00:00:00+00:00"><meta itemprop=dateModified content="2020-06-21T00:00:00+00:00"><meta itemprop=wordCount content="763"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Some xgboost notes so far"><meta name=twitter:description content="Let&rsquo;s summarize I want to just summarize some learnings from some of my recent notebooks yea.
I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.
This is mainly around navigating XGBoost.
There are two XGBoost APIs With the sklearn API you can
import xgboost as xgb from xgboost import XGBClassifier import numpy as np from sklearn."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">michal.piekarczyk.xyz</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/handy/ title="Handy page">Handy</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/post/ title="Post page">Post</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/project/ title="Side Projects page">Side Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/foo/ title="The Foos page">The Foos</a></li></ul></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POST</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/&text=Some%20xgboost%20notes%20so%20far" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://michal.piekarczyk.xyz/post/2020-06-21-notes-xgboost/&title=Some%20xgboost%20notes%20so%20far" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">Some xgboost notes so far</h1><time class="f6 mv4 dib tracked" datetime=2020-06-21T00:00:00Z>June 21, 2020</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h4 id=lets-summarize>Let&rsquo;s summarize</h4><p>I want to just summarize some learnings <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-quick-mvp-xgboost--snapshot-2020-06-10T0239Z.md>from</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-10-again.md>some</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-12--snapshot-2020-06-14T2258Z.md>of</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-14.md>my</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-16.md>recent</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-19.md>notebooks</a> <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-20.md>yea</a>.</p><p>I have picked up my bike share data learning project from earlier, to try to redo it after having gathered more experience. I want to just jot down some ad hoc thoughts here.</p><p>This is mainly around navigating XGBoost.</p><h4 id=there-are-two-xgboost-apis>There are two XGBoost APIs</h4><p>With the sklearn API you can</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> xgboost <span style=color:#66d9ef>as</span> xgb
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> xgboost <span style=color:#f92672>import</span> XGBClassifier
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> accuracy_score, log_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>random(size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>3</span>)), np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>], size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1000</span>,), replace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> XGBClassifier()<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>y_pred_prob <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict_proba(X_test)
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(y_pred_prob, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>acc <span style=color:#f92672>=</span> accuracy_score(y_test, y_pred)
</span></span><span style=display:flex><span>print(acc)
</span></span></code></pre></div><p>And you can also just</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>params <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#39;eta&#39;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;objective&#39;</span>: <span style=color:#e6db74>&#39;binary:logistic&#39;</span>}
</span></span><span style=display:flex><span>dtrain <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(X_train, labels<span style=color:#f92672>=</span>y_train)
</span></span><span style=display:flex><span>dtest <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(X_test)
</span></span><span style=display:flex><span>bst <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>train(params, dtrain, num_round<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> bst<span style=color:#f92672>.</span>predict(dtest)
</span></span></code></pre></div><p>The sklearn API I believe uses <code>100</code> rounds/iterations/epochs (I have seen these used interchangeably) by default, and the <code>xg.train</code> method let&rsquo;s you specify that. I am not sure how to override that w/ the sklearn wrapper.</p><p>The sklearn API starts you off with a bunch of default parameters that appear to be different than the raw parameters.</p><h4 id=learning-continuation-is-not-a-good-method-for-batch-learning>Learning continuation is not a good method for batch learning</h4><p>I tried to split up my data into chunks and use this approach to batch learn..</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>batch_train</span>(X, y)
</span></span><span style=display:flex><span>    parts <span style=color:#f92672>=</span> [range(<span style=color:#ae81ff>1000</span>), range(<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>2000</span>), range(<span style=color:#ae81ff>2000</span>, <span style=color:#ae81ff>3000</span>)]
</span></span><span style=display:flex><span>    prev_model_loc <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> XGBClassifier()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, part <span style=color:#f92672>in</span> enumerate(parts):
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>fit(X[part], y[part], xgb_model<span style=color:#f92672>=</span>prev_model_loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        prev_model_loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;model.xg&#39;</span>
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>save_model(prev_model_loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model
</span></span></code></pre></div><p>With about half a million rows, training was doable in one go and it took maybe <code>7 minutes</code>. But when I tried to &ldquo;batch&rdquo; this into <code>10k</code> sections, this lasted for <code>8 hours</code> . Luckily I was sleeping and kept my laptop on with Amphetamine, but indeed that was crazy.</p><p><a href=https://stackoverflow.com/a/44922590>This answer</a> around pickling/unpickling seems to even say incremental learning with the sklearn API is not possible. So indeed I feel like I want to try it with the functional API.</p><p>I am thinking the lesson is the data caching approach mentioned <a href=https://stackoverflow.com/questions/43972009/how-to-load-a-big-train-csv-for-xgboost>here</a> for example, is the way to go, but I have not been able to find how to use it with the sklearn api.</p><p>I tried that in my <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-12.md#i-ended-up-trying-out-the-external-memory-approach>notebook here</a> but my results were very different. I think this was something to do with a very different set of default parameters.</p><p>Or something tells me using <code>num_rounds=2</code> could have been the culprit. I would like to retry this with more rounds! (In next <a href=#using-the-functional-xgboost-api-with-caching-seems-to-be-hit-or-miss>section</a> I did try <code>num_rounds=100</code> but that did not help )</p><p>Somehow the caching feature is not mentioned in <a href=https://towardsdatascience.com/build-xgboost-lightgbm-models-on-large-datasets-what-are-the-possible-solutions-bf882da2c27d>this blogpost</a> .</p><h5 id=using-the-functional-xgboost-api-with-caching-seems-to-be-hit-or-miss>Using the functional Xgboost api with caching seems to be hit or miss</h5><p>I did try the functional xgboost api w/ <code>num_rounds=100</code> in <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-21.md>this notebook</a> , although it feels like something&rsquo;s wrong. The verbose xgboost output looks like no learning is happening. Going to have to try to pick that apart. According to the <a href=https://xgboost.readthedocs.io/en/latest/parameter.html>parameters documentation</a> , as far as the <em>&ldquo;tree construction algorithm&rdquo;</em> goes, <em>&ldquo;Experimental support for external memory is available for approx and gpu_hist.&rdquo;</em> for the <code>tree_method</code> parameters.</p><p>Later on <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-03-aws.md>here</a> I can see learning does happen as long as that &ldquo;cache&rdquo; feature is not used. Indeed very odd.</p><h4 id=parallelism>Parallelism</h4><p>One anecdote around parallelism. In these two notebooks, <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-03-aws.md>2020-07-03-aws</a> and <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-04-aws.md>2020-07-04-aws</a>, I used the same data and same xgboost parameters except in the first go I used the functional API and in the second go I used the sklearn API.</p><p>Amazingly, the accuracy and logloss on the test set was exactly the same to several decimal places in the two cases (I passed <code>seed=42</code> in both cases as a parameter but I didn&rsquo;t expect such a high level of determinism!).</p><p>The walltime difference was <code>4min 18s</code> vs <code>49min 6s</code> ! (What a difference multithreading makes!)</p><p>The first case used <code>2 threads</code>. I didn&rsquo;t actually set the <code>nthread</code> parameter, but I read in <a href=https://xgboost.readthedocs.io/en/latest/parameter.html>the docs</a> it defaults to the max. The <a href=https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier>sklearn doc</a> seems to show <code>n_jobs</code> is the equivalent parameter here, but it does not appear to describe the default.</p><p>Nicely, the magic func <code>%%time</code> shows the parallelization as</p><pre tabindex=0><code>CPU times: user 8min 24s, sys: 1.24 s, total: 8min 26s
Wall time: 4min 18s
</code></pre><p>vs</p><pre tabindex=0><code>CPU times: user 49min 10s, sys: 1.15 s, total: 49min 11s
Wall time: 49min 6s
</code></pre><h4 id=side-note>Side note</h4><ul><li>In the multiclass problem I have here, this is really making me think of the difference between <a href=https://ballotpedia.org/Winner-take-all>winner take all political elections</a> vs <a href=https://ballotpedia.org/Proportional_representation>proportional representation</a> elections, as binary vs multi-label problems.</li></ul><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://michal.piekarczyk.xyz/>&copy; michal.piekarczyk.xyz 2023</a><div></div></div></footer></body></html>