<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Tuning Results | michal.piekarczyk.xyz</title><meta name=keywords content><meta name=description content="Looking at hyperparameter tuning results"><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2020-07-24-understanding-tuning-results/><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-86TSXEXWB5"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-86TSXEXWB5",{anonymize_ip:!1})}</script><meta property="og:title" content="Understanding Tuning Results"><meta property="og:description" content="Looking at hyperparameter tuning results"><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2020-07-24-understanding-tuning-results/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-07-24T00:00:00+00:00"><meta property="article:modified_time" content="2020-07-24T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Tuning Results"><meta name=twitter:description content="Looking at hyperparameter tuning results"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Understanding Tuning Results","item":"https://michal.piekarczyk.xyz/post/2020-07-24-understanding-tuning-results/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Tuning Results","name":"Understanding Tuning Results","description":"Looking at hyperparameter tuning results","keywords":[],"articleBody":"This a mini post, part of this project\nTo explore some of the hyperparameter boundaries with this model, I ran some tests in these notebooks, here and here.\nAnd to get a better understanding of the overfitting-ness / underfitting-ness of models , here , I took the model artifacts and recalculated metrics on the training set, to generate some stats comparing training and testing performance.\nHere below, we see the effect of the row subsample ratio and tree max depth on test logloss. And also the effect on test accuracy. And the effect on balanced test accuracy. The tuning took several days to complete, but I started plotting early results in this notebook\nLearning rate had some drastic effects for sure!\nMaybe this one below did not have enough data points yet, but slightly surprising perhaps. I had expected the smaller learning rate to take longer.\nTrain / Test Comparisons Keeping a few parameters fixed, I started looking at train vs test accuracy across the number of rounds used during training,\nimport fresh.plot as fp keep_fixed = { 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bylevel': 0.1, 'colsample_bynode': 1, 'colsample_bytree': 0.1, 'subsample': 0.1, #'num_round': 10, } # alldf is the combined train+test metrics on model artifacts fp.compare_train_test(alldf, feature_col='num_round', metric_cols=['acc', 'train_acc'], keep_fixed=keep_fixed) And sorting the metrics dataframe by test accuracy, I also plotted that,\nbest_params = dict(alldf.sort_values(by='acc').iloc[-1]) best_params {'train_acc': 0.12693459297270465, 'train_balanced_acc': 0.11012147901980039, 'i': 755, 'train_logloss': 3.4301962566050057, 'train_karea': 0.76345208497788, 'max_depth': 4, 'learning_rate': 0.1, 'objective': 'multi:softprob', 'num_class': 54, 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 1, 'colsample_bytree': 1.0, 'gamma': 0, 'max_delta_step': 0, 'min_child_weight': 1, 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': 42, 'subsample': 0.4, 'verbosity': 0, 'acc': 0.12304248437307332, 'balanced_acc': 0.10551953202851949, 'logloss': 3.4480742986458592, 'walltime': 1918.593945, 'karea': 0.75845582462009, 'num_round': 100} # Taking the best params ^^ here is what the \"learning curve\" seems to look like keep_fixed = { 'max_depth': 4, 'learning_rate': 0.1, 'colsample_bylevel': 1.0, 'colsample_bynode': 1, 'colsample_bytree': 1.0, 'subsample': 0.4, #'num_round': 10, } fp.compare_train_test(alldf, feature_col='num_round', metric_cols=['acc', 'train_acc', 'balanced_acc', 'train_balanced_acc'], keep_fixed=keep_fixed) The difference between train accuracy and test accuracy does not look crazy so I think it is safe to say this model is not overfitting .\n","wordCount":"351","inLanguage":"en","datePublished":"2020-07-24T00:00:00Z","dateModified":"2020-07-24T00:00:00Z","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2020-07-24-understanding-tuning-results/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/tags/ title=tags><span>tags</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Understanding Tuning Results</h1><div class=post-meta><span title='2020-07-24 00:00:00 +0000 UTC'>242424-24-2412</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;351 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><div class=post-content><p>This a mini post, part of <a href=/project/2020-10-20-bike-share-learn-reboot/>this project</a></p><p>To explore some of the hyperparameter boundaries with this model, I ran some tests in these notebooks, <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-09-aws.md>here</a> and <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-10-aws.md>here</a>.</p><p>And to get a better understanding of the overfitting-ness / underfitting-ness of models , <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-16-local.md#2020-07-18>here</a> , I took the model artifacts and recalculated metrics on the training set, to generate some stats comparing training and testing performance.</p><p>Here below, we see the effect of the row subsample ratio and tree max depth on test logloss.</p><p>And also the effect on test accuracy.</p><p>And the effect on balanced test accuracy.</p><p>The tuning took several days to complete, but I started plotting early results in <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md>this notebook</a></p><p>Learning rate had some drastic effects for sure!</p><p>Maybe this one below did not have enough data points yet, but slightly surprising perhaps. I had expected the smaller learning rate to take longer.</p><h3 id=train--test-comparisons>Train / Test Comparisons<a hidden class=anchor aria-hidden=true href=#train--test-comparisons>#</a></h3><p>Keeping a few parameters fixed, I started looking at train vs test accuracy across the number of rounds used during training,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> fresh.plot <span style=color:#66d9ef>as</span> fp
</span></span><span style=display:flex><span>keep_fixed <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bylevel&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bynode&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bytree&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;subsample&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#75715e>#&#39;num_round&#39;: 10,</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span><span style=color:#75715e># alldf is the combined train+test metrics on model artifacts</span>
</span></span><span style=display:flex><span>fp<span style=color:#f92672>.</span>compare_train_test(alldf, feature_col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;num_round&#39;</span>,
</span></span><span style=display:flex><span>                      metric_cols<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;acc&#39;</span>, <span style=color:#e6db74>&#39;train_acc&#39;</span>],
</span></span><span style=display:flex><span>                      keep_fixed<span style=color:#f92672>=</span>keep_fixed)
</span></span></code></pre></div><p>And sorting the metrics dataframe by test accuracy, I also plotted that,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>best_params <span style=color:#f92672>=</span> dict(alldf<span style=color:#f92672>.</span>sort_values(by<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;acc&#39;</span>)<span style=color:#f92672>.</span>iloc[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>best_params
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;train_acc&#39;</span>: <span style=color:#ae81ff>0.12693459297270465</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_balanced_acc&#39;</span>: <span style=color:#ae81ff>0.11012147901980039</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;i&#39;</span>: <span style=color:#ae81ff>755</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_logloss&#39;</span>: <span style=color:#ae81ff>3.4301962566050057</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_karea&#39;</span>: <span style=color:#ae81ff>0.76345208497788</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;objective&#39;</span>: <span style=color:#e6db74>&#39;multi:softprob&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;num_class&#39;</span>: <span style=color:#ae81ff>54</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;base_score&#39;</span>: <span style=color:#ae81ff>0.5</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;booster&#39;</span>: <span style=color:#e6db74>&#39;gbtree&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bylevel&#39;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bynode&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bytree&#39;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;gamma&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_delta_step&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;min_child_weight&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;random_state&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;reg_alpha&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;reg_lambda&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;scale_pos_weight&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;seed&#39;</span>: <span style=color:#ae81ff>42</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;subsample&#39;</span>: <span style=color:#ae81ff>0.4</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;verbosity&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;acc&#39;</span>: <span style=color:#ae81ff>0.12304248437307332</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;balanced_acc&#39;</span>: <span style=color:#ae81ff>0.10551953202851949</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;logloss&#39;</span>: <span style=color:#ae81ff>3.4480742986458592</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;walltime&#39;</span>: <span style=color:#ae81ff>1918.593945</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;karea&#39;</span>: <span style=color:#ae81ff>0.75845582462009</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;num_round&#39;</span>: <span style=color:#ae81ff>100</span>}
</span></span><span style=display:flex><span><span style=color:#75715e># Taking the best params ^^ here is what the &#34;learning curve&#34; seems to look like</span>
</span></span><span style=display:flex><span>keep_fixed <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bylevel&#39;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bynode&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bytree&#39;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;subsample&#39;</span>: <span style=color:#ae81ff>0.4</span>,
</span></span><span style=display:flex><span> <span style=color:#75715e>#&#39;num_round&#39;: 10,</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fp<span style=color:#f92672>.</span>compare_train_test(alldf, feature_col<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;num_round&#39;</span>,
</span></span><span style=display:flex><span>                      metric_cols<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;acc&#39;</span>, <span style=color:#e6db74>&#39;train_acc&#39;</span>, <span style=color:#e6db74>&#39;balanced_acc&#39;</span>,
</span></span><span style=display:flex><span>                                  <span style=color:#e6db74>&#39;train_balanced_acc&#39;</span>],
</span></span><span style=display:flex><span>                      keep_fixed<span style=color:#f92672>=</span>keep_fixed)
</span></span></code></pre></div><p>The difference between train accuracy and test accuracy does not look crazy so I think it is safe to say this model is not overfitting .</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/project/2020-10-20-bike-share-learn-reboot/><span class=title>« Prev</span><br><span>Bike Share Learn Reboot</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2020-07-13-multi-multi-class/><span class=title>Next »</span><br><span>Notes on multi-multi-class classifiers</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>