<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Spark Weekend | My blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."><meta name=generator content="Hugo 0.103.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><meta property="og:title" content="Spark Weekend"><meta property="og:description" content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-01-23T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-23T00:00:00+00:00"><meta property="og:site_name" content="My blog"><meta itemprop=name content="Spark Weekend"><meta itemprop=description content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."><meta itemprop=datePublished content="2021-01-23T00:00:00+00:00"><meta itemprop=dateModified content="2021-01-23T00:00:00+00:00"><meta itemprop=wordCount content="3414"><meta itemprop=keywords content="spark,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Spark Weekend"><meta name=twitter:description content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">My blog</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/handy/ title="Handy page">Handy</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/post/ title="Post page">Post</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/project/ title="Side Projects page">Side Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/foo/ title="The Foos page">The Foos</a></li></ul></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POST</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/&text=Spark%20Weekend" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/&title=Spark%20Weekend" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">Spark Weekend</h1><time class="f6 mv4 dib tracked" datetime=2021-01-23T00:00:00Z>January 23, 2021</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h3 id=trying-out-spark-this-weekend>Trying out Spark this weekend</h3><p><em>These are just my casual notes from doing that, updating them as I go along.</em></p><h4 id=following-this-post-to-get-kubernetes-running-in-docker-for-mac>Following this post to get kubernetes running in Docker for mac</h4><ul><li>Per <a href=https://medium.com/faun/apache-spark-on-kubernetes-docker-for-mac-2501cc72e659>this post</a> , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings.</li><li>Kubernetes is taking quite a while to start up though . several minutes. kind of weird?</li></ul><h4 id=download-spark-image>Download spark image</h4><ul><li>From <a href=https://spark.apache.org/downloads.html>here</a></li></ul><h3 id=2021-01-24>2021-01-24</h3><h4 id=ok-backup-my-docker-images>ok backup my docker images</h4><ul><li>Per <a href=https://corgibytes.com/blog/2019/05/13/docker-for-mac-safely-reset-from-factory-defaults/>notes</a> , I backed up local docker images,</li><li>Like this&mldr;</li></ul><pre tabindex=0><code>docker save citibike-learn:0.9
# image:citibike-learn, tag:latest, image-id:1ff5cd891f00
# image:citibike-learn, tag:0.9, imageid:c8d430e84654
</code></pre><ul><li>Then I did the factory reset.</li><li>And Enabled Kubernetes and wow! Nice finally got the green light.</li><li>And restoring with <code>docker load</code> like this</li></ul><pre tabindex=0><code>docker load -i  citibike-learn-0.9.tar
</code></pre><h4 id=ok-now-i-can-continue-trying-to-get-spark-setup>Ok now I can continue trying to get spark setup..</h4><ul><li>Per the <a href=https://medium.com/faun/apache-spark-on-kubernetes-docker-for-mac-2501cc72e659>post</a> , I grabbed spark albeit <code>3.0.1</code> , instead of <code>2.x</code> ( from <a href=https://www.apache.org/dyn/closer.lua/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz>here</a> ) , because according to the <a href=https://databricks.com/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html>release notes</a> , 3.0 and 2.x are sounding very compatible.</li></ul><pre tabindex=0><code>./bin/docker-image-tool.sh -t spark-docker build
</code></pre><ul><li>&mldr; following along&mldr;</li></ul><pre tabindex=0><code>kubectl create serviceaccount spark
# serviceaccount/spark created

kubectl create clusterrolebinding spark-role --clusterrole=edit  --serviceaccount=default:spark --namespace=default
# clusterrolebinding.rbac.authorization.k8s.io/spark-role created
</code></pre><ul><li>And submitting an example job</li></ul><pre tabindex=0><code>bin/spark-submit  \
    --master k8s://https://localhost:6443  \
    --deploy-mode cluster  \
    --conf spark.executor.instances=1  \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.container.image=spark:spark-docker  \
    --class org.apache.spark.examples.SparkPi  \
    --name spark-pi  \
    local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar
</code></pre><ul><li><p>Taking <code>4 minutes</code> so far. Not sure how long this is meant to take haha.</p></li><li><p>I tried https://localhost:6443/ from my browser but got denied for now, as below&mldr;</p></li></ul><pre tabindex=0><code>{
kind: &#34;Status&#34;,
apiVersion: &#34;v1&#34;,
metadata: { },
status: &#34;Failure&#34;,
message: &#34;forbidden: User &#34;system:anonymous&#34; cannot get path &#34;/&#34;&#34;,
reason: &#34;Forbidden&#34;,
details: { },
code: 403
}
</code></pre><ul><li>I tried the <code>kubectl get pods</code> command and I can see the run time so far..</li></ul><pre tabindex=0><code>$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
spark-pi-4df4497735de91a1-driver   1/1     Running   0          6m1s
spark-pi-79033a7735deb0a4-exec-1   0/1     Pending   0          5m52s
</code></pre><ul><li>Likely something is blocking. (Actually I noticed my Dropbox was being pretty aggressive. so I paused that.)</li></ul><h5 id=enabling-port-forwarding-to-get-access-to-the-dashboard>enabling port forwarding to get access to the dashboard..</h5><pre tabindex=0><code>kubectl get pods -n kube-system
</code></pre><pre tabindex=0><code>NAME                                     READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-pdx99                  1/1     Running   0          30m
coredns-f9fd979d6-vjpfp                  1/1     Running   0          30m
etcd-docker-desktop                      1/1     Running   0          29m
kube-apiserver-docker-desktop            1/1     Running   0          29m
kube-controller-manager-docker-desktop   1/1     Running   0          29m
kube-proxy-42wws                         1/1     Running   0          30m
kube-scheduler-docker-desktop            1/1     Running   0          29m
storage-provisioner                      1/1     Running   0          29m
vpnkit-controller                        1/1     Running   0          29m
</code></pre><ul><li><p>and hmm I cant run <code>kubectl port-forward kubernetes-dashboard-7b9c7bc8c9-ckfmr 8443:8443 -n kube-system</code> because I dont have that running looks like .</p></li><li><p>Ah according to <a href=https://medium.com/backbase/kubernetes-in-local-the-easy-way-f8ef2b98be68>here</a> the kubernetes dashboard does not come <em>out of the box</em></p></li><li><p>Per <a href=https://stackoverflow.com/a/30094032/472876>here</a> tried killing</p></li></ul><pre tabindex=0><code># ./bin/spark-class org.apache.spark.deploy.Client kill &lt;master url&gt; &lt;driver ID&gt;
./bin/spark-class org.apache.spark.deploy.Client kill k8s://https://localhost:6443 spark-pi-4df4497735de91a1-driver

WARNING: This client is deprecated and will be removed in a future version of Spark
Use ./bin/spark-submit with &#34;--master spark://host:port&#34;
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

...
Exception in thread &#34;main&#34; org.apache.spark.SparkException: Invalid master URL: spark://k8s://https://localhost:6443
</code></pre><ul><li><p>Crashed&mldr; anyway just <code>Ctrl-C</code> for now</p></li><li><p>But when looking around I see per <a href=https://issues.apache.org/jira/browse/SPARK-11909>here</a> that the master url in that command should be <code>spark://localhost:6443</code> instead.</p></li><li><p>And per <a href=https://sparkbyexamples.com/spark/spark-how-to-kill-running-application/>this note</a> , yarn is mentioned too. I dont have that yet however.</p></li><li><p>TRy to get that dashboard , following from <a href=https://medium.com/backbase/kubernetes-in-local-the-easy-way-f8ef2b98be68>here</a></p></li><li><p>It is here, <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.0.5>https://github.com/kubernetes/dashboard/releases/tag/v2.0.5</a></p></li></ul><pre tabindex=0><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml

namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
</code></pre><ul><li>Hmm I did not see the dashboard with <code>kubectl get pods -n kube-system</code>, but the mentioned to look using <code>kubectl get pods --all-namespaces</code> , and I do see it indeed , in its own namespace indeed&mldr; not in the <code>kube-system</code> namespace</li></ul><pre tabindex=0><code>NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kubernetes-dashboard   kubernetes-dashboard-6f65cb5c64-kbq8d        1/1     Running   0          2m46s
</code></pre><ul><li>Not seeing anything listening on <code>8443</code> with <code>netstat -an |grep LIST</code> however, as mentioned <a href=https://medium.com/faun/apache-spark-on-kubernetes-docker-for-mac-2501cc72e659>here</a></li><li>But the other blog post is telling me to go here , http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy</li><li>After first starting a local proxy that is..</li></ul><pre tabindex=0><code>kubectl proxy
# Starting to serve on 127.0.0.1:8001
</code></pre><ul><li>As mentioned, when I visited this url, I saw the screen asking for a token.</li><li>And running the one liner ,</li></ul><pre tabindex=0><code>kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk &#39;/^deployment-controller-token-/{print $1}&#39;) | awk &#39;$1==&#34;token:&#34;{print $2}&#39;
</code></pre><ul><li>Yielded a token, which was accepted.</li></ul><h4 id=let-me-retry-that-earlier-example-job->let me retry that earlier example job ..</h4><ul><li>Since now I can look at the dashboard. Maybe I will see why that job was stalling..</li><li>trying again</li></ul><pre tabindex=0><code>bin/spark-submit  \
--master k8s://https://localhost:6443  \
--deploy-mode cluster  \
--conf spark.executor.instances=1  \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
--conf spark.kubernetes.container.image=spark:spark-docker  \
--class org.apache.spark.examples.SparkPi  \
--name spark-pi  \
local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar
</code></pre><ul><li>One of the early outputs&mldr;</li></ul><pre tabindex=0><code>21/01/24 16:03:01 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
</code></pre><ul><li>And basically now stuck in <code>Pending</code>.</li></ul><h5 id=insufficient-memory>Insufficient Memory!</h5><ul><li>Ok so when I looked around in the Dashboard, I see oddly &mldr; the first attempt could not succeed because of memory</li></ul><img src="https://s3.amazonaws.com/my-blog-content/2021-01-23-spark-weekend/Screen Shot 2021-01-24 at 4.15.17 PM-insufficient-memory.png" width=50%><ul><li>Oh and it is hanging around still blocking resources.</li></ul><pre tabindex=0><code>$ kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
default                spark-pi-4df4497735de91a1-driver             1/1     Running   0          116m
default                spark-pi-79033a7735deb0a4-exec-1             0/1     Pending   0          116m
default                spark-pi-df12a57736350578-driver             0/1     Pending   0          21m
default                spark-pi-e333f47736434a39-driver             0/1     Pending   0          6m16s
</code></pre><ul><li>So actually <code>Ctrl-C</code> was not enough to kill it.</li><li>When I look at the logs for this driver pod, I&rsquo;m seeing</li></ul><pre tabindex=0><code>21/01/24 21:23:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
</code></pre><ul><li>Not sure how to know what resources are needed though.</li></ul><h5 id=deleting-this-pod>Deleting this pod</h5><ul><li>Kind of handy , when I try deleting that pod in the dashboard , I&rsquo;m seeing a handy note that</li></ul><pre tabindex=0><code>This action is equivalent to:kubectl delete -n default pod spark-pi-4df4497735de91a1-driver
</code></pre><ul><li>And as soon as that was terminated, the Pending job is running. So yea none of my <code>Ctrl-C</code> were useful haha.</li><li>Trying that CLI delete instead then</li></ul><pre tabindex=0><code>kubectl delete -n default pod spark-pi-df12a57736350578-driver
</code></pre><ul><li>Ok that seems to have worked.</li></ul><h4 id=how-to-try-this-again-without-the-memory-issue>How to try this again without the memory issue?</h4><ul><li>not sure but&mldr;</li></ul><h4 id=read-about-the-pyspark-shell-being-in-the-base-spark-so-trying>Read about the pyspark shell being in the base spark so trying</h4><ul><li>nice ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>$</span> <span style=color:#f92672>./</span>bin<span style=color:#f92672>/</span>pyspark
</span></span><span style=display:flex><span>Python <span style=color:#ae81ff>3.7.2</span> (default, Dec <span style=color:#ae81ff>29</span> <span style=color:#ae81ff>2018</span>, <span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>04</span>)
</span></span><span style=display:flex><span>[Clang <span style=color:#ae81ff>4.0.1</span> (tags<span style=color:#f92672>/</span>RELEASE_401<span style=color:#f92672>/</span>final)] :: Anaconda, Inc<span style=color:#f92672>.</span> on darwin
</span></span><span style=display:flex><span>Type <span style=color:#e6db74>&#34;help&#34;</span>, <span style=color:#e6db74>&#34;copyright&#34;</span>, <span style=color:#e6db74>&#34;credits&#34;</span> <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;license&#34;</span> <span style=color:#66d9ef>for</span> more information<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>21</span><span style=color:#f92672>/</span><span style=color:#ae81ff>01</span><span style=color:#f92672>/</span><span style=color:#ae81ff>24</span> <span style=color:#ae81ff>17</span>:<span style=color:#ae81ff>21</span>:<span style=color:#ae81ff>27</span> WARN NativeCodeLoader: Unable to load native<span style=color:#f92672>-</span>hadoop library <span style=color:#66d9ef>for</span> your platform<span style=color:#f92672>...</span> using builtin<span style=color:#f92672>-</span>java classes where applicable
</span></span><span style=display:flex><span>Using Spark<span style=color:#e6db74>&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties</span>
</span></span><span style=display:flex><span>Setting default log level to <span style=color:#e6db74>&#34;WARN&#34;</span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>To adjust logging level use sc<span style=color:#f92672>.</span>setLogLevel(newLevel)<span style=color:#f92672>.</span> For SparkR, use setLogLevel(newLevel)<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Welcome to
</span></span><span style=display:flex><span>      ____              __
</span></span><span style=display:flex><span>     <span style=color:#f92672>/</span> __<span style=color:#f92672>/</span>__  ___ _____<span style=color:#f92672>/</span> <span style=color:#f92672>/</span>__
</span></span><span style=display:flex><span>    _\ \<span style=color:#f92672>/</span> _ \<span style=color:#f92672>/</span> _ <span style=color:#960050;background-color:#1e0010>`</span><span style=color:#f92672>/</span> __<span style=color:#f92672>/</span>  <span style=color:#e6db74>&#39;_/</span>
</span></span><span style=display:flex><span>   <span style=color:#f92672>/</span>__ <span style=color:#f92672>/</span> <span style=color:#f92672>.</span>__<span style=color:#f92672>/</span>\_,_<span style=color:#f92672>/</span>_<span style=color:#f92672>/</span> <span style=color:#f92672>/</span>_<span style=color:#f92672>/</span>\_\   version <span style=color:#ae81ff>3.0.1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>/</span>_<span style=color:#f92672>/</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Using Python version <span style=color:#ae81ff>3.7.2</span> (default, Dec <span style=color:#ae81ff>29</span> <span style=color:#ae81ff>2018</span> <span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>04</span>)
</span></span><span style=display:flex><span>SparkSession available <span style=color:#66d9ef>as</span> <span style=color:#e6db74>&#39;spark&#39;</span><span style=color:#f92672>.</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.context <span style=color:#f92672>import</span> SparkContext
</span></span><span style=display:flex><span>sc <span style=color:#f92672>=</span> SparkContext(<span style=color:#e6db74>&#39;local&#39;</span>, <span style=color:#e6db74>&#39;test&#39;</span>)
</span></span></code></pre></div><p>Oops</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;stdin&gt;&#34;</span>, line <span style=color:#ae81ff>1</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py&#34;</span>, line <span style=color:#ae81ff>133</span>, <span style=color:#f92672>in</span> __init__
</span></span><span style=display:flex><span>    SparkContext<span style=color:#f92672>.</span>_ensure_initialized(self, gateway<span style=color:#f92672>=</span>gateway, conf<span style=color:#f92672>=</span>conf)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py&#34;</span>, line <span style=color:#ae81ff>341</span>, <span style=color:#f92672>in</span> _ensure_initialized
</span></span><span style=display:flex><span>    callsite<span style=color:#f92672>.</span>function, callsite<span style=color:#f92672>.</span>file, callsite<span style=color:#f92672>.</span>linenum))
</span></span><span style=display:flex><span><span style=color:#a6e22e>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app<span style=color:#f92672>=</span>PySparkShell, master<span style=color:#f92672>=</span>local[<span style=color:#f92672>*</span>]) created by <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span> at <span style=color:#f92672>/</span>Users<span style=color:#f92672>/</span>michal<span style=color:#f92672>/</span>Downloads<span style=color:#f92672>/</span>spark<span style=color:#f92672>-</span><span style=color:#ae81ff>3.0.1</span><span style=color:#f92672>-</span>bin<span style=color:#f92672>-</span>hadoop3<span style=color:#ae81ff>.2</span><span style=color:#f92672>/</span>python<span style=color:#f92672>/</span>pyspark<span style=color:#f92672>/</span>shell<span style=color:#f92672>.</span>py:<span style=color:#ae81ff>41</span>
</span></span></code></pre></div><ul><li>oh interesting it is already pre defined</li></ul><pre tabindex=0><code>&gt;&gt;&gt; sc
&lt;SparkContext master=local[*] appName=PySparkShell&gt;
</code></pre><ul><li>Will try something basic..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>])
</span></span><span style=display:flex><span>rdd
</span></span><span style=display:flex><span><span style=color:#75715e># ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>*</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># PythonRDD[1] at RDD at PythonRDD.scala:53</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#75715e># [1, 2, 3, 4]</span>
</span></span></code></pre></div><ul><li>ok haha not quite right.</li><li>Ah duh of course have to compose/chain that..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>*</span><span style=color:#ae81ff>3</span>)<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#75715e># [3, 6, 9, 12]                                                                   </span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#75715e># [1, 2, 3, 4]</span>
</span></span></code></pre></div><ul><li>Ok excellent!</li><li>Going to look more through these docs <a href=spark.apache.org/docs/latest/api/python/index.html>here</a></li></ul><h4 id=next>Next</h4><ul><li>I would like to try some more basic transformations and actions.</li></ul><h3 id=2021-01-31>2021-01-31</h3><h4 id=try-some-things-on-this-covid19-dataset>try some things on this covid19 dataset</h4><ul><li>from <a href=https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf>here</a></li><li>This is a <code>1.59GiB</code> file , so perfect, how do I use Spark to split this up and perform some basic statistics</li><li><code>COVID-19_Case_Surveillance_Public_Use_Data.csv</code></li><li>Specifically, I think a good idea to test if random sampling this data, the <code>onset_dt</code> or onset date of symptoms, what is the onset rate by age bin, which is already binned as <code>age_group</code>.</li><li>Ah and looks like you need to be explicit with specifying a header is present.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>printSchema()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>root
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> cdc_case_earliest_dt : string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> cdc_report_dt: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> pos_spec_dt: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> onset_dt: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> current_status: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> sex: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> age_group: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> race_ethnicity_combined: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> hosp_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> icu_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> death_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> medcond_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span></code></pre></div><h3 id=2021-02-07>2021-02-07</h3><h4 id=symptomatic-by-age-group>Symptomatic by age group.</h4><ul><li>Hmm interesting that <a href=https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc>in docs</a> is says that so does that mean that all the partitions are running in parallel? How do you only run based on the number of workers you can run simultaneously?</li></ul><blockquote><p>Note: Don’t create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;age_group&#39;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[Row(age_group<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;0 - 9 Years&#39;</span>, count<span style=color:#f92672>=</span><span style=color:#ae81ff>9</span>)]                                         
</span></span></code></pre></div><ul><li>Try w/ a column that has more variation.. and <code>1000</code> rows instead.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [Row(sex=&#39;Female&#39;, count=446), Row(sex=&#39;Unknown&#39;, count=30), Row(sex=&#39;Missing&#39;, count=3), Row(sex=&#39;Male&#39;, count=520)]</span>
</span></span></code></pre></div><ul><li>And how do I apply a custom <code>apply</code> function with my group by</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>apply(foo)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;stdin&gt;&#34;</span>, line <span style=color:#ae81ff>2</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py&#34;</span>, line <span style=color:#ae81ff>70</span>, <span style=color:#f92672>in</span> apply
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Invalid udf: the udf argument must be a pandas_udf of type &#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ValueError</span>: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP<span style=color:#f92672>.</span>
</span></span></code></pre></div><ul><li>hmm oops</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> pandas_udf, PandasUDFType
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType, StringType, LongType, DoubleType, StructField
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(<span style=color:#e6db74>&#39;sex&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>                     StructField(<span style=color:#e6db74>&#39;onset_dt&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pandas_udf</span>(schema, PandasUDFType<span style=color:#f92672>.</span>GROUPED_MAP)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;stdin&gt;&#34;</span>, line <span style=color:#ae81ff>1</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py&#34;</span>, line <span style=color:#ae81ff>325</span>, <span style=color:#f92672>in</span> pandas_udf
</span></span><span style=display:flex><span>    require_minimum_pyarrow_version()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py&#34;</span>, line <span style=color:#ae81ff>54</span>, <span style=color:#f92672>in</span> require_minimum_pyarrow_version
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;it was not found.&#34;</span> <span style=color:#f92672>%</span> minimum_pyarrow_version)
</span></span><span style=display:flex><span><span style=color:#a6e22e>ImportError</span>: PyArrow <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0.15.1</span> must be installed; however, it was <span style=color:#f92672>not</span> found<span style=color:#f92672>.</span>
</span></span></code></pre></div><ul><li>hmm</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(pandars3) <span style=color:#960050;background-color:#1e0010>$</span> pip install  PyArrow
</span></span><span style=display:flex><span>Collecting PyArrow
</span></span><span style=display:flex><span>  Downloading https:<span style=color:#f92672>//</span>files<span style=color:#f92672>.</span>pythonhosted<span style=color:#f92672>.</span>org<span style=color:#f92672>/</span>packages<span style=color:#f92672>/</span><span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>5</span>f<span style=color:#f92672>/</span><span style=color:#ae81ff>1</span>fb0c604636d46257af3c3075955e860161e8c41386405467f073df73f91<span style=color:#f92672>/</span>pyarrow<span style=color:#f92672>-</span><span style=color:#ae81ff>3.0.0</span><span style=color:#f92672>-</span>cp37<span style=color:#f92672>-</span>cp37m<span style=color:#f92672>-</span>macosx_10_13_x86_64<span style=color:#f92672>.</span>whl (<span style=color:#ae81ff>14.1</span>MB)
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>100</span><span style=color:#f92672>%</span> <span style=color:#f92672>|</span><span style=color:#960050;background-color:#1e0010>████████████████████████████████</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>14.1</span>MB <span style=color:#ae81ff>1.6</span>MB<span style=color:#f92672>/</span>s
</span></span><span style=display:flex><span>Collecting numpy<span style=color:#f92672>&gt;=</span><span style=color:#ae81ff>1.16.6</span> (<span style=color:#f92672>from</span> PyArrow)
</span></span><span style=display:flex><span>  Downloading https:<span style=color:#f92672>//</span>files<span style=color:#f92672>.</span>pythonhosted<span style=color:#f92672>.</span>org<span style=color:#f92672>/</span>packages<span style=color:#f92672>/</span><span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>30</span><span style=color:#f92672>/</span>a8ce4cb0c084cc1442408807dde60f9796356ea056ca6ef81c865a3d4e62<span style=color:#f92672>/</span>numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.20.1</span><span style=color:#f92672>-</span>cp37<span style=color:#f92672>-</span>cp37m<span style=color:#f92672>-</span>macosx_10_9_x86_64<span style=color:#f92672>.</span>whl (<span style=color:#ae81ff>16.0</span>MB)
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>100</span><span style=color:#f92672>%</span> <span style=color:#f92672>|</span><span style=color:#960050;background-color:#1e0010>████████████████████████████████</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>16.0</span>MB <span style=color:#ae81ff>1.3</span>MB<span style=color:#f92672>/</span>s
</span></span><span style=display:flex><span>tensorboard <span style=color:#ae81ff>1.14.0</span> has requirement setuptools<span style=color:#f92672>&gt;=</span><span style=color:#ae81ff>41.0.0</span>, but you<span style=color:#e6db74>&#39;ll have setuptools 40.6.3 which is incompatible.</span>
</span></span><span style=display:flex><span>Installing collected packages: numpy, PyArrow
</span></span><span style=display:flex><span>  Found existing installation: numpy <span style=color:#ae81ff>1.16.0</span>
</span></span><span style=display:flex><span>    Uninstalling numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.16.0</span>:
</span></span><span style=display:flex><span>      Successfully uninstalled numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.16.0</span>
</span></span><span style=display:flex><span>Successfully installed PyArrow<span style=color:#f92672>-</span><span style=color:#ae81ff>3.0.0</span> numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.20.1</span>
</span></span></code></pre></div><ul><li>Ok cool now this worked ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> pandas_udf, PandasUDFType
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType, StringType, LongType, DoubleType, StructField
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(<span style=color:#e6db74>&#39;sex&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>                     StructField(<span style=color:#e6db74>&#39;onset_dt&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pandas_udf</span>(schema, PandasUDFType<span style=color:#f92672>.</span>GROUPED_MAP)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>apply(foo)
</span></span></code></pre></div><ul><li>Really weird error though haha&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> out<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#ae81ff>21</span><span style=color:#f92672>/</span><span style=color:#ae81ff>02</span><span style=color:#f92672>/</span><span style=color:#ae81ff>07</span> <span style=color:#ae81ff>23</span>:<span style=color:#ae81ff>33</span>:<span style=color:#ae81ff>31</span> ERROR Executor: <span style=color:#a6e22e>Exception</span> <span style=color:#f92672>in</span> task <span style=color:#ae81ff>60.0</span> <span style=color:#f92672>in</span> stage <span style=color:#ae81ff>6.0</span> (TID <span style=color:#ae81ff>205</span>)]
</span></span><span style=display:flex><span>org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>api<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>PythonException: Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>605</span>, <span style=color:#f92672>in</span> main
</span></span><span style=display:flex><span>    process()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>597</span>, <span style=color:#f92672>in</span> process
</span></span><span style=display:flex><span>    serializer<span style=color:#f92672>.</span>dump_stream(out_iter, outfile)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>255</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ArrowStreamSerializer<span style=color:#f92672>.</span>dump_stream(self, init_stream_yield_batches(), stream)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>88</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>248</span>, <span style=color:#f92672>in</span> init_stream_yield_batches
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> series <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>210</span>, <span style=color:#f92672>in</span> load_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> [self<span style=color:#f92672>.</span>arrow_to_pandas(c) <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> pa<span style=color:#f92672>.</span>Table<span style=color:#f92672>.</span>from_batches([batch])<span style=color:#f92672>.</span>itercolumns()]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>210</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>listcomp<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> [self<span style=color:#f92672>.</span>arrow_to_pandas(c) <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> pa<span style=color:#f92672>.</span>Table<span style=color:#f92672>.</span>from_batches([batch])<span style=color:#f92672>.</span>itercolumns()]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>236</span>, <span style=color:#f92672>in</span> arrow_to_pandas
</span></span><span style=display:flex><span>    s <span style=color:#f92672>=</span> super(ArrowStreamPandasUDFSerializer, self)<span style=color:#f92672>.</span>arrow_to_pandas(arrow_column)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>128</span>, <span style=color:#f92672>in</span> arrow_to_pandas
</span></span><span style=display:flex><span>    s <span style=color:#f92672>=</span> arrow_column<span style=color:#f92672>.</span>to_pandas(date_as_object<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/array.pxi&#34;</span>, line <span style=color:#ae81ff>751</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>_PandasConvertible<span style=color:#f92672>.</span>to_pandas
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/table.pxi&#34;</span>, line <span style=color:#ae81ff>224</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>ChunkedArray<span style=color:#f92672>.</span>_to_pandas
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/array.pxi&#34;</span>, line <span style=color:#ae81ff>1310</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>_array_like_to_pandas
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/error.pxi&#34;</span>, line <span style=color:#ae81ff>116</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>check_status
</span></span><span style=display:flex><span>pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>ArrowException: Unknown error: Wrapping <span style=color:#ae81ff>2020</span><span style=color:#f92672>/</span><span style=color:#ae81ff>03</span><span style=color:#f92672>/</span><span style=color:#960050;background-color:#1e0010>�</span><span style=color:#ae81ff>9</span> failed
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>api<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>BasePythonRunner<span style=color:#960050;background-color:#1e0010>$</span>ReaderIterator<span style=color:#f92672>.</span>handlePythonException(PythonRunner<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>503</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>PythonArrowOutput<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>1.</span>read(PythonArrowOutput<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>99</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>PythonArrowOutput<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>1.</span>read(PythonArrowOutput<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>49</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>api<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>BasePythonRunner<span style=color:#960050;background-color:#1e0010>$</span>ReaderIterator<span style=color:#f92672>.</span>hasNext(PythonRunner<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>456</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>InterruptibleIterator<span style=color:#f92672>.</span>hasNext(InterruptibleIterator<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>37</span>)
</span></span><span style=display:flex><span>	at scala<span style=color:#f92672>.</span>collection<span style=color:#f92672>.</span>Iterator<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>11.</span>hasNext(Iterator<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>489</span>)
</span></span><span style=display:flex><span>	at scala<span style=color:#f92672>.</span>collection<span style=color:#f92672>.</span>Iterator<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>10.</span>hasNext(Iterator<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>458</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>SparkPlan<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>getByteArrayRdd<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>1</span>(SparkPlan<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>340</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>SparkPlan<span style=color:#960050;background-color:#1e0010>$$</span>Lambda<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2055</span><span style=color:#f92672>/</span><span style=color:#ae81ff>64856516.</span>apply(Unknown Source)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>mapPartitionsInternal<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2</span>(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>872</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>mapPartitionsInternal<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2</span><span style=color:#960050;background-color:#1e0010>$</span>adapted(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>872</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#960050;background-color:#1e0010>$$</span>Lambda<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2051</span><span style=color:#f92672>/</span><span style=color:#ae81ff>1858155754.</span>apply(Unknown Source)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>MapPartitionsRDD<span style=color:#f92672>.</span>compute(MapPartitionsRDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>52</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span>computeOrReadCheckpoint(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>349</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span>iterator(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>313</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>scheduler<span style=color:#f92672>.</span>ResultTask<span style=color:#f92672>.</span>runTask(ResultTask<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>90</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>scheduler<span style=color:#f92672>.</span>Task<span style=color:#f92672>.</span>run(Task<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>127</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>executor<span style=color:#f92672>.</span>Executor<span style=color:#960050;background-color:#1e0010>$</span>TaskRunner<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>run<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>3</span>(Executor<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>446</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>executor<span style=color:#f92672>.</span>Executor<span style=color:#960050;background-color:#1e0010>$</span>TaskRunner<span style=color:#960050;background-color:#1e0010>$$</span>Lambda<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2018</span><span style=color:#f92672>/</span><span style=color:#ae81ff>1084937392.</span>apply(Unknown Source)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>Utils<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#f92672>.</span>tryWithSafeFinally(Utils<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>1377</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>executor<span style=color:#f92672>.</span>Executor<span style=color:#960050;background-color:#1e0010>$</span>TaskRunner<span style=color:#f92672>.</span>run(Executor<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>449</span>)
</span></span><span style=display:flex><span>	at java<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>concurrent<span style=color:#f92672>.</span>ThreadPoolExecutor<span style=color:#f92672>.</span>runWorker(ThreadPoolExecutor<span style=color:#f92672>.</span>java:<span style=color:#ae81ff>1142</span>)
</span></span><span style=display:flex><span>	at java<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>concurrent<span style=color:#f92672>.</span>ThreadPoolExecutor<span style=color:#960050;background-color:#1e0010>$</span>Worker<span style=color:#f92672>.</span>run(ThreadPoolExecutor<span style=color:#f92672>.</span>java:<span style=color:#ae81ff>617</span>)
</span></span><span style=display:flex><span>	at java<span style=color:#f92672>.</span>lang<span style=color:#f92672>.</span>Thread<span style=color:#f92672>.</span>run(Thread<span style=color:#f92672>.</span>java:<span style=color:#ae81ff>745</span>)
</span></span></code></pre></div><h3 id=2021-02-20>2021-02-20</h3><h4 id=going-to-attempt-to-use-ipython-w-pyspark>Going to attempt to use ipython w/ pyspark</h4><ul><li>According to <a href=https://stackoverflow.com/questions/31862293/how-to-load-ipython-shell-with-pyspark#31863595>stackoverflow</a></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>PYSPARK_DRIVER_PYTHON<span style=color:#f92672>=</span>ipython <span style=color:#f92672>./</span>bin<span style=color:#f92672>/</span>pyspark
</span></span></code></pre></div><ul><li>Ok nice worked. Just had to make sure to <code>source activate pandars3</code> my conda environment which actually has <code>ipython</code> ..</li></ul><h4 id=hmm-maybe-since-i-had-errors-w-group-by--i-can-try---reducebykey-intead>Hmm maybe since i had errors w/ group by , I can try <code>reduceByKey</code> intead?</h4><ul><li>oh actually, when looking at the doc for the group by with <code>help(df.groupBy('sex'))</code> , I read in the <code>apply</code> description that it is depracated and <code>applyInPandas</code> is recommended instead.</li><li>And in the <a href=https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-function-apis>apache spark doc here</a> , I&rsquo;m reading that " Using PandasUDFType will be deprecated in the future." so then the complicated decorator looking code I was trying above, maybe that is getting phased out anyway.</li><li>The only thing new here is that I need to pass the schema of the dataframe to <code>applyInPandas</code></li><li>My particualr dataset is actually all categorical data and dates.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)     
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType, StringType, LongType, DoubleType, StructField
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let me try to treat them all as nullable strings for now...</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(x, StringType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>=> ok now error I got is actually more clear&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>PythonException:
</span></span><span style=display:flex><span>  An exception was thrown <span style=color:#f92672>from</span> the Python worker<span style=color:#f92672>.</span> Please see the stack trace below<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>605</span>, <span style=color:#f92672>in</span> main
</span></span><span style=display:flex><span>    process()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>597</span>, <span style=color:#f92672>in</span> process
</span></span><span style=display:flex><span>    serializer<span style=color:#f92672>.</span>dump_stream(out_iter, outfile)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>255</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ArrowStreamSerializer<span style=color:#f92672>.</span>dump_stream(self, init_stream_yield_batches(), stream)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>88</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>248</span>, <span style=color:#f92672>in</span> init_stream_yield_batches
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> series <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>429</span>, <span style=color:#f92672>in</span> mapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(keys, vals)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>175</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>lambda</span><span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>lambda</span> k, v: [(wrapped(k, v), to_arrow_type(return_type))]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>167</span>, <span style=color:#f92672>in</span> wrapped
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pandas.DataFrame, but is </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(type(result)))
</span></span><span style=display:flex><span><span style=color:#a6e22e>TypeError</span>: Return type of the user<span style=color:#f92672>-</span>defined function should be pandas<span style=color:#f92672>.</span>DataFrame, but <span style=color:#f92672>is</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>class</span> <span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>pandas</span><span style=color:#f92672>.</span>core<span style=color:#f92672>.</span>series<span style=color:#f92672>.</span>Series<span style=color:#e6db74>&#39;&gt;</span>
</span></span></code></pre></div><ul><li>So let me make sure to return a dataframe in my <code>foo</code> func</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>limit(<span style=color:#ae81ff>1</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(x, StringType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>Now getting the error..</li></ul><pre tabindex=0><code>PythonException:
  An exception was thrown from the Python worker. Please see the stack trace below.
  ...
AttributeError: &#39;DataFrame&#39; object has no attribute &#39;limit&#39;
</code></pre><ul><li>Hmm so literally the input is a vanilla pandas dataframe I think oh that&rsquo;s why!</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(x, StringType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>hmm..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>PythonException:
</span></span><span style=display:flex><span>  An exception was thrown <span style=color:#f92672>from</span> the Python worker<span style=color:#f92672>.</span> Please see the stack trace below<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>605</span>, <span style=color:#f92672>in</span> main
</span></span><span style=display:flex><span>    process()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>597</span>, <span style=color:#f92672>in</span> process
</span></span><span style=display:flex><span>    serializer<span style=color:#f92672>.</span>dump_stream(out_iter, outfile)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>255</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ArrowStreamSerializer<span style=color:#f92672>.</span>dump_stream(self, init_stream_yield_batches(), stream)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>88</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>248</span>, <span style=color:#f92672>in</span> init_stream_yield_batches
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> series <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>429</span>, <span style=color:#f92672>in</span> mapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(keys, vals)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>175</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>lambda</span><span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>lambda</span> k, v: [(wrapped(k, v), to_arrow_type(return_type))]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>160</span>, <span style=color:#f92672>in</span> wrapped
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> f(pd<span style=color:#f92672>.</span>concat(value_series, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py&#34;</span>, line <span style=color:#ae81ff>107</span>, <span style=color:#f92672>in</span> wrapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(<span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;ipython-input-54-736ec161f4f7&gt;&#34;</span>, line <span style=color:#ae81ff>4</span>, <span style=color:#f92672>in</span> foo
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/frame.py&#34;</span>, line <span style=color:#ae81ff>392</span>, <span style=color:#f92672>in</span> __init__
</span></span><span style=display:flex><span>    mgr <span style=color:#f92672>=</span> init_dict(data, index, columns, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>212</span>, <span style=color:#f92672>in</span> init_dict
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> arrays_to_mgr(arrays, data_names, index, columns, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>56</span>, <span style=color:#f92672>in</span> arrays_to_mgr
</span></span><span style=display:flex><span>    arrays <span style=color:#f92672>=</span> _homogenize(arrays, index, dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>277</span>, <span style=color:#f92672>in</span> _homogenize
</span></span><span style=display:flex><span>    raise_cast_failure<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>642</span>, <span style=color:#f92672>in</span> sanitize_array
</span></span><span style=display:flex><span>    value, len(index), dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py&#34;</span>, line <span style=color:#ae81ff>1187</span>, <span style=color:#f92672>in</span> construct_1d_arraylike_from_scalar
</span></span><span style=display:flex><span>    subarr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>empty(length, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span><span style=color:#a6e22e>TypeError</span>: Cannot interpret <span style=color:#e6db74>&#39;&lt;attribute &#39;</span>dtype<span style=color:#e6db74>&#39; of &#39;</span>numpy<span style=color:#f92672>.</span>generic<span style=color:#e6db74>&#39; objects&gt;&#39;</span> <span style=color:#66d9ef>as</span> a data type
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(<span style=color:#e6db74>&#39;sex&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>                     StructField(<span style=color:#e6db74>&#39;count&#39;</span>, LongType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>group to try the string schema usage instead</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;, &#39;</span><span style=color:#f92672>.</span>join([<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74> string&#39;</span> <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns]); schema                                                               
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;cdc_case_earliest_dt string, cdc_report_dt string, pos_spec_dt string, onset_dt string, current_status string, sex string, age_group string, race_ethnicity_combined string, hosp_yn string, icu_yn string, death_yn string, medcond_yn string&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li><p>Dang same error. Maybe doesnt like string type group bys?</p></li><li><p>Randomly reading this may be something to do w/ old pandas version?</p></li></ul><pre tabindex=0><code>
In [68]: pd.__version__                                                                                                                
Out[68]: &#39;0.24.2&#39;
</code></pre><ul><li>I upgraded to <code>1.0.5</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;sex string, count int&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>now a different error..</li></ul><pre tabindex=0><code>pyarrow.lib.ArrowException: Unknown error: Wrapping 2020/03/�6 failed
</code></pre><ul><li>Makes me think I have some garbage data</li><li>Trying the 10 line datafile i have instead</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># everything else is the same ..</span>
</span></span></code></pre></div><ul><li>WOw now a scala/java error..</li></ul><pre tabindex=0><code>21/02/20 22:19:32 ERROR Executor: Exception in task 60.0 in stage 8.0 (TID 406)]
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
...
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)
at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)
at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
at org.apache.spark.sql.execution.SparkPlan$$Lambda$2055/1769623532.apply(Unknown Source)
at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
at org.apache.spark.rdd.RDD$$Lambda$2051/917090051.apply(Unknown Source)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
at org.apache.spark.scheduler.Task.run(Task.scala:127)
at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
at org.apache.spark.executor.Executor$TaskRunner$$Lambda$2018/644307005.apply(Unknown Source)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
at java.io.DataInputStream.readInt(DataInputStream.java:392)
at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
... 22 more



21/02/20 22:19:32 ERROR TaskSetManager: Task 159 in stage 8.0 failed 1 times; aborting job
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-3-e6065af68166&gt; in &lt;module&gt;
     15 schema = &#39;sex string, count int&#39;
     16 #
---&gt; 17 df.groupBy(&#39;sex&#39;).applyInPandas(foo, schema).collect()
     18

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py in collect(self)
    594         &#34;&#34;&#34;
    595         with SCCallSiteSync(self._sc) as css:
--&gt; 596             sock_info = self._jdf.collectToPython()
    597         return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))
    598

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306
   1307         for temp_arg in temp_args:

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &#34;An error occurred while calling {0}{1}{2}.\n&#34;.
--&gt; 328                     format(target_id, &#34;.&#34;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o148.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 159 in stage 8.0 failed 1 times, most recent failure: Lost task 159.0 in stage 8.0 (TID 409, 192.168.16.173, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
</code></pre><ul><li>hahaha that is great.</li></ul><h4 id=the-toy-example-does-work-though>The toy example does work though</h4><ul><li>from the <a href=https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#grouped-map>docs</a></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(
</span></span><span style=display:flex><span>    [(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1.0</span>), (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2.0</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3.0</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>5.0</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>10.0</span>)],
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;v&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>subtract_mean</span>(pdf):
</span></span><span style=display:flex><span>    <span style=color:#75715e># pdf is a pandas.DataFrame</span>
</span></span><span style=display:flex><span>    v <span style=color:#f92672>=</span> pdf<span style=color:#f92672>.</span>v
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pdf<span style=color:#f92672>.</span>assign(v<span style=color:#f92672>=</span>v <span style=color:#f92672>-</span> v<span style=color:#f92672>.</span>mean())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#34;id&#34;</span>)<span style=color:#f92672>.</span>applyInPandas(subtract_mean, schema<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;id long, v double&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>+----+----+</span>                                                                     
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  id<span style=color:#f92672>|</span>   v<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+----+----+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>   <span style=color:#ae81ff>1</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>0.0</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>   <span style=color:#ae81ff>2</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>0.0</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+----+----+</span>
</span></span></code></pre></div><ul><li>Hmm so my guess is the string group by is not appreciated..?</li></ul><h5 id=uummm-tried-again-w-the-small-file-and-this-time-worked-well-didnt-crash-at-least>uummm tried again w/ the small file and this time worked&mldr; well didnt crash at least..</h5><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;sex string, count int&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><pre tabindex=0><code>[Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=3),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=1),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2)]
</code></pre><ul><li>But now this output looks like well not what I would expect.</li><li>I expect two rows since you know, this is a group by. So hmm</li><li>But in any case, at least it is not crashing! so major improvement.</li><li>Hmm unless this is a partitioned group by&mldr; hmm that would be exciting. So the group by has to be combined?</li><li>So could it be I have <code>12</code> partitions here? But the file only has <code>9</code> rows. Weird.</li></ul><h4 id=oh-the-apply-func-can-take-the-key-as-an-arg->oh the apply func can take the key as an arg ?</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(key, dfx):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        key: tuple of the group by keys.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dfx: pandas df for the given group by key.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;sex string, count int&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><ul><li>result is same, but since using <code>show()</code> instead of <code>collect()</code> this time, the output looks slightly different</li><li>Still don&rsquo;t know why more than two rows though ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>+------+-----+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>   sex<span style=color:#f92672>|</span>count<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+------+-----+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>3</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  Male<span style=color:#f92672>|</span>    <span style=color:#ae81ff>2</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  Male<span style=color:#f92672>|</span>    <span style=color:#ae81ff>2</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  Male<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+------+-----+</span>
</span></span><span style=display:flex><span>only showing top <span style=color:#ae81ff>20</span> rows
</span></span></code></pre></div><ul class=pa0><li class=list><a href=/tags/spark class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">spark</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://michal.piekarczyk.xyz/>&copy; My blog 2022</a><div></div></div></footer></body></html>