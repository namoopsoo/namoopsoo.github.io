<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Spark Weekend | michal.piekarczyk.xyz</title><meta name=keywords content="spark"><meta name=description content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Spark Weekend"><meta property="og:description" content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-01-23T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-23T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Spark Weekend"><meta name=twitter:description content="Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.
Following this post to get kubernetes running in Docker for mac Per this post , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this&mldr; docker save citibike-learn:0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Spark Weekend","item":"https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Spark Weekend","name":"Spark Weekend","description":"Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.\nFollowing this post to get kubernetes running in Docker for mac Per this post , I just ticked the \u0026ldquo;Enable Kubernetes\u0026rdquo; option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this\u0026hellip; docker save citibike-learn:0.","keywords":["spark"],"articleBody":"Trying out Spark this weekend These are just my casual notes from doing that, updating them as I go along.\nFollowing this post to get kubernetes running in Docker for mac Per this post , I just ticked the “Enable Kubernetes” option in the docker settings. Kubernetes is taking quite a while to start up though . several minutes. kind of weird? Download spark image From here 2021-01-24 ok backup my docker images Per notes , I backed up local docker images, Like this… docker save citibike-learn:0.9 # image:citibike-learn, tag:latest, image-id:1ff5cd891f00 # image:citibike-learn, tag:0.9, imageid:c8d430e84654 Then I did the factory reset. And Enabled Kubernetes and wow! Nice finally got the green light. And restoring with docker load like this docker load -i citibike-learn-0.9.tar Ok now I can continue trying to get spark setup.. Per the post , I grabbed spark albeit 3.0.1 , instead of 2.x ( from here ) , because according to the release notes , 3.0 and 2.x are sounding very compatible. ./bin/docker-image-tool.sh -t spark-docker build … following along… kubectl create serviceaccount spark # serviceaccount/spark created kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default # clusterrolebinding.rbac.authorization.k8s.io/spark-role created And submitting an example job bin/spark-submit \\ --master k8s://https://localhost:6443 \\ --deploy-mode cluster \\ --conf spark.executor.instances=1 \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.container.image=spark:spark-docker \\ --class org.apache.spark.examples.SparkPi \\ --name spark-pi \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar Taking 4 minutes so far. Not sure how long this is meant to take haha.\nI tried https://localhost:6443/ from my browser but got denied for now, as below…\n{ kind: \"Status\", apiVersion: \"v1\", metadata: { }, status: \"Failure\", message: \"forbidden: User \"system:anonymous\" cannot get path \"/\"\", reason: \"Forbidden\", details: { }, code: 403 } I tried the kubectl get pods command and I can see the run time so far.. $ kubectl get pods NAME READY STATUS RESTARTS AGE spark-pi-4df4497735de91a1-driver 1/1 Running 0 6m1s spark-pi-79033a7735deb0a4-exec-1 0/1 Pending 0 5m52s Likely something is blocking. (Actually I noticed my Dropbox was being pretty aggressive. so I paused that.) enabling port forwarding to get access to the dashboard.. kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-f9fd979d6-pdx99 1/1 Running 0 30m coredns-f9fd979d6-vjpfp 1/1 Running 0 30m etcd-docker-desktop 1/1 Running 0 29m kube-apiserver-docker-desktop 1/1 Running 0 29m kube-controller-manager-docker-desktop 1/1 Running 0 29m kube-proxy-42wws 1/1 Running 0 30m kube-scheduler-docker-desktop 1/1 Running 0 29m storage-provisioner 1/1 Running 0 29m vpnkit-controller 1/1 Running 0 29m and hmm I cant run kubectl port-forward kubernetes-dashboard-7b9c7bc8c9-ckfmr 8443:8443 -n kube-system because I dont have that running looks like .\nAh according to here the kubernetes dashboard does not come out of the box\nPer here tried killing\n# ./bin/spark-class org.apache.spark.deploy.Client kill ./bin/spark-class org.apache.spark.deploy.Client kill k8s://https://localhost:6443 spark-pi-4df4497735de91a1-driver WARNING: This client is deprecated and will be removed in a future version of Spark Use ./bin/spark-submit with \"--master spark://host:port\" log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. ... Exception in thread \"main\" org.apache.spark.SparkException: Invalid master URL: spark://k8s://https://localhost:6443 Crashed… anyway just Ctrl-C for now\nBut when looking around I see per here that the master url in that command should be spark://localhost:6443 instead.\nAnd per this note , yarn is mentioned too. I dont have that yet however.\nTRy to get that dashboard , following from here\nIt is here, https://github.com/kubernetes/dashboard/releases/tag/v2.0.5\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created Hmm I did not see the dashboard with kubectl get pods -n kube-system, but the mentioned to look using kubectl get pods --all-namespaces , and I do see it indeed , in its own namespace indeed… not in the kube-system namespace NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard kubernetes-dashboard-6f65cb5c64-kbq8d 1/1 Running 0 2m46s Not seeing anything listening on 8443 with netstat -an |grep LIST however, as mentioned here But the other blog post is telling me to go here , http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy After first starting a local proxy that is.. kubectl proxy # Starting to serve on 127.0.0.1:8001 As mentioned, when I visited this url, I saw the screen asking for a token. And running the one liner , kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk '/^deployment-controller-token-/{print $1}') | awk '$1==\"token:\"{print $2}' Yielded a token, which was accepted. let me retry that earlier example job .. Since now I can look at the dashboard. Maybe I will see why that job was stalling.. trying again bin/spark-submit \\ --master k8s://https://localhost:6443 \\ --deploy-mode cluster \\ --conf spark.executor.instances=1 \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.container.image=spark:spark-docker \\ --class org.apache.spark.examples.SparkPi \\ --name spark-pi \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar One of the early outputs… 21/01/24 16:03:01 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image. And basically now stuck in Pending. Insufficient Memory! Ok so when I looked around in the Dashboard, I see oddly … the first attempt could not succeed because of memory Oh and it is hanging around still blocking resources. $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default spark-pi-4df4497735de91a1-driver 1/1 Running 0 116m default spark-pi-79033a7735deb0a4-exec-1 0/1 Pending 0 116m default spark-pi-df12a57736350578-driver 0/1 Pending 0 21m default spark-pi-e333f47736434a39-driver 0/1 Pending 0 6m16s So actually Ctrl-C was not enough to kill it. When I look at the logs for this driver pod, I’m seeing 21/01/24 21:23:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Not sure how to know what resources are needed though. Deleting this pod Kind of handy , when I try deleting that pod in the dashboard , I’m seeing a handy note that This action is equivalent to:kubectl delete -n default pod spark-pi-4df4497735de91a1-driver And as soon as that was terminated, the Pending job is running. So yea none of my Ctrl-C were useful haha. Trying that CLI delete instead then kubectl delete -n default pod spark-pi-df12a57736350578-driver Ok that seems to have worked. How to try this again without the memory issue? not sure but… Read about the pyspark shell being in the base spark so trying nice .. $ ./bin/pyspark Python 3.7.2 (default, Dec 29 2018, 00:00:04) [Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. 21/01/24 17:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 3.0.1 /_/ Using Python version 3.7.2 (default, Dec 29 2018 00:00:04) SparkSession available as 'spark'. from pyspark.context import SparkContext sc = SparkContext('local', 'test') Oops\nTraceback (most recent call last): File \"\", line 1, in \u003cmodule\u003e File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\", line 133, in __init__ SparkContext._ensure_initialized(self, gateway=gateway, conf=conf) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\", line 341, in _ensure_initialized callsite.function, callsite.file, callsite.linenum)) ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by \u003cmodule\u003e at /Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/shell.py:41 oh interesting it is already pre defined \u003e\u003e\u003e sc Will try something basic.. rdd = sc.parallelize([1, 2, 3, 4]) rdd # ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262 rdd.map(lambda x: x*3) # PythonRDD[1] at RDD at PythonRDD.scala:53 rdd.collect() # [1, 2, 3, 4] ok haha not quite right. Ah duh of course have to compose/chain that.. rdd.map(lambda x: x*3).collect() # [3, 6, 9, 12] rdd.collect() # [1, 2, 3, 4] Ok excellent! Going to look more through these docs here Next I would like to try some more basic transformations and actions. 2021-01-31 try some things on this covid19 dataset from here This is a 1.59GiB file , so perfect, how do I use Spark to split this up and perform some basic statistics COVID-19_Case_Surveillance_Public_Use_Data.csv Specifically, I think a good idea to test if random sampling this data, the onset_dt or onset date of symptoms, what is the onset rate by age bin, which is already binned as age_group. Ah and looks like you need to be explicit with specifying a header is present. workdir = '/Users/michal/Downloads/' loc = f'{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head.csv' df = spark.read.option(\"header\",True).csv(loc) df.printSchema() root |-- cdc_case_earliest_dt : string (nullable = true) |-- cdc_report_dt: string (nullable = true) |-- pos_spec_dt: string (nullable = true) |-- onset_dt: string (nullable = true) |-- current_status: string (nullable = true) |-- sex: string (nullable = true) |-- age_group: string (nullable = true) |-- race_ethnicity_combined: string (nullable = true) |-- hosp_yn: string (nullable = true) |-- icu_yn: string (nullable = true) |-- death_yn: string (nullable = true) |-- medcond_yn: string (nullable = true) 2021-02-07 Symptomatic by age group. Hmm interesting that in docs is says that so does that mean that all the partitions are running in parallel? How do you only run based on the number of workers you can run simultaneously? Note: Don’t create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems.\ndf.groupBy('age_group').count().collect() [Row(age_group='0 - 9 Years', count=9)] Try w/ a column that has more variation.. and 1000 rows instead. loc = f'{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv' df = spark.read.option(\"header\",True).csv(loc) df.groupBy('sex').count().collect() # [Row(sex='Female', count=446), Row(sex='Unknown', count=30), Row(sex='Missing', count=3), Row(sex='Male', count=520)] And how do I apply a custom apply function with my group by def foo(dfx): return dfx.count() df.groupBy('sex').apply(foo) Traceback (most recent call last): File \"\", line 2, in \u003cmodule\u003e File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\", line 70, in apply raise ValueError(\"Invalid udf: the udf argument must be a pandas_udf of type \" ValueError: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP. hmm oops from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import StructType, StringType, LongType, DoubleType, StructField schema = StructType([StructField('sex', StringType(), True), StructField('onset_dt', StringType(), True)]) @pandas_udf(schema, PandasUDFType.GROUPED_MAP) def foo(dfx): return dfx.count() Traceback (most recent call last): File \"\", line 1, in \u003cmodule\u003e File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\", line 325, in pandas_udf require_minimum_pyarrow_version() File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\", line 54, in require_minimum_pyarrow_version \"it was not found.\" % minimum_pyarrow_version) ImportError: PyArrow \u003e= 0.15.1 must be installed; however, it was not found. hmm (pandars3) $ pip install PyArrow Collecting PyArrow Downloading https://files.pythonhosted.org/packages/68/5f/1fb0c604636d46257af3c3075955e860161e8c41386405467f073df73f91/pyarrow-3.0.0-cp37-cp37m-macosx_10_13_x86_64.whl (14.1MB) 100% |████████████████████████████████| 14.1MB 1.6MB/s Collecting numpy\u003e=1.16.6 (from PyArrow) Downloading https://files.pythonhosted.org/packages/68/30/a8ce4cb0c084cc1442408807dde60f9796356ea056ca6ef81c865a3d4e62/numpy-1.20.1-cp37-cp37m-macosx_10_9_x86_64.whl (16.0MB) 100% |████████████████████████████████| 16.0MB 1.3MB/s tensorboard 1.14.0 has requirement setuptools\u003e=41.0.0, but you'll have setuptools 40.6.3 which is incompatible. Installing collected packages: numpy, PyArrow Found existing installation: numpy 1.16.0 Uninstalling numpy-1.16.0: Successfully uninstalled numpy-1.16.0 Successfully installed PyArrow-3.0.0 numpy-1.20.1 Ok cool now this worked .. from pyspark.sql.functions import pandas_udf, PandasUDFType from pyspark.sql.types import StructType, StringType, LongType, DoubleType, StructField schema = StructType([StructField('sex', StringType(), True), StructField('onset_dt', StringType(), True)]) @pandas_udf(schema, PandasUDFType.GROUPED_MAP) def foo(dfx): return dfx.count() workdir = '/Users/michal/Downloads/' loc = f'{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv' df = spark.read.option(\"header\",True).csv(loc) df.groupBy('sex').count().collect() # out = df.groupBy('sex').apply(foo) Really weird error though haha… \u003e\u003e\u003e out.collect() 21/02/07 23:33:31 ERROR Executor: Exception in task 60.0 in stage 6.0 (TID 205)] org.apache.spark.api.python.PythonException: Traceback (most recent call last): File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main process() File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process serializer.dump_stream(out_iter, outfile) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream for batch in iterator: File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches for series in iterator: File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 210, in load_stream yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()] File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 210, in \u003clistcomp\u003e yield [self.arrow_to_pandas(c) for c in pa.Table.from_batches([batch]).itercolumns()] File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 236, in arrow_to_pandas s = super(ArrowStreamPandasUDFSerializer, self).arrow_to_pandas(arrow_column) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 128, in arrow_to_pandas s = arrow_column.to_pandas(date_as_object=True) File \"pyarrow/array.pxi\", line 751, in pyarrow.lib._PandasConvertible.to_pandas File \"pyarrow/table.pxi\", line 224, in pyarrow.lib.ChunkedArray._to_pandas File \"pyarrow/array.pxi\", line 1310, in pyarrow.lib._array_like_to_pandas File \"pyarrow/error.pxi\", line 116, in pyarrow.lib.check_status pyarrow.lib.ArrowException: Unknown error: Wrapping 2020/03/�9 failed at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99) at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49) at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340) at org.apache.spark.sql.execution.SparkPlan$$Lambda$2055/64856516.apply(Unknown Source) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872) at org.apache.spark.rdd.RDD$$Lambda$2051/1858155754.apply(Unknown Source) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.executor.Executor$TaskRunner$$Lambda$2018/1084937392.apply(Unknown Source) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 2021-02-20 Going to attempt to use ipython w/ pyspark According to stackoverflow PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark Ok nice worked. Just had to make sure to source activate pandars3 my conda environment which actually has ipython .. Hmm maybe since i had errors w/ group by , I can try reduceByKey intead? oh actually, when looking at the doc for the group by with help(df.groupBy('sex')) , I read in the apply description that it is depracated and applyInPandas is recommended instead. And in the apache spark doc here , I’m reading that \" Using PandasUDFType will be deprecated in the future.\" so then the complicated decorator looking code I was trying above, maybe that is getting phased out anyway. The only thing new here is that I need to pass the schema of the dataframe to applyInPandas My particualr dataset is actually all categorical data and dates. def foo(dfx): return dfx.count() # workdir = '/Users/michal/Downloads/' loc = f'{workdir}/COVID-19_Case_Surveillance_Public_Use_Data.head.csv' df = spark.read.option(\"header\",True).csv(loc) from pyspark.sql.types import StructType, StringType, LongType, DoubleType, StructField # Let me try to treat them all as nullable strings for now... schema = StructType([StructField(x, StringType(), True) for x in df.columns ]) df.groupBy('sex').applyInPandas(foo, schema).collect() =\u003e ok now error I got is actually more clear… PythonException: An exception was thrown from the Python worker. Please see the stack trace below. Traceback (most recent call last): File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main process() File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process serializer.dump_stream(out_iter, outfile) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream for batch in iterator: File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches for series in iterator: File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 429, in mapper return f(keys, vals) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in \u003clambda\u003e return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))] File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in wrapped \"pandas.DataFrame, but is {}\".format(type(result))) TypeError: Return type of the user-defined function should be pandas.DataFrame, but is \u003cclass 'pandas.core.series.Series'\u003e So let me make sure to return a dataframe in my foo func import pandas as pd def foo(dfx): # This group by key key = dfx.limit(1)[0].sex return pd.DataFrame({'sex': key, 'count': dfx.count()}) # schema = StructType([StructField(x, StringType(), True) for x in df.columns ]) # df.groupBy('sex').applyInPandas(foo, schema).collect() Now getting the error.. PythonException: An exception was thrown from the Python worker. Please see the stack trace below. ... AttributeError: 'DataFrame' object has no attribute 'limit' Hmm so literally the input is a vanilla pandas dataframe I think oh that’s why! def foo(dfx): # This group by key key = dfx.iloc[0].sex return pd.DataFrame({'sex': key, 'count': dfx.count()}) # schema = StructType([StructField(x, StringType(), True) for x in df.columns ]) df.groupBy('sex').applyInPandas(foo, schema).collect() hmm.. PythonException: An exception was thrown from the Python worker. Please see the stack trace below. Traceback (most recent call last): File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main process() File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process serializer.dump_stream(out_iter, outfile) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream for batch in iterator: File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches for series in iterator: File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 429, in mapper return f(keys, vals) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in \u003clambda\u003e return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))] File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 160, in wrapped result = f(pd.concat(value_series, axis=1)) File \"/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper return f(*args, **kwargs) File \"\", line 4, in foo File \"/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/frame.py\", line 392, in __init__ mgr = init_dict(data, index, columns, dtype=dtype) File \"/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\", line 212, in init_dict return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype) File \"/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\", line 56, in arrays_to_mgr arrays = _homogenize(arrays, index, dtype) File \"/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\", line 277, in _homogenize raise_cast_failure=False) File \"/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py\", line 642, in sanitize_array value, len(index), dtype) File \"/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\", line 1187, in construct_1d_arraylike_from_scalar subarr = np.empty(length, dtype=dtype) TypeError: Cannot interpret '","wordCount":"3414","inLanguage":"en","datePublished":"2021-01-23T00:00:00Z","dateModified":"2021-01-23T00:00:00Z","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Spark Weekend</h1><div class=post-meta><span title='2021-01-23 00:00:00 +0000 UTC'>January 23, 2021</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;3414 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#trying-out-spark-this-weekend>Trying out Spark this weekend</a></li><li><a href=#2021-01-24>2021-01-24</a></li><li><a href=#2021-01-31>2021-01-31</a></li><li><a href=#2021-02-07>2021-02-07</a></li><li><a href=#2021-02-20>2021-02-20</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=trying-out-spark-this-weekend>Trying out Spark this weekend<a hidden class=anchor aria-hidden=true href=#trying-out-spark-this-weekend>#</a></h3><p><em>These are just my casual notes from doing that, updating them as I go along.</em></p><h4 id=following-this-post-to-get-kubernetes-running-in-docker-for-mac>Following this post to get kubernetes running in Docker for mac<a hidden class=anchor aria-hidden=true href=#following-this-post-to-get-kubernetes-running-in-docker-for-mac>#</a></h4><ul><li>Per <a href=https://medium.com/faun/apache-spark-on-kubernetes-docker-for-mac-2501cc72e659>this post</a> , I just ticked the &ldquo;Enable Kubernetes&rdquo; option in the docker settings.</li><li>Kubernetes is taking quite a while to start up though . several minutes. kind of weird?</li></ul><h4 id=download-spark-image>Download spark image<a hidden class=anchor aria-hidden=true href=#download-spark-image>#</a></h4><ul><li>From <a href=https://spark.apache.org/downloads.html>here</a></li></ul><h3 id=2021-01-24>2021-01-24<a hidden class=anchor aria-hidden=true href=#2021-01-24>#</a></h3><h4 id=ok-backup-my-docker-images>ok backup my docker images<a hidden class=anchor aria-hidden=true href=#ok-backup-my-docker-images>#</a></h4><ul><li>Per <a href=https://corgibytes.com/blog/2019/05/13/docker-for-mac-safely-reset-from-factory-defaults/>notes</a> , I backed up local docker images,</li><li>Like this&mldr;</li></ul><pre tabindex=0><code>docker save citibike-learn:0.9
# image:citibike-learn, tag:latest, image-id:1ff5cd891f00
# image:citibike-learn, tag:0.9, imageid:c8d430e84654
</code></pre><ul><li>Then I did the factory reset.</li><li>And Enabled Kubernetes and wow! Nice finally got the green light.</li><li>And restoring with <code>docker load</code> like this</li></ul><pre tabindex=0><code>docker load -i  citibike-learn-0.9.tar
</code></pre><h4 id=ok-now-i-can-continue-trying-to-get-spark-setup>Ok now I can continue trying to get spark setup..<a hidden class=anchor aria-hidden=true href=#ok-now-i-can-continue-trying-to-get-spark-setup>#</a></h4><ul><li>Per the <a href=https://medium.com/faun/apache-spark-on-kubernetes-docker-for-mac-2501cc72e659>post</a> , I grabbed spark albeit <code>3.0.1</code> , instead of <code>2.x</code> ( from <a href=https://www.apache.org/dyn/closer.lua/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz>here</a> ) , because according to the <a href=https://databricks.com/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html>release notes</a> , 3.0 and 2.x are sounding very compatible.</li></ul><pre tabindex=0><code>./bin/docker-image-tool.sh -t spark-docker build
</code></pre><ul><li>&mldr; following along&mldr;</li></ul><pre tabindex=0><code>kubectl create serviceaccount spark
# serviceaccount/spark created

kubectl create clusterrolebinding spark-role --clusterrole=edit  --serviceaccount=default:spark --namespace=default
# clusterrolebinding.rbac.authorization.k8s.io/spark-role created
</code></pre><ul><li>And submitting an example job</li></ul><pre tabindex=0><code>bin/spark-submit  \
    --master k8s://https://localhost:6443  \
    --deploy-mode cluster  \
    --conf spark.executor.instances=1  \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.container.image=spark:spark-docker  \
    --class org.apache.spark.examples.SparkPi  \
    --name spark-pi  \
    local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar
</code></pre><ul><li><p>Taking <code>4 minutes</code> so far. Not sure how long this is meant to take haha.</p></li><li><p>I tried https://localhost:6443/ from my browser but got denied for now, as below&mldr;</p></li></ul><pre tabindex=0><code>{
kind: &#34;Status&#34;,
apiVersion: &#34;v1&#34;,
metadata: { },
status: &#34;Failure&#34;,
message: &#34;forbidden: User &#34;system:anonymous&#34; cannot get path &#34;/&#34;&#34;,
reason: &#34;Forbidden&#34;,
details: { },
code: 403
}
</code></pre><ul><li>I tried the <code>kubectl get pods</code> command and I can see the run time so far..</li></ul><pre tabindex=0><code>$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
spark-pi-4df4497735de91a1-driver   1/1     Running   0          6m1s
spark-pi-79033a7735deb0a4-exec-1   0/1     Pending   0          5m52s
</code></pre><ul><li>Likely something is blocking. (Actually I noticed my Dropbox was being pretty aggressive. so I paused that.)</li></ul><h5 id=enabling-port-forwarding-to-get-access-to-the-dashboard>enabling port forwarding to get access to the dashboard..<a hidden class=anchor aria-hidden=true href=#enabling-port-forwarding-to-get-access-to-the-dashboard>#</a></h5><pre tabindex=0><code>kubectl get pods -n kube-system
</code></pre><pre tabindex=0><code>NAME                                     READY   STATUS    RESTARTS   AGE
coredns-f9fd979d6-pdx99                  1/1     Running   0          30m
coredns-f9fd979d6-vjpfp                  1/1     Running   0          30m
etcd-docker-desktop                      1/1     Running   0          29m
kube-apiserver-docker-desktop            1/1     Running   0          29m
kube-controller-manager-docker-desktop   1/1     Running   0          29m
kube-proxy-42wws                         1/1     Running   0          30m
kube-scheduler-docker-desktop            1/1     Running   0          29m
storage-provisioner                      1/1     Running   0          29m
vpnkit-controller                        1/1     Running   0          29m
</code></pre><ul><li><p>and hmm I cant run <code>kubectl port-forward kubernetes-dashboard-7b9c7bc8c9-ckfmr 8443:8443 -n kube-system</code> because I dont have that running looks like .</p></li><li><p>Ah according to <a href=https://medium.com/backbase/kubernetes-in-local-the-easy-way-f8ef2b98be68>here</a> the kubernetes dashboard does not come <em>out of the box</em></p></li><li><p>Per <a href=https://stackoverflow.com/a/30094032/472876>here</a> tried killing</p></li></ul><pre tabindex=0><code># ./bin/spark-class org.apache.spark.deploy.Client kill &lt;master url&gt; &lt;driver ID&gt;
./bin/spark-class org.apache.spark.deploy.Client kill k8s://https://localhost:6443 spark-pi-4df4497735de91a1-driver

WARNING: This client is deprecated and will be removed in a future version of Spark
Use ./bin/spark-submit with &#34;--master spark://host:port&#34;
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

...
Exception in thread &#34;main&#34; org.apache.spark.SparkException: Invalid master URL: spark://k8s://https://localhost:6443
</code></pre><ul><li><p>Crashed&mldr; anyway just <code>Ctrl-C</code> for now</p></li><li><p>But when looking around I see per <a href=https://issues.apache.org/jira/browse/SPARK-11909>here</a> that the master url in that command should be <code>spark://localhost:6443</code> instead.</p></li><li><p>And per <a href=https://sparkbyexamples.com/spark/spark-how-to-kill-running-application/>this note</a> , yarn is mentioned too. I dont have that yet however.</p></li><li><p>TRy to get that dashboard , following from <a href=https://medium.com/backbase/kubernetes-in-local-the-easy-way-f8ef2b98be68>here</a></p></li><li><p>It is here, <a href=https://github.com/kubernetes/dashboard/releases/tag/v2.0.5>https://github.com/kubernetes/dashboard/releases/tag/v2.0.5</a></p></li></ul><pre tabindex=0><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml

namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
</code></pre><ul><li>Hmm I did not see the dashboard with <code>kubectl get pods -n kube-system</code>, but the mentioned to look using <code>kubectl get pods --all-namespaces</code> , and I do see it indeed , in its own namespace indeed&mldr; not in the <code>kube-system</code> namespace</li></ul><pre tabindex=0><code>NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kubernetes-dashboard   kubernetes-dashboard-6f65cb5c64-kbq8d        1/1     Running   0          2m46s
</code></pre><ul><li>Not seeing anything listening on <code>8443</code> with <code>netstat -an |grep LIST</code> however, as mentioned <a href=https://medium.com/faun/apache-spark-on-kubernetes-docker-for-mac-2501cc72e659>here</a></li><li>But the other blog post is telling me to go here , http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy</li><li>After first starting a local proxy that is..</li></ul><pre tabindex=0><code>kubectl proxy
# Starting to serve on 127.0.0.1:8001
</code></pre><ul><li>As mentioned, when I visited this url, I saw the screen asking for a token.</li><li>And running the one liner ,</li></ul><pre tabindex=0><code>kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | awk &#39;/^deployment-controller-token-/{print $1}&#39;) | awk &#39;$1==&#34;token:&#34;{print $2}&#39;
</code></pre><ul><li>Yielded a token, which was accepted.</li></ul><h4 id=let-me-retry-that-earlier-example-job->let me retry that earlier example job ..<a hidden class=anchor aria-hidden=true href=#let-me-retry-that-earlier-example-job->#</a></h4><ul><li>Since now I can look at the dashboard. Maybe I will see why that job was stalling..</li><li>trying again</li></ul><pre tabindex=0><code>bin/spark-submit  \
--master k8s://https://localhost:6443  \
--deploy-mode cluster  \
--conf spark.executor.instances=1  \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
--conf spark.kubernetes.container.image=spark:spark-docker  \
--class org.apache.spark.examples.SparkPi  \
--name spark-pi  \
local:///opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar
</code></pre><ul><li>One of the early outputs&mldr;</li></ul><pre tabindex=0><code>21/01/24 16:03:01 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
</code></pre><ul><li>And basically now stuck in <code>Pending</code>.</li></ul><h5 id=insufficient-memory>Insufficient Memory!<a hidden class=anchor aria-hidden=true href=#insufficient-memory>#</a></h5><ul><li>Ok so when I looked around in the Dashboard, I see oddly &mldr; the first attempt could not succeed because of memory</li></ul><ul><li>Oh and it is hanging around still blocking resources.</li></ul><pre tabindex=0><code>$ kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
default                spark-pi-4df4497735de91a1-driver             1/1     Running   0          116m
default                spark-pi-79033a7735deb0a4-exec-1             0/1     Pending   0          116m
default                spark-pi-df12a57736350578-driver             0/1     Pending   0          21m
default                spark-pi-e333f47736434a39-driver             0/1     Pending   0          6m16s
</code></pre><ul><li>So actually <code>Ctrl-C</code> was not enough to kill it.</li><li>When I look at the logs for this driver pod, I&rsquo;m seeing</li></ul><pre tabindex=0><code>21/01/24 21:23:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
</code></pre><ul><li>Not sure how to know what resources are needed though.</li></ul><h5 id=deleting-this-pod>Deleting this pod<a hidden class=anchor aria-hidden=true href=#deleting-this-pod>#</a></h5><ul><li>Kind of handy , when I try deleting that pod in the dashboard , I&rsquo;m seeing a handy note that</li></ul><pre tabindex=0><code>This action is equivalent to:kubectl delete -n default pod spark-pi-4df4497735de91a1-driver
</code></pre><ul><li>And as soon as that was terminated, the Pending job is running. So yea none of my <code>Ctrl-C</code> were useful haha.</li><li>Trying that CLI delete instead then</li></ul><pre tabindex=0><code>kubectl delete -n default pod spark-pi-df12a57736350578-driver
</code></pre><ul><li>Ok that seems to have worked.</li></ul><h4 id=how-to-try-this-again-without-the-memory-issue>How to try this again without the memory issue?<a hidden class=anchor aria-hidden=true href=#how-to-try-this-again-without-the-memory-issue>#</a></h4><ul><li>not sure but&mldr;</li></ul><h4 id=read-about-the-pyspark-shell-being-in-the-base-spark-so-trying>Read about the pyspark shell being in the base spark so trying<a hidden class=anchor aria-hidden=true href=#read-about-the-pyspark-shell-being-in-the-base-spark-so-trying>#</a></h4><ul><li>nice ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>$</span> <span style=color:#f92672>./</span>bin<span style=color:#f92672>/</span>pyspark
</span></span><span style=display:flex><span>Python <span style=color:#ae81ff>3.7.2</span> (default, Dec <span style=color:#ae81ff>29</span> <span style=color:#ae81ff>2018</span>, <span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>04</span>)
</span></span><span style=display:flex><span>[Clang <span style=color:#ae81ff>4.0.1</span> (tags<span style=color:#f92672>/</span>RELEASE_401<span style=color:#f92672>/</span>final)] :: Anaconda, Inc<span style=color:#f92672>.</span> on darwin
</span></span><span style=display:flex><span>Type <span style=color:#e6db74>&#34;help&#34;</span>, <span style=color:#e6db74>&#34;copyright&#34;</span>, <span style=color:#e6db74>&#34;credits&#34;</span> <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;license&#34;</span> <span style=color:#66d9ef>for</span> more information<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>21</span><span style=color:#f92672>/</span><span style=color:#ae81ff>01</span><span style=color:#f92672>/</span><span style=color:#ae81ff>24</span> <span style=color:#ae81ff>17</span>:<span style=color:#ae81ff>21</span>:<span style=color:#ae81ff>27</span> WARN NativeCodeLoader: Unable to load native<span style=color:#f92672>-</span>hadoop library <span style=color:#66d9ef>for</span> your platform<span style=color:#f92672>...</span> using builtin<span style=color:#f92672>-</span>java classes where applicable
</span></span><span style=display:flex><span>Using Spark<span style=color:#e6db74>&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties</span>
</span></span><span style=display:flex><span>Setting default log level to <span style=color:#e6db74>&#34;WARN&#34;</span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>To adjust logging level use sc<span style=color:#f92672>.</span>setLogLevel(newLevel)<span style=color:#f92672>.</span> For SparkR, use setLogLevel(newLevel)<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Welcome to
</span></span><span style=display:flex><span>      ____              __
</span></span><span style=display:flex><span>     <span style=color:#f92672>/</span> __<span style=color:#f92672>/</span>__  ___ _____<span style=color:#f92672>/</span> <span style=color:#f92672>/</span>__
</span></span><span style=display:flex><span>    _\ \<span style=color:#f92672>/</span> _ \<span style=color:#f92672>/</span> _ <span style=color:#960050;background-color:#1e0010>`</span><span style=color:#f92672>/</span> __<span style=color:#f92672>/</span>  <span style=color:#e6db74>&#39;_/</span>
</span></span><span style=display:flex><span>   <span style=color:#f92672>/</span>__ <span style=color:#f92672>/</span> <span style=color:#f92672>.</span>__<span style=color:#f92672>/</span>\_,_<span style=color:#f92672>/</span>_<span style=color:#f92672>/</span> <span style=color:#f92672>/</span>_<span style=color:#f92672>/</span>\_\   version <span style=color:#ae81ff>3.0.1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>/</span>_<span style=color:#f92672>/</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Using Python version <span style=color:#ae81ff>3.7.2</span> (default, Dec <span style=color:#ae81ff>29</span> <span style=color:#ae81ff>2018</span> <span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>04</span>)
</span></span><span style=display:flex><span>SparkSession available <span style=color:#66d9ef>as</span> <span style=color:#e6db74>&#39;spark&#39;</span><span style=color:#f92672>.</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.context <span style=color:#f92672>import</span> SparkContext
</span></span><span style=display:flex><span>sc <span style=color:#f92672>=</span> SparkContext(<span style=color:#e6db74>&#39;local&#39;</span>, <span style=color:#e6db74>&#39;test&#39;</span>)
</span></span></code></pre></div><p>Oops</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;stdin&gt;&#34;</span>, line <span style=color:#ae81ff>1</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py&#34;</span>, line <span style=color:#ae81ff>133</span>, <span style=color:#f92672>in</span> __init__
</span></span><span style=display:flex><span>    SparkContext<span style=color:#f92672>.</span>_ensure_initialized(self, gateway<span style=color:#f92672>=</span>gateway, conf<span style=color:#f92672>=</span>conf)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py&#34;</span>, line <span style=color:#ae81ff>341</span>, <span style=color:#f92672>in</span> _ensure_initialized
</span></span><span style=display:flex><span>    callsite<span style=color:#f92672>.</span>function, callsite<span style=color:#f92672>.</span>file, callsite<span style=color:#f92672>.</span>linenum))
</span></span><span style=display:flex><span><span style=color:#a6e22e>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app<span style=color:#f92672>=</span>PySparkShell, master<span style=color:#f92672>=</span>local[<span style=color:#f92672>*</span>]) created by <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span> at <span style=color:#f92672>/</span>Users<span style=color:#f92672>/</span>michal<span style=color:#f92672>/</span>Downloads<span style=color:#f92672>/</span>spark<span style=color:#f92672>-</span><span style=color:#ae81ff>3.0.1</span><span style=color:#f92672>-</span>bin<span style=color:#f92672>-</span>hadoop3<span style=color:#ae81ff>.2</span><span style=color:#f92672>/</span>python<span style=color:#f92672>/</span>pyspark<span style=color:#f92672>/</span>shell<span style=color:#f92672>.</span>py:<span style=color:#ae81ff>41</span>
</span></span></code></pre></div><ul><li>oh interesting it is already pre defined</li></ul><pre tabindex=0><code>&gt;&gt;&gt; sc
&lt;SparkContext master=local[*] appName=PySparkShell&gt;
</code></pre><ul><li>Will try something basic..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>])
</span></span><span style=display:flex><span>rdd
</span></span><span style=display:flex><span><span style=color:#75715e># ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>*</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># PythonRDD[1] at RDD at PythonRDD.scala:53</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#75715e># [1, 2, 3, 4]</span>
</span></span></code></pre></div><ul><li>ok haha not quite right.</li><li>Ah duh of course have to compose/chain that..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>*</span><span style=color:#ae81ff>3</span>)<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#75715e># [3, 6, 9, 12]                                                                   </span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#75715e># [1, 2, 3, 4]</span>
</span></span></code></pre></div><ul><li>Ok excellent!</li><li>Going to look more through these docs <a href=spark.apache.org/docs/latest/api/python/index.html>here</a></li></ul><h4 id=next>Next<a hidden class=anchor aria-hidden=true href=#next>#</a></h4><ul><li>I would like to try some more basic transformations and actions.</li></ul><h3 id=2021-01-31>2021-01-31<a hidden class=anchor aria-hidden=true href=#2021-01-31>#</a></h3><h4 id=try-some-things-on-this-covid19-dataset>try some things on this covid19 dataset<a hidden class=anchor aria-hidden=true href=#try-some-things-on-this-covid19-dataset>#</a></h4><ul><li>from <a href=https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf>here</a></li><li>This is a <code>1.59GiB</code> file , so perfect, how do I use Spark to split this up and perform some basic statistics</li><li><code>COVID-19_Case_Surveillance_Public_Use_Data.csv</code></li><li>Specifically, I think a good idea to test if random sampling this data, the <code>onset_dt</code> or onset date of symptoms, what is the onset rate by age bin, which is already binned as <code>age_group</code>.</li><li>Ah and looks like you need to be explicit with specifying a header is present.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>printSchema()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>root
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> cdc_case_earliest_dt : string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> cdc_report_dt: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> pos_spec_dt: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> onset_dt: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> current_status: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> sex: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> age_group: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> race_ethnicity_combined: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> hosp_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> icu_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> death_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span><span style=display:flex><span> <span style=color:#f92672>|--</span> medcond_yn: string (nullable <span style=color:#f92672>=</span> true)
</span></span></code></pre></div><h3 id=2021-02-07>2021-02-07<a hidden class=anchor aria-hidden=true href=#2021-02-07>#</a></h3><h4 id=symptomatic-by-age-group>Symptomatic by age group.<a hidden class=anchor aria-hidden=true href=#symptomatic-by-age-group>#</a></h4><ul><li>Hmm interesting that <a href=https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc>in docs</a> is says that so does that mean that all the partitions are running in parallel? How do you only run based on the number of workers you can run simultaneously?</li></ul><blockquote><p>Note: Don’t create too many partitions in parallel on a large cluster; otherwise Spark might crash your external database systems.</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;age_group&#39;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[Row(age_group<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;0 - 9 Years&#39;</span>, count<span style=color:#f92672>=</span><span style=color:#ae81ff>9</span>)]                                         
</span></span></code></pre></div><ul><li>Try w/ a column that has more variation.. and <code>1000</code> rows instead.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [Row(sex=&#39;Female&#39;, count=446), Row(sex=&#39;Unknown&#39;, count=30), Row(sex=&#39;Missing&#39;, count=3), Row(sex=&#39;Male&#39;, count=520)]</span>
</span></span></code></pre></div><ul><li>And how do I apply a custom <code>apply</code> function with my group by</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>apply(foo)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;stdin&gt;&#34;</span>, line <span style=color:#ae81ff>2</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py&#34;</span>, line <span style=color:#ae81ff>70</span>, <span style=color:#f92672>in</span> apply
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Invalid udf: the udf argument must be a pandas_udf of type &#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ValueError</span>: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP<span style=color:#f92672>.</span>
</span></span></code></pre></div><ul><li>hmm oops</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> pandas_udf, PandasUDFType
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType, StringType, LongType, DoubleType, StructField
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(<span style=color:#e6db74>&#39;sex&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>                     StructField(<span style=color:#e6db74>&#39;onset_dt&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pandas_udf</span>(schema, PandasUDFType<span style=color:#f92672>.</span>GROUPED_MAP)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;stdin&gt;&#34;</span>, line <span style=color:#ae81ff>1</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>module<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py&#34;</span>, line <span style=color:#ae81ff>325</span>, <span style=color:#f92672>in</span> pandas_udf
</span></span><span style=display:flex><span>    require_minimum_pyarrow_version()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py&#34;</span>, line <span style=color:#ae81ff>54</span>, <span style=color:#f92672>in</span> require_minimum_pyarrow_version
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;it was not found.&#34;</span> <span style=color:#f92672>%</span> minimum_pyarrow_version)
</span></span><span style=display:flex><span><span style=color:#a6e22e>ImportError</span>: PyArrow <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0.15.1</span> must be installed; however, it was <span style=color:#f92672>not</span> found<span style=color:#f92672>.</span>
</span></span></code></pre></div><ul><li>hmm</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(pandars3) <span style=color:#960050;background-color:#1e0010>$</span> pip install  PyArrow
</span></span><span style=display:flex><span>Collecting PyArrow
</span></span><span style=display:flex><span>  Downloading https:<span style=color:#f92672>//</span>files<span style=color:#f92672>.</span>pythonhosted<span style=color:#f92672>.</span>org<span style=color:#f92672>/</span>packages<span style=color:#f92672>/</span><span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>5</span>f<span style=color:#f92672>/</span><span style=color:#ae81ff>1</span>fb0c604636d46257af3c3075955e860161e8c41386405467f073df73f91<span style=color:#f92672>/</span>pyarrow<span style=color:#f92672>-</span><span style=color:#ae81ff>3.0.0</span><span style=color:#f92672>-</span>cp37<span style=color:#f92672>-</span>cp37m<span style=color:#f92672>-</span>macosx_10_13_x86_64<span style=color:#f92672>.</span>whl (<span style=color:#ae81ff>14.1</span>MB)
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>100</span><span style=color:#f92672>%</span> <span style=color:#f92672>|</span><span style=color:#960050;background-color:#1e0010>████████████████████████████████</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>14.1</span>MB <span style=color:#ae81ff>1.6</span>MB<span style=color:#f92672>/</span>s
</span></span><span style=display:flex><span>Collecting numpy<span style=color:#f92672>&gt;=</span><span style=color:#ae81ff>1.16.6</span> (<span style=color:#f92672>from</span> PyArrow)
</span></span><span style=display:flex><span>  Downloading https:<span style=color:#f92672>//</span>files<span style=color:#f92672>.</span>pythonhosted<span style=color:#f92672>.</span>org<span style=color:#f92672>/</span>packages<span style=color:#f92672>/</span><span style=color:#ae81ff>68</span><span style=color:#f92672>/</span><span style=color:#ae81ff>30</span><span style=color:#f92672>/</span>a8ce4cb0c084cc1442408807dde60f9796356ea056ca6ef81c865a3d4e62<span style=color:#f92672>/</span>numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.20.1</span><span style=color:#f92672>-</span>cp37<span style=color:#f92672>-</span>cp37m<span style=color:#f92672>-</span>macosx_10_9_x86_64<span style=color:#f92672>.</span>whl (<span style=color:#ae81ff>16.0</span>MB)
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>100</span><span style=color:#f92672>%</span> <span style=color:#f92672>|</span><span style=color:#960050;background-color:#1e0010>████████████████████████████████</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>16.0</span>MB <span style=color:#ae81ff>1.3</span>MB<span style=color:#f92672>/</span>s
</span></span><span style=display:flex><span>tensorboard <span style=color:#ae81ff>1.14.0</span> has requirement setuptools<span style=color:#f92672>&gt;=</span><span style=color:#ae81ff>41.0.0</span>, but you<span style=color:#e6db74>&#39;ll have setuptools 40.6.3 which is incompatible.</span>
</span></span><span style=display:flex><span>Installing collected packages: numpy, PyArrow
</span></span><span style=display:flex><span>  Found existing installation: numpy <span style=color:#ae81ff>1.16.0</span>
</span></span><span style=display:flex><span>    Uninstalling numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.16.0</span>:
</span></span><span style=display:flex><span>      Successfully uninstalled numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.16.0</span>
</span></span><span style=display:flex><span>Successfully installed PyArrow<span style=color:#f92672>-</span><span style=color:#ae81ff>3.0.0</span> numpy<span style=color:#f92672>-</span><span style=color:#ae81ff>1.20.1</span>
</span></span></code></pre></div><ul><li>Ok cool now this worked ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.functions <span style=color:#f92672>import</span> pandas_udf, PandasUDFType
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType, StringType, LongType, DoubleType, StructField
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(<span style=color:#e6db74>&#39;sex&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>                     StructField(<span style=color:#e6db74>&#39;onset_dt&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pandas_udf</span>(schema, PandasUDFType<span style=color:#f92672>.</span>GROUPED_MAP)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>apply(foo)
</span></span></code></pre></div><ul><li>Really weird error though haha&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>&gt;&gt;&gt;</span> out<span style=color:#f92672>.</span>collect()
</span></span><span style=display:flex><span><span style=color:#ae81ff>21</span><span style=color:#f92672>/</span><span style=color:#ae81ff>02</span><span style=color:#f92672>/</span><span style=color:#ae81ff>07</span> <span style=color:#ae81ff>23</span>:<span style=color:#ae81ff>33</span>:<span style=color:#ae81ff>31</span> ERROR Executor: <span style=color:#a6e22e>Exception</span> <span style=color:#f92672>in</span> task <span style=color:#ae81ff>60.0</span> <span style=color:#f92672>in</span> stage <span style=color:#ae81ff>6.0</span> (TID <span style=color:#ae81ff>205</span>)]
</span></span><span style=display:flex><span>org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>api<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>PythonException: Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>605</span>, <span style=color:#f92672>in</span> main
</span></span><span style=display:flex><span>    process()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>597</span>, <span style=color:#f92672>in</span> process
</span></span><span style=display:flex><span>    serializer<span style=color:#f92672>.</span>dump_stream(out_iter, outfile)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>255</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ArrowStreamSerializer<span style=color:#f92672>.</span>dump_stream(self, init_stream_yield_batches(), stream)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>88</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>248</span>, <span style=color:#f92672>in</span> init_stream_yield_batches
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> series <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>210</span>, <span style=color:#f92672>in</span> load_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> [self<span style=color:#f92672>.</span>arrow_to_pandas(c) <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> pa<span style=color:#f92672>.</span>Table<span style=color:#f92672>.</span>from_batches([batch])<span style=color:#f92672>.</span>itercolumns()]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>210</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span>listcomp<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> [self<span style=color:#f92672>.</span>arrow_to_pandas(c) <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> pa<span style=color:#f92672>.</span>Table<span style=color:#f92672>.</span>from_batches([batch])<span style=color:#f92672>.</span>itercolumns()]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>236</span>, <span style=color:#f92672>in</span> arrow_to_pandas
</span></span><span style=display:flex><span>    s <span style=color:#f92672>=</span> super(ArrowStreamPandasUDFSerializer, self)<span style=color:#f92672>.</span>arrow_to_pandas(arrow_column)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>128</span>, <span style=color:#f92672>in</span> arrow_to_pandas
</span></span><span style=display:flex><span>    s <span style=color:#f92672>=</span> arrow_column<span style=color:#f92672>.</span>to_pandas(date_as_object<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/array.pxi&#34;</span>, line <span style=color:#ae81ff>751</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>_PandasConvertible<span style=color:#f92672>.</span>to_pandas
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/table.pxi&#34;</span>, line <span style=color:#ae81ff>224</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>ChunkedArray<span style=color:#f92672>.</span>_to_pandas
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/array.pxi&#34;</span>, line <span style=color:#ae81ff>1310</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>_array_like_to_pandas
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;pyarrow/error.pxi&#34;</span>, line <span style=color:#ae81ff>116</span>, <span style=color:#f92672>in</span> pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>check_status
</span></span><span style=display:flex><span>pyarrow<span style=color:#f92672>.</span>lib<span style=color:#f92672>.</span>ArrowException: Unknown error: Wrapping <span style=color:#ae81ff>2020</span><span style=color:#f92672>/</span><span style=color:#ae81ff>03</span><span style=color:#f92672>/</span><span style=color:#960050;background-color:#1e0010>�</span><span style=color:#ae81ff>9</span> failed
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>api<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>BasePythonRunner<span style=color:#960050;background-color:#1e0010>$</span>ReaderIterator<span style=color:#f92672>.</span>handlePythonException(PythonRunner<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>503</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>PythonArrowOutput<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>1.</span>read(PythonArrowOutput<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>99</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>PythonArrowOutput<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>1.</span>read(PythonArrowOutput<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>49</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>api<span style=color:#f92672>.</span>python<span style=color:#f92672>.</span>BasePythonRunner<span style=color:#960050;background-color:#1e0010>$</span>ReaderIterator<span style=color:#f92672>.</span>hasNext(PythonRunner<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>456</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>InterruptibleIterator<span style=color:#f92672>.</span>hasNext(InterruptibleIterator<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>37</span>)
</span></span><span style=display:flex><span>	at scala<span style=color:#f92672>.</span>collection<span style=color:#f92672>.</span>Iterator<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>11.</span>hasNext(Iterator<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>489</span>)
</span></span><span style=display:flex><span>	at scala<span style=color:#f92672>.</span>collection<span style=color:#f92672>.</span>Iterator<span style=color:#960050;background-color:#1e0010>$$</span>anon<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>10.</span>hasNext(Iterator<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>458</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>SparkPlan<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>getByteArrayRdd<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>1</span>(SparkPlan<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>340</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>execution<span style=color:#f92672>.</span>SparkPlan<span style=color:#960050;background-color:#1e0010>$$</span>Lambda<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2055</span><span style=color:#f92672>/</span><span style=color:#ae81ff>64856516.</span>apply(Unknown Source)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>mapPartitionsInternal<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2</span>(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>872</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>mapPartitionsInternal<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2</span><span style=color:#960050;background-color:#1e0010>$</span>adapted(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>872</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#960050;background-color:#1e0010>$$</span>Lambda<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2051</span><span style=color:#f92672>/</span><span style=color:#ae81ff>1858155754.</span>apply(Unknown Source)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>MapPartitionsRDD<span style=color:#f92672>.</span>compute(MapPartitionsRDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>52</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span>computeOrReadCheckpoint(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>349</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>RDD<span style=color:#f92672>.</span>iterator(RDD<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>313</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>scheduler<span style=color:#f92672>.</span>ResultTask<span style=color:#f92672>.</span>runTask(ResultTask<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>90</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>scheduler<span style=color:#f92672>.</span>Task<span style=color:#f92672>.</span>run(Task<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>127</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>executor<span style=color:#f92672>.</span>Executor<span style=color:#960050;background-color:#1e0010>$</span>TaskRunner<span style=color:#f92672>.</span><span style=color:#960050;background-color:#1e0010>$</span>anonfun<span style=color:#960050;background-color:#1e0010>$</span>run<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>3</span>(Executor<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>446</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>executor<span style=color:#f92672>.</span>Executor<span style=color:#960050;background-color:#1e0010>$</span>TaskRunner<span style=color:#960050;background-color:#1e0010>$$</span>Lambda<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#ae81ff>2018</span><span style=color:#f92672>/</span><span style=color:#ae81ff>1084937392.</span>apply(Unknown Source)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>Utils<span style=color:#960050;background-color:#1e0010>$</span><span style=color:#f92672>.</span>tryWithSafeFinally(Utils<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>1377</span>)
</span></span><span style=display:flex><span>	at org<span style=color:#f92672>.</span>apache<span style=color:#f92672>.</span>spark<span style=color:#f92672>.</span>executor<span style=color:#f92672>.</span>Executor<span style=color:#960050;background-color:#1e0010>$</span>TaskRunner<span style=color:#f92672>.</span>run(Executor<span style=color:#f92672>.</span>scala:<span style=color:#ae81ff>449</span>)
</span></span><span style=display:flex><span>	at java<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>concurrent<span style=color:#f92672>.</span>ThreadPoolExecutor<span style=color:#f92672>.</span>runWorker(ThreadPoolExecutor<span style=color:#f92672>.</span>java:<span style=color:#ae81ff>1142</span>)
</span></span><span style=display:flex><span>	at java<span style=color:#f92672>.</span>util<span style=color:#f92672>.</span>concurrent<span style=color:#f92672>.</span>ThreadPoolExecutor<span style=color:#960050;background-color:#1e0010>$</span>Worker<span style=color:#f92672>.</span>run(ThreadPoolExecutor<span style=color:#f92672>.</span>java:<span style=color:#ae81ff>617</span>)
</span></span><span style=display:flex><span>	at java<span style=color:#f92672>.</span>lang<span style=color:#f92672>.</span>Thread<span style=color:#f92672>.</span>run(Thread<span style=color:#f92672>.</span>java:<span style=color:#ae81ff>745</span>)
</span></span></code></pre></div><h3 id=2021-02-20>2021-02-20<a hidden class=anchor aria-hidden=true href=#2021-02-20>#</a></h3><h4 id=going-to-attempt-to-use-ipython-w-pyspark>Going to attempt to use ipython w/ pyspark<a hidden class=anchor aria-hidden=true href=#going-to-attempt-to-use-ipython-w-pyspark>#</a></h4><ul><li>According to <a href=https://stackoverflow.com/questions/31862293/how-to-load-ipython-shell-with-pyspark#31863595>stackoverflow</a></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>PYSPARK_DRIVER_PYTHON<span style=color:#f92672>=</span>ipython <span style=color:#f92672>./</span>bin<span style=color:#f92672>/</span>pyspark
</span></span></code></pre></div><ul><li>Ok nice worked. Just had to make sure to <code>source activate pandars3</code> my conda environment which actually has <code>ipython</code> ..</li></ul><h4 id=hmm-maybe-since-i-had-errors-w-group-by--i-can-try---reducebykey-intead>Hmm maybe since i had errors w/ group by , I can try <code>reduceByKey</code> intead?<a hidden class=anchor aria-hidden=true href=#hmm-maybe-since-i-had-errors-w-group-by--i-can-try---reducebykey-intead>#</a></h4><ul><li>oh actually, when looking at the doc for the group by with <code>help(df.groupBy('sex'))</code> , I read in the <code>apply</code> description that it is depracated and <code>applyInPandas</code> is recommended instead.</li><li>And in the <a href=https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-function-apis>apache spark doc here</a> , I&rsquo;m reading that " Using PandasUDFType will be deprecated in the future." so then the complicated decorator looking code I was trying above, maybe that is getting phased out anyway.</li><li>The only thing new here is that I need to pass the schema of the dataframe to <code>applyInPandas</code></li><li>My particualr dataset is actually all categorical data and dates.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dfx<span style=color:#f92672>.</span>count()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)     
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyspark.sql.types <span style=color:#f92672>import</span> StructType, StringType, LongType, DoubleType, StructField
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let me try to treat them all as nullable strings for now...</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(x, StringType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>=> ok now error I got is actually more clear&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>PythonException:
</span></span><span style=display:flex><span>  An exception was thrown <span style=color:#f92672>from</span> the Python worker<span style=color:#f92672>.</span> Please see the stack trace below<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>605</span>, <span style=color:#f92672>in</span> main
</span></span><span style=display:flex><span>    process()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>597</span>, <span style=color:#f92672>in</span> process
</span></span><span style=display:flex><span>    serializer<span style=color:#f92672>.</span>dump_stream(out_iter, outfile)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>255</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ArrowStreamSerializer<span style=color:#f92672>.</span>dump_stream(self, init_stream_yield_batches(), stream)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>88</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>248</span>, <span style=color:#f92672>in</span> init_stream_yield_batches
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> series <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>429</span>, <span style=color:#f92672>in</span> mapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(keys, vals)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>175</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>lambda</span><span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>lambda</span> k, v: [(wrapped(k, v), to_arrow_type(return_type))]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>167</span>, <span style=color:#f92672>in</span> wrapped
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pandas.DataFrame, but is </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(type(result)))
</span></span><span style=display:flex><span><span style=color:#a6e22e>TypeError</span>: Return type of the user<span style=color:#f92672>-</span>defined function should be pandas<span style=color:#f92672>.</span>DataFrame, but <span style=color:#f92672>is</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>class</span> <span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>pandas</span><span style=color:#f92672>.</span>core<span style=color:#f92672>.</span>series<span style=color:#f92672>.</span>Series<span style=color:#e6db74>&#39;&gt;</span>
</span></span></code></pre></div><ul><li>So let me make sure to return a dataframe in my <code>foo</code> func</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>limit(<span style=color:#ae81ff>1</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(x, StringType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>Now getting the error..</li></ul><pre tabindex=0><code>PythonException:
  An exception was thrown from the Python worker. Please see the stack trace below.
  ...
AttributeError: &#39;DataFrame&#39; object has no attribute &#39;limit&#39;
</code></pre><ul><li>Hmm so literally the input is a vanilla pandas dataframe I think oh that&rsquo;s why!</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(x, StringType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>hmm..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>PythonException:
</span></span><span style=display:flex><span>  An exception was thrown <span style=color:#f92672>from</span> the Python worker<span style=color:#f92672>.</span> Please see the stack trace below<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Traceback (most recent call last):
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>605</span>, <span style=color:#f92672>in</span> main
</span></span><span style=display:flex><span>    process()
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>597</span>, <span style=color:#f92672>in</span> process
</span></span><span style=display:flex><span>    serializer<span style=color:#f92672>.</span>dump_stream(out_iter, outfile)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>255</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ArrowStreamSerializer<span style=color:#f92672>.</span>dump_stream(self, init_stream_yield_batches(), stream)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>88</span>, <span style=color:#f92672>in</span> dump_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py&#34;</span>, line <span style=color:#ae81ff>248</span>, <span style=color:#f92672>in</span> init_stream_yield_batches
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> series <span style=color:#f92672>in</span> iterator:
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>429</span>, <span style=color:#f92672>in</span> mapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(keys, vals)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>175</span>, <span style=color:#f92672>in</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>lambda</span><span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>lambda</span> k, v: [(wrapped(k, v), to_arrow_type(return_type))]
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py&#34;</span>, line <span style=color:#ae81ff>160</span>, <span style=color:#f92672>in</span> wrapped
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> f(pd<span style=color:#f92672>.</span>concat(value_series, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/Users/michal/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py&#34;</span>, line <span style=color:#ae81ff>107</span>, <span style=color:#f92672>in</span> wrapper
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f(<span style=color:#f92672>*</span>args, <span style=color:#f92672>**</span>kwargs)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;&lt;ipython-input-54-736ec161f4f7&gt;&#34;</span>, line <span style=color:#ae81ff>4</span>, <span style=color:#f92672>in</span> foo
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/frame.py&#34;</span>, line <span style=color:#ae81ff>392</span>, <span style=color:#f92672>in</span> __init__
</span></span><span style=display:flex><span>    mgr <span style=color:#f92672>=</span> init_dict(data, index, columns, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>212</span>, <span style=color:#f92672>in</span> init_dict
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> arrays_to_mgr(arrays, data_names, index, columns, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>56</span>, <span style=color:#f92672>in</span> arrays_to_mgr
</span></span><span style=display:flex><span>    arrays <span style=color:#f92672>=</span> _homogenize(arrays, index, dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>277</span>, <span style=color:#f92672>in</span> _homogenize
</span></span><span style=display:flex><span>    raise_cast_failure<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/internals/construction.py&#34;</span>, line <span style=color:#ae81ff>642</span>, <span style=color:#f92672>in</span> sanitize_array
</span></span><span style=display:flex><span>    value, len(index), dtype)
</span></span><span style=display:flex><span>  File <span style=color:#e6db74>&#34;/usr/local/miniconda3/envs/pandars3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py&#34;</span>, line <span style=color:#ae81ff>1187</span>, <span style=color:#f92672>in</span> construct_1d_arraylike_from_scalar
</span></span><span style=display:flex><span>    subarr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>empty(length, dtype<span style=color:#f92672>=</span>dtype)
</span></span><span style=display:flex><span><span style=color:#a6e22e>TypeError</span>: Cannot interpret <span style=color:#e6db74>&#39;&lt;attribute &#39;</span>dtype<span style=color:#e6db74>&#39; of &#39;</span>numpy<span style=color:#f92672>.</span>generic<span style=color:#e6db74>&#39; objects&gt;&#39;</span> <span style=color:#66d9ef>as</span> a data type
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType([StructField(<span style=color:#e6db74>&#39;sex&#39;</span>, StringType(), <span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>                     StructField(<span style=color:#e6db74>&#39;count&#39;</span>, LongType(), <span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>                     ])
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>group to try the string schema usage instead</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;, &#39;</span><span style=color:#f92672>.</span>join([<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74> string&#39;</span> <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>columns]); schema                                                               
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;cdc_case_earliest_dt string, cdc_report_dt string, pos_spec_dt string, onset_dt string, current_status string, sex string, age_group string, race_ethnicity_combined string, hosp_yn string, icu_yn string, death_yn string, medcond_yn string&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li><p>Dang same error. Maybe doesnt like string type group bys?</p></li><li><p>Randomly reading this may be something to do w/ old pandas version?</p></li></ul><pre tabindex=0><code>
In [68]: pd.__version__                                                                                                                
Out[68]: &#39;0.24.2&#39;
</code></pre><ul><li>I upgraded to <code>1.0.5</code></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;sex string, count int&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><ul><li>now a different error..</li></ul><pre tabindex=0><code>pyarrow.lib.ArrowException: Unknown error: Wrapping 2020/03/�6 failed
</code></pre><ul><li>Makes me think I have some garbage data</li><li>Trying the 10 line datafile i have instead</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># everything else is the same ..</span>
</span></span></code></pre></div><ul><li>WOw now a scala/java error..</li></ul><pre tabindex=0><code>21/02/20 22:19:32 ERROR Executor: Exception in task 60.0 in stage 8.0 (TID 406)]
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
...
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)
at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:105)
at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)
at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
at org.apache.spark.sql.execution.SparkPlan$$Lambda$2055/1769623532.apply(Unknown Source)
at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
at org.apache.spark.rdd.RDD$$Lambda$2051/917090051.apply(Unknown Source)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
at org.apache.spark.scheduler.Task.run(Task.scala:127)
at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
at org.apache.spark.executor.Executor$TaskRunner$$Lambda$2018/644307005.apply(Unknown Source)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
at java.io.DataInputStream.readInt(DataInputStream.java:392)
at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:86)
... 22 more



21/02/20 22:19:32 ERROR TaskSetManager: Task 159 in stage 8.0 failed 1 times; aborting job
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-3-e6065af68166&gt; in &lt;module&gt;
     15 schema = &#39;sex string, count int&#39;
     16 #
---&gt; 17 df.groupBy(&#39;sex&#39;).applyInPandas(foo, schema).collect()
     18

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py in collect(self)
    594         &#34;&#34;&#34;
    595         with SCCallSiteSync(self._sc) as css:
--&gt; 596             sock_info = self._jdf.collectToPython()
    597         return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))
    598

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-&gt; 1305             answer, self.gateway_client, self.target_id, self.name)
   1306
   1307         for temp_arg in temp_args:

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py in deco(*a, **kw)
    126     def deco(*a, **kw):
    127         try:
--&gt; 128             return f(*a, **kw)
    129         except py4j.protocol.Py4JJavaError as e:
    130             converted = convert_exception(e.java_exception)

~/Downloads/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     &#34;An error occurred while calling {0}{1}{2}.\n&#34;.
--&gt; 328                     format(target_id, &#34;.&#34;, name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o148.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 159 in stage 8.0 failed 1 times, most recent failure: Lost task 159.0 in stage 8.0 (TID 409, 192.168.16.173, executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
</code></pre><ul><li>hahaha that is great.</li></ul><h4 id=the-toy-example-does-work-though>The toy example does work though<a hidden class=anchor aria-hidden=true href=#the-toy-example-does-work-though>#</a></h4><ul><li>from the <a href=https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#grouped-map>docs</a></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(
</span></span><span style=display:flex><span>    [(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1.0</span>), (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2.0</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3.0</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>5.0</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>10.0</span>)],
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;v&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>subtract_mean</span>(pdf):
</span></span><span style=display:flex><span>    <span style=color:#75715e># pdf is a pandas.DataFrame</span>
</span></span><span style=display:flex><span>    v <span style=color:#f92672>=</span> pdf<span style=color:#f92672>.</span>v
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pdf<span style=color:#f92672>.</span>assign(v<span style=color:#f92672>=</span>v <span style=color:#f92672>-</span> v<span style=color:#f92672>.</span>mean())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#34;id&#34;</span>)<span style=color:#f92672>.</span>applyInPandas(subtract_mean, schema<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;id long, v double&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>+----+----+</span>                                                                     
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  id<span style=color:#f92672>|</span>   v<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+----+----+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>   <span style=color:#ae81ff>1</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>0.0</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>   <span style=color:#ae81ff>2</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>0.0</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+----+----+</span>
</span></span></code></pre></div><ul><li>Hmm so my guess is the string group by is not appreciated..?</li></ul><h5 id=uummm-tried-again-w-the-small-file-and-this-time-worked-well-didnt-crash-at-least>uummm tried again w/ the small file and this time worked&mldr; well didnt crash at least..<a hidden class=anchor aria-hidden=true href=#uummm-tried-again-w-the-small-file-and-this-time-worked-well-didnt-crash-at-least>#</a></h5><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(dfx):
</span></span><span style=display:flex><span>    <span style=color:#75715e># This group by key</span>
</span></span><span style=display:flex><span>    key <span style=color:#f92672>=</span> dfx<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sex
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key, <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;sex string, count int&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>collect()
</span></span></code></pre></div><pre tabindex=0><code>[Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=3),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Female&#39;, count=7),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=1),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2),
 Row(sex=&#39;Male&#39;, count=2)]
</code></pre><ul><li>But now this output looks like well not what I would expect.</li><li>I expect two rows since you know, this is a group by. So hmm</li><li>But in any case, at least it is not crashing! so major improvement.</li><li>Hmm unless this is a partitioned group by&mldr; hmm that would be exciting. So the group by has to be combined?</li><li>So could it be I have <code>12</code> partitions here? But the file only has <code>9</code> rows. Weird.</li></ul><h4 id=oh-the-apply-func-can-take-the-key-as-an-arg->oh the apply func can take the key as an arg ?<a hidden class=anchor aria-hidden=true href=#oh-the-apply-func-can-take-the-key-as-an-arg->#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/COVID-19_Case_Surveillance_Public_Use_Data.head.csv&#39;</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>,<span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>foo</span>(key, dfx):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        key: tuple of the group by keys.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dfx: pandas df for the given group by key.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#39;sex&#39;</span>: key[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#39;count&#39;</span>: dfx<span style=color:#f92672>.</span>count()})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;sex string, count int&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#39;sex&#39;</span>)<span style=color:#f92672>.</span>applyInPandas(foo, schema)<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><ul><li>result is same, but since using <code>show()</code> instead of <code>collect()</code> this time, the output looks slightly different</li><li>Still don&rsquo;t know why more than two rows though ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>+------+-----+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>   sex<span style=color:#f92672>|</span>count<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+------+-----+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>3</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>Female<span style=color:#f92672>|</span>    <span style=color:#ae81ff>7</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  Male<span style=color:#f92672>|</span>    <span style=color:#ae81ff>2</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  Male<span style=color:#f92672>|</span>    <span style=color:#ae81ff>2</span><span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  Male<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>  null<span style=color:#f92672>|</span> null<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+------+-----+</span>
</span></span><span style=display:flex><span>only showing top <span style=color:#ae81ff>20</span> rows
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://michal.piekarczyk.xyz/tags/spark/>spark</a></li></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2021-03-06-book-summary-the-practice/><span class=title>« Prev</span><br><span>Book Summary of The Practice</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2021-01-07-steak-two/><span class=title>Next »</span><br><span>Steak Two</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>