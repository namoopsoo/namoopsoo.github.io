<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Trying Databricks | michal.piekarczyk.xyz</title><meta name=keywords content="spark,covid"><meta name=description content="https://databricks.com/try-databricks
2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2021-03-20-databricks/><link crossorigin=anonymous href=/assets/css/stylesheet.4c73b1b942ee612f2f6a56636bd60cf62223b2cdb42d501875d67bb952acf3c0.css integrity="sha256-THOxuULuYS8valZja9YM9iIjss20LVAYddZ7uVKs88A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Trying Databricks"><meta property="og:description" content="https://databricks.com/try-databricks
2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2021-03-20-databricks/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-03-21T00:00:00+00:00"><meta property="article:modified_time" content="2021-03-21T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Trying Databricks"><meta name=twitter:description content="https://databricks.com/try-databricks
2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Trying Databricks","item":"https://michal.piekarczyk.xyz/post/2021-03-20-databricks/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Trying Databricks","name":"Trying Databricks","description":"https://databricks.com/try-databricks\n2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language.","keywords":["spark","covid"],"articleBody":"https://databricks.com/try-databricks\n2021-03-21 Running A quick start notebook Based on the notes here, it is pretty easy to create an auto-scaling cluster. Not sure yet what events prompt the cluster to get more workers. But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare. I also like ethat this notebook supports SQL and also python , using what looks like first line as %python to indicate the language. Is this spark sql or sql ? From the quick start notebook… CREATE TABLE diamonds USING csv OPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\") 2021-04-03 Revisit my earlier problem Last time , I found this CDC dataset called “COVID-19_Case_Surveillance_Public_Use_Data.csv” My basic initial question I would like to answer is “how do the symptomatic rates compare by age bin”, since this dataset has an onset_dt column, which is eithr blank if no symptoms and has a date if symptoms. More dataset metadata.. 22.5 M rows each row is a de-identified patient created: 2020-05-15 updated 2021-03-31 (not sure what was being updated though) Temporal Applicability: 2020-01-01/2021-03-16 Update Frequency:\tMonthly columns Column Name Description Type cdc_case_earliest_dt Calculated date–the earliest available date for the record, taken from either the available set of clinical dates (date related to the illness or specimen collection) or the calculated date representing initial date case was received by CDC. This variable is optimized for completeness and may change for a given record from time to time as new information is submitted about a case. Date \u0026 Time cdc_report_dt Calculated date representing initial date case was reported to CDC. Depreciated; CDC recommends researchers use cdc_case_earliest_dt in time series and other time-based analyses. Date \u0026 Time pos_spec_dt Date of first positive specimen collection Date \u0026 Time onset_dt Symptom onset date, if symptomatic Date \u0026 Time current_status Case Status: Laboratory-confirmed case; Probable case Plain Text sex Sex: Male; Female; Unknown; Other Plain Text age_group Age Group: 0 - 9 Years; 10 - 19 Years; 20 - 39 Years; 40 - 49 Years; 50 - 59 Years; 60 - 69 Years; 70 - 79 Years; 80 + Years Plain Text race_ethnicity_combined Race and ethnicity (combined): Hispanic/Latino; American Indian / Alaska Native, Non-Hispanic; Asian, Non-Hispanic; Black, Non-Hispanic; Native Hawaiian / Other Pacific Islander, Non-Hispanic; White, Non-Hispanic; Multiple/Other, Non-Hispanic Plain Text hosp_yn Hospitalization status Plain Text icu_yn ICU admission status Plain Text death_yn Death status Plain Text medcond_yn Presence of underlying comorbidity or disease Plain Text Get data in there Per the Databricks web console I can specify an S3 bucket and create a table from my file like that And they refer to “DBFS” as “Databricks File System” from the example you can load from the File Store like sparkDF = spark.read.csv('/FileStore/tables/state_income-9f7c5.csv', header=\"true\", inferSchema=\"true\") # then you can create a temp table from that df sparkDF.createOrReplaceTempView(\"temp_table_name\") THere was also an interesting note in the help notebook about permanent tables available across cluster restarts… # Since this table is registered as a temp view, it will only be available to this notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame. # Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data. # To do so, choose your table name and uncomment the bottom line. permanent_table_name = \"{{table_name}}\" # df.write.format(\"{{table_import_type}}\").saveAsTable(permanent_table_name) I am looking for how to do this w/ s3… Ah according to docs you mount s3 files as regular files then proceed as usual ok will try that … aws_bucket_name = \"my-databricks-assets-alpha\" s3fn = \"s3://my-databricks-assets-alpha/cdc-dataset/COVID-19_Case_Surveillance_Public_Use_Data.csv\" s3fn = \"s3://my-databricks-assets-alpha/cdc-dataset/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv\" mount_name = \"blah\" dbutils.fs.mount(\"s3a://%s\" % aws_bucket_name, \"/mnt/%s\" % mount_name) display(dbutils.fs.ls(\"/mnt/%s\" % mount_name)) Funny thing I was trying to run this cell in the databricks notebook but it would not run and no error was given. But the reason I am pretty sure is that no cluster was attached to the notebook.\nThen when I started the cluster creation process and then tried executing a cell, I was seeing “Waiting for cluster to start: Starting Spark” in the output.\nNow AccessDenied. Will try to tweak the Role which I created for databricks . Ah I see it has no s3 access at all…\nHmm I tweaked permissions and tried again but now got a new error that the directory is already mounted.\nSo going to try just the second part..\nNow getting Access Denied for getFileStatus on s3a://my-databricks-assets-alpha/: ,\nI unmounted dbutils.fs.unmount(\"/mnt/%s\" % mount_name) , gave all the read s3 permissions available, but still getting that Access Denied for getFileStatus on s3a://my-databricks-assets-alpha/: ,\nBut oddly enough when I go to my AWS EC2 the workers which were created have no “IAM Role\"s associated with them. so that’s weird.\nTrying to use these docs to troubleshoot s3a\nOk going to just try the upload instead because cannot figure out the permissions. But I feel it is possibly because of the missing IAM Role on the ec2 workers.\nWait oops! At the very top of this sample notebook I completely ignored the link , https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html , which has super detailed IAM role instructions #$%*#$$#!! haha Ok going to try the upload route anyway just so I can possibly get started .. upload ok I stepped away from my upload of this 1.5 gig file, and when I came back there was no evidence of success or error hehe, I looked around the file system w/ the notebook and stumbled upon this interesting dir, which looks like it has my file import os print(os.listdir(\"/dbfs/FileStore/tables\")) # ['COVID_19_Case_Surveillance_Public_Use_Data.csv'] So going to try making a table from it… # File location and type file_location = \"/dbfs/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv\" file_type = \"csv\" # CSV options infer_schema = \"true\" first_row_is_header = \"true\" delimiter = \",\" # The applied options are for CSV files. For other file types, these will be ignored. df = spark.read.format(file_type) \\ .option(\"inferSchema\", infer_schema) \\ .option(\"header\", first_row_is_header) \\ .option(\"sep\", delimiter) \\ .load(file_location) display(df) Got … AnalysisException: Path does not exist: dbfs:/dbfs/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv; Maybe without the leading /dbfs/ ? Ah bingo.. so it’s not an absolute path but like a relative path.. # File location and type file_location = \"/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv\" file_type = \"csv\" # CSV options infer_schema = \"true\" first_row_is_header = \"true\" delimiter = \",\" # The applied options are for CSV files. For other file types, these will be ignored. df = spark.read.format(file_type) \\ .option(\"inferSchema\", infer_schema) \\ .option(\"header\", first_row_is_header) \\ .option(\"sep\", delimiter) \\ .load(file_location) display(df) Ok that was pretty fast..\nAnd create that table..\n# Since this table is registered as a temp view, it will only be available to this notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame. # Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data. # To do so, choose your table name and uncomment the bottom line. permanent_table_name = \"covid\" # table_import_type? df.write.format(\"json\").saveAsTable(permanent_table_name) Took 50 seconds try read .. select * from covid limit 10 wow ok actually worked … I am now seeing first ten rows .. quick stat %sql select age_group, count(1) from covid group by age_group order by age_group asc ok try that group by from last time.. select age_group, sum(case when onset_dt is null then 0 else 1 end)/count(1) as asymptomatic_rate from covid group by age_group order by age_group And side notes, in the databricks browser notebook , I generated this above plot by clicking “bar chart” option and generating a “Bokeh” looking graphic, then downloading it. But I had tried to export the notebook as ipynb and the images did not get saved . Instead it looked like only the tabular data was saved. And although this data is pretty sparse, look at the comorbidity data too hosp_yn\t|Hospitalization status|Plain Text icu_yn\t|ICU admission status|Plain Text death_yn\t|Death status|Plain Text medcond_yn\t|Presence of underlying comorbidity or disease|Plain Text select medcond_yn, count(1) from covid group by medcond_yn medcond_yn|count(1) --|-- Unknown|1290951 No|938017 Yes|944195 Missing|10242673 select sum(case when hosp_yn) from covid group by medcond_yn hosp_yn count(1) Unknown 2139571 No 4830822 Yes 703734 Missing 5741709 %sql select medcond_yn, sum(case when hosp_yn = 'Yes' then 1 else 0 end)/count(1) as hospYes_rate, sum(case when hosp_yn = 'No' then 1 else 0 end)/count(1) as hospNo_rate, sum(case when hosp_yn = 'Unknown' then 1 else 0 end)/count(1) as hospUnknown_rate, sum(case when hosp_yn = 'Missing' then 1 else 0 end)/count(1) as hospMissing_rate from covid group by medcond_yn medcond_yn hospMissing_rate hospYes_rate hospUnknown_rate hospNo_rate Unknown 0.12071333458822217 0.06108210148952207 0.6504840230186894 0.16772054090356645 No 0.22393943819781517 0.03035552660559457 0.021185117114082153 0.7245199180825082 Yes 0.10189102886585928 0.21121802170102574 0.04902694888238129 0.6378640005507337 Missing 0.5154523628744176 0.03875697291127033 0.12044365762726195 0.32534700658705007 ","wordCount":"1435","inLanguage":"en","datePublished":"2021-03-21T00:00:00Z","dateModified":"2021-03-21T00:00:00Z","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2021-03-20-databricks/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/tags/ title=tags><span>tags</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Trying Databricks</h1><div class=post-meta><span title='2021-03-21 00:00:00 +0000 UTC'>212121-21-2112</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1435 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><div class=post-content><p><a href=https://databricks.com/try-databricks>https://databricks.com/try-databricks</a></p><h3 id=2021-03-21>2021-03-21<a hidden class=anchor aria-hidden=true href=#2021-03-21>#</a></h3><h4 id=running-a-quick-start-notebook>Running A quick start notebook<a hidden class=anchor aria-hidden=true href=#running-a-quick-start-notebook>#</a></h4><ul><li>Based on the notes here, it is pretty easy to create an auto-scaling cluster.</li><li>Not sure yet what events prompt the cluster to get more workers.</li><li>But I would be curious to try a job that uses fewer workers and more workers, to see how the outcomes compare.</li><li>I also like ethat this notebook supports <code>SQL</code> and also <code>python</code> , using what looks like first line as <code>%python</code> to indicate the language.</li></ul><h4 id=is-this-spark-sql-or-sql->Is this spark sql or sql ?<a hidden class=anchor aria-hidden=true href=#is-this-spark-sql-or-sql->#</a></h4><ul><li>From the quick start notebook&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> diamonds
</span></span><span style=display:flex><span><span style=color:#66d9ef>USING</span> csv
</span></span><span style=display:flex><span><span style=color:#66d9ef>OPTIONS</span> (path <span style=color:#e6db74>&#34;/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv&#34;</span>, header <span style=color:#e6db74>&#34;true&#34;</span>)
</span></span></code></pre></div><h3 id=2021-04-03>2021-04-03<a hidden class=anchor aria-hidden=true href=#2021-04-03>#</a></h3><h4 id=revisit-my-earlier-problem>Revisit my earlier problem<a hidden class=anchor aria-hidden=true href=#revisit-my-earlier-problem>#</a></h4><ul><li><a href=https://michal.piekarczyk.xyz/2021/01/23/spark-weekend.html#2021-01-31>Last time</a> , I found this <a href=https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf>CDC dataset</a> called &ldquo;COVID-19_Case_Surveillance_Public_Use_Data.csv&rdquo;</li><li>My basic initial question I would like to answer is &ldquo;how do the symptomatic rates compare by age bin&rdquo;, since this dataset has an <code>onset_dt</code> column, which is eithr blank if no symptoms and has a date if symptoms.</li></ul><h4 id=more-dataset-metadata>More dataset metadata..<a hidden class=anchor aria-hidden=true href=#more-dataset-metadata>#</a></h4><ul><li>22.5 M rows</li><li>each row is a de-identified patient</li><li>created: <code>2020-05-15</code></li><li>updated <code>2021-03-31</code> (not sure what was being updated though)</li><li>Temporal Applicability: <code>2020-01-01/2021-03-16</code></li><li>Update Frequency: Monthly</li><li>columns</li></ul><table><thead><tr><th>Column Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>cdc_case_earliest_dt</td><td>Calculated date&ndash;the earliest available date for the record, taken from either the available set of clinical dates (date related to the illness or specimen collection) or the calculated date representing initial date case was received by CDC. This variable is optimized for completeness and may change for a given record from time to time as new information is submitted about a case.</td><td>Date & Time</td></tr><tr><td>cdc_report_dt</td><td>Calculated date representing initial date case was reported to CDC. Depreciated; CDC recommends researchers use cdc_case_earliest_dt in time series and other time-based analyses.</td><td>Date & Time</td></tr><tr><td>pos_spec_dt</td><td>Date of first positive specimen collection</td><td>Date & Time</td></tr><tr><td>onset_dt</td><td>Symptom onset date, if symptomatic</td><td>Date & Time</td></tr><tr><td>current_status</td><td>Case Status: Laboratory-confirmed case; Probable case</td><td>Plain Text</td></tr><tr><td>sex</td><td>Sex: Male; Female; Unknown; Other</td><td>Plain Text</td></tr><tr><td>age_group</td><td>Age Group: 0 - 9 Years; 10 - 19 Years; 20 - 39 Years; 40 - 49 Years; 50 - 59 Years; 60 - 69 Years; 70 - 79 Years; 80 + Years</td><td>Plain Text</td></tr><tr><td>race_ethnicity_combined</td><td>Race and ethnicity (combined): Hispanic/Latino; American Indian / Alaska Native, Non-Hispanic; Asian, Non-Hispanic; Black, Non-Hispanic; Native Hawaiian / Other Pacific Islander, Non-Hispanic; White, Non-Hispanic; Multiple/Other, Non-Hispanic</td><td>Plain Text</td></tr><tr><td>hosp_yn</td><td>Hospitalization status</td><td>Plain Text</td></tr><tr><td>icu_yn</td><td>ICU admission status</td><td>Plain Text</td></tr><tr><td>death_yn</td><td>Death status</td><td>Plain Text</td></tr><tr><td>medcond_yn</td><td>Presence of underlying comorbidity or disease</td><td>Plain Text</td></tr></tbody></table><h4 id=get-data-in-there>Get data in there<a hidden class=anchor aria-hidden=true href=#get-data-in-there>#</a></h4><ul><li>Per the Databricks web console I can specify an S3 bucket and create a table from my file like that</li><li>And they refer to &ldquo;DBFS&rdquo; as &ldquo;Databricks File System&rdquo;</li><li>from the example you can load from the File Store like</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sparkDF <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>csv(<span style=color:#e6db74>&#39;/FileStore/tables/state_income-9f7c5.csv&#39;</span>, header<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;true&#34;</span>, inferSchema<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;true&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># then you can create a temp table from that df</span>
</span></span><span style=display:flex><span>sparkDF<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;temp_table_name&#34;</span>)
</span></span></code></pre></div><ul><li>THere was also an interesting note in the help notebook about permanent tables available across cluster restarts&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Since this table is registered as a temp view, it will only be available to this notebook. If you&#39;d like other users to be able to query this table, you can also create a table from the DataFrame.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># To do so, choose your table name and uncomment the bottom line.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>permanent_table_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;{{table_name}}&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># df.write.format(&#34;{{table_import_type}}&#34;).saveAsTable(permanent_table_name)</span>
</span></span></code></pre></div><ul><li>I am looking for how to do this w/ s3&mldr;</li><li>Ah according to <a href=https://docs.databricks.com/data/data-sources/aws/amazon-s3.html>docs</a> you mount s3 files as regular files then proceed as usual</li><li>ok will try that &mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>aws_bucket_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;my-databricks-assets-alpha&#34;</span>
</span></span><span style=display:flex><span>s3fn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;s3://my-databricks-assets-alpha/cdc-dataset/COVID-19_Case_Surveillance_Public_Use_Data.csv&#34;</span>
</span></span><span style=display:flex><span>s3fn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;s3://my-databricks-assets-alpha/cdc-dataset/COVID-19_Case_Surveillance_Public_Use_Data.head1000.csv&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mount_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;blah&#34;</span>
</span></span><span style=display:flex><span>dbutils<span style=color:#f92672>.</span>fs<span style=color:#f92672>.</span>mount(<span style=color:#e6db74>&#34;s3a://</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> aws_bucket_name, <span style=color:#e6db74>&#34;/mnt/</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> mount_name)
</span></span><span style=display:flex><span>display(dbutils<span style=color:#f92672>.</span>fs<span style=color:#f92672>.</span>ls(<span style=color:#e6db74>&#34;/mnt/</span><span style=color:#e6db74>%s</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> mount_name))
</span></span></code></pre></div><ul><li><p>Funny thing I was trying to run this cell in the databricks notebook but it would not run and no error was given. But the reason I am pretty sure is that no cluster was attached to the notebook.</p></li><li><p>Then when I started the cluster creation process and then tried executing a cell, I was seeing &ldquo;Waiting for cluster to start: Starting Spark&rdquo; in the output.</p></li><li><p>Now <code>AccessDenied</code>. Will try to tweak the Role which I created for databricks . Ah I see it has no s3 access at all&mldr;</p></li><li><p>Hmm I tweaked permissions and tried again but now got a new error that the directory is already mounted.</p></li><li><p>So going to try just the second part..</p></li><li><p>Now getting Access Denied for <code>getFileStatus on s3a://my-databricks-assets-alpha/:</code> ,</p></li><li><p>I unmounted <code>dbutils.fs.unmount("/mnt/%s" % mount_name)</code> , gave all the read s3 permissions available, but still getting that Access Denied for <code>getFileStatus on s3a://my-databricks-assets-alpha/:</code> ,</p></li><li><p>But oddly enough when I go to my AWS EC2 the workers which were created have no &ldquo;IAM Role"s associated with them. so that&rsquo;s weird.</p></li><li><p>Trying to use these <a href=http://hadoop.apache.org/docs/r2.8.0/hadoop-aws/tools/hadoop-aws/index.html#Troubleshooting_S3A>docs</a> to troubleshoot s3a</p></li><li><p>Ok going to just try the upload instead because cannot figure out the permissions. But I feel it is possibly because of the missing IAM Role on the ec2 workers.</p></li></ul><h4 id=wait-oops>Wait oops!<a hidden class=anchor aria-hidden=true href=#wait-oops>#</a></h4><ul><li>At the very top of this sample notebook I completely ignored the link , <a href=https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html>https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html</a> , which has super detailed IAM role instructions #$%*#$$#!! haha</li><li>Ok going to try the upload route anyway just so I can possibly get started ..</li></ul><h4 id=upload>upload<a hidden class=anchor aria-hidden=true href=#upload>#</a></h4><ul><li>ok I stepped away from my upload of this 1.5 gig file, and when I came back there was no evidence of success or error hehe,</li><li>I looked around the file system w/ the notebook and stumbled upon this interesting dir, which looks like it has my file</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>print(os<span style=color:#f92672>.</span>listdir(<span style=color:#e6db74>&#34;/dbfs/FileStore/tables&#34;</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># [&#39;COVID_19_Case_Surveillance_Public_Use_Data.csv&#39;]</span>
</span></span></code></pre></div><ul><li>So going to try making a table from it&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># File location and type</span>
</span></span><span style=display:flex><span>file_location <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/dbfs/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv&#34;</span>
</span></span><span style=display:flex><span>file_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;csv&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># CSV options</span>
</span></span><span style=display:flex><span>infer_schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>first_row_is_header <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>delimiter <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;,&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># The applied options are for CSV files. For other file types, these will be ignored.</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(file_type) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;inferSchema&#34;</span>, infer_schema) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>, first_row_is_header) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;sep&#34;</span>, delimiter) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>load(file_location)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>display(df)
</span></span></code></pre></div><ul><li>Got &mldr;</li></ul><pre tabindex=0><code>AnalysisException: Path does not exist: dbfs:/dbfs/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv;
</code></pre><ul><li>Maybe without the leading <code>/dbfs/</code> ?</li><li>Ah bingo.. so it&rsquo;s not an absolute path but like a relative path..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># File location and type</span>
</span></span><span style=display:flex><span>file_location <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/FileStore/tables/COVID_19_Case_Surveillance_Public_Use_Data.csv&#34;</span>
</span></span><span style=display:flex><span>file_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;csv&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># CSV options</span>
</span></span><span style=display:flex><span>infer_schema <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>first_row_is_header <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>delimiter <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;,&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># The applied options are for CSV files. For other file types, these will be ignored.</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(file_type) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;inferSchema&#34;</span>, infer_schema) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;header&#34;</span>, first_row_is_header) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option(<span style=color:#e6db74>&#34;sep&#34;</span>, delimiter) \
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>load(file_location)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>display(df)
</span></span></code></pre></div><ul><li><p>Ok that was pretty fast..</p></li><li><p>And create that table..</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Since this table is registered as a temp view, it will only be available to this notebook. If you&#39;d like other users to be able to query this table, you can also create a table from the DataFrame.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># To do so, choose your table name and uncomment the bottom line.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>permanent_table_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;covid&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># table_import_type?</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;json&#34;</span>)<span style=color:#f92672>.</span>saveAsTable(permanent_table_name)
</span></span></code></pre></div><ul><li>Took 50 seconds</li><li>try read ..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>from</span> covid <span style=color:#66d9ef>limit</span> <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><ul><li>wow ok actually worked &mldr; I am now seeing first ten rows ..</li></ul><h4 id=quick-stat>quick stat<a hidden class=anchor aria-hidden=true href=#quick-stat>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#f92672>%</span><span style=color:#66d9ef>sql</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>select</span> age_group, <span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>from</span> covid
</span></span><span style=display:flex><span><span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> age_group
</span></span><span style=display:flex><span><span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> age_group <span style=color:#66d9ef>asc</span>
</span></span></code></pre></div><h4 id=ok-try-that-group-by-from-last-time>ok try that group by from last time..<a hidden class=anchor aria-hidden=true href=#ok-try-that-group-by-from-last-time>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span>   age_group, <span style=color:#66d9ef>sum</span>(<span style=color:#66d9ef>case</span> <span style=color:#66d9ef>when</span> onset_dt <span style=color:#66d9ef>is</span> <span style=color:#66d9ef>null</span> <span style=color:#66d9ef>then</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>end</span>)<span style=color:#f92672>/</span><span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>as</span> asymptomatic_rate
</span></span><span style=display:flex><span><span style=color:#66d9ef>from</span> covid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> age_group
</span></span><span style=display:flex><span><span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> age_group
</span></span></code></pre></div><ul><li>And side notes, in the databricks browser notebook , I generated this above plot by clicking &ldquo;bar chart&rdquo; option and generating a &ldquo;Bokeh&rdquo; looking graphic, then downloading it. But I had tried to export the notebook as <code>ipynb</code> and the images did not get saved . Instead it looked like only the tabular data was saved.</li></ul><h4 id=and-although-this-data-is-pretty-sparse-look-at-the-comorbidity-data-too>And although this data is pretty sparse, look at the comorbidity data too<a hidden class=anchor aria-hidden=true href=#and-although-this-data-is-pretty-sparse-look-at-the-comorbidity-data-too>#</a></h4><pre tabindex=0><code>hosp_yn	|Hospitalization status|Plain Text
icu_yn	|ICU admission status|Plain Text
death_yn	|Death status|Plain Text
medcond_yn	|Presence of underlying comorbidity or disease|Plain Text
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span> medcond_yn, <span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>from</span> covid
</span></span><span style=display:flex><span><span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> medcond_yn
</span></span></code></pre></div><pre tabindex=0><code>medcond_yn|count(1)
--|--
Unknown|1290951
No|938017
Yes|944195
Missing|10242673
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span> <span style=color:#66d9ef>sum</span>(<span style=color:#66d9ef>case</span> <span style=color:#66d9ef>when</span> hosp_yn)
</span></span><span style=display:flex><span><span style=color:#66d9ef>from</span> covid
</span></span><span style=display:flex><span><span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> medcond_yn
</span></span></code></pre></div><table><thead><tr><th>hosp_yn</th><th>count(1)</th></tr></thead><tbody><tr><td>Unknown</td><td>2139571</td></tr><tr><td>No</td><td>4830822</td></tr><tr><td>Yes</td><td>703734</td></tr><tr><td>Missing</td><td>5741709</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#f92672>%</span><span style=color:#66d9ef>sql</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>select</span> medcond_yn,
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span>(<span style=color:#66d9ef>case</span> <span style=color:#66d9ef>when</span> hosp_yn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Yes&#39;</span> <span style=color:#66d9ef>then</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>end</span>)<span style=color:#f92672>/</span><span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>as</span> hospYes_rate,
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span>(<span style=color:#66d9ef>case</span> <span style=color:#66d9ef>when</span> hosp_yn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;No&#39;</span> <span style=color:#66d9ef>then</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>end</span>)<span style=color:#f92672>/</span><span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>as</span> hospNo_rate,
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span>(<span style=color:#66d9ef>case</span> <span style=color:#66d9ef>when</span> hosp_yn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Unknown&#39;</span> <span style=color:#66d9ef>then</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>end</span>)<span style=color:#f92672>/</span><span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>as</span> hospUnknown_rate,
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span>(<span style=color:#66d9ef>case</span> <span style=color:#66d9ef>when</span> hosp_yn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Missing&#39;</span> <span style=color:#66d9ef>then</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>end</span>)<span style=color:#f92672>/</span><span style=color:#66d9ef>count</span>(<span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>as</span> hospMissing_rate
</span></span><span style=display:flex><span><span style=color:#66d9ef>from</span> covid
</span></span><span style=display:flex><span><span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> medcond_yn
</span></span></code></pre></div><table><thead><tr><th>medcond_yn</th><th>hospMissing_rate</th><th>hospYes_rate</th><th>hospUnknown_rate</th><th>hospNo_rate</th></tr></thead><tbody><tr><td>Unknown</td><td>0.12071333458822217</td><td>0.06108210148952207</td><td>0.6504840230186894</td><td>0.16772054090356645</td></tr><tr><td>No</td><td>0.22393943819781517</td><td>0.03035552660559457</td><td>0.021185117114082153</td><td>0.7245199180825082</td></tr><tr><td>Yes</td><td>0.10189102886585928</td><td>0.21121802170102574</td><td>0.04902694888238129</td><td>0.6378640005507337</td></tr><tr><td>Missing</td><td>0.5154523628744176</td><td>0.03875697291127033</td><td>0.12044365762726195</td><td>0.32534700658705007</td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags><li><a href=https://michal.piekarczyk.xyz/tags/spark/>spark</a></li><li><a href=https://michal.piekarczyk.xyz/tags/covid/>covid</a></li></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2021-03-28-vimeo-troubleshooting-frozen-uploads/><span class=title>« Prev</span><br><span>Troubleshooting Vimeo video upload freeze</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2021-01-23-spark-weekend/><span class=title>Next »</span><br><span>Spark Weekend</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>