<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Back prop from scratch 2022-10-02 | michal.piekarczyk.xyz</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."><meta name=generator content="Hugo 0.110.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><link rel=stylesheet href=/css/custom_code_style.css><meta property="og:title" content="Back prop from scratch 2022-10-02"><meta property="og:description" content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-10-02T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-02T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta itemprop=name content="Back prop from scratch 2022-10-02"><meta itemprop=description content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."><meta itemprop=datePublished content="2022-10-02T00:00:00+00:00"><meta itemprop=dateModified content="2022-10-02T00:00:00+00:00"><meta itemprop=wordCount content="811"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Back prop from scratch 2022-10-02"><meta name=twitter:description content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">michal.piekarczyk.xyz</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/handy/ title="Handy page">Handy</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/post/ title="Post page">Post</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/project/ title="Side Projects page">Side Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/foo/ title="The Foos page">The Foos</a></li></ul></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POST</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/&text=Back%20prop%20from%20scratch%202022-10-02" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/&title=Back%20prop%20from%20scratch%202022-10-02" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">Back prop from scratch 2022-10-02</h1><time class="f6 mv4 dib tracked" datetime=2022-10-02T00:00:00Z>October 2, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><div id=content><ul><li>my backprop SGD from scratch 2022-Aug<ul><li>14:13 ok reviewing from last time ,<ul><li>Yea so I had switched from relu to sigmoid on commit <code>b88ef76daf</code> , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0.5 anyway.</li><li>So at this point one thought I have for sure is whether this network is just one layer more complicated than would be needed for a problem set this simple. The thought arose after seeing that weight output from last training,<p><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-09-26T002513-weights_1664736071405_0.png title=2022-09-26T002513-weights.png><br></p></li><li>But in any case, I think for now I am curious if I can find more bugs.</li></ul></li><li>So, we are underfitting here. So the loss is just increasing steadily and I see the layer 1 and layer 2 weights are just increasing steadily as well. So makes me think this is related.<ul><li>16:02 let me try to observe the updates ,<p><br></p><pre><code data-lang=python class=python>  import network as n
  import dataset
  import plot
  import runner
  import ipdb
  import matplotlib.pyplot as plt
  import pylab
  from collections import Counter
  from utils import utc_now, utc_ts
  
  data = dataset.build_dataset_inside_outside_circle(0.5)
  parameters = {&quot;learning_rate&quot;: 0.01,
                &quot;steps&quot;: 50,
                &quot;log_loss_every_k_steps&quot;: 10
  
               }
  
  runner.train_and_analysis(data, parameters)
  

</code></pre></li><li></li></ul></li></ul></li><li>17:26 ah well spotted one silly bug in tracking the metrics, so I had the train and validation loss I was logging flipped,<p><br></p><pre><code data-lang=python class=python>  if step % log_loss_every_k_steps == 0:
      _, total_loss = loss(model, data.X_validation, data.Y_validation)
      metrics[&quot;train&quot;][&quot;loss_vec&quot;].append(total_loss)
  
      _, total_loss = loss(model, data.X_train, data.Y_train)
      metrics[&quot;validation&quot;][&quot;loss_vec&quot;].append(total_loss)

</code></pre><p>fixed now so it is<br></p><pre><code data-lang=python class=python>  if step % log_loss_every_k_steps == 0:
      _, total_loss = loss(model, data.X_validation, data.Y_validation)
      metrics[&quot;validation&quot;][&quot;loss_vec&quot;].append(total_loss)
  
      _, total_loss = loss(model, data.X_train, data.Y_train)
      metrics[&quot;train&quot;][&quot;loss_vec&quot;].append(total_loss)
  

</code></pre><p>A bug indeed, but would not affect the training itself .<br></p></li><li>Ok I think good thing to do next, continue my low level debugging such that as I calculate <code>g</code> , if gradient descent is working properly, then I should be able to write an assert that I think the loss at least for the single example should decrease after applying <code>g</code> update, otherwise something is wrong !</li><li>19:27 ok to check this I then have to calculate the loss on the micro-batch I'm using here,<ul><li>ok, first here is how I would reshape a single example to obtain its loss,<p><br></p><pre><code data-lang=python class=python>  i = 0 
  x, y = data.X_train[i], data.Y_train[i]
  x.shape, y.shape
  
  Y_actual, total_loss = n.loss(model, x.reshape((1, -1)), y.reshape((1, 1)))
  print(&quot;(x, y)&quot;, (x, y))
  print(&quot;Y_actual&quot;, Y_actual)
  print(&quot;loss&quot;, total_loss)

</code></pre><pre><code data-lang=python class=python>  (x, y) (array([ -7.55637702, -12.67353685]), 1)                                                                      
  Y_actual [0.93243955]
  loss 0.06995095896007311

</code></pre></li><li>And side not I realized technically I'm not plotting the training loss, since the training set has <code>9,000</code> rows and I'm only really using <code>500</code> or so of them so far. So I will adjust the training loss calculation for specifically that portion I use.</li><li>20:27 ok cool, going to try out this new code where I also now am logging the before and after for each microbatch loss<pre><code data-lang=python class=python>  import network as n
  import dataset
  import plot
  import runner
  import ipdb
  import matplotlib.pyplot as plt
  import pylab
  from collections import Counter
  from utils import utc_now, utc_ts
  
  data = dataset.build_dataset_inside_outside_circle(0.5)
  parameters = {&quot;learning_rate&quot;: 0.01,
                &quot;steps&quot;: 500,
                &quot;log_loss_every_k_steps&quot;: 10
               }
  
  model, artifacts, metrics = runner.train_and_analysis(data, parameters)
   outer: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [00:12&lt;00:00, 40.35it/s]
                                                                                                                     saving to 2022-10-03T003158.png                                                                                      
  2022-10-03T003158.png
  2022-10-03T003159-weights.png
  2022-10-03T003200-hist.png
  saving to 2022-10-03T003201-scatter.png
  2022-10-03T003201-scatter.png

</code></pre><p>And let me look at those micro batch updates then<br></p><pre><code data-lang=python class=python>  In [8]: metrics[&quot;micro_batch_updates&quot;][:5]
  Out[8]: 
  [{&apos;loss_before&apos;: 0.43903926069642474,
    &apos;y_actual_before&apos;: array([0.64465547]),
    &apos;x&apos;: array([-9.44442228,  1.4129736 ]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.43757904199626413,
    &apos;y_actual_after&apos;: array([0.6455975])},
   {&apos;loss_before&apos;: 1.0263273283159982,
    &apos;y_actual_before&apos;: array([0.64167946]),
    &apos;x&apos;: array([-3.4136343 , 17.13301918]),
    &apos;y&apos;: 0,
    &apos;loss_after&apos;: 1.0309406841349795,
    &apos;y_actual_after&apos;: array([0.64332871])},
   {&apos;loss_before&apos;: 0.4300753021013386,
    &apos;y_actual_before&apos;: array([0.65046011]),
    &apos;x&apos;: array([-2.26675345, -5.20582749]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.4285424015973017,
    &apos;y_actual_after&apos;: array([0.65145797])},
   {&apos;loss_before&apos;: 1.0544704530873739,
    &apos;y_actual_before&apos;: array([0.65162314]),
    &apos;x&apos;: array([ 14.74873303, -16.34664216]),
    &apos;y&apos;: 0,
    &apos;loss_after&apos;: 1.0598453040833464,
    &apos;y_actual_after&apos;: array([0.65349059])},
   {&apos;loss_before&apos;: 0.42370781274874675,
    &apos;y_actual_before&apos;: array([0.65461512]),
    &apos;x&apos;: array([  1.71615885, -11.0142264 ]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.42217509911520096,
    &apos;y_actual_after&apos;: array([0.65561923])}]
  
  import matplotlib.pyplot as plt
  from utils import utc_now, utc_ts
  import pylab
  
  deltas = [x[&quot;loss_after&quot;] - x[&quot;loss_before&quot;] for x in metrics[&quot;micro_batch_updates&quot;]]
  with plt.style.context(&quot;fivethirtyeight&quot;):
      plt.hist(deltas, bins=50)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()
  
  # saving to 2022-10-03T005623-micro-batch-loss-deltas.png

</code></pre><p><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-10-03T005623-micro-batch-loss-deltas_1664758696808_0.png title=2022-10-03T005623-micro-batch-loss-deltas.png><br></p></li><li>Wow fascinating, so a lot of the loss is getting reduced, at least slightly more than not haha,<pre><code data-lang=python class=python>  In [17]: from collections import Counter
      ...: Counter([&quot;loss_reduction&quot; if x &lt; 0 else &quot;loss_increase&quot; for x in [y for y in deltas if y != 0]])
  Out[17]: Counter({&apos;loss_reduction&apos;: 260, &apos;loss_increase&apos;: 240})

</code></pre><p>And<br></p><pre><code data-lang=python class=python>  with plt.style.context(&quot;fivethirtyeight&quot;):
      plt.plot(deltas)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()

</code></pre><p>But wow, this next plot is fascinating!<br><br></p><pre><code data-lang=python class=python>  with plt.style.context(&quot;fivethirtyeight&quot;):
      fig = plt.figure(figsize =(20, 9))
  
      plt.plot(deltas, linewidth=0.7)
      plt.title(&quot;Microbatch loss_after - loss_before&quot;)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()

</code></pre><p><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-10-03T010957-micro-batch-loss-deltas-over-steps_1664759458065_0.png title=2022-10-03T010957-micro-batch-loss-deltas-over-steps.png><br></p></li><li>So according to the above, yes the microbatch delta loss is ping ponging back and forth and basically getting worse, for the different microbatch inputs . Wow. so glad I looked at this chronological kind of plot !</li><li></li><li></li></ul></li></ul></div><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://michal.piekarczyk.xyz/>&copy; michal.piekarczyk.xyz 2023</a><div></div></div></footer></body></html>