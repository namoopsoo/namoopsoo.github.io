<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Back prop from scratch 2022-10-02 | michal.piekarczyk.xyz</title><meta name=keywords content><meta name=description content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/><link crossorigin=anonymous href=/assets/css/stylesheet.4c73b1b942ee612f2f6a56636bd60cf62223b2cdb42d501875d67bb952acf3c0.css integrity="sha256-THOxuULuYS8valZja9YM9iIjss20LVAYddZ7uVKs88A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Back prop from scratch 2022-10-02"><meta property="og:description" content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-10-02T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-02T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Back prop from scratch 2022-10-02"><meta name=twitter:description content="my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Back prop from scratch 2022-10-02","item":"https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Back prop from scratch 2022-10-02","name":"Back prop from scratch 2022-10-02","description":"my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0.","keywords":[],"articleBody":" my backprop SGD from scratch 2022-Aug 14:13 ok reviewing from last time , Yea so I had switched from relu to sigmoid on commit b88ef76daf , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0.5 anyway. So at this point one thought I have for sure is whether this network is just one layer more complicated than would be needed for a problem set this simple. The thought arose after seeing that weight output from last training, But in any case, I think for now I am curious if I can find more bugs. So, we are underfitting here. So the loss is just increasing steadily and I see the layer 1 and layer 2 weights are just increasing steadily as well. So makes me think this is related. 16:02 let me try to observe the updates , import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\"learning_rate\": 0.01, \"steps\": 50, \"log_loss_every_k_steps\": 10 } runner.train_and_analysis(data, parameters) 17:26 ah well spotted one silly bug in tracking the metrics, so I had the train and validation loss I was logging flipped, if step % log_loss_every_k_steps == 0: _, total_loss = loss(model, data.X_validation, data.Y_validation) metrics[\"train\"][\"loss_vec\"].append(total_loss) _, total_loss = loss(model, data.X_train, data.Y_train) metrics[\"validation\"][\"loss_vec\"].append(total_loss) fixed now so it is if step % log_loss_every_k_steps == 0: _, total_loss = loss(model, data.X_validation, data.Y_validation) metrics[\"validation\"][\"loss_vec\"].append(total_loss) _, total_loss = loss(model, data.X_train, data.Y_train) metrics[\"train\"][\"loss_vec\"].append(total_loss) A bug indeed, but would not affect the training itself . Ok I think good thing to do next, continue my low level debugging such that as I calculate g , if gradient descent is working properly, then I should be able to write an assert that I think the loss at least for the single example should decrease after applying g update, otherwise something is wrong ! 19:27 ok to check this I then have to calculate the loss on the micro-batch I'm using here, ok, first here is how I would reshape a single example to obtain its loss, i = 0 x, y = data.X_train[i], data.Y_train[i] x.shape, y.shape Y_actual, total_loss = n.loss(model, x.reshape((1, -1)), y.reshape((1, 1))) print(\"(x, y)\", (x, y)) print(\"Y_actual\", Y_actual) print(\"loss\", total_loss) (x, y) (array([ -7.55637702, -12.67353685]), 1) Y_actual [0.93243955] loss 0.06995095896007311 And side not I realized technically I'm not plotting the training loss, since the training set has 9,000 rows and I'm only really using 500 or so of them so far. So I will adjust the training loss calculation for specifically that portion I use. 20:27 ok cool, going to try out this new code where I also now am logging the before and after for each microbatch loss import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\"learning_rate\": 0.01, \"steps\": 500, \"log_loss_every_k_steps\": 10 } model, artifacts, metrics = runner.train_and_analysis(data, parameters) outer: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [00:12\u003c00:00, 40.35it/s] saving to 2022-10-03T003158.png 2022-10-03T003158.png 2022-10-03T003159-weights.png 2022-10-03T003200-hist.png saving to 2022-10-03T003201-scatter.png 2022-10-03T003201-scatter.png And let me look at those micro batch updates then In [8]: metrics[\"micro_batch_updates\"][:5] Out[8]: [{'loss_before': 0.43903926069642474, 'y_actual_before': array([0.64465547]), 'x': array([-9.44442228, 1.4129736 ]), 'y': 1, 'loss_after': 0.43757904199626413, 'y_actual_after': array([0.6455975])}, {'loss_before': 1.0263273283159982, 'y_actual_before': array([0.64167946]), 'x': array([-3.4136343 , 17.13301918]), 'y': 0, 'loss_after': 1.0309406841349795, 'y_actual_after': array([0.64332871])}, {'loss_before': 0.4300753021013386, 'y_actual_before': array([0.65046011]), 'x': array([-2.26675345, -5.20582749]), 'y': 1, 'loss_after': 0.4285424015973017, 'y_actual_after': array([0.65145797])}, {'loss_before': 1.0544704530873739, 'y_actual_before': array([0.65162314]), 'x': array([ 14.74873303, -16.34664216]), 'y': 0, 'loss_after': 1.0598453040833464, 'y_actual_after': array([0.65349059])}, {'loss_before': 0.42370781274874675, 'y_actual_before': array([0.65461512]), 'x': array([ 1.71615885, -11.0142264 ]), 'y': 1, 'loss_after': 0.42217509911520096, 'y_actual_after': array([0.65561923])}] import matplotlib.pyplot as plt from utils import utc_now, utc_ts import pylab deltas = [x[\"loss_after\"] - x[\"loss_before\"] for x in metrics[\"micro_batch_updates\"]] with plt.style.context(\"fivethirtyeight\"): plt.hist(deltas, bins=50) out_loc = f\"{utc_ts(utc_now())}-micro-batch-loss-deltas.png\" print(\"saving to\", out_loc) pylab.savefig(out_loc, bbox_inches=\"tight\") pylab.close() plt.close() # saving to 2022-10-03T005623-micro-batch-loss-deltas.png Wow fascinating, so a lot of the loss is getting reduced, at least slightly more than not haha, In [17]: from collections import Counter ...: Counter([\"loss_reduction\" if x \u003c 0 else \"loss_increase\" for x in [y for y in deltas if y != 0]]) Out[17]: Counter({'loss_reduction': 260, 'loss_increase': 240}) And with plt.style.context(\"fivethirtyeight\"): plt.plot(deltas) out_loc = f\"{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png\" print(\"saving to\", out_loc) pylab.savefig(out_loc, bbox_inches=\"tight\") pylab.close() plt.close() But wow, this next plot is fascinating! with plt.style.context(\"fivethirtyeight\"): fig = plt.figure(figsize =(20, 9)) plt.plot(deltas, linewidth=0.7) plt.title(\"Microbatch loss_after - loss_before\") out_loc = f\"{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png\" print(\"saving to\", out_loc) pylab.savefig(out_loc, bbox_inches=\"tight\") pylab.close() plt.close() So according to the above, yes the microbatch delta loss is ping ponging back and forth and basically getting worse, for the different microbatch inputs . Wow. so glad I looked at this chronological kind of plot ! ","wordCount":"811","inLanguage":"en","datePublished":"2022-10-02T00:00:00Z","dateModified":"2022-10-02T00:00:00Z","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2022-10-02-backprop-scratch/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Back prop from scratch 2022-10-02</h1><div class=post-meta><span title='2022-10-02 00:00:00 +0000 UTC'>2022-02-212</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;811 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><div class=post-content><div id=content><ul><li>my backprop SGD from scratch 2022-Aug<ul><li>14:13 ok reviewing from last time ,<ul><li>Yea so I had switched from relu to sigmoid on commit <code>b88ef76daf</code> , but yea log loss is still going up during training, so for sure got rid of the bug of how it did not make sense to map that relu output to a sigmoid since a relu only produces positive numbers and so the sigmoid therefore was only able to produce values greater than 0.5 anyway.</li><li>So at this point one thought I have for sure is whether this network is just one layer more complicated than would be needed for a problem set this simple. The thought arose after seeing that weight output from last training,<p><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-09-26T002513-weights_1664736071405_0.png title=2022-09-26T002513-weights.png><br></p></li><li>But in any case, I think for now I am curious if I can find more bugs.</li></ul></li><li>So, we are underfitting here. So the loss is just increasing steadily and I see the layer 1 and layer 2 weights are just increasing steadily as well. So makes me think this is related.<ul><li>16:02 let me try to observe the updates ,<p><br></p><pre><code data-lang=python class=python>  import network as n
  import dataset
  import plot
  import runner
  import ipdb
  import matplotlib.pyplot as plt
  import pylab
  from collections import Counter
  from utils import utc_now, utc_ts
  
  data = dataset.build_dataset_inside_outside_circle(0.5)
  parameters = {&quot;learning_rate&quot;: 0.01,
                &quot;steps&quot;: 50,
                &quot;log_loss_every_k_steps&quot;: 10
  
               }
  
  runner.train_and_analysis(data, parameters)
  

</code></pre></li><li></li></ul></li></ul></li><li>17:26 ah well spotted one silly bug in tracking the metrics, so I had the train and validation loss I was logging flipped,<p><br></p><pre><code data-lang=python class=python>  if step % log_loss_every_k_steps == 0:
      _, total_loss = loss(model, data.X_validation, data.Y_validation)
      metrics[&quot;train&quot;][&quot;loss_vec&quot;].append(total_loss)
  
      _, total_loss = loss(model, data.X_train, data.Y_train)
      metrics[&quot;validation&quot;][&quot;loss_vec&quot;].append(total_loss)

</code></pre><p>fixed now so it is<br></p><pre><code data-lang=python class=python>  if step % log_loss_every_k_steps == 0:
      _, total_loss = loss(model, data.X_validation, data.Y_validation)
      metrics[&quot;validation&quot;][&quot;loss_vec&quot;].append(total_loss)
  
      _, total_loss = loss(model, data.X_train, data.Y_train)
      metrics[&quot;train&quot;][&quot;loss_vec&quot;].append(total_loss)
  

</code></pre><p>A bug indeed, but would not affect the training itself .<br></p></li><li>Ok I think good thing to do next, continue my low level debugging such that as I calculate <code>g</code> , if gradient descent is working properly, then I should be able to write an assert that I think the loss at least for the single example should decrease after applying <code>g</code> update, otherwise something is wrong !</li><li>19:27 ok to check this I then have to calculate the loss on the micro-batch I'm using here,<ul><li>ok, first here is how I would reshape a single example to obtain its loss,<p><br></p><pre><code data-lang=python class=python>  i = 0 
  x, y = data.X_train[i], data.Y_train[i]
  x.shape, y.shape
  
  Y_actual, total_loss = n.loss(model, x.reshape((1, -1)), y.reshape((1, 1)))
  print(&quot;(x, y)&quot;, (x, y))
  print(&quot;Y_actual&quot;, Y_actual)
  print(&quot;loss&quot;, total_loss)

</code></pre><pre><code data-lang=python class=python>  (x, y) (array([ -7.55637702, -12.67353685]), 1)                                                                      
  Y_actual [0.93243955]
  loss 0.06995095896007311

</code></pre></li><li>And side not I realized technically I'm not plotting the training loss, since the training set has <code>9,000</code> rows and I'm only really using <code>500</code> or so of them so far. So I will adjust the training loss calculation for specifically that portion I use.</li><li>20:27 ok cool, going to try out this new code where I also now am logging the before and after for each microbatch loss<pre><code data-lang=python class=python>  import network as n
  import dataset
  import plot
  import runner
  import ipdb
  import matplotlib.pyplot as plt
  import pylab
  from collections import Counter
  from utils import utc_now, utc_ts
  
  data = dataset.build_dataset_inside_outside_circle(0.5)
  parameters = {&quot;learning_rate&quot;: 0.01,
                &quot;steps&quot;: 500,
                &quot;log_loss_every_k_steps&quot;: 10
               }
  
  model, artifacts, metrics = runner.train_and_analysis(data, parameters)
   outer: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [00:12&lt;00:00, 40.35it/s]
                                                                                                                     saving to 2022-10-03T003158.png                                                                                      
  2022-10-03T003158.png
  2022-10-03T003159-weights.png
  2022-10-03T003200-hist.png
  saving to 2022-10-03T003201-scatter.png
  2022-10-03T003201-scatter.png

</code></pre><p>And let me look at those micro batch updates then<br></p><pre><code data-lang=python class=python>  In [8]: metrics[&quot;micro_batch_updates&quot;][:5]
  Out[8]: 
  [{&apos;loss_before&apos;: 0.43903926069642474,
    &apos;y_actual_before&apos;: array([0.64465547]),
    &apos;x&apos;: array([-9.44442228,  1.4129736 ]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.43757904199626413,
    &apos;y_actual_after&apos;: array([0.6455975])},
   {&apos;loss_before&apos;: 1.0263273283159982,
    &apos;y_actual_before&apos;: array([0.64167946]),
    &apos;x&apos;: array([-3.4136343 , 17.13301918]),
    &apos;y&apos;: 0,
    &apos;loss_after&apos;: 1.0309406841349795,
    &apos;y_actual_after&apos;: array([0.64332871])},
   {&apos;loss_before&apos;: 0.4300753021013386,
    &apos;y_actual_before&apos;: array([0.65046011]),
    &apos;x&apos;: array([-2.26675345, -5.20582749]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.4285424015973017,
    &apos;y_actual_after&apos;: array([0.65145797])},
   {&apos;loss_before&apos;: 1.0544704530873739,
    &apos;y_actual_before&apos;: array([0.65162314]),
    &apos;x&apos;: array([ 14.74873303, -16.34664216]),
    &apos;y&apos;: 0,
    &apos;loss_after&apos;: 1.0598453040833464,
    &apos;y_actual_after&apos;: array([0.65349059])},
   {&apos;loss_before&apos;: 0.42370781274874675,
    &apos;y_actual_before&apos;: array([0.65461512]),
    &apos;x&apos;: array([  1.71615885, -11.0142264 ]),
    &apos;y&apos;: 1,
    &apos;loss_after&apos;: 0.42217509911520096,
    &apos;y_actual_after&apos;: array([0.65561923])}]
  
  import matplotlib.pyplot as plt
  from utils import utc_now, utc_ts
  import pylab
  
  deltas = [x[&quot;loss_after&quot;] - x[&quot;loss_before&quot;] for x in metrics[&quot;micro_batch_updates&quot;]]
  with plt.style.context(&quot;fivethirtyeight&quot;):
      plt.hist(deltas, bins=50)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()
  
  # saving to 2022-10-03T005623-micro-batch-loss-deltas.png

</code></pre><p><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-10-03T005623-micro-batch-loss-deltas_1664758696808_0.png title=2022-10-03T005623-micro-batch-loss-deltas.png><br></p></li><li>Wow fascinating, so a lot of the loss is getting reduced, at least slightly more than not haha,<pre><code data-lang=python class=python>  In [17]: from collections import Counter
      ...: Counter([&quot;loss_reduction&quot; if x &lt; 0 else &quot;loss_increase&quot; for x in [y for y in deltas if y != 0]])
  Out[17]: Counter({&apos;loss_reduction&apos;: 260, &apos;loss_increase&apos;: 240})

</code></pre><p>And<br></p><pre><code data-lang=python class=python>  with plt.style.context(&quot;fivethirtyeight&quot;):
      plt.plot(deltas)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()

</code></pre><p>But wow, this next plot is fascinating!<br><br></p><pre><code data-lang=python class=python>  with plt.style.context(&quot;fivethirtyeight&quot;):
      fig = plt.figure(figsize =(20, 9))
  
      plt.plot(deltas, linewidth=0.7)
      plt.title(&quot;Microbatch loss_after - loss_before&quot;)
      out_loc = f&quot;{utc_ts(utc_now())}-micro-batch-loss-deltas-over-steps.png&quot;
      print(&quot;saving to&quot;, out_loc)
      pylab.savefig(out_loc, bbox_inches=&quot;tight&quot;)
      pylab.close()
      plt.close()

</code></pre><p><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-02-Back-prop-from-scratch-2022-10-02/2022-10-03T010957-micro-batch-loss-deltas-over-steps_1664759458065_0.png title=2022-10-03T010957-micro-batch-loss-deltas-over-steps.png><br></p></li><li>So according to the above, yes the microbatch delta loss is ping ponging back and forth and basically getting worse, for the different microbatch inputs . Wow. so glad I looked at this chronological kind of plot !</li><li></li><li></li></ul></li></ul></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2022-10-06-iphone-unavailable/><span class=title>« Prev</span><br><span>Not sure how I managed to catch my bus to DC this morning</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2022-09-25-backprop-scratch/><span class=title>Next »</span><br><span>Backprop and SGD From Scratch 2022-09-25</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>