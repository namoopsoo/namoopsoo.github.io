<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Back prop from scratch 2022-10-12 | michal.piekarczyk.xyz</title>
<meta name=keywords content><meta name=description content="ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[&#34;loss_after&#34;] - x[&#34;loss_before&#34;] for x in metrics[&#34;micro_batch_updates&#34;]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all?"><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2022-10-12-backprop-scratch/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Back prop from scratch 2022-10-12"><meta property="og:description" content="ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[&#34;loss_after&#34;] - x[&#34;loss_before&#34;] for x in metrics[&#34;micro_batch_updates&#34;]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all?"><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2022-10-12-backprop-scratch/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-10-12T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-26T18:33:54-05:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Back prop from scratch 2022-10-12"><meta name=twitter:description content="ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[&#34;loss_after&#34;] - x[&#34;loss_before&#34;] for x in metrics[&#34;micro_batch_updates&#34;]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Back prop from scratch 2022-10-12","item":"https://michal.piekarczyk.xyz/post/2022-10-12-backprop-scratch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Back prop from scratch 2022-10-12","name":"Back prop from scratch 2022-10-12","description":"ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[\u0026quot;loss_after\u0026quot;] - x[\u0026quot;loss_before\u0026quot;] for x in metrics[\u0026quot;micro_batch_updates\u0026quot;]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all?","keywords":[],"articleBody":" ok [[my backprop SGD from scratch 2022-Aug]] looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing. deltas = [x[\"loss_after\"] - x[\"loss_before\"] for x in metrics[\"micro_batch_updates\"]] although initially the values were some negatives, as well. But I wonder does it indeed something is terribly wrong if this number ever goes up at all? I think maybe yes unless this indicates the learning rate is still too high ? I am using 0.01 , but maybe it is still too high when using a single example at a time. Ok let me try even smaller learning rate, import network as n import dataset import plot import runner import ipdb import matplotlib.pyplot as plt import pylab from collections import Counter from utils import utc_now, utc_ts data = dataset.build_dataset_inside_outside_circle(0.5) parameters = {\"learning_rate\": 0.001, \"steps\": 1000, \"log_loss_every_k_steps\": 10 } model, artifacts, metrics = runner.train_and_analysis(data, parameters) outer: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:11\u003c00:00, 83.82it/s] saving to 2022-10-12T175402.png 2022-10-12T175402.png 2022-10-12T175403-weights.png 2022-10-12T175404-hist.png saving to 2022-10-12T175404-scatter.png 2022-10-12T175404-scatter.png saving to 2022-10-12T175404-micro-batch-loss-deltas-over-steps.png 2022-10-12T175404-micro-batch-loss-deltas-over-steps.png 14:01 well the worsening of the loss trend is still there and only thing that seems to have changed is the scale difference in the loss is now proportionally smaller, following the reduction of the learning rate from 0.01 to 0.001 I suppose So yea wondering if I ought to next just look for more bugs or consider increasing the batch size from one to more. Oh yea and in any case the fact that the train loss is slightly worse than the validation loss is another red flag. And of course loss in both cases is going up so yea still fundamental problems. Matt Mazur article. will look again. 14:46 going to do a super simple test of the feed forward now. Unclear what the problem is maybe there is some fundamental matrix multiplication bug? ok , starting with a blank network, with random weights, going to follow one or two inputs to the end, import network as n parameters = {\"learning_rate\": 0.01} model = n.initialize_model(parameters) def feed_forward_manually(model, x): x1, x2 = x[0], x[1] w1, w2, w3 = model.layers[0].weights[0] w4, w5, w6 = model.layers[0].weights[1] h1 = n.logit_to_prob(x1*w1 + x2*w4 + 1) h2 = n.logit_to_prob(x1*w2 + x2*w5 + 1) h3 = n.logit_to_prob(x1*w3 + x2*w6 + 1) w7, w8 = model.layers[1].weights[0] w9, w10 = model.layers[1].weights[1] w11, w12 = model.layers[1].weights[2] h4 = n.logit_to_prob(h1*w7 + h2*w9 + h3*w11 + 1) h5 = n.logit_to_prob(h1*w8 + h1*w10 + h3*w12 + 1) w13 = model.layers[2].weights[0][0] w14 = model.layers[2].weights[1][0] y_prob = n.logit_to_prob(h4*w13 + h5*w14 + 1) return y_prob x = [1, 2] y_prob_manually = feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) print(\"y_prob_manually\", y_prob_manually, \"y_prob_mat_mul\", y_prob_mat_mul) # y_prob_manually 0.7103884357305136 y_prob_mat_mul 0.47438553403530753 15:57 ok well is this a bug? not sure why this is different. But maybe this is a good test then?! And in any case sort of perhaps I should be randomizing and also doing updates on the bias term as well. But yea first should make sure this feed forward works as expected. And in addition I'm seeing, kind of weird but for some hand selected inputs basically the outputs seem to be kind of tightly constrained. Not much movement here , that's not ideal , In [32]: x = [-1, 20] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\"y_prob_manually\", y_prob_manually, \"y_prob_mat_mul\", y_prob_mat_mul) y_prob_manually 0.711842713079452 y_prob_mat_mul 0.4761151735416374 In [33]: x = [10, 20] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\"y_prob_manually\", y_prob_manually, \"y_prob_mat_mul\", y_prob_mat_mul) y_prob_manually 0.7105989141221917 y_prob_mat_mul 0.47474962375739577 In [34]: x = [10, 200] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\"y_prob_manually\", y_prob_manually, \"y_prob_mat_mul\", y_prob_mat_mul) y_prob_manually 0.711961055211584 y_prob_mat_mul 0.4762497657849592 In [35]: x = [0, 0] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\"y_prob_manually\", y_prob_manually, \"y_prob_mat_mul\", y_prob_mat_mul) y_prob_manually 0.7105745880018329 y_prob_mat_mul 0.4745660467842152 In [36]: x = [-5, -5] ...: y_prob_manually = feed_forward_manually(model, x) ...: y_prob_mat_mul = n.feed_forward(x, model.layers) ...: print(\"y_prob_manually\", y_prob_manually, \"y_prob_mat_mul\", y_prob_mat_mul) y_prob_manually 0.7111368385474176 y_prob_mat_mul 0.4751433263136077 maybe I can pinpoint which layer has the bug? from pprint import pprint import test_feed_forward x = [-5, -5] frozen = test_feed_forward.feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) pprint([ [\"--\", \"manually\", \"matmul\"], [\"h1\", frozen[\"h1\"], model.layers[0].nodes[\"h1\"]], [\"h2\", frozen[\"h2\"], model.layers[0].nodes[\"h2\"]], [\"h3\", frozen[\"h3\"], model.layers[0].nodes[\"h3\"]], [\"h4\", frozen[\"h4\"], model.layers[1].nodes[\"h4\"]], [\"h5\", frozen[\"h5\"], model.layers[1].nodes[\"h5\"]], [\"y_prob\", frozen[\"y_prob\"], y_prob_mat_mul], ]) [['--', 'manually', 'matmul'], ['h1', 0.44128214463701015, 0.44128214463701015], ['h2', 0.9894985068325902, 0.9894985068325902], ['h3', 0.7973686345809591, 0.7973686345809591], ['h4', 0.7643256444514099, 0.7643256444514099], ['h5', 0.7752070858908596, 0.7604738710471179], ['y_prob', 0.7111368385474176, 0.4751433263136077]] ok So h1, h2, h3, h4 are matching and then h5, is where the problem starts hmm . 16:24 ok think I found a small bug , from pprint import pprint import test_feed_forward x = [-5, -5] frozen = test_feed_forward.feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) pprint([ [\"--\", \"manually\", \"matmul\"], [\"h1\", frozen[\"h1\"], model.layers[0].nodes[\"h1\"]], [\"h2\", frozen[\"h2\"], model.layers[0].nodes[\"h2\"]], [\"h3\", frozen[\"h3\"], model.layers[0].nodes[\"h3\"]], [\"h4\", frozen[\"h4\"], model.layers[1].nodes[\"h4\"]], [\"h5\", frozen[\"h5\"], model.layers[1].nodes[\"h5\"]], [\"y_prob\", frozen[\"y_prob\"], y_prob_mat_mul], ]) [['--', 'manually', 'matmul'], ['h1', 0.44128214463701015, 0.44128214463701015], ['h2', 0.9894985068325902, 0.9894985068325902], ['h3', 0.7973686345809591, 0.7973686345809591], ['h4', 0.7643256444514099, 0.7643256444514099], ['h5', 0.7604738710471179, 0.7604738710471179], ['y_prob', 0.7110504493887152, 0.4751433263136077]] So weird. ok now h5 matches. just not y_prob. that bug was in my test func. hmm ok reran one more time, so I had in my test func , an extra bias term I was adding , in the final logit, but not in the main matmul feed forward func. 16:44 ok well then theres no bug in the feed forward func, but it is weird how tight the outputs are . Something tells me actually this is related to the hard coded bias values of 1 ? Let me loosen up the bias, maybe that helps. Ok so before, data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]] pprint([[x, n.feed_forward(x, model.layers)] for x in data]) [[[0, 0], 0.4745660467842152], [[4, 5], 0.4738876007756717], [[-4, 5], 0.4762211475351283], [[-5, -5], 0.4751433263136077], [[5, -5], 0.4737412140572382], [[-20, -20], 0.4754772076192018], [[100, 100], 0.4748013350557756]] And , import numpy as np model.layers[0] = model.layers[0]._replace(bias=np.array([0.1])) model.layers[1] = model.layers[1]._replace(bias=np.array([0.1])) data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]] pprint([[x, n.feed_forward(x, model.layers)] for x in data]) [[[0, 0], 0.4813065143876082], [[4, 5], 0.4804879738767632], [[-4, 5], 0.48321160815195635], [[-5, -5], 0.4823729185086716], [[5, -5], 0.4793507289434747], [[-20, -20], 0.4822354979795412], [[100, 100], 0.4810180811163541]] hmm doesn't seem to have helped. Let me go lower, model.layers[0] = model.layers[0]._replace(bias=np.array([0.01])) model.layers[1] = model.layers[1]._replace(bias=np.array([0.01])) print(\"biases, \", model.layers[0].bias, model.layers[1].bias, model.layers[2].bias) # biases, [0.01] [0.01] [0] data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]] pprint([[x, n.feed_forward(x, model.layers)] for x in data]) [[[0, 0], 0.4820947777588216], [[4, 5], 0.48128620914361286], [[-4, 5], 0.4839510956396274], [[-5, -5], 0.4831921147269756], [[5, -5], 0.4800346864013735], [[-20, -20], 0.48300320046986295], [[100, 100], 0.48173375233047927]] 17:03 ok so weird not even adjustments to bias helped . Just double check , with the manual feedforward too, from pprint import pprint import test_feed_forward x = [-5, -5] frozen = test_feed_forward.feed_forward_manually(model, x) y_prob_mat_mul = n.feed_forward(x, model.layers) pprint([ [\"--\", \"manually\", \"matmul\"], [\"h1\", frozen[\"h1\"], model.layers[0].nodes[\"h1\"]], [\"h2\", frozen[\"h2\"], model.layers[0].nodes[\"h2\"]], [\"h3\", frozen[\"h3\"], model.layers[0].nodes[\"h3\"]], [\"h4\", frozen[\"h4\"], model.layers[1].nodes[\"h4\"]], [\"h5\", frozen[\"h5\"], model.layers[1].nodes[\"h5\"]], [\"y_prob\", frozen[\"y_prob\"], y_prob_mat_mul], ]) [['--', 'manually', 'matmul'], ['h1', 0.22688927424734276, 0.22688927424734276], ['h2', 0.9722312068731663, 0.9722312068731663], ['h3', 0.5938559073859204, 0.5938559073859204], ['h4', 0.51632734891332, 0.51632734891332], ['h5', 0.5124838141374718, 0.5124838141374718], ['y_prob', 0.4831921147269756, 0.4831921147269756]] Ok yea, so looks like no bug and reducing the bias has not diminished how frozen the outputs seem to be. 17:11 so yea for now , feels like it is good I verified the feed forward func does what it is supposed to, but it is super weird that the network is really tightly configured. Super weird. Maybe I should not have activation functions on the inner layers? Nah I don't think that's the problem. Makes me wonder what about something about this particular multi-layer network architecture that is being weird? Maybe I should try different architectures? Do they have similar properties? ","wordCount":"1280","inLanguage":"en","datePublished":"2022-10-12T00:00:00Z","dateModified":"2023-02-26T18:33:54-05:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2022-10-12-backprop-scratch/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Back prop from scratch 2022-10-12</h1><div class=post-meta>&lt;span title='2022-10-12 00:00:00 +0000 UTC'>October 12, 2022&lt;/span>&amp;nbsp;·&amp;nbsp;7 min&amp;nbsp;·&amp;nbsp;1280 words&amp;nbsp;·&amp;nbsp;Michal Piekarczyk</div></header><div class=post-content><div id=content><ul><li>ok <a class=tag>[[my backprop SGD from scratch 2022-Aug]]</a><ul><li>looking over results from last time, indeed so strange how microbatch loss was going back and forth and eventually trending that the plot of my change in loss, is increasing.<pre><code data-lang=python class=python>		    deltas = [x[&quot;loss_after&quot;] - x[&quot;loss_before&quot;] for x in metrics[&quot;micro_batch_updates&quot;]]

</code></pre><p>although initially the values were some negatives, as well.<br></p><ul><li>But I wonder does it indeed something is terribly wrong if this number ever goes up at all? I think maybe yes unless this indicates the learning rate is still too high ? I am using <code>0.01</code> , but maybe it is still too high when using a single example at a time.</li><li>Ok let me try even smaller learning rate,<pre><code data-lang=python class=python>			  
import network as n
import dataset
import plot
import runner
import ipdb
import matplotlib.pyplot as plt
import pylab
from collections import Counter
from utils import utc_now, utc_ts

data = dataset.build_dataset_inside_outside_circle(0.5)
parameters = {&quot;learning_rate&quot;: 0.001,
              &quot;steps&quot;: 1000,
              &quot;log_loss_every_k_steps&quot;: 10
             }

model, artifacts, metrics = runner.train_and_analysis(data, parameters)

</code></pre><pre><code>			  outer: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:11&lt;00:00, 83.82it/s]
			                                                                                                             saving to 2022-10-12T175402.png                                                                                
			  2022-10-12T175402.png
			  2022-10-12T175403-weights.png
			  2022-10-12T175404-hist.png
			  saving to 2022-10-12T175404-scatter.png
			  2022-10-12T175404-scatter.png
			  saving to 2022-10-12T175404-micro-batch-loss-deltas-over-steps.png
			  2022-10-12T175404-micro-batch-loss-deltas-over-steps.png

</code></pre><p><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175402_1665597462150_0.png title=2022-10-12T175402.png><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175403-weights_1665597472123_0.png title=2022-10-12T175403-weights.png><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175404-hist_1665597478395_0.png title=2022-10-12T175404-hist.png><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175404-scatter_1665597484623_0.png title=2022-10-12T175404-scatter.png><br><img src=https://s3.amazonaws.com/my-blog-content/2022/2022-10-12-Back-prop-from-scratch-2022-10-12/2022-10-12T175404-micro-batch-loss-deltas-over-steps_1665597490756_0.png title=2022-10-12T175404-micro-batch-loss-deltas-over-steps.png><br></p></li><li>14:01 well the worsening of the loss trend is still there and only thing that seems to have changed is the scale difference in the loss is now proportionally smaller, following the reduction of the learning rate from <code>0.01</code> to <code>0.001</code> I suppose</li><li>So yea wondering if I ought to next just look for more bugs or consider increasing the batch size from one to more.</li><li>Oh yea and in any case the fact that the train loss is slightly worse than the validation loss is another red flag. And of course loss in both cases is going up so yea still fundamental problems.</li></ul></li><li>Matt Mazur article. will look again.</li><li>14:46 going to do a super simple test of the feed forward now. Unclear what the problem is maybe there is some fundamental matrix multiplication bug?<ul><li>ok , starting with a blank network, with random weights, going to follow one or two inputs to the end,<p><br></p><pre><code data-lang=python class=python>			  
import network as n
parameters = {&quot;learning_rate&quot;: 0.01}
model = n.initialize_model(parameters)

def feed_forward_manually(model, x):
    x1, x2 = x[0], x[1]
    w1, w2, w3 = model.layers[0].weights[0]
    w4, w5, w6 = model.layers[0].weights[1]
    h1 = n.logit_to_prob(x1*w1 + x2*w4 + 1)
    h2 = n.logit_to_prob(x1*w2 + x2*w5 + 1)
    h3 = n.logit_to_prob(x1*w3 + x2*w6 + 1)

    
    w7, w8 = model.layers[1].weights[0]
    w9, w10 = model.layers[1].weights[1]
    w11, w12 = model.layers[1].weights[2]
    h4 = n.logit_to_prob(h1*w7 + h2*w9 + h3*w11 + 1)
    h5 = n.logit_to_prob(h1*w8 + h1*w10 + h3*w12 + 1)
    
    w13 = model.layers[2].weights[0][0]
    w14 = model.layers[2].weights[1][0]
    
    y_prob = n.logit_to_prob(h4*w13 + h5*w14 + 1)
    return y_prob
    
x = [1, 2]
y_prob_manually = feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)

# y_prob_manually 0.7103884357305136 y_prob_mat_mul 0.47438553403530753
			  

</code></pre></li><li>15:57 ok well is this a bug? not sure why this is different. But maybe this is a good test then?!</li><li>And in any case sort of perhaps I should be randomizing and also doing updates on the bias term as well. But yea first should make sure this feed forward works as expected.</li><li>And in addition I'm seeing, kind of weird but for some hand selected inputs basically the outputs seem to be kind of tightly constrained. Not much movement here , that's not ideal ,<p><br></p><pre><code data-lang=python class=python>			  
In [32]: x = [-1, 20]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.711842713079452 y_prob_mat_mul 0.4761151735416374

In [33]: x = [10, 20]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.7105989141221917 y_prob_mat_mul 0.47474962375739577

In [34]: x = [10, 200]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.711961055211584 y_prob_mat_mul 0.4762497657849592

In [35]: x = [0, 0]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.7105745880018329 y_prob_mat_mul 0.4745660467842152

In [36]: x = [-5, -5]
    ...: y_prob_manually = feed_forward_manually(model, x)
    ...: y_prob_mat_mul = n.feed_forward(x, model.layers)
    ...: print(&quot;y_prob_manually&quot;, y_prob_manually, &quot;y_prob_mat_mul&quot;, y_prob_mat_mul)
y_prob_manually 0.7111368385474176 y_prob_mat_mul 0.4751433263136077
			  

</code></pre></li><li>maybe I can pinpoint which layer has the bug?<pre><code data-lang=python class=python>			  
from pprint import pprint
import test_feed_forward
x = [-5, -5]
frozen = test_feed_forward.feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
pprint([
  [&quot;--&quot;, &quot;manually&quot;, &quot;matmul&quot;],
  [&quot;h1&quot;, frozen[&quot;h1&quot;], model.layers[0].nodes[&quot;h1&quot;]],
  [&quot;h2&quot;, frozen[&quot;h2&quot;], model.layers[0].nodes[&quot;h2&quot;]],
  [&quot;h3&quot;, frozen[&quot;h3&quot;], model.layers[0].nodes[&quot;h3&quot;]],
  [&quot;h4&quot;, frozen[&quot;h4&quot;], model.layers[1].nodes[&quot;h4&quot;]],
  [&quot;h5&quot;, frozen[&quot;h5&quot;], model.layers[1].nodes[&quot;h5&quot;]],
  [&quot;y_prob&quot;, frozen[&quot;y_prob&quot;], y_prob_mat_mul],
])

</code></pre><p><br></p><pre><code data-lang=python class=python>			  
[[&apos;--&apos;, &apos;manually&apos;, &apos;matmul&apos;],
 [&apos;h1&apos;, 0.44128214463701015, 0.44128214463701015],
 [&apos;h2&apos;, 0.9894985068325902, 0.9894985068325902],
 [&apos;h3&apos;, 0.7973686345809591, 0.7973686345809591],
 [&apos;h4&apos;, 0.7643256444514099, 0.7643256444514099],
 [&apos;h5&apos;, 0.7752070858908596, 0.7604738710471179],
 [&apos;y_prob&apos;, 0.7111368385474176, 0.4751433263136077]]

</code></pre><p>ok So h1, h2, h3, h4 are matching and then h5, is where the problem starts hmm .<br></p></li><li>16:24 ok think I found a small bug ,<pre><code data-lang=python class=python>			  
from pprint import pprint
import test_feed_forward
x = [-5, -5]
frozen = test_feed_forward.feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
pprint([
  [&quot;--&quot;, &quot;manually&quot;, &quot;matmul&quot;],
  [&quot;h1&quot;, frozen[&quot;h1&quot;], model.layers[0].nodes[&quot;h1&quot;]],
  [&quot;h2&quot;, frozen[&quot;h2&quot;], model.layers[0].nodes[&quot;h2&quot;]],
  [&quot;h3&quot;, frozen[&quot;h3&quot;], model.layers[0].nodes[&quot;h3&quot;]],
  [&quot;h4&quot;, frozen[&quot;h4&quot;], model.layers[1].nodes[&quot;h4&quot;]],
  [&quot;h5&quot;, frozen[&quot;h5&quot;], model.layers[1].nodes[&quot;h5&quot;]],
  [&quot;y_prob&quot;, frozen[&quot;y_prob&quot;], y_prob_mat_mul],
])

</code></pre><p><br></p><pre><code data-lang=python class=python>			  
[[&apos;--&apos;, &apos;manually&apos;, &apos;matmul&apos;],
 [&apos;h1&apos;, 0.44128214463701015, 0.44128214463701015],
 [&apos;h2&apos;, 0.9894985068325902, 0.9894985068325902],
 [&apos;h3&apos;, 0.7973686345809591, 0.7973686345809591],
 [&apos;h4&apos;, 0.7643256444514099, 0.7643256444514099],
 [&apos;h5&apos;, 0.7604738710471179, 0.7604738710471179],
 [&apos;y_prob&apos;, 0.7110504493887152, 0.4751433263136077]]

</code></pre><p>So weird. ok now h5 matches. just not y_prob. that bug was in my test func.<br></p></li><li>hmm ok reran one more time, so I had in my test func , an extra bias term I was adding , in the final logit, but not in the main matmul feed forward func.</li></ul></li><li>16:44 ok well then theres no bug in the feed forward func, but it is weird how tight the outputs are . Something tells me actually this is related to the hard coded bias values of 1 ?<ul><li>Let me loosen up the bias, maybe that helps.<p>Ok so before,<br></p><pre><code data-lang=python class=python>			  
data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]]
pprint([[x, n.feed_forward(x, model.layers)] for x in data])
[[[0, 0], 0.4745660467842152],
 [[4, 5], 0.4738876007756717],
 [[-4, 5], 0.4762211475351283],
 [[-5, -5], 0.4751433263136077],
 [[5, -5], 0.4737412140572382],
 [[-20, -20], 0.4754772076192018],
 [[100, 100], 0.4748013350557756]]

</code></pre><p>And ,<br></p><pre><code data-lang=python class=python>			  
import numpy as np
model.layers[0] = model.layers[0]._replace(bias=np.array([0.1]))
model.layers[1] = model.layers[1]._replace(bias=np.array([0.1]))

data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]]
pprint([[x, n.feed_forward(x, model.layers)] for x in data])

</code></pre><p><br></p><pre><code data-lang=python class=python>			  [[[0, 0], 0.4813065143876082],
			   [[4, 5], 0.4804879738767632],
			   [[-4, 5], 0.48321160815195635],
			   [[-5, -5], 0.4823729185086716],
			   [[5, -5], 0.4793507289434747],
			   [[-20, -20], 0.4822354979795412],
			   [[100, 100], 0.4810180811163541]]

</code></pre><p>hmm doesn't seem to have helped. Let me go lower,<br></p><pre><code data-lang=python class=python>			  
model.layers[0] = model.layers[0]._replace(bias=np.array([0.01]))
model.layers[1] = model.layers[1]._replace(bias=np.array([0.01]))
print(&quot;biases, &quot;, model.layers[0].bias, model.layers[1].bias, model.layers[2].bias)
# biases,  [0.01] [0.01] [0]

data = [[0, 0], [4, 5], [-4, 5], [-5, -5], [5, -5], [-20, -20], [100, 100]]
pprint([[x, n.feed_forward(x, model.layers)] for x in data])

</code></pre><p><br></p><pre><code data-lang=python class=python>			  [[[0, 0], 0.4820947777588216],
			   [[4, 5], 0.48128620914361286],
			   [[-4, 5], 0.4839510956396274],
			   [[-5, -5], 0.4831921147269756],
			   [[5, -5], 0.4800346864013735],
			   [[-20, -20], 0.48300320046986295],
			   [[100, 100], 0.48173375233047927]]

</code></pre></li><li>17:03 ok so weird not even adjustments to bias helped . Just double check , with the manual feedforward too,<p><br></p><pre><code data-lang=python class=python>			  
from pprint import pprint
import test_feed_forward
x = [-5, -5]
frozen = test_feed_forward.feed_forward_manually(model, x)
y_prob_mat_mul = n.feed_forward(x, model.layers)
pprint([
  [&quot;--&quot;, &quot;manually&quot;, &quot;matmul&quot;],
  [&quot;h1&quot;, frozen[&quot;h1&quot;], model.layers[0].nodes[&quot;h1&quot;]],
  [&quot;h2&quot;, frozen[&quot;h2&quot;], model.layers[0].nodes[&quot;h2&quot;]],
  [&quot;h3&quot;, frozen[&quot;h3&quot;], model.layers[0].nodes[&quot;h3&quot;]],
  [&quot;h4&quot;, frozen[&quot;h4&quot;], model.layers[1].nodes[&quot;h4&quot;]],
  [&quot;h5&quot;, frozen[&quot;h5&quot;], model.layers[1].nodes[&quot;h5&quot;]],
  [&quot;y_prob&quot;, frozen[&quot;y_prob&quot;], y_prob_mat_mul],
])
			  

</code></pre><pre><code data-lang=python class=python>			  [[&apos;--&apos;, &apos;manually&apos;, &apos;matmul&apos;],
			   [&apos;h1&apos;, 0.22688927424734276, 0.22688927424734276],
			   [&apos;h2&apos;, 0.9722312068731663, 0.9722312068731663],
			   [&apos;h3&apos;, 0.5938559073859204, 0.5938559073859204],
			   [&apos;h4&apos;, 0.51632734891332, 0.51632734891332],
			   [&apos;h5&apos;, 0.5124838141374718, 0.5124838141374718],
			   [&apos;y_prob&apos;, 0.4831921147269756, 0.4831921147269756]]
			  

</code></pre><p>Ok yea, so looks like no bug and reducing the bias has not diminished how frozen the outputs seem to be.<br></p></li><li>17:11 so yea for now , feels like it is good I verified the feed forward func does what it is supposed to, but it is super weird that the network is really tightly configured. Super weird.</li><li>Maybe I should not have activation functions on the inner layers? Nah I don't think that's the problem.</li><li>Makes me wonder what about something about this particular multi-layer network architecture that is being weird? Maybe I should try different architectures? Do they have similar properties?</li><li></li><li></li></ul></li></ul></li></ul></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2022-10-13-backprop-scratch/><span class=title>« Prev</span><br><span>Backprop and SGD From Scratch 2022-10-13</span>
</a><a class=next href=https://michal.piekarczyk.xyz/post/2022-10-06-iphone-unavailable/><span class=title>Next »</span><br><span>Not sure how I managed to catch my bus to DC this morning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>