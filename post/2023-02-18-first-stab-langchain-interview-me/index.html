<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using langchain to interview myself about my skills | michal.piekarczyk.xyz</title><meta name=keywords content="openai,langchain"><meta name=description content="Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]
So I&rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,
Ok, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Using langchain to interview myself about my skills"><meta property="og:description" content="Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]
So I&rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,
Ok, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-18T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using langchain to interview myself about my skills"><meta name=twitter:description content="Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]
So I&rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,
Ok, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"Using langchain to interview myself about my skills","item":"https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using langchain to interview myself about my skills","name":"Using langchain to interview myself about my skills","description":"Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]\nSo I\u0026rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,\nOk, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime.","keywords":["openai","langchain"],"articleBody":"Premise Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]\nSo I’m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the Link examples that provide references, citations,\nOk, so for accumulating my information, import yaml import tempfile from pathlib import Path from datetime import datetime import pytz def utc_now(): return datetime.utcnow().replace(tzinfo=pytz.UTC) def utc_ts(dt): return dt.strftime(\"%Y-%m-%dT%H%M%S\") def read_yaml(loc): with open(loc) as fd: return yaml.safe_load(fd) from pathlib import Path import os repos_dir = Path(os.getenv(\"REPOS_DIR\")) assert repos_dir.is_dir() experience_loc = repos_dir / \"my-challenges-and-accomplishments/experience.yaml\" experiences_dict = read_yaml(experience_loc)[\"Descriptions\"] sections = [] for project, detail in experiences_dict.items(): section = \"\" if detail.get(\"company\"): company = detail.get(\"company\") section = (f\"When I worked at {company}, \" f\"there was a project in {detail['year']}, {project}.\") elif detail.get(\"project\"): project = detail.get(\"project\") section = f\"In {detail['year']}, I had a side project, {project}. \" section += \". \".join([x for x in detail.get(\"one-liners\", [])]) section += \". \".join([x for x in detail.get(\"stories\", [])]) sections.append(section) workdir = repos_dir / \"2023-interview-me\" path = workdir / f\"{utc_ts(utc_now())}-the-story-blurb.txt\" path.write_text(\"\\n\\n\\n\".join(sections)) Ok let me run now some of the basic question answer chains Use my environment from before,\nsource ~/.python_venvs/langchainz/bin/activate But when trying to use this just like per the qa with sources example in that here,\nfrom langchain.embeddings.openai import OpenAIEmbeddings from langchain.embeddings.cohere import CohereEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch from langchain.vectorstores import Chroma from langchain.docstore.document import Document from langchain.prompts import PromptTemplate from pathlib import Path workdir = str(repos_dir / \"2023-interview-me\" ) story_path = repos_dir / \"2023-interview-me\" / \"2023-02-19T011846-the-story-blurb.txt\" my_story = story_path.read_text() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_text(my_story) embeddings = OpenAIEmbeddings() docsearch = Chroma.from_texts( texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]) from langchain.chains.qa_with_sources import load_qa_with_sources_chain from langchain.llms import OpenAI query = \"What kind of projects have I worked on with tensor flow?\" docs = docsearch.similarity_search(query) chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\") chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) this line,\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]) yielded a new install request, so that’s what I did,\nValueError: Could not import chromadb python package. Please it install it with `pip install chromadb`. In [4]: !pip install chromadb Ok wow, so I reran that again and I am getting,\nCreated a chunk of size 1474, which is longer than the specified 1000 Created a chunk of size 2846, which is longer than the specified 1000 Created a chunk of size 2348, which is longer than the specified 1000 Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. Out[1]: {'output_text': \" I have worked on projects with Tensor Flow that involve predicting pilot's states of awareness, predicting bike share rider's destinations, and using Tensor Flow LSTM to answer questions about a CDC Covid dataset.\\nSOURCES: 14, 15, 0\"} which is not bad\nSo , also I realized the really cool part about the approach here, is that the numbers, correspond simply to the indices of the texts, and the texts split, actually intuitively haha by a double new line, \\n\\n because at least that is how I split them just naturally. In [2]: len(texts) Out[2]: 16 In [6]: text_splitter._separator Out[6]: '\\n\\n' So since the above output says 14, 15, 0, I think also this is somewhat sorted by relevance, so let me see what is in those,\nIn [9]: for i in [14, 15, 0]: ...: print(\"i\", i, texts[i], \"\\n===================================================\\n\\n\") ...: Okay when I looked at that ^^ I noticed actually some texts were getting actually smushed together? There was a lot of information. Going to create my text corpus one more time but this time separating by three newlines .\nOk let’s see , I now created 2023-interview-me/2023-02-19T015128-the-story-blurb.txt , lets see if there are more than 16 texts?\nstory_path = repos_dir / \"2023-interview-me\" / \"2023-02-19T015128-the-story-blurb.txt\" my_story = Path(loc).read_text() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_text(my_story) In [11]: len(texts) Out[11]: 16 Ah ok still 16 texts, but actually maybe this is based on the chunk_size ?\nOk I want to try the chain which gives the intermediate results, Also needed one more package, here for the below,\nValueError: Could not import tiktoken python package. This is needed in order to calculate get_num_tokens. Please it install it with `pip install tiktoken`. query = \"What kind of projects have I worked on with tensor flow?\" docs = docsearch.similarity_search(query) chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True) chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) Ok this looks like it only returns the relevant statements, nice.\n{'intermediate_steps': [\" In 2019, I had a side project, Reducing Commercial Aviation Fatalities (kaggle). Designed and built a Tensor Flow LSTM based model to predict pilot's states of awareness, given time series physiological data.\", ' None.', ' None', ' None.'], 'output_text': \" I have worked on a Tensor Flow LSTM based model to predict pilot's states of awareness, given time series physiological data.\\nSOURCES: 14\"} And the important thing is that this code preserves the original text so I can nicely look for myself exactly at the primary source !\nLet me try another query, query = \"What is my experience with Docker?\" docs = docsearch.similarity_search(query) chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True) chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True) {'intermediate_steps': [' \"Dockerized our production underwriting stack and split from the main company git repo to give us the flexibility to deploy both scikit learn and XGBoost models with AWS SageMaker.\"', ' \"Helped make better underwriting decisions on returning customers, by optimizing /re-engineering /versioning our SQL based logistic regression model, with a python + Docker + SQL pipeline, cutting runtime from 6+ hours to under an hour.\"', ' None.', ' None'], 'output_text': ' I have no experience with Docker.\\nSOURCES: 0, 14'} Hah I feel like I need to do some kind of tuning here? But it is cool to see the intermediate step here even though it is not reflected in the final output.\n","wordCount":"985","inLanguage":"en","datePublished":"2023-02-18T00:00:00Z","dateModified":"2023-02-18T00:00:00Z","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>Using langchain to interview myself about my skills</h1><div class=post-meta><span title='2023-02-18 00:00:00 +0000 UTC'>February 18, 2023</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;985 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#premise>Premise</a></li><li><a href=#ok-so-for-accumulating-my-information>Ok, so for accumulating my information,</a></li><li><a href=#ok-let-me-run-now-some-of-the-basic-question-answer-chains>Ok let me run now some of the basic question answer chains</a></li><li><a href=#ok-i-want-to-try-the-chain-which-gives-the-intermediate-results>Ok I want to try the chain which gives the intermediate results,</a></li><li><a href=#let-me-try-another-query>Let me try another query,</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=premise>Premise<a hidden class=anchor aria-hidden=true href=#premise>#</a></h3><p>Ok, got this half baked idea , combine my #brag-document with the available [[langchain]] QA chains into a proof of concept maybe I can call [[langchain interview me 2023-feb]]</p><p>So I&rsquo;m going to throw a bunch of my source material together, language based, accessible as plain text doc, and then I will run the <a href=https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/qa_with_sources.html>Link</a> examples that provide references, citations,</p><h3 id=ok-so-for-accumulating-my-information>Ok, so for accumulating my information,<a hidden class=anchor aria-hidden=true href=#ok-so-for-accumulating-my-information>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> yaml
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tempfile
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pytz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>utc_now</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> datetime<span style=color:#f92672>.</span>utcnow()<span style=color:#f92672>.</span>replace(tzinfo<span style=color:#f92672>=</span>pytz<span style=color:#f92672>.</span>UTC)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>utc_ts</span>(dt):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dt<span style=color:#f92672>.</span>strftime(<span style=color:#e6db74>&#34;%Y-%m-</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>T%H%M%S&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_yaml</span>(loc):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(loc) <span style=color:#66d9ef>as</span> fd:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> yaml<span style=color:#f92672>.</span>safe_load(fd)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>repos_dir <span style=color:#f92672>=</span> Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> repos_dir<span style=color:#f92672>.</span>is_dir()      
</span></span><span style=display:flex><span>experience_loc <span style=color:#f92672>=</span> repos_dir <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;my-challenges-and-accomplishments/experience.yaml&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>experiences_dict <span style=color:#f92672>=</span> read_yaml(experience_loc)[<span style=color:#e6db74>&#34;Descriptions&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sections <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> project, detail <span style=color:#f92672>in</span> experiences_dict<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    section <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> detail<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;company&#34;</span>):
</span></span><span style=display:flex><span>        company <span style=color:#f92672>=</span> detail<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;company&#34;</span>)
</span></span><span style=display:flex><span>        section <span style=color:#f92672>=</span> (<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;When I worked at </span><span style=color:#e6db74>{</span>company<span style=color:#e6db74>}</span><span style=color:#e6db74>, &#34;</span>
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;there was a project in </span><span style=color:#e6db74>{</span>detail[<span style=color:#e6db74>&#39;year&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>, </span><span style=color:#e6db74>{</span>project<span style=color:#e6db74>}</span><span style=color:#e6db74>.&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> detail<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;project&#34;</span>):
</span></span><span style=display:flex><span>        project <span style=color:#f92672>=</span> detail<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;project&#34;</span>)
</span></span><span style=display:flex><span>        section <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;In </span><span style=color:#e6db74>{</span>detail[<span style=color:#e6db74>&#39;year&#39;</span>]<span style=color:#e6db74>}</span><span style=color:#e6db74>, I had a side project, </span><span style=color:#e6db74>{</span>project<span style=color:#e6db74>}</span><span style=color:#e6db74>. &#34;</span>
</span></span><span style=display:flex><span>    section <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;. &#34;</span><span style=color:#f92672>.</span>join([x <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> detail<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;one-liners&#34;</span>, [])])
</span></span><span style=display:flex><span>    section <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#34;. &#34;</span><span style=color:#f92672>.</span>join([x <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> detail<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;stories&#34;</span>, [])])
</span></span><span style=display:flex><span>    sections<span style=color:#f92672>.</span>append(section)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> repos_dir <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-interview-me&#34;</span>    
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> workdir <span style=color:#f92672>/</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>utc_ts(utc_now())<span style=color:#e6db74>}</span><span style=color:#e6db74>-the-story-blurb.txt&#34;</span> 
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>path<span style=color:#f92672>.</span>write_text(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join(sections))
</span></span></code></pre></div><h3 id=ok-let-me-run-now-some-of-the-basic-question-answer-chains>Ok let me run now some of the basic question answer chains<a hidden class=anchor aria-hidden=true href=#ok-let-me-run-now-some-of-the-basic-question-answer-chains>#</a></h3><p>Use my environment from before,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>source ~/.python_venvs/langchainz/bin/activate
</span></span></code></pre></div><p>But when trying to use this just like per the qa with sources example in that <a href=https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/qa_with_sources.html>here</a>,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.embeddings.openai <span style=color:#f92672>import</span> OpenAIEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.embeddings.cohere <span style=color:#f92672>import</span> CohereEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> CharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.vectorstores.elastic_vector_search <span style=color:#f92672>import</span> ElasticVectorSearch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.vectorstores <span style=color:#f92672>import</span> Chroma
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.docstore.document <span style=color:#f92672>import</span> Document
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> PromptTemplate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> str(repos_dir <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-interview-me&#34;</span>  )  
</span></span><span style=display:flex><span>story_path <span style=color:#f92672>=</span> repos_dir <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-interview-me&#34;</span> <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-02-19T011846-the-story-blurb.txt&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_story <span style=color:#f92672>=</span> story_path<span style=color:#f92672>.</span>read_text()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> CharacterTextSplitter(chunk_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, chunk_overlap<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>texts <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_text(my_story)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> OpenAIEmbeddings()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docsearch <span style=color:#f92672>=</span> Chroma<span style=color:#f92672>.</span>from_texts(
</span></span><span style=display:flex><span>    texts, 
</span></span><span style=display:flex><span>    embeddings, 
</span></span><span style=display:flex><span>    metadatas<span style=color:#f92672>=</span>[{<span style=color:#e6db74>&#34;source&#34;</span>: str(i)} <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(texts))])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.chains.qa_with_sources <span style=color:#f92672>import</span> load_qa_with_sources_chain
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.llms <span style=color:#f92672>import</span> OpenAI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What kind of projects have I worked on with tensor flow?&#34;</span>
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> docsearch<span style=color:#f92672>.</span>similarity_search(query)
</span></span><span style=display:flex><span>chain <span style=color:#f92672>=</span> load_qa_with_sources_chain(OpenAI(temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>), chain_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;stuff&#34;</span>)
</span></span><span style=display:flex><span>chain({<span style=color:#e6db74>&#34;input_documents&#34;</span>: docs, <span style=color:#e6db74>&#34;question&#34;</span>: query}, return_only_outputs<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>this line,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>docsearch <span style=color:#f92672>=</span> Chroma<span style=color:#f92672>.</span>from_texts(texts, embeddings, metadatas<span style=color:#f92672>=</span>[{<span style=color:#e6db74>&#34;source&#34;</span>: str(i)} <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(texts))])
</span></span></code></pre></div><p>yielded a new install request, so that&rsquo;s what I did,</p><pre tabindex=0><code>ValueError: Could not import chromadb python package. Please it install it with `pip install chromadb`.

In [4]: !pip install chromadb
</code></pre><p>Ok wow, so I reran that again and I am getting,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Created a chunk of size <span style=color:#ae81ff>1474</span>, which <span style=color:#f92672>is</span> longer than the specified <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>Created a chunk of size <span style=color:#ae81ff>2846</span>, which <span style=color:#f92672>is</span> longer than the specified <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>Created a chunk of size <span style=color:#ae81ff>2348</span>, which <span style=color:#f92672>is</span> longer than the specified <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>Running Chroma using direct local API<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Using DuckDB <span style=color:#f92672>in</span><span style=color:#f92672>-</span>memory <span style=color:#66d9ef>for</span> database<span style=color:#f92672>.</span> Data will be transient<span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>1</span>]: {<span style=color:#e6db74>&#39;output_text&#39;</span>: <span style=color:#e6db74>&#34; I have worked on projects with Tensor Flow that involve predicting pilot&#39;s states of awareness, predicting bike share rider&#39;s destinations, and using Tensor Flow LSTM to answer questions about a CDC Covid dataset.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>SOURCES: 14, 15, 0&#34;</span>}
</span></span></code></pre></div><p>which is not bad</p><h4 id=so--also-i-realized-the-really-cool-part-about-the-approach-here-is-that-the-numbers-correspond-simply-to-the-indices-of-the-texts-and-the-texts-split-actually-intuitively-haha-by-a-double-new-line-nn-because-at-least-that-is-how-i-split-them-just-naturally>So , also I realized the really cool part about the approach here, is that the numbers, correspond simply to the indices of the texts, and the texts split, actually intuitively haha by a double new line, <code>\n\n</code> because at least that is how I split them just naturally.<a hidden class=anchor aria-hidden=true href=#so--also-i-realized-the-really-cool-part-about-the-approach-here-is-that-the-numbers-correspond-simply-to-the-indices-of-the-texts-and-the-texts-split-actually-intuitively-haha-by-a-double-new-line-nn-because-at-least-that-is-how-i-split-them-just-naturally>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>2</span>]: len(texts)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>2</span>]: <span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>6</span>]: text_splitter<span style=color:#f92672>.</span>_separator
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>6</span>]: <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#39;</span>
</span></span></code></pre></div><p>So since the above output says <code>14, 15, 0</code>, I think also this is somewhat sorted by relevance, so let me see what is in those,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>9</span>]: <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> [<span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>0</span>]:
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>:     print(<span style=color:#e6db74>&#34;i&#34;</span>, i, texts[i], <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>===================================================</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      
</span></span></code></pre></div><p>Okay when I looked at that ^^ I noticed actually some texts were getting actually smushed together? There was a lot of information. Going to create my text corpus one more time but this time separating by three newlines .</p><p>Ok let&rsquo;s see , I now created <code>2023-interview-me/2023-02-19T015128-the-story-blurb.txt</code> , lets see if there are more than 16 texts?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>story_path <span style=color:#f92672>=</span> repos_dir <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-interview-me&#34;</span> <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-02-19T015128-the-story-blurb.txt&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_story <span style=color:#f92672>=</span> Path(loc)<span style=color:#f92672>.</span>read_text()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> CharacterTextSplitter(chunk_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, chunk_overlap<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>texts <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_text(my_story)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>11</span>]: len(texts)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>11</span>]: <span style=color:#ae81ff>16</span>
</span></span></code></pre></div><p>Ah ok still 16 texts, but actually maybe this is based on the <code>chunk_size</code> ?</p><h3 id=ok-i-want-to-try-the-chain-which-gives-the-intermediate-results>Ok I want to try the chain which gives the intermediate results,<a hidden class=anchor aria-hidden=true href=#ok-i-want-to-try-the-chain-which-gives-the-intermediate-results>#</a></h3><p>Also needed one more package, here for the below,</p><pre tabindex=0><code>ValueError: Could not import tiktoken python package. This is needed in order to calculate get_num_tokens. Please it install it with `pip install tiktoken`.
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What kind of projects have I worked on with tensor flow?&#34;</span>
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> docsearch<span style=color:#f92672>.</span>similarity_search(query)
</span></span><span style=display:flex><span>chain <span style=color:#f92672>=</span> load_qa_with_sources_chain(OpenAI(temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>), 
</span></span><span style=display:flex><span>                                   chain_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;map_reduce&#34;</span>, 
</span></span><span style=display:flex><span>                                   return_intermediate_steps<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>chain({<span style=color:#e6db74>&#34;input_documents&#34;</span>: docs, <span style=color:#e6db74>&#34;question&#34;</span>: query}, return_only_outputs<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>Ok this looks like it only returns the relevant statements, nice.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>{<span style=color:#e6db74>&#39;intermediate_steps&#39;</span>: [<span style=color:#e6db74>&#34; In 2019, I had a side project, Reducing Commercial Aviation Fatalities (kaggle). Designed and built a Tensor Flow LSTM based model to predict pilot&#39;s states of awareness, given time series physiological data.&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39; None.&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39; None&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39; None.&#39;</span>],
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;output_text&#39;</span>: <span style=color:#e6db74>&#34; I have worked on a Tensor Flow LSTM based model to predict pilot&#39;s states of awareness, given time series physiological data.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>SOURCES: 14&#34;</span>}
</span></span></code></pre></div><p>And the important thing is that this code preserves the original text so I can nicely look for myself exactly at the primary source !</p><h3 id=let-me-try-another-query>Let me try another query,<a hidden class=anchor aria-hidden=true href=#let-me-try-another-query>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What is my  experience with Docker?&#34;</span>
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> docsearch<span style=color:#f92672>.</span>similarity_search(query)
</span></span><span style=display:flex><span>chain <span style=color:#f92672>=</span> load_qa_with_sources_chain(OpenAI(temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>), 
</span></span><span style=display:flex><span>                                   chain_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;map_reduce&#34;</span>, 
</span></span><span style=display:flex><span>                                   return_intermediate_steps<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>chain({<span style=color:#e6db74>&#34;input_documents&#34;</span>: docs, <span style=color:#e6db74>&#34;question&#34;</span>: query}, return_only_outputs<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;intermediate_steps&#39;</span>: [<span style=color:#e6db74>&#39; &#34;Dockerized our production underwriting stack and split from the main company git repo to give us the flexibility to deploy both scikit learn and XGBoost models with AWS SageMaker.&#34;&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39; &#34;Helped make better underwriting decisions on returning customers, by optimizing /re-engineering /versioning our SQL based logistic regression model, with a python + Docker + SQL pipeline, cutting runtime from 6+ hours to under an hour.&#34;&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39; None.&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39; None&#39;</span>],
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;output_text&#39;</span>: <span style=color:#e6db74>&#39; I have no experience with Docker.</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>SOURCES: 0, 14&#39;</span>}
</span></span></code></pre></div><p>Hah I feel like I need to do some kind of tuning here? But it is cool to see the intermediate step here even though it is not reflected in the final output.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://michal.piekarczyk.xyz/tags/openai/>openai</a></li><li><a href=https://michal.piekarczyk.xyz/tags/langchain/>langchain</a></li></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2023-02-18-fact-checking-dark-humor/><span class=title>« Prev</span><br><span>fact checking dark humor</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2023-02-03-policy-document-understanding/><span class=title>Next »</span><br><span>Notes from a recent hackathon</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>