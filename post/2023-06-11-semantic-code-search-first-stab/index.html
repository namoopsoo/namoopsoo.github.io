<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>semantic code search first stab | michal.piekarczyk.xyz</title><meta name=keywords content><meta name=description content="The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.
The documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2023-06-11-semantic-code-search-first-stab/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="semantic code search first stab"><meta property="og:description" content="The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.
The documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2023-06-11-semantic-code-search-first-stab/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-06-11T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-11T00:00:00+00:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="semantic code search first stab"><meta name=twitter:description content="The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.
The documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"semantic code search first stab","item":"https://michal.piekarczyk.xyz/post/2023-06-11-semantic-code-search-first-stab/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"semantic code search first stab","name":"semantic code search first stab","description":"The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.\nThe documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first.","keywords":[],"articleBody":"The idea here was to try out Sentence Transformers , https://sbert.net , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.\nThe documentation at https://www.sbert.net/examples/applications/semantic-search/README.html#python was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first.\nAlso, the test results here look pretty decent, but the next step will be to create a reference set of the line numbers that were expected to be found, to get a good precision/recall score for this.\nInitial Learnings Learned about [[symmetric vs asymmetric semantic search]] Here is the explanation, https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search . This distinction refers to #[[sentence similarity task]] where the query is of the same size or asymmetrically, smaller size, such as a one or two word query. And wow that is exactly what I was looking for ! So apparently this includes the “msmarco” models.\nFinal test run Had a few test runs today, iterating on the approach, using queries from files other than the ones I built a corpus for , #moment/doh haha . Also weirdly the msmarco model documented as the v3 that should be used is MIA somehow, but the v2 seems fine. And “msmarco-MiniLM-L-6-v3” is fine too.\nBut here is the last run for today. 18:43 lets try just a few files, so I had initially passed queries I wrote out for the CrossEncode.py file , #moment/doh haha , so that is why below, I am pulling 'msmarco-MiniLM-L-6-v3' and re-encoding because I was puzzled w/ the initial lackluster results and was trying a different model. But yea I realized and just trying this model with the good set of queries instead, id:: 64864e08-de92-4127-9162-8b5b946b021b id:: 6486561e-d050-4bca-bc74-64fa5b475ab4\ncorpus = [x[\"line\"] for x in dataset if \"Sentence\" in x[\"path\"]] embedder = SentenceTransformer( 'msmarco-MiniLM-L-6-v3', use_auth_token=hf_token, ) corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) In [27]: set([x[\"path\"] for x in dataset if \"Sentence\" in x[\"path\"]]) Out[27]: {'docs/package_reference/SentenceTransformer.md', 'sentence_transformers/SentenceTransformer.py', 'sentence_transformers/datasets/ParallelSentencesDataset.py', 'sentence_transformers/datasets/SentenceLabelDataset.py', 'sentence_transformers/datasets/SentencesDataset.py', 'sentence_transformers/evaluation/SentenceEvaluator.py', 'sentence_transformers/readers/LabelSentenceReader.py'} In [28]: len(corpus) Out[28]: 1266 In [34]: %%time ...: corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) ...: ...: # CPU times: user 22.3 s, sys: 4.25 s, total: 26.5 s # Wall time: 14.8 s queries = [ \"snapshot_download\", \"SentenceEvaluator\", \"get_torch_home\", \"os.path.join\", \"flax_model.msgpack\", \"torch.nn.functional.normalize\", ] top_k = min(5, len(corpus)) for query in queries: query_embedding = embedder.encode(query, convert_to_tensor=True) # We use cosine-similarity and torch.topk to find the highest 5 scores cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0] top_results = torch.topk(cos_scores, k=top_k) print(\"\\n\\n======================\\n\\n\") print(\"Query:\", query) print(\"\\nTop 5 most similar sentences in corpus:\") for score, idx in zip(top_results[0], top_results[1]): print(corpus[idx].strip(), \"(Score: {:.4f})\".format(score)) \"\"\" # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5) hits = hits[0] #Get the hits for the first query for hit in hits: print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score'])) \"\"\" 19:11 ok nice, this looks really good ! id:: 64864f3c-17ce-41b1-a279-39e89b1952e8\n====================== Query: snapshot_download Top 5 most similar sentences in corpus: snapshot_download(model_name_or_path, (Score: 0.6807) from .util import import_from_string, batch_to_device, fullname, snapshot_download (Score: 0.5153) # Download from hub with caching (Score: 0.4511) optimizer.step() (Score: 0.3790) cache_folder = os.path.join(torch_cache_home, 'sentence_transformers') (Score: 0.3619) ====================== Query: SentenceEvaluator Top 5 most similar sentences in corpus: class SentenceEvaluator: (Score: 0.8183) evaluator: SentenceEvaluator = None, (Score: 0.7497) from .evaluation import SentenceEvaluator (Score: 0.7004) the evaluator (Score: 0.5470) # SentenceTransformer (Score: 0.5083) ====================== Query: get_torch_home Top 5 most similar sentences in corpus: from torch.hub import _get_torch_home (Score: 0.8654) torch_cache_home = _get_torch_home() (Score: 0.8592) torch_cache_home = os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch'))) (Score: 0.6630) import torch (Score: 0.6166) import torch (Score: 0.6166) ====================== Query: os.path.join Top 5 most similar sentences in corpus: file_path = os.path.join(root, filename) (Score: 0.8010) if not os.path.exists(os.path.join(model_path, 'modules.json')): (Score: 0.7379) if os.path.exists(os.path.join(model_path, 'modules.json')): #Load as SentenceTransformer model (Score: 0.7044) if os.path.exists(model_card_path): (Score: 0.6783) if os.path.exists(model_name_or_path): (Score: 0.6707) ====================== Query: flax_model.msgpack Top 5 most similar sentences in corpus: model = SentenceTransformer('model-name') (Score: 0.4369) ignore_files=['flax_model.msgpack', 'rust_model.ot', 'tf_model.h5'], (Score: 0.4280) model: SentenceTransformer (Score: 0.4155) the model to evaluate (Score: 0.4112) Evaluate the model (Score: 0.4007) ====================== Query: torch.nn.functional.normalize Top 5 most similar sentences in corpus: torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm) (Score: 0.7218) torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm) (Score: 0.7218) embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1) (Score: 0.6244) scaler = torch.cuda.amp.GradScaler() (Score: 0.5749) from torch import nn, Tensor, device (Score: 0.5742) And code for building that corpus ok cool, let me just focus on markdown and python id:: 648728d8-5958-4df6-95d4-b81de6665974 Put the following into a file code_search.py\nfrom pathlib import Path from itertools import chain def build_texts_from_repository(repo_dir): \"\"\"Return a dataset of the code \"\"\" dataset = [] file_types = [] for path in chain( Path(repo_dir).glob(\"**/*.py\"), Path(repo_dir).glob(\"**/*.md\"), ): assert path.is_file() and path.suffix lines = path.read_text().splitlines() dataset.extend( [{\"line_number\": i, \"line\": line, \"path\": str(path.relative_to(repo_dir))} for i, line in enumerate(lines) ] ) return dataset import os import code_search as cs from pathlib import Path repos_dir = os.getenv(\"REPOS_DIR\") target_dir = Path(repos_dir) / \"sentence-transformers\" dataset = cs.build_texts_from_repository(target_dir) double checking ,\nIn [12]: dataset[:10] Out[12]: [{'line_number': 0, 'line': 'from setuptools import setup, find_packages', 'path': 'setup.py'}, {'line_number': 1, 'line': '', 'path': 'setup.py'}, {'line_number': 2, 'line': 'with open(\"README.md\", mode=\"r\", encoding=\"utf-8\") as readme_file:', 'path': 'setup.py'}, {'line_number': 3, 'line': ' readme = readme_file.read()', 'path': 'setup.py'}, {'line_number': 4, 'line': '', 'path': 'setup.py'}, {'line_number': 5, 'line': '', 'path': 'setup.py'}, {'line_number': 6, 'line': '', 'path': 'setup.py'}, {'line_number': 7, 'line': 'setup(', 'path': 'setup.py'}, {'line_number': 8, 'line': ' name=\"sentence-transformers\",', 'path': 'setup.py'}, {'line_number': 9, 'line': ' version=\"2.2.2\",', 'path': 'setup.py'}] In [13]: set([Path(x[\"path\"]).suffix for x in dataset]) Out[13]: {'.md', '.py'} ","wordCount":"895","inLanguage":"en","datePublished":"2023-06-11T00:00:00Z","dateModified":"2023-06-11T00:00:00Z","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2023-06-11-semantic-code-search-first-stab/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>semantic code search first stab</h1><div class=post-meta><span title='2023-06-11 00:00:00 +0000 UTC'>June 11, 2023</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;895 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#learned-about-symmetric-vs-asymmetric-semantic-search>Learned about [[symmetric vs asymmetric semantic search]]</a></li></ul><ul><li><a href=#and-code-for-building-that-corpus>And code for building that corpus</a></li></ul></nav></div></details></div><div class=post-content><p>The idea here was to try out Sentence Transformers , <a href=https://sbert.net>https://sbert.net</a> , on source code search. And as a first stab, a corpus was built with, hey why not, the code from the #sentence-transformers repo.</p><p>The documentation at <a href=https://www.sbert.net/examples/applications/semantic-search/README.html#python>https://www.sbert.net/examples/applications/semantic-search/README.html#python</a> was used for the basic test here. And a small bit of code at the bottom here, shows how the lines from the python source files were written to a python list first.</p><p>Also, the test results here look pretty decent, but the next step will be to create a reference set of the line numbers that were expected to be found, to get a good precision/recall score for this.</p><h1 id=initial-learnings>Initial Learnings<a hidden class=anchor aria-hidden=true href=#initial-learnings>#</a></h1><h2 id=learned-about-symmetric-vs-asymmetric-semantic-search>Learned about [[symmetric vs asymmetric semantic search]]<a hidden class=anchor aria-hidden=true href=#learned-about-symmetric-vs-asymmetric-semantic-search>#</a></h2><p>Here is the explanation, <a href=https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search>https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search</a> .
This distinction refers to #[[sentence similarity task]] where the query is of the same size or asymmetrically, smaller size, such as a one or two word query.
And wow that is exactly what I was looking for !
So apparently this includes the &ldquo;msmarco&rdquo; models.</p><h1 id=final-test-run>Final test run<a hidden class=anchor aria-hidden=true href=#final-test-run>#</a></h1><p>Had a few test runs today, iterating on the approach, using queries from files other than the ones I built a corpus for , #moment/doh haha . Also weirdly the msmarco model documented as the v3 that should be used is MIA somehow, but the v2 seems fine. And &ldquo;msmarco-MiniLM-L-6-v3&rdquo; is fine too.</p><p>But here is the last run for today.
18:43 lets try just a few files, so I had initially passed queries I wrote out for the <code>CrossEncode.py</code> file , #moment/doh haha , so that is why below, I am pulling <code>'msmarco-MiniLM-L-6-v3'</code> and re-encoding because I was puzzled w/ the initial lackluster results and was trying a different model. But yea I realized and just trying this model with the good set of queries instead,
id:: 64864e08-de92-4127-9162-8b5b946b021b
id:: 6486561e-d050-4bca-bc74-64fa5b475ab4</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>corpus <span style=color:#f92672>=</span> [x[<span style=color:#e6db74>&#34;line&#34;</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> dataset <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;Sentence&#34;</span> <span style=color:#f92672>in</span> x[<span style=color:#e6db74>&#34;path&#34;</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>embedder <span style=color:#f92672>=</span> SentenceTransformer(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;msmarco-MiniLM-L-6-v3&#39;</span>,
</span></span><span style=display:flex><span>    use_auth_token<span style=color:#f92672>=</span>hf_token,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>corpus_embeddings <span style=color:#f92672>=</span> embedder<span style=color:#f92672>.</span>encode(corpus, convert_to_tensor<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>27</span>]: set([x[<span style=color:#e6db74>&#34;path&#34;</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> dataset <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;Sentence&#34;</span> <span style=color:#f92672>in</span> x[<span style=color:#e6db74>&#34;path&#34;</span>]])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>27</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;docs/package_reference/SentenceTransformer.md&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sentence_transformers/SentenceTransformer.py&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sentence_transformers/datasets/ParallelSentencesDataset.py&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sentence_transformers/datasets/SentenceLabelDataset.py&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sentence_transformers/datasets/SentencesDataset.py&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sentence_transformers/evaluation/SentenceEvaluator.py&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sentence_transformers/readers/LabelSentenceReader.py&#39;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>28</span>]: len(corpus)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>28</span>]: <span style=color:#ae81ff>1266</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>34</span>]: <span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: corpus_embeddings <span style=color:#f92672>=</span> embedder<span style=color:#f92672>.</span>encode(corpus, convert_to_tensor<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span><span style=color:#75715e># CPU times: user 22.3 s, sys: 4.25 s, total: 26.5 s</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Wall time: 14.8 s</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>queries <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;snapshot_download&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;SentenceEvaluator&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;get_torch_home&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;os.path.join&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;flax_model.msgpack&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;torch.nn.functional.normalize&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>top_k <span style=color:#f92672>=</span> min(<span style=color:#ae81ff>5</span>, len(corpus))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> query <span style=color:#f92672>in</span> queries:
</span></span><span style=display:flex><span>    query_embedding <span style=color:#f92672>=</span> embedder<span style=color:#f92672>.</span>encode(query, convert_to_tensor<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># We use cosine-similarity and torch.topk to find the highest 5 scores</span>
</span></span><span style=display:flex><span>    cos_scores <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>cos_sim(query_embedding, corpus_embeddings)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    top_results <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>topk(cos_scores, k<span style=color:#f92672>=</span>top_k)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>======================</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Query:&#34;</span>, query)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Top 5 most similar sentences in corpus:&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> score, idx <span style=color:#f92672>in</span> zip(top_results[<span style=color:#ae81ff>0</span>], top_results[<span style=color:#ae81ff>1</span>]):
</span></span><span style=display:flex><span>        print(corpus[idx]<span style=color:#f92672>.</span>strip(), <span style=color:#e6db74>&#34;(Score: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>)&#34;</span><span style=color:#f92672>.</span>format(score))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    hits = hits[0]      #Get the hits for the first query
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    for hit in hits:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        print(corpus[hit[&#39;corpus_id&#39;]], &#34;(Score: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>)&#34;.format(hit[&#39;score&#39;]))
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span></code></pre></div><p>19:11 ok nice, this looks really good !
id:: 64864f3c-17ce-41b1-a279-39e89b1952e8</p><pre tabindex=0><code>======================


Query: snapshot_download

Top 5 most similar sentences in corpus:
snapshot_download(model_name_or_path, (Score: 0.6807)
from .util import import_from_string, batch_to_device, fullname, snapshot_download (Score: 0.5153)
# Download from hub with caching (Score: 0.4511)
optimizer.step() (Score: 0.3790)
cache_folder = os.path.join(torch_cache_home, &#39;sentence_transformers&#39;) (Score: 0.3619)


======================


Query: SentenceEvaluator

Top 5 most similar sentences in corpus:
class SentenceEvaluator: (Score: 0.8183)
evaluator: SentenceEvaluator = None, (Score: 0.7497)
from .evaluation import SentenceEvaluator (Score: 0.7004)
the evaluator (Score: 0.5470)
# SentenceTransformer (Score: 0.5083)


======================


Query: get_torch_home

Top 5 most similar sentences in corpus:
from torch.hub import _get_torch_home (Score: 0.8654)
torch_cache_home = _get_torch_home() (Score: 0.8592)
torch_cache_home = os.path.expanduser(os.getenv(&#39;TORCH_HOME&#39;, os.path.join(os.getenv(&#39;XDG_CACHE_HOME&#39;, &#39;~/.cache&#39;), &#39;torch&#39;))) (Score: 0.6630)
import torch (Score: 0.6166)
import torch (Score: 0.6166)


======================


Query: os.path.join

Top 5 most similar sentences in corpus:
file_path = os.path.join(root, filename) (Score: 0.8010)
if not os.path.exists(os.path.join(model_path, &#39;modules.json&#39;)): (Score: 0.7379)
if os.path.exists(os.path.join(model_path, &#39;modules.json&#39;)):    #Load as SentenceTransformer model (Score: 0.7044)
if os.path.exists(model_card_path): (Score: 0.6783)
if os.path.exists(model_name_or_path): (Score: 0.6707)


======================


Query: flax_model.msgpack

Top 5 most similar sentences in corpus:
model = SentenceTransformer(&#39;model-name&#39;) (Score: 0.4369)
ignore_files=[&#39;flax_model.msgpack&#39;, &#39;rust_model.ot&#39;, &#39;tf_model.h5&#39;], (Score: 0.4280)
model: SentenceTransformer (Score: 0.4155)
the model to evaluate (Score: 0.4112)
Evaluate the model (Score: 0.4007)


======================


Query: torch.nn.functional.normalize

Top 5 most similar sentences in corpus:
torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm) (Score: 0.7218)
torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm) (Score: 0.7218)
embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1) (Score: 0.6244)
scaler = torch.cuda.amp.GradScaler() (Score: 0.5749)
from torch import nn, Tensor, device (Score: 0.5742)
</code></pre><h2 id=and-code-for-building-that-corpus>And code for building that corpus<a hidden class=anchor aria-hidden=true href=#and-code-for-building-that-corpus>#</a></h2><p>ok cool, let me just focus on markdown and python
id:: 648728d8-5958-4df6-95d4-b81de6665974
Put the following into a file <code>code_search.py</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> itertools <span style=color:#f92672>import</span> chain
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_texts_from_repository</span>(repo_dir):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Return a dataset of the code
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    file_types <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> path <span style=color:#f92672>in</span> chain(
</span></span><span style=display:flex><span>        Path(repo_dir)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#34;**/*.py&#34;</span>),
</span></span><span style=display:flex><span>        Path(repo_dir)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#34;**/*.md&#34;</span>),
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> path<span style=color:#f92672>.</span>is_file() <span style=color:#f92672>and</span> path<span style=color:#f92672>.</span>suffix
</span></span><span style=display:flex><span>        lines <span style=color:#f92672>=</span> path<span style=color:#f92672>.</span>read_text()<span style=color:#f92672>.</span>splitlines()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        dataset<span style=color:#f92672>.</span>extend(
</span></span><span style=display:flex><span>            [{<span style=color:#e6db74>&#34;line_number&#34;</span>: i,
</span></span><span style=display:flex><span>               <span style=color:#e6db74>&#34;line&#34;</span>: line,
</span></span><span style=display:flex><span>               <span style=color:#e6db74>&#34;path&#34;</span>: str(path<span style=color:#f92672>.</span>relative_to(repo_dir))}
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i, line <span style=color:#f92672>in</span> enumerate(lines)
</span></span><span style=display:flex><span>         ]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dataset
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> code_search <span style=color:#66d9ef>as</span> cs
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>repos_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)
</span></span><span style=display:flex><span>target_dir <span style=color:#f92672>=</span> Path(repos_dir) <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;sentence-transformers&#34;</span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> cs<span style=color:#f92672>.</span>build_texts_from_repository(target_dir)
</span></span></code></pre></div><p>double checking ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>12</span>]: dataset[:<span style=color:#ae81ff>10</span>]
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>12</span>]: 
</span></span><span style=display:flex><span>[{<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;from setuptools import setup, find_packages&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;with open(&#34;README.md&#34;, mode=&#34;r&#34;, encoding=&#34;utf-8&#34;) as readme_file:&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;    readme = readme_file.read()&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>4</span>, <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>5</span>, <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>6</span>, <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>7</span>, <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;setup(&#39;</span>, <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;    name=&#34;sentence-transformers&#34;,&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;line_number&#39;</span>: <span style=color:#ae81ff>9</span>, <span style=color:#e6db74>&#39;line&#39;</span>: <span style=color:#e6db74>&#39;    version=&#34;2.2.2&#34;,&#39;</span>, <span style=color:#e6db74>&#39;path&#39;</span>: <span style=color:#e6db74>&#39;setup.py&#39;</span>}]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>13</span>]: set([Path(x[<span style=color:#e6db74>&#34;path&#34;</span>])<span style=color:#f92672>.</span>suffix <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> dataset])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>13</span>]: {<span style=color:#e6db74>&#39;.md&#39;</span>, <span style=color:#e6db74>&#39;.py&#39;</span>}
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://michal.piekarczyk.xyz/post/2023-06-10-spinoza-vs-descartes/><span class=title>Next »</span><br><span>Decartes vs Spinoza</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>