<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>langchain interview me 2023 feb | michal.piekarczyk.xyz</title><meta name=keywords content><meta name=description content="type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18
Note This is not a blog post but kind of a landing page I&rsquo;m using to aggregate on-going project notes here
Vision Broadly would like to do here something like the following
compare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since that‚Äôs a moving target ."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="langchain interview me 2023 feb"><meta property="og:description" content="type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18
Note This is not a blog post but kind of a landing page I&rsquo;m using to aggregate on-going project notes here
Vision Broadly would like to do here something like the following
compare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since that‚Äôs a moving target ."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-02T09:33:08-04:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary"><meta name=twitter:title content="langchain interview me 2023 feb"><meta name=twitter:description content="type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18
Note This is not a blog post but kind of a landing page I&rsquo;m using to aggregate on-going project notes here
Vision Broadly would like to do here something like the following
compare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since that‚Äôs a moving target ."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://michal.piekarczyk.xyz/post/"},{"@type":"ListItem","position":2,"name":"langchain interview me 2023 feb","item":"https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"langchain interview me 2023 feb","name":"langchain interview me 2023 feb","description":"type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18\nNote This is not a blog post but kind of a landing page I\u0026rsquo;m using to aggregate on-going project notes here\nVision Broadly would like to do here something like the following\ncompare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since that‚Äôs a moving target .","keywords":[],"articleBody":"type:: #project-type status:: #in-progress-status blogDate:: 2023-02-18\nNote This is not a blog post but kind of a landing page I‚Äôm using to aggregate on-going project notes here\nVision Broadly would like to do here something like the following\ncompare against arbitrary #job-listings , #job-description , collapsed:: true And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since that‚Äôs a moving target . And you can interview yourself too haha .\nI can use the [[my projects/personal/langchain-interview-me-2023-feb]] stuff concepts to see , what roles online do I align with and am I progressing towards them at #Humana or stagnating?\nMaking updating your #brag-document like a #fun-factor #[[having fun]] experience üòÄ And original intent was a UI to actually ask questions Also better #TellMeAboutYourself , #[[tell a story]] . Since the #brag-document has lots of cool stories, and also #chronological-story , this could be a cool way to weave together the personal story. collapsed:: true And for [[my projects/personal/langchain-interview-me-2023-feb]] thing, so I was in this [[May 28th, 2023]] too. Would be cool to make it easier for an individual to construct their [[TellMeAboutYourself]] since this is so important and at least to myself cannot rely on my memory haha\nGetting feedback about your text corpus of your experience Maybe the documents out there can help inform you, of other relevant terms that you forgot to discuss. Also maybe there is low-information density in your corpus. Take out the stop words haha.\nmy blog posts initial post with the #question-answer-task 20:55 So I have the #blog-post from [[Feb 18th, 2023]] here, where I put together my technical background , create embeddings from them and run a #question-answer-task #langchain , with one of the chains called ‚Äúload_qa_with_sources_chain‚Äù that gives intermediate source text results too.\nAlso this one collapsed:: true [[blogpost/2023-06-25-everybody-loves-reynauds]] https://michal.piekarczyk.xyz/post/2023-06-25-everybody-loves-reynauds with a comparison across a few embedding models, to suss out which of them do or do not have medical vocabulary\nAlso applying sentence transformers to code search part one and part two\nresearch went through that [[article/Getting Started With Embeddings]] , which was useful to start learning about #sentence-transformers library collapsed:: true And more recently, I went through the #[[hugging face]] example around #Medicare and with the #article-type , [[article/Getting Started With Embeddings]] , link,\nAnd used the ‚Äúlangchainz‚Äù virtual env I have, and I used the https://api-inference.huggingface.co REST API specifying to use the ‚Äúsentence-transformers/all-MiniLM-L6-v2‚Äù model to produce embeddings , and then the #sentence-transformers library, semantic_search , ( from sentence_transformers.util import semantic_search ) , to a question to a set of frequently asked questions\nI have this question, is the #sentence-transformers #average-pooling noisy? Can I use better #NER [[Named Entity Recognition NER]] ? Maybe help from https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da ?\nattempts on [[May 28th, 2023]], I started defining the #job-description comparison concept, and I ran a comparison of my blurb ‚Äú2023-02-19T011846-the-story-blurb.txt‚Äù against ‚Äú2023-05-28-enigma-mle.txt‚Äù . The results were maybe somewhat not easy to read. Perhaps a lot of text. Maybe I need shorter sentences? collapsed:: true\nMotivation / plan So, now , let me create a quick tool, to cross the sentences of a brag document, against like 10 job description embeddings, and help match them, to understand say, two kinds of problems,\n(1) Which job descriptions match the best, (2) but then also, for a specific job description, which sentences are matched and which are not matched. (3) So help you know, say even if you are not necessarily looking for something right now, can you know do you align with recent postings in your field?\nOutcome 21:50 okay here‚Äôs a quick example,\nimport torch import json from pathlib import Path import os import requests from sentence_transformers.util import semantic_search model_id = \"sentence-transformers/all-MiniLM-L6-v2\" api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\" headers = {\"Authorization\": f\"Bearer {hf_token}\"} def query(texts): response = requests.post(api_url, headers=headers, json={\"inputs\": texts, \"options\":{\"wait_for_model\":True}}) return response.json() repos_dir = os.getenv(\"REPOS_DIR\") workdir = str(Path(repos_dir) / \"2023-interview-me\" ) loc = str(Path(repos_dir) / \"2023-interview-me/2023-02-19T011846-the-story-blurb.txt\") my_story_vec = Path(loc).read_text().split(\"\\n\") folder = \"job_descriptions\" jd1 = Path(folder) / \"2023-05-28-enigma-mle.txt\" texts = jd1.read_text().split(\"\\n\") output = query(my_story_vec) my_story_embeddings = torch.FloatTensor(output) output = query(texts) jd_embeddings = torch.FloatTensor(output) hits = semantic_search(my_story_embeddings, jd_embeddings, top_k=10) 22:24 Ah interesting, so since unlike the #Medicare #faq tutorial, where one question was given, I am passing an array now so my output is now also multi-dimensional\n# [[texts[x[\"corpus_id\"]], x[\"corpus_id\"]] for x in hits[0] ] for i, row in enumerate(hits): print(f\"({i})\", \"matching,\", my_story_vec[i], \":\") hmm = [[texts[x[\"corpus_id\"]], x[\"corpus_id\"], x[\"score\"]] for x in row[:3] ] print(hmm, \"\\n\\n\") (1) matching, : [['', 34, 1.0], ['', 1, 0.9999997615814209], ['', 9, 0.9999997615814209]] (2) matching, When I worked at zibby1, there was a project in 2015, various earlier projects.Created a Vagrant virtual machine based staging environment that developers can quickly use to stage code, to help us transition from personalized AWS staging environments which can potentially help us save several hundreds of dollars a month.. : [['‚Ä¢ Has experience working with distributed computing and building CI/CD tools.', 26, 0.34121203422546387], ['‚Ä¢ Engineers best-in-class solutions that enables data scientists to develop, test, explain, deploy and monitor statistical models to production environments (we use PySpark)', 14, 0.3387572765350342], ['As a member of Machine Learning team, you will build the ML systems and infrastructure at the core of our small business data product. Your impact will be measured by the performance, testability and reliability of our ML systems.', 10, 0.28739088773727417]] (3) matching, Implemented the retailer lead list reporting, so that big data heavy retailers like Sears could finally be more involved in following up with customers who were not originating their preapprovals.. : [['‚Ä¢ Is driven to work with customers to have an impact on the real world', 29, 0.3841177821159363], ['‚Ä¢ Impact: your work product will have a direct impact on hundreds of millions of significant decisions within the massive small business economy', 21, 0.28213953971862793], ['This is a critical and exciting time at Enigma. We are hearing from repeated customers that our product is creating tremendous value for them and is aligned perfectly with their needs. This creates an urgent need to accelerate the build out of our machine learning capabilities', 4, 0.2662915289402008]] Okay there is some beginnings of something here. Got to do some more preprocessing on this data though, do get way more cleaner comparisons .\nand on [[Jun 18th, 2023]] , how about #spacy and #[[Named Entity Recognition NER]] , collapsed:: true Think because yea I saw that #sentence-transformers #[[cosine similarity]] between my #brag-document sentences and #job-description was super low, so thinking hey how about extract entities and then attempt matches using that instead, Initially I saw that the first extraction was pulling only very few entities for this job description for instance, 19:33 hmm ok but , this is not capturing all the entities, hmm weird,\nimport spacy nlp = spacy.load(\"en_core_web_sm\") In [3]: jd = \"\"\" ...: Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our pro ...: ducts need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information r ...: etrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; ...: the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projec ...: ts as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across t ...: he full-stack as we continue to push technology forward. ...: ...: With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions ...: . ...: ...: The web is what you make of it and our team is helping the world make more of the web. From open-source pros to user-experience extraordinaires, we develop products that help ...: users connect, communicate and collaborate with others. Our consumer products and cloud platforms are giving millions of users at homes, businesses, universities and nonprofit ...: s around the world the tools that shape their web experience -- and changing the way they think about computing. ...: \"\"\" In [4]: doc = nlp(jd) In [5]: for ent in doc.ents: ...: print(ent.text, ent.start_char, ent.end_char, ent.label_) ...: Google 1 7 ORG billions 86 94 CARDINAL UI 504 506 GPE Google‚Äôs 641 649 ORG millions 1396 1404 CARDINAL and on [[Jun 25th, 2023]] the [[blogpost/2023-06-25-everybody-loves-reynauds]] So in that mini blogpost, I tried out multiple #[[embedding space]] using different embedding models. And it looked like only ‚Äúall-MiniLM-L12-v2‚Äù appeared to have some kind of [[medical-condition]] knowledge .\n[[Jul 6th, 2023]] , can I do a #[[supervised fine-tuning]] #[[my first]] , collapsed:: true yea so just starting , going through , between https://www.sbert.net/docs/training/overview.html and [[article/Train and Fine-Tune Sentence Transformers Models]]\n08:35 so yea if a particular out of the box model uses [[average-pooling]] then for sure that yells at me that [[stop-words]] should be removed hmm 08:39 ACtually looking at https://www.kaggle.com/datasets?search=job and hmm I do see job related datasets. Maybe there are some relevant ones !? How about, say, https://www.kaggle.com/datasets/niyamatalmass/google-job-skills , obtained by way of #selenium . Ok cool so this gives me hope that maybe in the future I can pull some more posts in the future, hopefully [[web-scrape]] is still possible later. 08:55 ok that is actually pretty decent, looking at the ‚Äújob_skills.csv‚Äù . Some nice jargon in there ! 09:04 ok so of the 4 dataset cases in https://huggingface.co/blog/how-to-train-sentence-transformers , I think makes most sense to use Case 2, where instead of assigning a number from 0 to 1 for similarity, I can just choose sentences that. I feel are similar to beuild a dataset. These are ‚Äúpositive pairs‚Äù [[positive pair]] So https://huggingface.co/datasets/embedding-data/sentence-compression here is a reference example that uses this. #[[Lossy compression]] perhaps . Kind of cool since yea #summarization-task is kind of this. Some details are missed yes but get the main idea #TLDR . I see pretty simple, each row is a json looking pair. Ah ok [[json lines]] right. Learned about this from [[Michael Light]]. https://jsonlines.org/examples/ nice. 09:13 ok so I can write some dataset building code like this,\nfrom sentence_transformers import InputExample from torch.utils.data import DataLoader train_examples = [] dataset = read_json_lines(...) for i, x in enumerate(dataset): s1, s2 = x train_examples.append( InputExample(texts=[s1, s2])) train_dataloader = DataLoader( train_examples, shuffle=True, batch_size=16) feels like I should use my [[my projects/personal/manage-my-photos]] labeler to help me kind of somewhat quickly build some labels. Ok and for case 2 of [[positive pair]] looks like https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss [[Multiple negatives ranking loss]] #[[loss function]] should be used\nfrom sentence_transformers import losses from sentence_transformers import SentenceTransformer model_id = \"sentence-transformers/all-MiniLM-L6-v2\" model = SentenceTransformer(model_id) train_loss = losses.MultipleNegativesRankingLoss(model=model) # fine tune model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10) just try for a handful then?\nimport pandas as pd import os from pathlib import Path loc = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/job_skills.csv\") df = pd.read_csv(loc) 09:30 ok stopping here. next can continue to try out the first example of this fine tuning.\n[[Jul 7th, 2023]] some [[paraphrase-mining]] hmm how can I build my dataset 08:40 [[my projects/personal/langchain-interview-me-2023-feb]]\ncollapsed:: true so yea next was going to write that csv data, see can I do a fine tune , first try haha, Wonder if I can dump out all the sentences , use out of the box similarity to see what looks like might be related, and maybe I can use the labeling annotation system I have to refine? Ok, what are the closest right now?\nimport pandas as pd import os from pathlib import Path from functools import reduce from collections import Counter loc = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/job_skills.csv\") df = pd.read_csv(loc) raw_sentences = reduce(lambda x, y: x + y, [re.split(r\"[\\n\\.]\", df.iloc[i][col]) for i in range(df.shape[0]) for col in [\"Responsibilities\", 'Minimum Qualifications', 'Preferred Qualifications'] if not pd.isnull(df.iloc[i][col]) ] ) sentences = list(set(raw_sentences)) hmm ok\nIn [101]: dict(Counter(raw_sentences).most_common(5)) Out[101]: {'': 14038, 'BA/BS degree or equivalent practical experience': 521, 'g': 261, \"Bachelor's degree or equivalent practical experience\": 71, ' Specific responsibilities are assigned to interns at the start of the program': 69} In [102]: len(raw_sentences), len(sentences) Out[102]: (31424, 9421) 09:13 ok and similarities , [[paraphrase-mining]]\n%%time from sentence_transformers import SentenceTransformer, util model = SentenceTransformer('all-MiniLM-L6-v2') paraphrases = util.paraphrase_mining(model, sentences) for paraphrase in paraphrases[0:10]: score, i, j = paraphrase print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score)) 09:21 wow that was pretty fast .\nWork with Google Cloud Platform Partners to develop campaigns Work with Google Cloud Platform partners to develop campaigns Score: 1.0000 Collect customer support data from partners and derive insights for cross-functional teams Collect customer support data from partners and derive insights for cross-functional teams Score: 1.0000 10 years of partner programs experience at an enterprise software (or Cloud) company and experience with competitive partner programs 10 years of partner programs experience at an Enterprise Software (or Cloud) company and experience with competitive partner programs Score: 1.0000 9 years of experience serving in the capacity of a technical sales engineer in a cloud computing environment or equivalent experience in a customer facing role (including working as a member of a professional services or systems engineering team) 9 years of experience serving in the capacity of a Technical Sales Engineer in a cloud computing environment or equivalent experience in a customer facing role (including working as a member of a professional services or systems engineering team) Score: 1.0000 Identify, engage, and advise Google-caliber talent with a focus on creating a great experience for each candidate Identify, engage, and advise Google-caliber talent with a focus on creating a great experience for each candidate Score: 1.0000 Manage a team of software engineers, including task planning and code reviews Manage a team of Software Engineers, including task planning and code reviews Score: 1.0000 Perform an array of administrative tasks (Manage calendars, book travel, and schedule facilities and equipment) Perform an array of administrative tasks (manage calendars, book travel, and schedule facilities and equipment) Score: 1.0000 Understanding of solution architecture within web and mobile environments and technical experience of web/internet related technologies, architecture across SAAS, PAAS and IAAS and competitive cloud productivity suites Understanding of solution architecture within web and mobile environments and technical experience of web/internet related technologies, architecture across SaaS, PaaS and IaaS and competitive cloud productivity suites Score: 1.0000 Extensive knowledge of UNIX/Linux environments Extensive knowledge of Unix/Linux environments Score: 1.0000 Experience working towards strategic business goals Experience working towards strategic business goals Score: 1.0000 CPU times: user 1min 41s, sys: 3.35 s, total: 1min 44s Wall time: 57.3 s Ok seeing even though I used ‚Äúset‚Äù I still have dupes . Ok seeing , should also use strip and lower case too 09:25 ok\nstripped_raw_sentences = [x.strip().lower() for x in raw_sentences] sentences = list(set(stripped_raw_sentences)) print(len(raw_sentences), len(set(raw_sentences)), len(sentences)) # 31424 9421 9325 %%time from sentence_transformers import SentenceTransformer, util model = SentenceTransformer('all-MiniLM-L6-v2') paraphrases = util.paraphrase_mining(model, sentences) for paraphrase in paraphrases[0:10]: score, i, j = paraphrase print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score)) cpa / ca or other professional accounting accreditation cpa/ca or other professional accounting accreditation Score: 1.0000 10 years of partner programs experience at a enterprise software (or cloud) company and experience with competitive partner programs 10 years of partner programs experience at an enterprise software (or cloud) company and experience with competitive partner programs Score: 1.0000 5 years of partner programs experience at a enterprise software (or cloud) company 5 years of partner programs experience at an enterprise software (or cloud) company Score: 1.0000 technically minded, with an understanding of the technology and cloud computing market, and a passion for google cloud products (g-suite, google cloud platform) technically minded, with a understanding of the technology and cloud computing market, and a passion for google cloud products (g-suite, google cloud platform) Score: 0.9999 shape google‚Äôs approach to partnership strategy with stakeholders in partner programs, product management, engineering, sales, and marketing; support regional engagement with strategic global and regional partners shape google‚Äôs approach to partnership strategy with stakeholders in partner programs, product management, engineering, sales and marketing; support regional engagement with strategic global and regional partners Score: 0.9998 a combination of hr experience in the following areas: organizational design, succession planning, performance management, diversity and inclusion, business consulting, coaching and development, talent management, data analysis and employee relations a combination of hr experience in the following areas: organizational design, succession planning, performance management, diversity and inclusion, business consulting, coaching and development, talent management, data analysis, and employee relations Score: 0.9998 assist clients in the adoption of new products via upgrades and migrations to develop their long term success and improve product offerings by providing client feedback on features to product management and engineering assist clients in the adoption of new products via upgrades and migrations to develop their long-term success and improve product offerings by providing client feedback on features to product management and engineering Score: 0.9995 build strong relationships and operating rhythms with leaders inside and outside their core product team to efficiently implement user experiences that are cohesive, inclusive and well-informed build strong relationships and operating rhythms with leaders inside and outside their core product team to efficiently implement user experiences that are cohesive, inclusive, and well-informed Score: 0.9995 take responsibility for technical aspects of solutions to include such activities as supporting bid responses, product and solution briefings, proof-of-concept work and the coordination of supporting technical resources take responsibility for technical aspects of solutions to include such activities as supporting bid responses, product and solution briefings, proof-of-concept work, and the coordination of supporting technical resources Score: 0.9994 experience serving in the capacity of a technical sales engineer in a cloud computing environment or equivalent experience in a customer facing role (including working as a member of a professional services or systems engineering team) experience serving in the capacity of a technical sales engineer in a cloud computing environment or equivalent experience in a customer-facing role (including working as a member of a professional services or systems engineering team) Score: 0.9994 CPU times: user 1min 45s, sys: 2.84 s, total: 1min 48s Wall time: 1min 09:41 ok haha I can see there are some arbitrary internal space differences as well haha 08:49 ok lets find 10 [[positive pair]] , and use that ,\npairs = [ [] ] [[Jul 10th, 2023]] looked at the vocabulary misses and job titles collapsed:: true\nnoticed that the paraphrase mining output is not full looks like more or less we get the better matches first\nthe model I‚Äôm testing with does have technical data sources 08:56 haha this is not simple, so many sentences, is there any way of getting around hand labeling?\nMaybe I can look for technical terms which I suspect are not part of the #vocabulary , hmm So https://huggingface.co/datasets/code_search_net and [[stack exchange]] duplicate questions and actually many other technical datasets are used per https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 ,\nhmm oh the AutoTokenizer is a way to get tokens and vocabulary in the model collapsed:: true 09:14 tokenizer?\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [00:00\u003c00:00, 36.5kB/s] Downloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00\u003c00:00, 7.02MB/s] Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00\u003c00:00, 8.44MB/s] Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112/112 [00:00\u003c00:00, 28.4kB/s] In [129]: tokenizer.vocab_files_names Out[129]: {'vocab_file': 'vocab.txt', 'tokenizer_file': 'tokenizer.json'} well that looks good ! Like a nice way perhaps to see the vocabulary,\nIn [131]: vocabulary = tokenizer.get_vocab() In [133]: len(vocabulary) Out[133]: 30522 In [135]: print(list(vocabulary.keys())[:30]) ['##iq', \"##'\", '1723', 'italians', 'caretaker', 'debbie', 'bloomberg', 'enforcing', 'sex', 'flicking', 'likes', 'glimpse', 'relax', 'coward', 'eyelids', 'worth', 'dynamics', '##¬π', 'recognizes', 'arcadia', 'deportivo', 'pointedly', 'iowa', '##rio', 'moved', '—è', 'news', 'whoever', 'blossom', 'preserved'] 09:21 Okay let me look for like if a few vocabulary terms in job descriptions are there,\njob_terms = [ \"html\", \"databricks\", \"python\", \"css\", \"api\", \"postgresql\", \"database\", \"mysql\", \"clojure\", \"java\", \"javascript\", \"angular\", \"idempotent\", \"azure\", \"github\", \"git\", \"concurrency\", \"asyncio\", \"dbutils\", \"ipython\", \"docker\", \"pipeline\", \"sklearn\", \"tensorflow\", \"pytorch\", \"numpy\", \"pandas\", \"ec2\", \"ecs\", \"aws\", \"sagemaker\", \"nginx\", \"redis\", \"cli\", \"auc\", \"xgboost\", \"repository\"] from tqdm import tqdm hits = [] no_hits = [] for term in job_terms: for token in tqdm(vocabulary.keys()): if term in token: hits.append([term, token]) no_hits = list(set(job_terms) - set([x[0] for x in hits])) In [137]: len(hits) Out[137]: 117 In [138]: hits Out[138]: [['api', 'rapidly'], ['api', 'shapiro'], ['api', 'shaping'], ['database', 'database'], ['ecs', 'ecstasy'], ['git', 'illegitimate'], ['angular', 'triangular'], ['git', 'digits'], ['cli', 'clicks'], ['cli', 'inclination'], ['api', 'apical'], ['java', 'java'], ['cli', 'cycling'], ['cli', 'clip'], ['cli', 'clit'], ['api', 'capitals'], ['cli', 'clicked'], ['cli', 'cliff'], ['concurrency', 'concurrency'], ['auc', 'caucus'], ['java', 'javanese'], ['cli', 'clifton'], ['cli', 'client'], ['git', 'legitimacy'], ['api', 'capita'], ['cli', 'clicking'], ['git', 'digit'], ['api', 'capitalist'], ['aws', 'flaws'], ['cli', 'incline'], ['cli', 'climbs'], ['cli', 'inclined'], ['git', 'digitally'], ['git', 'legitimate'], ['cli', 'decline'], ['cli', 'clinical'], ['git', 'longitude'], ['cli', 'declining'], ['pipeline', 'pipeline'], ['cli', 'climax'], ['cli', 'clinics'], ['api', 'capitol'], ['aws', 'laws'], ['aws', 'claws'], ['api', 'rapid'], ['azure', 'azure'], ['api', 'dilapidated'], ['angular', 'rectangular'], ['api', 'api'], ['api', 'gaping'], ['auc', 'caucasus'], ['redis', 'rediscovered'], ['cli', 'declines'], ['cli', 'eclipse'], ['git', 'agitated'], ['auc', 'bureaucracy'], ['api', 'scraping'], ['cli', 'clive'], ['database', 'databases'], ['api', 'therapist'], ['git', 'longitudinal'], ['cli', 'cyclist'], ['cli', 'climates'], ['cli', 'clinging'], ['auc', 'caucasian'], ['angular', 'angular'], ['cli', 'radcliffe'], ['cli', 'clinched'], ['git', 'agitation'], ['api', 'capitalism'], ['cli', 'recycling'], ['aws', 'lawson'], ['git', 'fugitive'], ['cli', 'cyclists'], ['api', 'capital'], ['python', 'python'], ['aws', 'paws'], ['cli', 'clint'], ['cli', 'clifford'], ['cli', '##cliff'], ['auc', 'auction'], ['cli', 'circling'], ['repository', 'repository'], ['auc', 'sauce'], ['html', 'html'], ['cli', 'clips'], ['aws', 'outlaws'], ['cli', '##cliffe'], ['api', 'escaping'], ['aws', 'lawsuit'], ['cli', 'clinch'], ['api', 'leaping'], ['api', 'rapids'], ['cli', 'clients'], ['auc', 'auckland'], ['cli', 'climb'], ['cli', 'climate'], ['cli', 'cyclic'], ['aws', 'dawson'], ['cli', 'declined'], ['cli', 'click'], ['cli', 'climatic'], ['cli', 'clinic'], ['aws', 'lawsuits'], ['cli', 'climbing'], ['api', 'napier'], ['aws', 'draws'], ['api', 'landscaping'], ['aws', 'jaws'], ['git', 'digital'], ['cli', 'clinton'], ['redis', 'redistribution'], ['git', '##git'], ['cli', 'climbed'], ['cli', 'clipped'], ['cli', 'cliffs'], ['cli', 'euclidean']] ok this is interesting then\nIn [140]: no_hits Out[140]: ['github', 'databricks', 'mysql', 'pytorch', 'sklearn', 'postgresql', 'docker', 'nginx', 'idempotent', 'sagemaker', 'xgboost', 'css', 'clojure', 'dbutils', 'tensorflow', 'asyncio', 'pandas', 'numpy', 'ec2', 'ipython', 'javascript'] and job titles, maybe I could group along that first collapsed:: true 09:36 also another thing for trying next is I should also cut up and do paraphrase mining perhaps within the particular job title, (printing just a sample below )\nIn [144]: print(df[\"Title\"].unique().tolist()[:20]) ['Google Cloud Program Manager', 'Supplier Development Engineer (SDE), Cable/Connector', 'Data Analyst, Product and Tools Operations, Google Technical Services', 'Developer Advocate, Partner Engineering', 'Program Manager, Audio Visual (AV) Deployments', 'Associate Account Strategist (Czech/Slovak), Global Customer Experience', 'Supplier Development Engineer, Camera, Consumer Hardware', 'Strategic Technology Partner Manager, Healthcare and Life Sciences', 'Manufacturing Business Manager, Google Hardware', 'Solutions Architect, Healthcare and Life Sciences, Google Cloud', 'Data Analyst, Consumer Hardware', 'Partner Onboarding Manager (Americas)', 'Associate Account Strategist (Ukrainian), GMS Sales', 'Survey Lead, Google Cloud Support', 'Solution Architect, Google Cloud Platform (Apigee)', 'Manufacturing Test Engineer', 'Machine Learning Product Specialist, Google Cloud (EMEA)', 'Software Engineering Manager, Cloud Storage, Site Reliability Engineering', 'Global Supply Chain Manager, Display/Touch, Consumer Hardware', 'Technical Program Manager, ASIC Development'] In [145]: len(df[\"Title\"].unique().tolist()) Out[145]: 794 this list of titles is pretty extensive and might have duplicates also\nthoughts for later use vocabulary misses maybe to figure out what to fine tune with\n[[Jul 11th, 2023]] refined the nohits per the vocabulary of the model and used it to tokenize to verify they are unknown collapsed:: true\nyea no hits first let me use more precise way of looking for hits,\njob_terms = [ \"html\", \"databricks\", \"python\", \"css\", \"api\", \"postgresql\", \"database\", \"mysql\", \"clojure\", \"java\", \"javascript\", \"angular\", \"idempotent\", \"azure\", \"github\", \"git\", \"concurrency\", \"asyncio\", \"dbutils\", \"ipython\", \"docker\", \"pipeline\", \"sklearn\", \"tensorflow\", \"pytorch\", \"numpy\", \"pandas\", \"ec2\", \"ecs\", \"aws\", \"sagemaker\", \"nginx\", \"redis\", \"cli\", \"auc\", \"xgboost\", \"repository\"] from tqdm import tqdm hits = [] no_hits = [] for term in job_terms: for token in tqdm(vocabulary.keys()): if term == token.strip(\"#\"): hits.append([term, token]) no_hits = list(set(job_terms) - set([x[0] for x in hits])) 08:58 yea thats a lot of nohits,\nIn [151]: hits Out[151]: [['html', 'html'], ['python', 'python'], ['api', 'api'], ['database', 'database'], ['java', 'java'], ['angular', 'angular'], ['azure', 'azure'], ['git', '##git'], ['concurrency', 'concurrency'], ['pipeline', 'pipeline'], ['repository', 'repository']] In [152]: no_hits Out[152]: ['github', 'databricks', 'ecs', 'mysql', 'pytorch', 'sklearn', 'auc', 'aws', 'postgresql', 'docker', 'nginx', 'idempotent', 'sagemaker', 'cli', 'xgboost', 'css', 'clojure', 'dbutils', 'tensorflow', 'asyncio', 'pandas', 'redis', 'numpy', 'ec2', 'ipython', 'javascript'] and drafting looking for the nohits in the dataset, So do I see job descriptions that have those job terms I did not find vocabulary hits for?\nimport pandas as pd import os from pathlib import Path loc = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/job_skills.csv\") jobsdf = pd.read_csv(loc) import utils as ut columns = [\"Responsibilities\", 'Minimum Qualifications', 'Preferred Qualifications'] raw_sentences = ut.extract_raw_sentences(jobsdf, columns) sentences_with_oov_tokens = [] for sentence in raw_sentences: words = re.split(r\"[^a-zA-Z0-9]\", sentence) words = [x for x in words if x] # for token in no_hits: realized how to tokenize an arbitrary sentence and therefore I can see this model indeed does not know about that vocabulary ! 09:23 let me just try to tokenize a fabricated sentence that has the no hit tokens,\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') sentence = \"familiar with xgboost pandas and tensorflow including docker and other technologies\" sentences = [sentence] encoded_input = tokenizer( sentences, padding=True, truncation=True, return_tensors='pt') {'input_ids': tensor([[ 101, 5220, 2007, 1060, 18259, 9541, 3367, 25462, 2015, 1998, 23435, 12314, 2164, 8946, 2121, 1998, 2060, 6786, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} hmm not really clear since these are numeric encodings, how to get the vocabulary debugging part here. 09:38 ah ok never mind found it in the docs here https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization 09:38 ok so here is how to #debugging #tokenizer #tokenization\nsentence = \"familiar with xgboost pandas and tensorflow including docker and other technologies\" tokenizer.tokenize(sentence) In [178]: print(tokenizer.tokenize(sentence)) ['familiar', 'with', 'x', '##gb', '##oo', '##st', 'panda', '##s', 'and', 'tensor', '##flow', 'including', 'dock', '##er', 'and', 'other', 'technologies'] so yea super interesting ! if a particular word is not recognized in the vocabulary, it just gets split up into stuff that is known or the ## is used perhaps to create some kinds of smaller #subword-tokenization . I was reading the documentation of that tokenizer and found this section, #+BEGIN_QUOTE\nis_split_into_words (`bool`, *optional*, defaults to `False`): Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace) which it will tokenize. This is useful for NER or token classification. #+END_QUOTE which I think is pretty cool, referring to #[[Named Entity Recognition NER]] , used with this, 09:41 next ok yea thinking would love to inform this model of the entities, vocabulary t hat is missing.\n[[Jul 12th, 2023]] ok started building up code to capture a mini corpus, of the sentences, which have words that are not part of the vocabulary, collapsed:: true 08:17 [[my projects/personal/langchain-interview-me-2023-feb]]\n09:05 ok wow organized earlier notes a bit ! So should I therefore, collect the sentences that have the no hits and at least see what happens if I fine tune with those, if the new #sentence-transformers model has the new vocabulary ? ok, so to build a corpus, thinking for each sentence in this dataset I am working with right now, if I tokenize it using the AutoTokenizer from 'sentence-transformers/all-MiniLM-L6-v2' , and the output does not include my desired tokens or tokens prefixed with ## but the sentence does have the words in question visible in plain text, then that sentence is a candidate for the fine tuning set I think ! 09:16 Also I just glanced through what this out put, looks like,\nfrom transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') vocabulary = tokenizer.get_vocab() print(vocabulary.keys()) And I don‚Äôt see anything upper case so pretty sure I can stick to lower case ! So first the slower way, and maybe I can find a faster #PyTorch way later, 09:34 ok drafting this on the side still. but high level concept yea, find sentences that have the one or more of the desired terms in plain text, that actually might be good enough, as long as I have checked indeed the words are no hits against the model vocabulary but can also tokenize such sentences and verify that the expected tokens do not exist 09:36 have a high level #question though, per #subword-tokenization how do you contain #[[Named Entity Recognition NER]] concepts if they can end up being broken up? #card Like even in the example in https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization , somehow ‚Äútransformer‚Äù is not in\n\"bert-base-cased\" isn‚Äôt that kind of silly?\nand [[Jul 13th, 2023]] , got a bunch of the no hit sentences, at least for some definition, 08:41 [[my projects/personal/langchain-interview-me-2023-feb]]\ncollapsed:: true hm ok,\nimport utils as ut nohit_list = ut.current_nohit_list(\"sentence-transformers/all-MiniLM-L6-v2\") raw_sentences = ut.extract_raw_sentences(jobsdf, columns) 09:02 ok will filter oov words like this\nIn [191]: for x in raw_sentences[:4]: ...: print(\"=============\") ...: print(x, \"\\n\", ut.sequence_from_sentence(x), \"\\n\") ...: ============= lead projects from start to finish and manage all issues that impact design ['lead', 'projects', 'from', 'start', 'to', 'finish', 'and', 'manage', 'all', 'issues', 'that', 'impact', 'design'] ============= break the mold, and bring creativity and innovation in strategy and tactics ['break', 'the', 'mold', 'and', 'bring', 'creativity', 'and', 'innovation', 'in', 'strategy', 'and', 'tactics'] ============= become a brand advocate; engage and influence internal and external relationships; build, customize and deliver solutions through forums to achieve outcomes in support of the brand advertising annual plan ['become', 'a', 'brand', 'advocate', 'engage', 'and', 'influence', 'internal', 'and', 'external', 'relationships', 'build', 'customize', 'and', 'deliver', 'solutions', 'through', 'forums', 'to', 'achieve', 'outcomes', 'in', 'support', 'of', 'the', 'brand', 'advertising', 'annual', 'plan'] ============= experience in java and/or python development ['experience', 'in', 'java', 'and', 'or', 'python', 'development'] 09:08 only search some of the technical roles maybe, to try to get faster results,\nraw_titles = ut.extract_raw_sentences(jobsdf, [\"Title\"]) title_vocab = reduce(lambda x, y: x + y, [ut.sequence_from_sentence(x) for x in raw_titles] ) print(Counter(title_vocab).most_common(25)) [('manager', 300), ('google', 237), ('cloud', 167), ('and', 127), ('sales', 89), ('marketing', 87), ('engineer', 79), ('technical', 71), ('account', 64), ('lead', 64), ('business', 63), ('partner', 62), ('solutions', 61), ('operations', 59), ('product', 57), ('specialist', 57), ('services', 53), ('english', 52), ('analyst', 52), ('hardware', 51), ('associate', 48), ('global', 46), ('program', 44), ('customer', 43), ('development', 42)] ok based off of that, ‚Äúengineer‚Äù feels like a safe assumption here, 09:17 ok so just the engineer sentences then,\nIn [203]: jobsdf[jobsdf[\"Title\"].str.contains(\"engineer\")].shape Out[203]: (0, 7) In [204]: jobsdf[jobsdf[\"Title\"].str.contains(\"Engineer\")].shape Out[204]: (140, 7) In [205]: jobsdf.shape Out[205]: (1250, 7) columns = [\"Responsibilities\", 'Minimum Qualifications', 'Preferred Qualifications'] engineer_df = jobsdf[jobsdf[\"Title\"].str.contains(\"Engineer\")].copy() raw_sentences = ut.extract_raw_sentences(engineer_df, columns) nohit_sentences = ut.find_nohit_sentences(raw_sentences, nohit_list) In [211]: len(raw_sentences), len(nohit_sentences) Out[211]: (1217, 38) In [212]: nohit_sentences Out[212]: [['programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby', ['javascript']], ['experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver', ['javascript']], ['experience with java for android, objective-c for ios, html, javascript', ['javascript']], ['experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)', ['tensorflow']], ['software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)', ['javascript']], ['familiarity in one or more common web or mobile development language such as java, python, go, php, javascript, etc', ['javascript']], ['experience with front-end web technologies (html5, css3, and javascript)', ['javascript']], ['technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar', ['mysql']], ['familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)', ['tensorflow']], ['html5, css3, and javascript development experience', ['javascript']], ['java, c/c++, c#, python, javascript, or go)', ['javascript']], ['experience with web technologies (object-oriented javascript, html, css), and experience with the latest web standards including html5 and css3', ['javascript', 'css']], ['experience programming in one of the following: java, javascript and/or c++', ['javascript']], ['4 years of relevant work experience, including web application experience or skills using ajax, html, css or javascript', ['javascript', 'css']], [', sql, mysql, mapreduce, hadoop)', ['mysql']], ['experience working with deployment and orchestration technologies (such as pxe, docker, kubernetes, puppet, chef, salt, ansible, jenkins)', ['docker']], ['development experience in c, c++ or java and experience designing modular, object-oriented javascript', ['javascript']], ['expert html and css skills', ['css']], [', unit, functional, integration, stress testing) for your code, using one or more of the following: c, c++, c#, java, javascript, go, or python', ['javascript']], ['experience in writing software in one or more languages such as java, c++, python, go, javascript', ['javascript']], ['experience with one or more general purpose programming languages including but not limited to: c/c++, c#, python, javascript, go, objective-c, swift', ['javascript']], ['fluency in one or more of the following: python, javascript, java, php, perl, or c++', ['javascript']], ['previous tech internships or relevant work experience programming in c, c++, c#, java, javascript, go or python', ['javascript']], [', object-oriented javascript, html, css)', ['javascript', 'css']], ['restful, soap, etc), and javascript', ['javascript']], ['experience in backend development and using one or more cloud platform services (aws, azure, gcp)', ['aws']], ['1 year of experience in software engineering and coding, working with two or more of the following languages: java, c/c++, c#, objective-c, python, javascript, php, ruby and/or go', ['javascript']], ['4 years of experience working with front end languages such as html5, css, javascript (angularjs)', ['javascript', 'css']], ['experience with web technologies such as html, css, javascript, and http', ['javascript', 'css']], ['software development platforms and solutions to include j2ee, java servlets, javascript, python, go, php, asp, cgi, ajax', ['javascript']], [', r, python, matlab, pandas) and database languages (e', ['pandas']], ['experience with modern javascript frameworks (such as backbone, angular, or ember) and css pre-processing frameworks (such as sass or less)', ['javascript', 'css']], ['experience in writing code fixes and tools to solve problems in c, c++, c#, java, javascript, go or python (e', ['javascript']], ['net, python, shell, perl, javascript)', ['javascript']], ['programming experience in one or more of the following languages/platforms: android, java, kotlin, ios, javascript', ['javascript']], ['experience with one or more general purpose programming languages including but not limited to: java, c/c++, c#, objective c, python, javascript, or go', ['javascript']], ['experience in writing software in one or more languages such as java, python, go, javascript, c++, or similar', ['javascript']], ['experience with java for android, and objective-c for ios, html and javascript', ['javascript']]] 09:25 hmm also actually seeing sometimes splitting on a \".\" is not quite accurate. okay so next, since this is not looking terribly like a whole lot of sentences, can manually assign the ones that are similar, say, and try a fit.\n[[Jul 15th, 2023]] finally tried the [[supervised fine-tuning]] but didn‚Äôt seem to add to the vocabulary collapsed:: true\ncreated clusters manually, by looking at my no hit list from earlier, of sentences containing words that were not in the vocabulary, collapsed:: true 20:07 going to just manually create some groups,\n# web stuff, front end leaning group1 = [ 'programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby', 'experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver', 'software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)', 'experience with front-end web technologies (html5, css3, and javascript)', 'html5, css3, and javascript development experience', 'experience with web technologies (object-oriented javascript, html, css), and experience with the latest web standards including html5 and css3', '4 years of relevant work experience, including web application experience or skills using ajax, html, css or javascript', 'expert html and css skills', 'restful, soap, etc), and javascript', '4 years of experience working with front end languages such as html5, css, javascript (angularjs)', 'experience with web technologies such as html, css, javascript, and http', 'software development platforms and solutions to include j2ee, java servlets, javascript, python, go, php, asp, cgi, ajax', 'experience with modern javascript frameworks (such as backbone, angular, or ember) and css pre-processing frameworks (such as sass or less)', ] # feeling more back end mle ish, group3 = [ 'experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)', 'technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar', 'familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)', ', sql, mysql, mapreduce, hadoop)', 'experience working with deployment and orchestration technologies (such as pxe, docker, kubernetes, puppet, chef, salt, ansible, jenkins)', 'experience in backend development and using one or more cloud platform services (aws, azure, gcp)', ', r, python, matlab, pandas) and database languages ', ] # mobile dev group2 = [ 'experience with java for android, objective-c for ios, html, javascript', 'familiarity in one or more common web or mobile development language such as java, python, go, php, javascript, etc', 'java, c/c++, c#, python, javascript, or go)', 'experience programming in one of the following: java, javascript and/or c++', 'development experience in c, c++ or java and experience designing modular, object-oriented javascript', ', unit, functional, integration, stress testing) for your code, using one or more of the following: c, c++, c#, java, javascript, go, or python', 'experience in writing software in one or more languages such as java, c++, python, go, javascript', 'experience with one or more general purpose programming languages including but not limited to: c/c++, c#, python, javascript, go, objective-c, swift', 'fluency in one or more of the following: python, javascript, java, php, perl, or c++', '1 year of experience in software engineering and coding, working with two or more of the following languages: java, c/c++, c#, objective-c, python, javascript, php, ruby and/or go', 'experience in writing code fixes and tools to solve problems in c, c++, c#, java, javascript, go or python ', 'programming experience in one or more of the following languages/platforms: android, java, kotlin, ios, javascript', 'experience with one or more general purpose programming languages including but not limited to: java, c/c++, c#, objective c, python, javascript, or go', 'experience in writing software in one or more languages such as java, python, go, javascript, c++, or similar', 'experience with java for android, and objective-c for ios, html and javascript', ] Then created a dataset from that, and ran fit with the out of the box ‚Äòall-MiniLM-L6-v2‚Äô sentence transformer model collapsed:: true 20:36 since https://huggingface.co/datasets/embedding-data/sentence-compression/tree/main is given as the example and since I see those [[json lines]] , but it is with [[git-lfs]] , let me try pull it as appropriate,\nok file was ‚Äúsentence-compression_compressed.jsonl.gz‚Äù, internally looks like this\n$ head data/kaggle-google-job-skills/sentence-compression_compressed.jsonl {\"set\": [\"The USHL completed an expansion draft on Monday as 10 players who were on the rosters of USHL teams during the 2009-10 season were selected by the League's two newest entries, the Muskegon Lumberjacks and Dubuque Fighting Saints.\", \"USHL completes expansion draft\"]} {\"set\": [\"Major League Baseball Commissioner Bud Selig will be speaking at St. Norbert College next month.\", \"Bud Selig to speak at St. Norbert College\"]} {\"set\": [\"It's fresh cherry time in Michigan and the best time to enjoy this delicious and nutritious fruit.\", \"It's cherry time\"]} {\"set\": [\"An Evesham man is facing charges in Pennsylvania after he allegedly dragged his girlfriend from the side of his pickup truck on the campus of Kutztown University in the early morning hours of Dec. 5, police said.\", \"Evesham man faces charges for Pa.\"]} {\"set\": [\"NRT LLC, one of the nation's largest residential real estate brokerage companies, announced several executive appointments within its Coldwell Banker Residential Brokerage operations in Southern California.\", \"NRT announces executive appointments at its Coldwell Banker operations in Southern California\"]} {\"set\": [\"THE JSE kept toying with an all time high by midday today as resources continued to fuel the bourse.\", \"JSE keeps toying with all time high\"]} {\"set\": [\"The government is defending the latest police crime statistics despite a worrying rise in the recorded amount of violent offending.\", \"Government defends crime statistics\"]} {\"set\": [\"The renovated Marappalam bridge, which had been opened for two-wheelers last week, was opened for other vehicles also on Friday.\", \"Marappalam bridge opened\"]} {\"set\": [\"A new survey shows 30 percent of Californians use Twitter, and more and more of us are using our smart phones to go online.\", \"Survey: 30 percent of Californians use Twitter\"]} {\"set\": [\"Brightpoint ,a provider of logistic services to the mobile industry, has started operations in the Turkish market.\", \"Brightpoint starts operations on Turkish market\"]} 20:50 ah ok the literal word ‚Äúset‚Äù really is in there okay !\nimport os import utils as u from pathlib import Path path = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl\") dataset = u.make_positive_pairs_from_groups(group1, group2, group3) path.write_text(\"\\n\".join([json.dumps(x) for x in dataset])) from sentence_transformers import InputExample from torch.utils.data import DataLoader train_examples = [] for i, x in enumerate(dataset): train_examples.append( InputExample(texts=[x[\"set\"][0], x[\"set\"][1]]) ) train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) # MultipleNegativesRankingLoss from sentence_transformers import losses model = SentenceTransformer('all-MiniLM-L6-v2') train_loss = losses.MultipleNegativesRankingLoss(model=model) model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10) 21:31 ok started that . Actually going pretty fast as I expected since yea my dataset is small for a proof of concept ,\nIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:16\u003c00:00, 1.26s/it] Epoch: 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1/10 [00:16\u003c02:27, 16.36s/it] Iteration: 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12/13 [00:19\u003c00:01, 1.64s/it] ... ... CPU times: user 4min 8s, sys: 38.8 s, total: 4min 47s Wall time: 2min 43s hmm but new vocabulary does not seem to reflect new terms somehow And yea curious if I can see the vocabulary now as different,\npath = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/2023-07-15-fine-tuned-on-pairs.foo\") model.save(path) 21:42 oh nice, I see the vocab.txt got saved, in that folder,\n# 2023-07-15-fine-tuned-on-pairs.foo/vocab.txt\" path = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/2023-07-15-fine-tuned-on-pairs.foo\" / \"vocab.txt\") vocab = path.read_text() In [250]: set(nohit_list) \u0026 set(vocab) Out[250]: set() In [251]: set([f\"##{x}\" for x in nohit_list]) \u0026 set(vocab) Out[251]: set() 21:48 hmm not seeing any words from the nohit list in the dumped out vocab though. hmm ok back to the drawing board then? haha\n[[Jul 16th, 2023]] yea tried a different take on adding tokens to a tokenizer and that seemed to do it. collapsed:: true\nyea it was not ‚Äútokenizer.json‚Äù 11:17 ok so next though,\n11:23 hmm interesting, I also looked at the ‚Äútokenizer.json‚Äù file that got created when doing model.save(), next to the ‚Äúvocab.txt‚Äù. They have the same tokens looks like except ‚Äútokenizer.json‚Äù also refers to the input ids [[tokenized-input-ids]] , 11:30 hmm but maybe fine tuning simply does not update the vocabulary?\nbut ‚Äúadd_tokens‚Äù collapsed:: true 12:01 ok super interesting, reading here on medium someone kind of confirming that to expand the vocabulary [[add to transformer vocabulary]], and prevent [[out-of-vocabulary-words-OOV]], you need another approach,\n12:32 lets try their recommendation, just took out the for-loop since yea looking at current documentation for add_tokens function, you can add a list instead. Incorporating w/ a check of what is my hit list and no hit list ,\nfrom transformers import AutoTokenizer, AutoModel model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') nohit_list = ut.current_nohit_list(\"sentence-transformers/all-MiniLM-L6-v2\") # Before vocabulary_before = list(tokenizer.get_vocab().keys()) tokenizer.add_tokens(nohit_list) # add new embeddings to the embedding matrix of the transformer model model.resize_token_embeddings(len(tokenizer)) # After vocabulary_after = list(tokenizer.get_vocab().keys()) # Did it work? 14:09 hmm got an error,\nAttributeError: 'SentenceTransformer' object has no attribute 'resize_token_embeddings' when trying to resize . Maybe need to go one level down, to the lower layer.\nfor child in model.children(): print(child, hasattr(child, \"resize_token_embeddings\"), \"\\n\") Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel False Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}) False Normalize() False 14:13 hmm nope, weird. But ok looks like this part worked,\nIn [263]: print(set(vocabulary_after) - set(vocabulary_before)) {'github', 'databricks', 'nlp', 'ecs', 'pytorch', 'pyspark', 'sklearn', 'auc', 'aws', 'postgresql', 'docker', 'nginx', 'idempotent', 'sagemaker', 'xgboost', 'cli', 'css', 'clojure', 'spacy', 'ipython', 'dbutils', 'tensorflow', 'asyncio', 'redis', 'pandas', 'numpy', 'ec2', 'mysql', 'javascript'} How about the tokenize command?\nsentence = \"familiar with xgboost pandas and tensorflow including docker and other technologies\" print(tokenizer.tokenize(sentence)) ['familiar', 'with', 'xgboost', 'pandas', 'and', 'tensorflow', 'including', 'docker', 'and', 'other', 'technologies'] 14:18 ok nice #moment/satisfaction well that does seem to work. so next, question is then, I should attempt to do some #[[cosine similarity]] , before and after, to understand did this really help üòÄ\n[[Jul 17th, 2023]] Reading more, I learn you do likely need to train a new tokenizer and you can‚Äôt just simply update its vocabulary collapsed:: true\nQuick side question I had about this last tokenizer and its case awareness, out of curiosity, does tokenize now show this for upper case too now? Should be yes right since this is a uncased model\nsentence = \"familiar with XGBoost pandas and TensorFlow including Docker and other technologies\" print(tokenizer.tokenize(sentence)) ['familiar', 'with', 'xgboost', 'pandas', 'and', 'tensorflow', 'including', 'docker', 'and', 'other', 'technologies'] 08:47 nice . answer is yes.\nyea hugging face docs, collapsed:: true So I suppose that now okay this is how you add tokens to this tokenizer, but two problems still.\nWell one obvious problem is the tokenizer now needs to be thrown back into the model, But also, so what if the tokenizer now has this vocabulary, I think now the [[supervised fine-tuning]] step next can help tell this model what is the association of these new tokens in the [[embedding space]] right? Otherwise, without that, I‚Äôm curious what would the output vector , embedding, even look like for sentences with those new words? Like a undefined error? or like a equivalent of a zero vector ? 09:03 ok wow so the answer is in their nice course here, chapter 6 , on training [[tokenizer]] [[train new tokenizer from an old one]] So funny enough, the example being used here is [[code understanding]] , [[source code embedding]] and so this dataset is used, to update the tokenizer of gpt-2,\nraw_datasets = load_dataset(\"code_search_net\", \"python\") from transformers import AutoTokenizer old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) fastinating side note mentioned here is that there are tokenizers that can be written in python, which are slow and also can be written in #Rust-lang and also #cuda . hmm so ok then you can save that tokenizer,\ntokenizer.save_pretrained(\"code-search-net-tokenizer\") but how about updating the original model then ? 09:21 ok well conceptually, skipping ahead in the [[hugging face]] course there, I see here in chapter 7, that you can use a Trainer from\nfrom transformers import Trainer in order to fine tune a model and pass a tokenizer as an input, So per above I suspect that is the answer to my question!\nSo thinking about next steps Ok so a conceptual update here, I think maybe I need to hunt down some datasets or build a dataset which has additional technical language, and then use that to fine tune a tokenizer, and not just add vocabulary to it with tokenizer.add_tokens haha that was not a full answer. Yea and then I would need to use some of the tips in chapter 6 and 7 of the [[hugging face]] course to fine tune a model but a sentence transformer model say, with the tokenizer that I updated.\n[[Jul 18th, 2023]] reading more about subword tokenization and purpose of tokenizer tuning collapsed:: true not sure if my use case where technical terms are lacking from a tokenizer warrants this but maybe 09:07 So then I‚Äôm not sure why the https://huggingface.co/blog/how-to-train-sentence-transformers method I used earlier for fine tuning, earlier here the other day , did not update the vocabulary. ,\n09:13 Hmm ok maybe the explanation they give up front here helps to confirm that fine tuning a transformer model will not update the tokenizer. My impression is that fine tuning a transformer model is really just going to update the weights and since we know that many transformer models use [[subword-tokenization]], although you have new words, the fine tuning weight adjustments are made off of the subword tokens that are less meaningful than if they had concepts mapped out for those new words Although I am also slightly getting the impression that training a tokenizer is less about vocabulary and more about like #grammar because in their phrasing they refer not to the differences in the vocabulary between #English and #Japanese but to the differences in punctuation . And they are fine tuning a tokenizer on [[source code]] which definitely has different #grammar . 09:26 yea and additionally here they spell out that [[train new tokenizer from an old one]] is really about deciding what are good [[subword-tokenization]] to use, actually what sub-words. hmm But a tokenizer does indeed have a vocabulary so hmm, is my problem a tokenizer vocabulary problem or is it really I should be looking at this as a [[Named Entity Recognition NER]] problem ?\nbut yea next should try, 09:32 anyway I should still just try this,\nso next let me apply the mini corpus I had from last time, to this, and lets see what the new tokenizer does\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) [[Jul 19th, 2023]] Ran the tokenizer fine tuning with a small dataset The vocabulary of the output of train_new_from_iterator had only the new data. So ‚Äúmission accomplished‚Äù haha I got the new vocabulary in there but at the cost of missing the original vocabulary üòÖ\nTrying this out collapsed:: true so let‚Äôs follow along per hugging face nlp chapter 6 ,\n09:23 what does the data they are passing in for their use case look like?\nfrom datasets import load_dataset # This can take a few minutes to load, so grab a coffee or tea while you wait! raw_datasets = load_dataset(\"code_search_net\", \"python\") training_corpus = ( raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000) ) ok haha I don‚Äôt want to download the whole thing because I am using a laptop tethered to my phone [[phone-tether]] #moment/haha . But I can just look at it online, https://huggingface.co/datasets/code_search_net , per the above, each row is a record and ‚Äúwhole_func_string‚Äù is a string of the function definition. So their training_corpus data would look like a list of strings basically. Ok let me repurpose my mini [[positive pair]] dataset from earlier for this then,\nimport os import json import utils as u from functools import reduce from pathlib import Path path = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl\") \"2023-07-15-positive-pairs.jsonl\" training_corpus = list(set( reduce( lambda x, y: x + y, [json.loads(x)[\"set\"] for x in Path(path).read_text().splitlines()] ) )) 09:40 ok so the second argument to\ntrain_new_from_iterator is the vocab_size . And currently it is,\nfrom transformers import AutoTokenizer old_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') old_tokenizer.vocab_size # Out[275]: 30522 ok the 52,000 in that doc is a lot more. Curious what happens ,\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) [00:00:00] Pre-processing sequences ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0 / 0 [00:00:00] Tokenize words ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 182 / 182 [00:00:00] Count pairs ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 182 / 182 [00:00:00] Compute merges ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 527 / 527 In [278]: tokenizer.vocab_size Out[278]: 601 09:44 hmm interesting, so perhaps for this to work properly, I would have needed all the original data also, concatenated with the new data? Let me just last thing, look at the vocab and hit or no hit with my terms at least,\nold_vocabulary = old_tokenizer.get_vocab() vocabulary = tokenizer.get_vocab() len(old_vocabulary), len(vocabulary) # Out[281]: (30522, 601) import utils as ut job_terms = ut.get_nohit_job_terms() print(job_terms) # ['html', 'databricks', 'python', 'css', 'api', 'postgresql', 'database', 'mysql', 'clojure', 'java', 'javascript', 'angular', 'idempotent', 'azure', 'github', 'git', 'concurrency', 'asyncio', 'dbutils', 'ipython', 'docker', 'pipeline', 'sklearn', 'tensorflow', 'pytorch', 'numpy', 'pandas', 'ec2', 'ecs', 'aws', 'sagemaker', 'nginx', 'redis', 'cli', 'auc', 'xgboost', 'repository', 'pyspark', 'nlp', 'spacy'] In [285]: set(job_terms) \u0026 set(vocabulary) Out[285]: {'angular', 'aws', 'azure', 'css', 'database', 'docker', 'html', 'java', 'javascript', 'mysql', 'pandas', 'python', 'tensorflow'} ok haha well good to know at least this does indeed update the vocabulary for at least the new stuff. But since vocab is super small, makes me think yea this needs to be built using original and new data , from scratch perhaps.\nThoughts summary thinkings here\nI think I should go back to the original task of evaluating how good is a model at matching a brag document to job description texts, perhaps also building my own dataset that can be used for this evaluation on a out of the box model, comparing then to something new\n[[Jul 20th, 2023]] did bit of reading learning , research mode Think I am now convinced that yes, having new terminology is a good reason for a new tokenizer, because otherwise a tokenizer that does not have the new words, will do excessive splitting and am embedding model will be less likely to get useful signal from them,\nmain benefit of train_new_from_iterator , is lets you quickly use the same class as an earlier tokenizer, but yea this is not a fine tuning step like I thought before Looking at notes from yesterday, and that new tokenizer, yea it has some of the new vocabulary,\nSome of it is not sub-worded, remaining intact\nprint(set(job_terms) \u0026 set(vocabulary)) {'aws', 'tensorflow', 'angular', 'docker', 'pandas', 'database', 'java', 'mysql', 'azure', 'css', 'python', 'html', 'javascript'} And yea some of it underwent [[subword-tokenization]]\nprint(set(job_terms) - set(vocabulary)) {'databricks', 'api', 'nlp', 'ecs', 'concurrency', 'pyspark', 'pytorch', 'sklearn', 'auc', 'pipeline', 'postgresql', 'nginx', 'idempotent', 'sagemaker', 'cli', 'xgboost', 'git', 'repository', 'clojure', 'spacy', 'dbutils', 'asyncio', 'redis', 'numpy', 'ec2', 'ipython', 'github'} 09:11 So this sentence become,\nprint(tokenizer.tokenize( \"Within databricks, you can use pyspark or scala, but to use tensorflow or pytorch in databricks, you need to stick to pyspark.\")) ['with', '##i', '##n', 'data', '##b', '##ri', '##ck', '##s', ',', 'yo', '##u', 'c', '##a', '##n', 'us', '##e', 'py', '##s', '##p', '##ark', 'or', 'sca', '##la', ',', 'but', 'to', 'us', '##e', 'tensorflow', 'or', 'py', '##t', '##or', '##ch', 'in', 'data', '##b', '##ri', '##ck', '##s', ',', 'yo', '##u', 'ne', '##e', '##d', 'to', 'st', '##ic', '##k', 'to', 'py', '##s', '##p', '##ark', '[UNK]'] Ok so I suppose the advantage of the form,\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) is you are using the specific class of that earlier tokenizer,\nprint(old_tokenizer.__class__.__name__, tokenizer.__class__.__name__) BertTokenizerFast BertTokenizerFast and training a tokenizer from scratch, I can find,\nfrom tokenizers import BertWordPieceTokenizer but I don‚Äôt see BertTokenizerFast in that same namespace\nand learned more precisely why , [[why a custom tokenizer]] , and also actually how a typical [[byte-pair encoding]] [[tokenizer]] algorithm relies on sub word frequencies when building a vocabulary 09:56 ok so then I am missing the primary reason maybe also of the custom tokenizer? [[why a custom tokenizer]]\nPerson with this medium post, is also under the impression that a custom tokenizer [[tokenizer]] is useful as a way of focusing on your unique vocabulary,\nour domain is very specific, words and concepts about clothes, shapes, colors, ‚Ä¶ Therefore, we are interested in defining our own tokenizer created from our specific vocabulary, avoiding including more common words from other domains or use cases that are irrelevant for our final purpose. 10:13 ok reading, here for some more detail, So the focus around tokenizing\n\"Don't you love ü§ó Transformers? We sure do.\" w.r.t. [\"Don't\", \"you\",] versus [\"Don\", \"'\", \"t\"] versus [\"Do\", \"n't\"] does hint that yes you do want to help extract units of meaning, And they say that yes you ideally want a smaller vocabulary size to help constrain computation, but using say [[character tokenization]] although ends up w/ a small vocabulary, will be less expressive and capturing the meaning of words will be more difficult. Feels like in the case of jargon words, like #acronym or just brand new words like ‚ÄúTensorflow‚Äù or ‚Äúpytorch‚Äù , I can see sometimes subword tokenization will be helpful, since say it would be great if the word ‚Äútensor‚Äù was tokenized as meaningfully related to ‚Äútensorflow‚Äù, similarly between ‚Äúpytorch‚Äù and ‚Äúpython‚Äù. And I get that if you are using a subword tokenizer with a good bit of language as a input, then you should have enough to prevent the [[out-of-vocabulary-words-OOV]] misses , 10:51 Also they describe the example of BertTokenizer tokenizing an acronym it has not seen,\nfrom transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") tokenizer.tokenize(\"I have a new GPU!\") [\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"] And they only mention the benefit, of not having the vocabulary miss, but no mention of the meaning of the concept of #GPU getting missed . Oh and they explain that the #double-hashtag allows tokenization to be reversible since now you know to re-attach the subwords. But not every tokenizer is reversible I think. Or at least not every tokenizer has the same syntax, since they point out , the use of the underscore _ instead, in XLNetTokenizer\nfrom transformers import XLNetTokenizer tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\") tokenizer.tokenize(\"Don't you love ü§ó Transformers? We sure do.\") [\"‚ñÅDon\", \"'\", \"t\", \"‚ñÅyou\", \"‚ñÅlove\", \"‚ñÅ\", \"ü§ó\", \"‚ñÅ\", \"Transform\", \"ers\", \"?\", \"‚ñÅWe\", \"‚ñÅsure\", \"‚ñÅdo\", \".\"] 11:10 going back to chapter 6 here, their statement helps with [[why a custom tokenizer]] , They highlight extreme reasons like your language is different than the original languages used in a model, or that your corpus is ‚Äúvery different‚Äù üòÄ. So I am leaning more that this is about the statistics of your text data as it relates to allowing language models to extract meaning , but without being a performance burden. 12:36 continue reading there, see if I missed something, collapsed:: true Ah interesting so per here, the [[vocabulary size]] is a hyper parameter, that is like a ceiling for splitting words, so if when just say space-splitting, we have too many words, then we split until the vocabulary size is under the input there. collapsed:: true But I know per my earlier attempt, yesterday, where I provided ,\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) I ended up with python In [278]: tokenizer.vocab_size Out[278]: 601 and still I had quite a lot of [[subword-tokenization]] going on, so there are other details looks like. So at least the [[byte-pair encoding]] #algorithm-type , will start with a base vocabulary which consists of just the characters. collapsed:: true 12:51 ok actually glad I kept reading this is interesting, so this algo, will iteratively, find the most frequent symbol pair, adding the merger to its vocabulary, then finding the next most frequent symbol pair after that. So symbols start out as just letters, but after one iteration, a symbol would consist of two characters together. And larger clumps can form after that. So their example, started out with space split words with frequencies,\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) and starting with a base vocabulary of\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"] and after performing three merges, having a vocabulary of\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"] and at that point they showed that the original set of words and their frequencies would be represented as\n(\"hug\", 10), (\"p\", \"ug\", 5), (\"p\", \"un\", 12), (\"b\", \"un\", 4), (\"hug\", \"s\", 5) and that a new test word like \"mug\" say would end up getting represented as [\"\", \"ug\"], since for this case \"m\" just was not part of the initial vocabulary. 13:10 and ultimately the size of the vocabulary for a tokenizer, [[vocabulary size]] will be the size of the base vocabulary plus the number of merges before it was decided to stop . And clearly, haha we can have some absurd algorithm implementation that never stops and we end up with a vocabulary that includes all the full words that were encountered, and then therefore we would get fewer computational benefits . 13:21 ok so [[why a custom tokenizer]], in the video link #video-type, embedded in chapter 6 link, the lean is now to yes train a tokenizer from scratch if there is new jargon yes as in a new ‚Äúdomain‚Äù , #card So high level four good reasons, for [[why a custom tokenizer]] [[derive-from-scratch]] #card new language, new characters ( with accents) new domain (medical, technical, legal) new style (haha like [[Old English]] or [[Old French]] ) #take-away ohhh and #moment/aha a really good example is explained that a tokenizer unfamiliar with a #corpus will excessively split and that is not good because #[[input sequence]] is limited [[context-window]] [[maximum-context-size]] ! And so you will risk not capturing the full sentence you want to pass to a #LLM . Nice. Excessive tokenizer splitting, can impact model performance, too, #question #card , why though? Maybe the argument is similar to like ‚Äú‚Äù those unknowns, in that there is less information being captured. My intuitive reasoning is that tiny subwords embedding representations will be likely meaningless . The #attention will get thrown off by basically letter chunks that will end up being as common as the word ‚Äúthe‚Äù , so perhaps you will have just #stop-words at that point with low information. Example of this particular model tokenizer missing a lot of #unicode characters from [[Bangla]] #language-type . And yea [[out-of-vocabulary-words-OOV]] ‚Äú‚Äù , has no useful information for the model to use there. And [[excessive splitting by tokenizer]] , for say #[[biomedical Glossary]] And for the other example given, of using code-search-net python dataset to train a tokenizer, I like the question that gets asked is [[performance-lift]] at least eye-balling. And in below example, she does show it is desirable to capture a concept as one token, but I think this will ultimately only happen if that example is more frequent , relative to other patterns when doing merges per [[byte-pair encoding]] [[Jul 21st, 2023]] mainly just brief thoughts about building a new dataset Thinking next I should look through various datasets out there , and choose which would have good english language breadth but also depth into the technical jargon world.\nSo for sure a dataset from job descriptions from that nice #Kaggle data. , https://www.kaggle.com/datasets/niyamatalmass/google-job-skills?resource=download maybe there are others like that , but for sure I think this is something that would need to be updated at least once or twice a year because technical terms change frequently 09:29 side note this very much feels like a [[data-drift]] problem but feels like [[nlp-drift]]\n[[Jul 22nd, 2023]] one brief look model card for, 'sentence-transformers/all-MiniLM-L6-v2' Think I am seeing that this model did not update the tokenizer of the pretrained model that it fine tuned. So what are the dataset sources in 'sentence-transformers/all-MiniLM-L6-v2' which has been my goto model recently.\nSo per https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 here, haha there are a billion sentence tuples in there haha. Not sure how easy it will be to perform the same fine tuning steps, but maybe the tokenizer step requires way less data. 13:31 So next want to answer yea what data was used for the tokenizer there. Ok so looking at https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/blob/main/train_script.py the train_script.py in there, it doesn‚Äôt look like they modified the tokenizer that was from the pre-trained model, given as an input at the bottom,\n#python train_many_data_files_v2.py --steps 1000000 --batch_size 128 \\ --model nreimers/MiniLM-L6-H384-uncased train_data_configs/all_datasets_v4.json output/all_datasets_v4_MiniLM-L6-H384-uncased-batch128 ok so then perhaps I should look up the card for that then.\n[[Jul 23rd, 2023]] More practical dive today Refreshed my story blurb texts after several months. Pulled another kaggle job description dataset for analysis. And another run of cosine similarity for the corpori corpi (what is plural of corpus haha )\nwrapping up thoughts on tokenizers from other day 09:53 ok two thoughts, so\ncollapsed:: true I can look at nreimers/MiniLM-L6-H384-uncased and if a tokenizer is described there, And I can also look at that chapter 7 fine tuning link that takes a tokenizer passed in . 10:07 I also do want to think more high level, about the overall goal , task and reevaluate approaches. But ok, quick look, of possible, at https://huggingface.co/nreimers/MiniLM-L6-H384-uncased , 10:13 only points to https://huggingface.co/microsoft/MiniLM-L12-H384-uncased but not much else to go on 10:18 and that points to this paper, https://arxiv.org/abs/2002.10957 , hmm [[LLM distillation]] with a [[LLM distillation/teacher model and teacher assistant]] 10:28 well looking at their train code in github , it looks like for each teacher assistant model (where the MiniLM on hugging face is one of them) , they appear to use these tokenizers directly,\nfrom transformers import ( BertTokenizer, DistilBertTokenizer, XLMTokenizer, XLMRobertaTokenizer, ) without modification.\nDatasets 10:31 ok hmm think I should define dataset and problem bit more now\ncollapsed:: true ok going back to a core early example, from here , I ran [[cosine similarity]] per [[sentence-transformers]] , let me try it again , ( [[May 28th, 2023]] ) This time with the new dataset I have and maybe I will add another one too. So look at that dataset job titles again,\nfrom sentence_transformers.util import semantic_search import pandas as pd import os from functools import reduce from collections import Counter from pathlib import Path import utils as ut loc = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/job_skills.csv\") jobsdf = pd.read_csv(loc) import utils as ut columns = [\"Responsibilities\", 'Minimum Qualifications', 'Preferred Qualifications'] raw_titles = ut.extract_raw_sentences(jobsdf, [\"Title\"]) title_vocab = reduce(lambda x, y: x + y, [ut.sequence_from_sentence(x) for x in raw_titles] ) print(Counter(title_vocab).most_common(25)) print(\"total\", len(set(title_vocab))) [('manager', 300), ('google', 237), ('cloud', 167), ('and', 127), ('sales', 89), ('marketing', 87), ('engineer', 79), ('technical', 71), ('account', 64), ('lead', 64), ('business', 63), ('partner', 62), ('solutions', 61), ('operations', 59), ('product', 57), ('specialist', 57), ('services', 53), ('english', 52), ('analyst', 52), ('hardware', 51), ('associate', 48), ('global', 46), ('program', 44), ('customer', 43), ('development', 42)] total 624 Picking a few more that stand out,\ntechnical_job_title_terms = [ \"engineer\", \"developer\", \"research\", \"technical\", \"analyst\", \"engineering\", \"data\", \"sciences\", \"ux\", \"analytics\", \"systems\", \"architect\", \"researcher\", \"web\", \"infrastructure\", \"intelligence\", \"quantitative\", \"learning\", \"software\", \"scientist\", ] 11:48 Maybe can see if more #MLE jobs in this kaggle dataset of #Amazon job descriptions. 12:47 ok so lets run cosine similarity, ranked, between my corpus and these, descriptions, 14:36 my last blurb was a few months ago, here , so refreshing slightly,\nimport yaml import tempfile from pathlib import Path from datetime import datetime import pytz import os import utils as ut def utc_now(): return datetime.utcnow().replace(tzinfo=pytz.UTC) def utc_ts(dt): return dt.strftime(\"%Y-%m-%dT%H%M%S\") def read_yaml(loc): with open(loc) as fd: return yaml.safe_load(fd) repos_dir = Path(os.getenv(\"REPOS_DIR\")) assert repos_dir.is_dir() experience_loc = repos_dir / \"my-challenges-and-accomplishments/experience.yaml\" experiences_dict = read_yaml(experience_loc)[\"Descriptions\"] my_sentences = ut.build_my_blurb(experiences_dict) 15:44 ok and compare with those datasets, Mini filter example,\nimport pandas as pd vec = [ {\"title\": \"Software Engineer yea\"}, {\"title\": \"Some Scientist\"}, {\"title\": \"Product Manager\"}, {\"title\": \"Industrial Designer\"} ] df = pd.DataFrame.from_records(vec) In [31]: df Out[31]: title 0 Software Engineer yea 1 Some Scientist 2 Product Manager 3 Industrial Designer In [32]: df.query(\"title.str.contains('software', case=False) or title.str.contains('scientist', case=False)\") Out[32]: title 0 Software Engineer yea 1 Some Scientist 16:16 ok put that into a func, ( putting it here )\ndef filter_pandas_multiple_contains(df, column, vec, case=False): \"\"\"filter dataframe for column containing any string from list vec given. Example \u003e\u003e\u003e vec = [ ... {\"title\": \"Software Engineer yea\"}, ... {\"title\": \"Some Scientist\"}, ... {\"title\": \"Product Manager\"}, ... {\"title\": \"Industrial Designer\"} ] \u003e\u003e\u003e df = pd.DataFrame.from_records(vec) \u003e\u003e\u003e df title 0 Software Engineer yea 1 Some Scientist 2 Product Manager 3 Industrial Designer \u003e\u003e\u003e import utils as ut \u003e\u003e\u003e ut.filter_pandas_multiple_contains(df, \"title\", [\"engineer\", \"scientist\"]) title 0 Software Engineer yea 1 Some Scientist \"\"\" query = \" or \".join( [f\"{column}.str.contains('{x}', case={case})\" for x in vec]) return df.query(query) In [39]: ut.filter_pandas_multiple_contains(df, \"title\", [\"engineer\", \"scientist\"]) Out[39]: title 0 Software Engineer yea 1 Some Scientist import pandas as pd import os from pathlib import Path import utils as ut raw_sentences = [] technical_job_title_terms = [ \"engineer\", \"developer\", \"research\", \"technical\", \"analyst\", \"engineering\", \"data\", \"sciences\", \"ux\", \"analytics\", \"systems\", \"architect\", \"researcher\", \"web\", \"infrastructure\", \"intelligence\", \"quantitative\", \"learning\", \"software\", \"scientist\", ] loc = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-google-job-skills/job_skills.csv\") jobsdf = ut.filter_pandas_multiple_contains( pd.read_csv(loc), \"Title\", technical_job_title_terms) columns = [\"Responsibilities\", 'Minimum Qualifications', 'Preferred Qualifications'] raw_sentences.extend(ut.extract_raw_sentences( jobsdf, columns)) print(\"len raw_sentences after ingestion\", len(raw_sentences)) loc = (Path(os.getenv(\"REPOS_DIR\")) / \"data\" / \"kaggle-amazon-job-skills/amazon_jobs_dataset.csv\") jobsdf = ut.filter_pandas_multiple_contains( pd.read_csv(loc), \"Title\", technical_job_title_terms) columns = [ 'DESCRIPTION', 'BASIC QUALIFICATIONS', 'PREFERRED QUALIFICATIONS'] raw_sentences.extend(ut.extract_raw_sentences( jobsdf, columns)) print(\"len raw_sentences after ingestion\", len(raw_sentences)) len raw_sentences after ingestion 3259 len raw_sentences after ingestion 33081 17:44 ok yea and doing that top k cosine similarity then\nimport json from pathlib import Path import os import requests from sentence_transformers.util import semantic_search model_id = \"sentence-transformers/all-MiniLM-L6-v2\" hf_token = os.getenv(\"HF_TOKEN\") my_story_embeddings = ut.vec_to_embeddings(model_id, hf_token, my_sentences) jd_embeddings = ut.vec_to_embeddings(model_id, hf_token, raw_sentences[:1000]) # this line took # Wall time: 7.05 s hits = semantic_search(my_story_embeddings, jd_embeddings, top_k=100) for i, row in enumerate(hits[:5]): print(f\"({i})\", \"matching,\", my_sentences[i], \":\") hmm = [[raw_sentences[x[\"corpus_id\"]], x[\"corpus_id\"], x[\"score\"]] for x in row[:3] ] print(hmm, \"\\n\\n\") 18:07\nIn [20]: for i, row in enumerate(hits[:5]): ...: print(f\"({i})\", \"matching,\", my_sentences[i], \":\") ...: hmm = [[raw_sentences[x[\"corpus_id\"]], x[\"corpus_id\"], x[\"score\"]] for x in row[:3] ] ...: print(hmm, \"\\n\\n\") ...: (0) matching, Built our initial method for model hosting, transitioning from a purely business rule based flow : [['experience in practical business modeling or financial modeling\\ndistinctive problem solving and analytical skills, combined with impeccable business judgment', 740, 0.45160090923309326], ['review and fulfill managed software requests to ensure products meet business needs, while overseeing programmatic compliance with associated software licenses/other agreements, contractual terms, and policies', 394, 0.433631956577301], ['experience in business planning and/or financial modeling', 425, 0.4310281574726105]] (1) matching, Created a Vagrant virtual machine based staging environment that developers can quickly use to stage code, to help us transition from personalized AWS staging environments which can potentially help us save several hundreds of dollars a month. : [['experience architecting, developing software, or internet scale production-grade big data solutions in virtualized environments such as google cloud platform', 753, 0.4532877206802368], ['establish and drive planning and execution steps towards production deployments', 959, 0.4100092053413391], ['experience in designing and implementing build automation, and configuration management for operating system platforms.', 859, 0.3592562973499298]] (2) matching, Implemented the retailer lead list reporting, so that big data heavy retailers like Sears could finally be more involved in following up with customers who were not originating their preapprovals. : [['create effective, scalable, and easy to understand reporting solutions (e.g', 262, 0.45110809803009033], ['lead global analysis of in-store demo device analytics', 833, 0.4380985200405121], ['demonstrated understanding of customer support verticals', 961, 0.42506536841392517]] (3) matching, Troubleshooted and fixed rare and difficult to detect buyout bugs. When customers had multiple payments being taken on a day when they also did a buyout, for example, there was a bug where we were incorrectly discounting the additional payments that they made that day. : [['serve as a central coordination point for customer bugs and issues escalated by internal sales teams', 941, 0.4794199466705322], ['author test plans with the goal of catching issues and fixing them at early design stage to improve the overall product quality and meet aggressive schedule', 313, 0.39714837074279785], ['support product implementation and help partners in their day to day challenges by delivering innovative and scalable solutions to their problems and troubleshooting their issues', 796, 0.3908129930496216]] (4) matching, Refactored payment processing to reflect a better interpretation of the law around customer suspense dollars. Previously, customers would pay down their next months payment. In the change, any payments that are made outside of the due date count towards the suspense account. This change required splitting out the plan shifting, away from the payment processing, into its own separate task, to simplify the new implementation of the payment law. : [['work with partner teams to re-engineer process workflows around demand planning, supply planning, ordering, and fulfillment', 219, 0.32384926080703735], ['experience in devising and implementing strategies and business improvements.', 863, 0.31515663862228394], ['consult with internal account management teams and customers to track the progress and impact', 621, 0.30947157740592957]] Ok so definitely still not impressed with the hits I‚Äôm getting here.\nThoughts A few follow up items coming to mind,\ncollapsed:: true I think per above experiment I ran, I want to find a few false negative matches , look at the scores they are producing, and then probably take a closer dive into the [[average-pooling]] . I want to really answer the question, do I need to do pre-processing, removing [[stop-words]] so the [[sentence-transformers]] [[cosine similarity]] after average pooling does not suffer? And then after doing the preprocessing if necessary , excluding it as an issue or acting to remove stop words or fluff words, then lets run cosine similarity like that. And then maybe a refined , more granular approach would be to think about using [[Named Entity Recognition NER]] maybe to better remove stop words , especially if I do not perhaps have the luxury of fine tuning. but yea side note I think fine tuning would be really helpful to help with embedding these interesting jargon words close to each other if they are indeed related And as a visual debugging I really should plot out or at least someone must have some nice tool to visualize embeddings\n[[Jul 24th, 2023]] started a nice debug session today , false negative analysis here, Ok per my notes from [[Jul 23rd, 2023]], let me hunt down one good [[false negative]],\nFor sure I am realizing yes a lot of my sentences in my personal corpus are lacking a succinctness and there is a lot of filler in there ideally I should cut out. Actually it would be cool to have that kind of feedback actually as a tool, ranking sentences by fluff haha that should be improved. And even, how many sentences are used in describing each individual project/story, can be helpful to see analyzed too.\nimport random # Using \"my_sentences\" defined last time for i, x in enumerate(random.choices(my_sentences, k=4)): print(i, x) 0 So the behavior was changed and I had this now, erroneously haha, so I needed to now remove it 1 Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions 2 Improved our sklearn model training that was crashing on a laptop, by cutting up the data into chunks and using python multiprocessing 3 (7) \"humana hackathon exploration of langchain against health plan documents\" ( I have described this in more detail earlier above ) Ok in any case, so let me search the corpus I have, for terms, say, spark, cluster pyspark, databricks . Of course I am realizing Google and Amazon have their own options for clustering and these job descriptions might not mention pyspark, but let‚Äôs see,\nimport pandas as pd import utils as ut # Using \"raw_sentences\" defined last time df = pd.DataFrame({\"description\": raw_sentences}) In [29]: df.iloc[:5] Out[29]: description 0 app scripts, spreadsheet software, etc) 1 leadership, problem solving and analysis exper... 2 hands-on experience using and/or managing data... 3 background in solving complex challenges and d... 4 travel frequently around emea for meetings, te... terms = [\"pyspark\", \"spark\", \"databricks\", \"multiprocessing\", \"cluster\"] resultsdf = ut.filter_pandas_multiple_contains(df, \"description\", terms) In [34]: resultsdf.shape Out[34]: (154, 1) In [36]: resultsdf.iloc[:10][\"description\"].tolist() Out[36]: ['hands-on experience using and/or managing databases, or cloud technologies such as sql, nosql, hadoop or spark', 'familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)\\ntechnical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar', 'regression, classification, clustering, etc)', 'know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 'experience using and/or managing databases, and with one or all of the following: mapreduce, hadoop, spark, flume, hive, impala, spark sql and/or bigquery', 'experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)', 'experience with data processing software (such as hadoop, spark, pig, hive) and data processing algorithms (mapreduce, flume)', 'technologies you will employ to solve these complex real-world business problems include natural language processing, machine learning, image recognition, elastic computing, spark, and a host of other state of the art aws technologies', 'in order to drive expansion of the amazon catalog, we use cluster-computing technologies like mapreduce, spark and hive to process billions of products and algorithmically find products not already sold on amazon', 'experience working with scala/python/java on spark to build and deploy ml models in production'] 09:19 Ok cool, so there are a few meaty sentences here that would be good to compare more directly with debugging eyes . for example,\nfrom sentence_transformers.util import semantic_search, cos_sim from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') model_id = \"sentence-transformers/all-MiniLM-L6-v2\" hf_token = os.getenv(\"HF_TOKEN\") s1 = \"Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\" s2 = 'know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery' s3 = 'experience working with scala/python/java on spark to build and deploy ml models in production' for s in [s1, s2, s3]: print(s, tokenizer.tokenize(s), \"\\n\\n\") embeddings = ut.vec_to_embeddings(model_id, hf_token, [s1, s2, s3]) In [45]: cos_sim(embeddings[0,:], embeddings[1, :]) Out[45]: tensor([[0.2323]]) In [46]: cos_sim(embeddings[0,:], embeddings[2, :]) Out[46]: tensor([[0.2320]]) also try the other way too ,\nIn [43]: sentences = [s1, s2, s3] ...: hits = semantic_search(embeddings, embeddings, top_k=3) ...: for i, row in enumerate(hits[:5]): ...: print(f\"({i})\", \"matching,\", sentences[i], \":\") ...: hmm = [[sentences[x[\"corpus_id\"]], x[\"corpus_id\"], x[\"score\"]] for x in row[:3] ] ...: print(hmm, \"\\n\\n\") ...: (0) matching, Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions : [['Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions', 0, 1.0], ['know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 1, 0.23231448233127594], ['experience working with scala/python/java on spark to build and deploy ml models in production', 2, 0.2320040762424469]] (1) matching, know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery : [['know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 1, 0.9999999403953552], ['experience working with scala/python/java on spark to build and deploy ml models in production', 2, 0.4644332528114319], ['Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions', 0, 0.23231443762779236]] (2) matching, experience working with scala/python/java on spark to build and deploy ml models in production : [['experience working with scala/python/java on spark to build and deploy ml models in production', 2, 1.000000238418579], ['know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 1, 0.4644332826137543], ['Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions', 0, 0.2320040762424469]] Ok so that gave around same result. Ok cool, no matter which func was used, Ok cool, so next want to keep diving deeper, probably ultimately looking at [[average-pooling]] here.\n[[Jul 25th, 2023]] Stop word removal experiment So for the question yesterday, why was it that two cosine similarity comparisons had basically same score, 0.2323 and 0.2320, maybe that is a clue.\nI think I have seen that completely unrelated sentences can have a zero score comparison right?\ns1 = \"If a tree falls in the forest and there is no one there to hear it, then does it make a sound?\" s2 = \"A bagel with cream cheese with some lox and some capers would go great with coffee.\" for s in [s1, s2]: print(s, tokenizer.tokenize(s), \"\\n\\n\") set(tokenizer.tokenize(s1)) \u0026 set(tokenizer.tokenize(s2)) # {'a', 'and'} embeddings = ut.vec_to_embeddings(model_id, hf_token, [s1, s2]) cos_sim(embeddings[0,:], embeddings[1, :]) # tensor([[0.0240]]) 09:03 ok cool So for example from yesterday, lets see what happens if we remove [[stop-words]] . ( Side note stacko link someone suggests also trying to mask stop words ) .\nimport nltk from sentence_transformers.util import semantic_search, cos_sim from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') model_id = \"sentence-transformers/all-MiniLM-L6-v2\" hf_token = os.getenv(\"HF_TOKEN\") from nltk.corpus import stopwords s1 = \"Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions\" s2 = 'know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery' s3 = 'experience working with scala/python/java on spark to build and deploy ml models in production' def dont_stop(s): stop_words = stopwords.words('english') return \" \".join([x for x in s.split() if x.lower() not in stop_words]) for s in [s1, s2, s3]: print(s, \"\\n\", dont_stop(s), \"\\n\\n\") Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery experience working with scala/python/java on spark to build and deploy ml models in production experience working scala/python/java spark build deploy ml models production 09:23 hmm not a whole lot of stop words there but lets try anyway,\nsentences = [s1, s2, s3] stopped = [dont_stop(x) for x in sentences] embeddings = ut.vec_to_embeddings(model_id, hf_token, stopped) hits = semantic_search(embeddings, embeddings, top_k=3) for i, row in enumerate(hits[:5]): print(f\"({i})\", \"matching,\", stopped[i], \":\") hmm = [[stopped[x[\"corpus_id\"]], x[\"corpus_id\"], x[\"score\"]] for x in row[:3] ] print(hmm, \"\\n\\n\") (0) matching, Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions : [['Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions', 0, 1.0], ['know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 1, 0.3553411066532135], ['experience working scala/python/java spark build deploy ml models production', 2, 0.25089341402053833]] (1) matching, know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery : [['know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 1, 1.0000001192092896], ['experience working scala/python/java spark build deploy ml models production', 2, 0.37388813495635986], ['Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions', 0, 0.3553411364555359]] (2) matching, experience working scala/python/java spark build deploy ml models production : [['experience working scala/python/java spark build deploy ml models production', 2, 1.0000001192092896], ['know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery', 1, 0.3738881051540375], ['Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions', 0, 0.25089341402053833]] 09:33 ok well I am seeing improvements undeniably here,\n0.3553 vs 0.2323 0.25089 vs 0.23200 so therefore [[sentence-transformers]] #take-away is not internally penalizing stop words as I had thought a bit that it might before. Ok I think next I still want to just dissect the way [[sentence-transformers]] [[cosine similarity]] gets the result and reproduce it manually see if I get the same thing\n[[Jul 26th, 2023]] some more experimentation, cosine similarity and stop words ok next, yea let‚Äôs try reproducing that [[cosine similarity]] ,\nfrom last time, yea this model produces embeddings with this size,\nIn [63]: embeddings[0,:].shape Out[63]: torch.Size([384]) 08:36 just double checking,\nn [68]: np.dot(embeddings[0, :], embeddings[1, :]), cos_sim(embeddings[0, :], embeddings[1, :]) Out[68]: (0.35534108, tensor([[0.3553]])) Ok next, just out of curiosity, since blog post also mentions that #spacy has a longer #stop-words list,\nfrom nltk.corpus import stopwords as sw_nltk import spacy en = spacy.load('en_core_web_sm') sw_spacy = en.Defaults.stop_words def dont_stop_both(s): en = spacy.load('en_core_web_sm') sw_spacy = en.Defaults.stop_words stop_words = set(sw_nltk.words('english') ) | set(sw_spacy) return \" \".join([x for x in s.split() if x.lower() not in stop_words]) sentences = [s1, s2, s3] stopped = [dont_stop(x) for x in sentences] stopped_both = [dont_stop_both(x) for x in sentences] [len(x) for x in stopped], [len(x) for x in stopped_both] # ([158, 82, 76], [153, 75, 76]) 08:57 ok so compare all three ways, for completeness\nresults = [] for name, a_list in [(\"original\", sentences), (\"nltk\", stopped), (\"nltk+spacy\", stopped_both)]: embeddings = ut.vec_to_embeddings(model_id, hf_token, a_list) results.append({\"what\": name, \"one\": cos_sim(embeddings[0, :], embeddings[1, :]), \"two\": cos_sim(embeddings[0, :], embeddings[2, :]), }) pd.DataFrame.from_records(results) what one two 0 original [[tensor(0.2323)]] [[tensor(0.2320)]] 1 nltk [[tensor(0.3553)]] [[tensor(0.2509)]] 2 nltk+spacy [[tensor(0.3329)]] [[tensor(0.2501)]] 09:08 Ok haha can be slightly hit or miss then . But if I did this other extreme stop removal, say specifically with information I know about these sentences,\njargon = [\"python\", \"databricks\", \"cluster\", \"pyspark\", \"scala\", \"answer\", \"question\", \"dataset\", \"map\", \"reduce\", \"hadoop\", \"spark\", \"flume\", \"hive\", \"impala\", \"sparksql\", \"bigquery\", \"java\", \"build\", \"deploy\", \"ml\", \"models\", \"production\"] def talk_jargon_to_me(jargon, s): return \" \".join([ (x if x.lower() in jargon else \"blah\") for x in s.split()]) only_jargon_sentences = [talk_jargon_to_me(jargon, x) for x in sentences] results = [] for name, a_list in [ (\"original\", sentences), (\"nltk\", stopped), (\"nltk+spacy\", stopped_both), (\"only_jargon\", only_jargon_sentences) ]: embeddings = ut.vec_to_embeddings(model_id, hf_token, a_list) results.append({\"what\": name, \"one\": cos_sim(embeddings[0, :], embeddings[1, :]), \"two\": cos_sim(embeddings[0, :], embeddings[2, :]), }) pd.DataFrame.from_records(results) what one two 0 original [[tensor(0.2323)]] [[tensor(0.2320)]] 1 nltk [[tensor(0.3553)]] [[tensor(0.2509)]] 2 nltk+spacy [[tensor(0.3329)]] [[tensor(0.2501)]] 3 only_jargon [[tensor(0.3406)]] [[tensor(0.5675)]] 09:36 Ok so sentence transformers can be a bit of a blunt tool for sure but I think I am verifying that with some pre-processing , I can get better use from them . And improving the jargon game can be helpful too.\nOne more thing out of curiosity,\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\" vocabulary = ut.vocabulary_of_model(model_id) [(x, x in vocabulary) for x in [\"spark\", \"pyspark\", \"python\"]] # [('spark', True), ('pyspark', False), ('python', True)] [(x, tokenizer.tokenize(x)) for x in [\"spark\", \"pyspark\", \"python\"]] [('spark', ['spark']), ('pyspark', ['p', '##ys', '##park']), ('python', ['python'])] terms = [\"spark\", \"pyspark\", \"python\"] embeddings = ut.vec_to_embeddings(model_id, hf_token, terms) [ [\"spark, pyspark\", cos_sim(embeddings[0, :], embeddings[1, :])], [\"spark, python\", cos_sim(embeddings[0, :], embeddings[2, :])], [\"pyspark, python\", cos_sim(embeddings[1, :], embeddings[2, :])], ] [['spark, pyspark', tensor([[0.5201]])], ['spark, python', tensor([[0.2316]])], ['pyspark, python', tensor([[0.4150]])]] 09:47 might be worth poking at this a bit more, so if ‚Äúpyspark‚Äù is not in the vocabulary here, but hmm maybe through all the fine tuning, on a billion sentences, maybe the embeddings still ended up being meaningfully close? Want to better understand this\n[[Jul 27th, 2023]] hmm what is the subword tokenization multiple , when thinking about truncation So it was cool to see you can get a better #[[cosine similarity]] score when taking out stop words and also when replacing the non-jargon words with blah words. Although that second part hmm might have actually artificially increased the score thre now that I think about it, ü§î\nMaybe thinking about sentences is hmm not going to be as useful as thinking about the entire job description itself maybe.\nSo we have space only for 384 tokens but that can still be a good number of sentences, 10 mmaybe . Ah yea and that is another good reason to get rid of the stop words Like from my example from yesterday and day before, how many word-pieces in those?\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') [{\"num_words\": len(x.split(\" \")), \"num_tokens\": len(tokenizer.tokenize(x)), \"token_inflation_factor\": len(tokenizer.tokenize(x))/len(x.split(\" \"))} for x in [s1, s2, s3]] [{'num_words': 34, 'num_tokens': 46, 'token_inflation_factor': 1.3529411764705883}, {'num_words': 13, 'num_tokens': 27, 'token_inflation_factor': 2.076923076923077}, {'num_words': 14, 'num_tokens': 18, 'token_inflation_factor': 1.2857142857142858}] So yea if we have a 384 length input window then yea maybe 10 sentences or so on a good day. Not bad. 09:09 yea side note about that [[average-pooling]],\nReading the https://www.sbert.net/examples/applications/computing-embeddings/README.html section here again for insight. This is a good page Interesting looking at that mean_pooling function, it takes the attention mask into accoount. kinda cool,\n[[Jul 28th, 2023]] looking more closely on how sentence transformer model pools ok thing I got it this time, per https://www.sbert.net/examples/applications/computing-embeddings/README.html\nIn [109]: #Sentences we want sentence embeddings for ...: sentences = ['This framework generates embeddings for each input sentence', ...: 'Sentences are passed as a list of string.', ...: 'The quick brown fox jumps over the lazy dog.'] ...: ...: #Load AutoModel from huggingface model repository ...: tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\") ...: model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\") ...: ...: #Tokenize sentences ...: encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt') ...: ...: #Compute token embeddings ...: with torch.no_grad(): ...: model_output = model(**encoded_input) ...: Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.9M/90.9M [00:56\u003c00:00, 1.61MB/s] Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90.9M/90.9M [00:56\u003c00:00, 1.93MB/s] In [110]: 08:53 ok let‚Äôs try to understand the function they show there, So model_output,\nIn [123]: type(encoded_input) Out[123]: transformers.tokenization_utils_base.BatchEncoding In [124]: vars(encoded_input) Out[124]: {'data': {'input_ids': tensor([[ 101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102], [ 101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0], [ 101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}, '_encodings': [Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])], '_n_sequences': 1} 09:16 ok so looks somewhat close, to doing it manually, below,\nprint(\"what was encoded,\", encoded_input.data[\"input_ids\"][0], ) print(\"back to tokens though,\", tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0])) print(\"original sentence,\", sentences[0]) tokens = tokenizer.tokenize(sentences[0]) print(\"sentence to tokens\", tokens) print(\"tokens to ids\", tokenizer.convert_tokens_to_ids(tokens)) what was encoded, tensor([ 101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102]) back to tokens though, ['[CLS]', 'this', 'framework', 'generates', 'em', '##bed', '##ding', '##s', 'for', 'each', 'input', 'sentence', '[SEP]'] original sentence, This framework generates embeddings for each input sentence sentence to tokens ['this', 'framework', 'generates', 'em', '##bed', '##ding', '##s', 'for', 'each', 'input', 'sentence'] tokens to ids [2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251] Only difference is I see when going back from input ids to tokens , there is an additional [CLS] at the start and a [SEP] at the end. no pad ?\nencoded_input_no_pad = tokenizer(sentences, padding=False, truncation=True, max_length=128, return_tensors='pt') print(\"without pad, tokens, \", tokenizer.convert_ids_to_tokens( encoded_input_no_pad.data[\"input_ids\"][0])) ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected). Ah ok so that doesn‚Äôt even work then so padding required.\n([[{\"num_words\": len(x.split(\" \")), \"len_tokens\": len(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][i])), \"len_input_ids\": encoded_input.data[\"input_ids\"][i,:].shape, \"len_mask\": encoded_input.data[\"attention_mask\"][i,:].shape, }, x, tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][i]), encoded_input.data[\"input_ids\"][i,:], encoded_input.data[\"attention_mask\"][i,:] ] for i, x in enumerate(sentences)]) [[{'num_words': 8, 'len_tokens': 13, 'len_input_ids': torch.Size([13]), 'len_mask': torch.Size([13])}, 'This framework generates embeddings for each input sentence', ['[CLS]', 'this', 'framework', 'generates', 'em', '##bed', '##ding', '##s', 'for', 'each', 'input', 'sentence', '[SEP]'], tensor([ 101, 2023, 7705, 19421, 7861, 8270, 4667, 2015, 2005, 2169, 7953, 6251, 102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])], [{'num_words': 8, 'len_tokens': 13, 'len_input_ids': torch.Size([13]), 'len_mask': torch.Size([13])}, 'Sentences are passed as a list of string.', ['[CLS]', 'sentences', 'are', 'passed', 'as', 'a', 'list', 'of', 'string', '.', '[SEP]', '[PAD]', '[PAD]'], tensor([ 101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])], [{'num_words': 9, 'len_tokens': 13, 'len_input_ids': torch.Size([13]), 'len_mask': torch.Size([13])}, 'The quick brown fox jumps over the lazy dog.', ['[CLS]', 'len_tokens': 13, 'len_input_ids': torch.Size([13]), 'len_mask': torch.Size([13])}, 'The quick brown fox jumps over the lazy dog.', ['[CLS]', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', tensor([ 101, 11746, 2024, 2979, 2004, 1037, 2862, 1997, 5164, 1012, 102, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])], [{'num_words': 9, 'len_tokens': 13, 'len_input_ids': torch.Size([13]), 'len_mask': torch.Size([13])}, 'The quick brown fox jumps over the lazy dog.', ['[CLS]', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '[SEP]', '[PAD]'], tensor([ 101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])]] 09:36 ah interesting so the [PAD] is separate actually, and that corresponds to the additional 0 in the [[attention-mask]]\n[[Jul 29th, 2023]] hmm so back to that cool document, https://www.sbert.net/examples/applications/computing-embeddings/README.html\nit was cool I saw you can easily do tokenizer.convert_ids_to_tokens and tokenizer.convert_tokens_to_ids, because I was able to veriffy that running\nencoded_input = tokenizer( sentences, padding=True, truncation=True, return_tensors='pt') will add some additional '[CLS]' and '[SEP]' tokens at the beginning and end. I noticed since the length of the output was weird especially looking at the [[attention-mask]] . So that attention mask basically was showing that the three sentences being encoded had a final length that was the same number of ids and tokens , both 13, but there were some 0s at the end of some of the sentences. So that ended up being just yet another [PAD] token. Ok so '[CLS] means special #BERT token for start of sequence and [SEP] is the separator between sequences . Yea and [PAD] just says, nothing to do here. And the padding=True option is not about padding up to like [[context-window]] limit, it just says if you are passing multiple sentences to be encoded, to make them all equal length. Anyway 384 is the size of the embedding, in this case, and it is not yet clear to me what is the [[what is relationship between size of input token sequence and dimension of embedding]] , there might be some [[dimensionality reduction]] right. 12:58 so continuing along then, next step was\n#Compute token embeddings with torch.no_grad(): model_output = model(**encoded_input) In [143]: vars(model_output) Out[143]: {'last_hidden_state': tensor([[[ 0.2913, -0.2685, -0.2250, ..., 0.4261, 0.0493, -0.2095], [-0.6272, -0.0421, -0.2452, ..., 0.5336, 1.3115, 0.5999], [ 0.0023, -0.2805, -0.4198, ..., -0.2900, 1.5808, -0.4912], ..., [ 0.1802, -0.5567, 0.0146, ..., 0.9311, 0.5940, -0.3536], [ 0.0603, -0.2502, 0.5959, ..., 0.9435, 0.9465, -1.0680], [-0.3356, 0.0650, 0.1109, ..., 1.0801, 0.2653, -0.2762]], [[ 0.0856, 0.1876, 0.0488, ..., 0.1204, -0.0907, -0.1662], [ 0.1291, -0.0266, 0.6318, ..., 0.7958, 0.1555, -1.2737], [ 0.0062, 0.2263, 0.1851, ..., 0.3981, 0.6461, -0.2192], ..., [ 0.3036, 0.3740, 0.2523, ..., 0.6319, 0.5731, -0.2901], [-0.2124, 0.2626, 0.6867, ..., 0.5504, 0.7065, -0.4728], [-0.2220, 0.2086, 0.6693, ..., 0.5410, 0.5683, -0.3963]], [[ 0.0464, 0.3381, 0.2082, ..., 0.2766, -0.0861, -0.0358], [ 0.1162, 0.2264, 0.1021, ..., 0.1858, 0.4895, 1.2175], [ 0.1537, 0.1730, 0.5151, ..., 1.3720, 0.3621, 0.5758], ..., [ 0.3883, 0.2813, 0.0309, ..., 0.3264, -0.1039, 0.5856], [ 0.3477, 0.0940, 0.2564, ..., 0.1463, 0.1743, 0.5586], [ 0.1911, -0.0142, 0.3021, ..., 0.1814, 0.2111, 0.2329]]]), 'pooler_output': tensor([[-0.0417, -0.0041, 0.0332, ..., 0.0117, -0.0634, -0.0058], [-0.0227, -0.0248, -0.0112, ..., 0.0482, -0.1108, 0.0122], [-0.0663, 0.0281, 0.0706, ..., 0.0258, -0.0222, -0.0608]]), 'hidden_states': None, 'past_key_values': None, 'attentions': None, 'cross_attentions': None} Ok cool, so seeing each input is embedded separately, we have size of 3 rows here, like 3 input encodings ,\nIn [146]: [model_output.last_hidden_state.shape, model_output.pooler_output.shape] Out[146]: [torch.Size([3, 13, 384]), torch.Size([3, 384])] Hmm and last hidden state, with the 13 wonder if those are 13 [[attention-head]] ? And then the pooler_output what combines these then ? 13:29 oh never mind #moment/doh #moment/duh that is 13 because thre are 13 tokens in each sentence ! [[moment/haha]] . üòÄ\n13:14 ok so final [[average-pooling]] mean pooling step\n#Perform pooling. In this case, mean pooling sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask']) Ok so you can use key or index to access these,\nIn [155]: np.allclose(model_output.last_hidden_state, model_output[0]), np.allclose(model_output.pooler_output, model_output[1]) Out[155]: (True, True) 13:24 Not yet clear why this additional unsqueeze [[numpy unsqueeze ]] step is done\nIn [163]: attention_mask = encoded_input['attention_mask'] ...: attention_mask.shape, attention_mask.unsqueeze(-1).shape Out[163]: (torch.Size([3, 13]), torch.Size([3, 13, 1])) Ah ok interesting,\nIn [166]: token_embeddings = model_output[0] In [167]: input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() In [168]: input_mask_expanded Out[168]: tensor([[[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]], [[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [0., 0., 0., ..., 0., 0., 0.]]]) In [169]: input_mask_expanded.shape Out[169]: torch.Size([3, 13, 384]) In [170]: (token_embeddings.size()) Out[170]: torch.Size([3, 13, 384]) In [171]: input_mask_expanded[0,0,:] Out[171]: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) So this interesting unsqueeze then expand pattern , to the 384 size of the embedding dimension, 13:30 so yea literally the output is\nIn [172]: token_embeddings.shape Out[172]: torch.Size([3, 13, 384]) In [173]: token_embeddings Out[173]: tensor([[[ 0.2913, -0.2685, -0.2250, ..., 0.4261, 0.0493, -0.2095], [-0.6272, -0.0421, -0.2452, ..., 0.5336, 1.3115, 0.5999], [ 0.0023, -0.2805, -0.4198, ..., -0.2900, 1.5808, -0.4912], ..., [ 0.1802, -0.5567, 0.0146, ..., 0.9311, 0.5940, -0.3536], [ 0.0603, -0.2502, 0.5959, ..., 0.9435, 0.9465, -1.0680], [-0.3356, 0.0650, 0.1109, ..., 1.0801, 0.2653, -0.2762]], [[ 0.0856, 0.1876, 0.0488, ..., 0.1204, -0.0907, -0.1662], [ 0.1291, -0.0266, 0.6318, ..., 0.7958, 0.1555, -1.2737], [ 0.0062, 0.2263, 0.1851, ..., 0.3981, 0.6461, -0.2192], ..., [ 0.3036, 0.3740, 0.2523, ..., 0.6319, 0.5731, -0.2901], [-0.2124, 0.2626, 0.6867, ..., 0.5504, 0.7065, -0.4728], [-0.2220, 0.2086, 0.6693, ..., 0.5410, 0.5683, -0.3963]], [[ 0.0464, 0.3381, 0.2082, ..., 0.2766, -0.0861, -0.0358], [ 0.1162, 0.2264, 0.1021, ..., 0.1858, 0.4895, 1.2175], [ 0.1537, 0.1730, 0.5151, ..., 1.3720, 0.3621, 0.5758], ..., [ 0.3883, 0.2813, 0.0309, ..., 0.3264, -0.1039, 0.5856], [ 0.3477, 0.0940, 0.2564, ..., 0.1463, 0.1743, 0.5586], [ 0.1911, -0.0142, 0.3021, ..., 0.1814, 0.2111, 0.2329]]]) at does the model_output.pooler_output mean then? Is that also doing a average of the 13 tokens? 13:33 The final step makes a bit more sense now\nsum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) because it is just saying, dont take into account the stuff that the mask masked away. And we add them, and divide by length for each sentence,\nIn [178]: sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) ...: sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) In [179]: pooled = sum_embeddings / sum_mask In [180]: pooled.shape Out[180]: torch.Size([3, 384]) too of course. 16:34 Ok so since I now basically know, the [[average-pooling]] of [[sentence-transformers]] indeed is just literally averaging each word, yes in the 384 dimensions but yea any unimportant word should most certainly be removed before average pooling\nAnd I still have that pretty critical two-part question, so if we are using [[subword-tokenization]] , and therefore a concept is going to be spread apart to multiple tokens, does that mean we are relying on this multi-dimensional averaging to somehow maintain the meaning of a word that was broken up into pieces? So the [[subword-tokenization]] clearly I now understand is a statistical procedure unrelated to the #[[supervised fine-tuning]] step and yea likely that the more common sub-words will end up being longer subwords after [[why a custom tokenizer]] , but most likely a concept still gets broken apart, into multiple sub-words, so then ultimately does that not really matter, because upon computing [[cosine similarity]], another sentence which has the same exact [[jargon]] word will be split up in the same way. And maybe even if a model is not #[[supervised fine-tuning]] with a new corpus, we may still benefit from at least words with a common word parts being close dimensionally, right, 18:20 hmm so a dead super simple test, of above sub-word theory,\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\" sentences = [ \"python\", \"pyspark\", ] embeddings = ut.vec_to_embeddings(model_id, sentences) In [189]: cos_sim(embeddings[0, :], embeddings[1,:]) Out[189]: tensor([[0.4150]]) Ok kind of thought so. 18:47 added one more option for myself there,\nreload(ut) model_id = \"sentence-transformers/all-MiniLM-L6-v2\" sentences = [ \"python\", \"pyspark\", ] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) ['[CLS]', 'python', '[SEP]', '[PAD]', '[PAD]'] ['[CLS]', 'p', '##ys', '##park', '[SEP]'] That‚Äôs terrible haha ok no wonder the cosine similarity is so low, 0.415 haha. 18:51 one more example to try, hmm, this one should be obvious ,\nsentences = [\"postgresql\", \"database\"] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ['[CLS]', 'post', '##gre', '##s', '##q', '##l', '[SEP]'] ['[CLS]', 'database', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] cosine similarity tensor([[0.5301]]) hmm haha thats really bad I think One more\nsentences = [\"postgresql\", \"sql\"] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ['[CLS]', 'post', '##gre', '##s', '##q', '##l', '[SEP]'] ['[CLS]', 'sql', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] cosine similarity tensor([[0.5085]]) hmm yea this is no good. Whatever this is, it is terrible I think It should be as good as this,\nIn [200]: sentences = [\"banana\", \"apple\"] ...: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) ...: print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ...: ['[CLS]', 'banana', '[SEP]'] ['[CLS]', 'apple', '[SEP]'] cosine similarity tensor([[0.4240]]) In [201]: sentences = [\"fruit\", \"apple\"] ...: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) ...: print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ['[CLS]', 'fruit', '[SEP]'] ['[CLS]', 'apple', '[SEP]'] cosine similarity tensor([[0.5372]]) In [202]: In [202]: sentences = [\"macintosh\", \"apple\"] ...: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) ...: print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ['[CLS]', 'macintosh', '[SEP]'] ['[CLS]', 'apple', '[SEP]'] cosine similarity tensor([[0.7044]]) 18:57 ok haha I‚Äôm confused. these are also kind of bad.\n[[Jul 31st, 2023]] ok interesting, removing the special tokens has no effect on the cosine similarity so following from [[Jul 29th, 2023]] I was going to remove the special tokens try again,\ntexts = [\"macintosh\", \"apple\"] encoded_input = tokenizer( texts, padding=True, truncation=True, max_length=128, cls_token=None, sep_token=None, return_tensors='pt') hmm that didn‚Äôt work,\nTypeError: _batch_encode_plus() got an unexpected keyword argument 'cls_token' how about,\ntexts = [\"macintosh\", \"apple\"] encoded_input = tokenizer( texts, padding=True, truncation=True, max_length=128, add_special_tokens=False, return_tensors='pt') 09:14 ok nice yes that did it !\nIn [206]: encoded_input Out[206]: {'input_ids': tensor([[22228], [ 6207]]), 'token_type_ids': tensor([[0], [0]]), 'attention_mask': tensor([[1], [1]])} Let‚Äôs then compare cosine similarity between these, with and without the special tokens present.\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\" embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) cosine similarity tensor([[0.7044]]) 09:27 ok looks like didn‚Äôt make a difference. Try one more,\nsentences = [\"fruit\", \"apple\"] model_id = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors='pt') embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\"add_special_tokens:\", add_special_tokens, \"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.2258]]) add_special_tokens: False cosine similarity tensor([[1.0000]]) 09:34 oops, have some kind of bug in the mean pooling code I think.\n[[Aug 1st, 2023]] trying a few more things wondering why weird cosine similarity 1 w/ single words encoded but different words ok what is bug from yesterday then ?\nsentences = [\"fruit\", \"apple\"] model_id = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors='pt') embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\"add_special_tokens:\", add_special_tokens, \"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) print(\"all close\", np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.5372]]) all close False add_special_tokens: False cosine similarity tensor([[1.0000]]) all close False In [217]: embeddings.shape Out[217]: torch.Size([2, 384]) hmm weird, yea spot checked, they don‚Äôt look identical actually, but similar,\nIn [218]: np.transpose(embeddings).shape Out[218]: torch.Size([384, 2]) In [219]: np.transpose(embeddings)[:5] Out[219]: tensor([[ 0.0025, 0.0021], [ 0.0335, 0.0337], [ 0.0014, 0.0013], [-0.0084, -0.0081], [-0.0206, -0.0208]]) n [220]: np.transpose(embeddings)[-5:] Out[220]: tensor([[ 0.0678, 0.0675], [ 0.1426, 0.1423], [-0.0018, -0.0016], [-0.1755, -0.1752], [-0.0967, -0.0969]]) but it is for sure a bug since, this happens for absurd cases,\nsentences = [\"fruit\", \"couch\"] model_id = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors='pt') embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\"add_special_tokens:\", add_special_tokens, \"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) print(\"all close\", np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.2795]]) all close False add_special_tokens: False cosine similarity tensor([[1.0000]]) all close False pdb trace,\nIn [224]: ipdb.runcall(ut.encoded_to_embeddings, encoded_input, model_id) ipdb\u003e p encoded_input {'input_ids': tensor([[5909], [6411]]), 'token_type_ids': tensor([[0], [0]]), 'attention_mask': tensor([[1], [1]])} ipdb\u003e p model_output.last_hidden_state.shape, model_output.pooler_output.shape (torch.Size([2, 1, 384]), torch.Size([2, 384])) cos_sim(model_output.last_hidden_state[0,:,:], model_output.last_hidden_state[1,:,:]) tensor([[1.0000]]) cos_sim(model_output.pooler_output[0, :], model_output.pooler_output[1, :]) tensor([[1.0000]]) np.allclose(model_output.pooler_output[0, :], model_output.pooler_output[1, :]) False weird ok so this happens before the mean_pooling func gets called. weird.\nsentences = [\"the fruit is edible\", \"this couch is on sale\"] model_id = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors='pt') embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\"add_special_tokens:\", add_special_tokens, \"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) print(\"all close\", np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.0687]]) all close False add_special_tokens: False cosine similarity tensor([[0.0756]]) all close False ok so then something weird going on w/ a single token hmm?\nsentences = [\"there is fruit on the table\", \"look at the table there is fruit\"] model_id = \"sentence-transformers/all-MiniLM-L6-v2\" tokenizer = AutoTokenizer.from_pretrained(model_id) for add_special_tokens in [True, False]: encoded_input = tokenizer( sentences, padding=True, truncation=True, max_length=128, add_special_tokens=add_special_tokens, return_tensors='pt') embeddings = ut.encoded_to_embeddings(encoded_input, model_id) print(\"add_special_tokens:\", add_special_tokens, \"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) print(\"all close\", np.allclose(embeddings[0, :], embeddings[1,:])) add_special_tokens: True cosine similarity tensor([[0.9155]]) all close False add_special_tokens: False cosine similarity tensor([[0.9066]]) all close False 09:34 dont know why the output from the model produces nearly same embedding, for a single word encoded, but multiple words, it seems to be working fine. guess more multi-word experiments then next .\n[[Aug 2nd, 2023]] interesting attempts around single and multiword embeddings Since like I saw yesterday, I can get high 0.90s score if I have a longer sentence, and for a single word, there is some kind of weird bug\nSo for just [\"fruit\", \"apple\"] I had\nsentences = [\"fruit\", \"apple\"] model_id = \"sentence-transformers/all-MiniLM-L6-v2\" embeddings = ut.vec_to_embeddings(model_id, sentences) cos_sim(embeddings[0, :], embeddings[1,:]) In [229]: sentences = [\"fruit\", \"apple\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) ...: Out[229]: tensor([[0.5372]]) In [230]: sentences = [\"a fruit\", \"my apple\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[230]: tensor([[0.4532]]) In [231]: sentences = [\"its some fruit\", \"here my apple\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[231]: tensor([[0.3700]]) In [232]: sentences = [\"its some fruit juice\", \"here my apple sauce\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[232]: tensor([[0.4277]]) In [233]: sentences = [\"its some fruit juice home made\", \"here my apple sauce custom recipe\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[233]: tensor([[0.3862]]) Ok I don‚Äôt know haha, might not solve this mystery right now. It might also be that hmm not all words are as close together as I thought? 09:05 ok yea haha, indeed I found some better single-word examples.\nIn [234]: sentences = [\"couch\", \"sofa\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[234]: tensor([[0.8564]]) In [235]: sentences = [\"hammock\", \"bed\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[235]: tensor([[0.2903]]) In [236]: sentences = [\"mattress\", \"bed\"] ...: model_id = \"sentence-transformers/all-MiniLM-L6-v2\" ...: embeddings = ut.vec_to_embeddings(model_id, sentences) ...: cos_sim(embeddings[0, :], embeddings[1,:]) Out[236]: tensor([[0.6860]]) So yea putting the single-word-bug theory to rest, at least when using CLS, SEP add_special_tokens=True there is no problem. And without CLS, SEP, using add_special_tokens=False then yea, the embeddings for both inputs are nearly the same producing cosine similarity of 1. That‚Äôs really weird. So I should stick to using add_special_tokens=True for now at least for this model. Ok back to big picture then,\nSo I have observed this model has bad performance when I try throwing technical [[jargon]] at it,\nreload(ut) model_id = \"sentence-transformers/all-MiniLM-L6-v2\" sentences = [ \"python\", \"pyspark\", ] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) ['[CLS]', 'python', '[SEP]', '[PAD]', '[PAD]'] ['[CLS]', 'p', '##ys', '##park', '[SEP]'] sentences = [\"postgresql\", \"database\"] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ['[CLS]', 'post', '##gre', '##s', '##q', '##l', '[SEP]'] ['[CLS]', 'database', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] cosine similarity tensor([[0.5301]]) sentences = [\"postgresql\", \"sql\"] encoded_input, embeddings = ut.vec_to_embeddings( model_id, sentences, return_tokenizer_output=True) tokenizer = AutoTokenizer.from_pretrained(model_id) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) print(\"cosine similarity\", cos_sim(embeddings[0, :], embeddings[1,:])) ['[CLS]', 'post', '##gre', '##s', '##q', '##l', '[SEP]'] ['[CLS]', 'sql', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] cosine similarity tensor([[0.5085]]) I have tried building a tokenizer from this model‚Äôs tokenizer, but that was problematic because it is not tokenizer fine tuning like model fine tuning, it just uses the same class. So initially I was thinking building a new tokenizer means I need all billion examples earlier, hmm but that‚Äôs just what went in to fine tuning the model. Maybe for a tokenizer, perhaps I just need to build a dataset that has a good sampling mix of English language and a healthy proportion of technical language. But I think before doing that, I would like to take another stab at understanding, how to answer the more general question, about [[subword-tokenization]] , so for non jargon language, any tokenizer will still end up having plenty of subwords, but they will still end up with good embeddings right? So since subwords are split up into multiple embeddings, then maybe is it that the model just associates those subword embeddings appropriately then? So is it like you identify that multi-syllable words have the same #etymology roots perhaps, like like, ‚Äúcharismatic‚Äù , ‚Äúcharisma‚Äù and ‚Äúcharacter‚Äù and ‚Äúcharacterization‚Äù ,\nIn [242]: sentences = [ ...: \"charismatic\" , \"charisma\", \"character\", \"characterization\" ...: ] In [243]: encoded_input, embeddings = ut.vec_to_embeddings( ...: model_id, sentences, return_tokenizer_output=True) ...: tokenizer = AutoTokenizer.from_pretrained(model_id) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][0, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][1, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][2, :])) ...: print(tokenizer.convert_ids_to_tokens(encoded_input.data[\"input_ids\"][3, :])) ...: ['[CLS]', 'charismatic', '[SEP]', '[PAD]', '[PAD]'] ['[CLS]', 'char', '##ism', '##a', '[SEP]'] ['[CLS]', 'character', '[SEP]', '[PAD]', '[PAD]'] ['[CLS]', 'characterization', '[SEP]', '[PAD]', '[PAD]'] In [244]: cos_sim(embeddings[0, :], embeddings[1,:]) Out[244]: tensor([[0.5882]]) In [245]: cos_sim(embeddings[0, :], embeddings[2,:]) Out[245]: tensor([[0.4623]]) In [246]: cos_sim(embeddings[1, :], embeddings[2,:]) Out[246]: tensor([[0.6060]]) yea maybe something like that happens with an embedding model then, it can still embed subwords well. Should try to do some reading on this .\nok\n","wordCount":"18532","inLanguage":"en","datePublished":"2023-02-18T00:00:00Z","dateModified":"2023-08-02T09:33:08-04:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;¬ª&nbsp;<a href=https://michal.piekarczyk.xyz/post/>Posts</a></div><h1 class=post-title>langchain interview me 2023 feb</h1><div class=post-meta><span title='2023-02-18 00:00:00 +0000 UTC'>February 18, 2023</span>&nbsp;¬∑&nbsp;88 min&nbsp;¬∑&nbsp;18532 words&nbsp;¬∑&nbsp;Michal Piekarczyk</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#note>Note</a></li><li><a href=#vision>Vision</a><ul><li><a href=#compare-against-arbitrary-job-listings--job-description->compare against arbitrary #job-listings , #job-description ,</a></li><li><a href=#making-updating-your-brag-document-like-a-fun-factor-having-fun-experience->Making updating your #brag-document like a #fun-factor #[[having fun]] experience üòÄ</a></li><li><a href=#and-original-intent-was-a-ui-to-actually-ask-questions>And original intent was a UI to actually ask questions</a></li><li><a href=#also-better-tellmeaboutyourself--tell-a-story--since-the-brag-document-has-lots-of-cool-stories-and-also-chronological-story--this-could-be-a-cool-way-to-weave-together-the-personal-story>Also better #TellMeAboutYourself , #[[tell a story]] . Since the #brag-document has lots of cool stories, and also #chronological-story , this could be a cool way to weave together the personal story.</a></li><li><a href=#getting-feedback-about-your-text-corpus-of-your-experience>Getting feedback about your text corpus of your experience</a></li></ul></li><li><a href=#my-blog-posts>my blog posts</a><ul><li><a href=#initial-post-with-the-question-answer-task>initial post with the #question-answer-task</a></li><li><a href=#also-this-one>Also this one</a></li><li><a href=#also--applying-sentence-transformers-to-code-search>Also applying sentence transformers to code search</a></li></ul></li><li><a href=#research>research</a><ul><li><a href=#went-through-that-articlegetting-started-with-embeddings--which-was-useful-to-start-learning-about-sentence-transformers-library>went through that [[article/Getting Started With Embeddings]] , which was useful to start learning about #sentence-transformers library</a></li><li><a href=#i-have-this-question-is-the-sentence-transformers-average-pooling-noisy>I have this question, is the #sentence-transformers #average-pooling noisy?</a></li><li><a href=#can-i-use-better-ner-named-entity-recognition-ner->Can I use better #NER [[Named Entity Recognition NER]] ?</a></li></ul></li><li><a href=#attempts>attempts</a><ul><li><a href=#on-may-28th-2023-i-started-defining-the-job-description-comparison-concept-and-i-ran-a-comparison-of-my-blurb-2023-02-19t011846-the-story-blurbtxt-against-2023-05-28-enigma-mletxt--the-results-were-maybe-somewhat-not-easy-to-read-perhaps-a-lot-of-text-maybe-i-need-shorter-sentences>on [[May 28th, 2023]], I started defining the #job-description comparison concept, and I ran a comparison of my blurb &ldquo;2023-02-19T011846-the-story-blurb.txt&rdquo; against &ldquo;2023-05-28-enigma-mle.txt&rdquo; . The results were maybe somewhat not easy to read. Perhaps a lot of text. Maybe I need shorter sentences?</a></li><li><a href=#and-on-jun-18th-2023--how-about-spacy-and-named-entity-recognition-ner->and on [[Jun 18th, 2023]] , how about #spacy and #[[Named Entity Recognition NER]] ,</a></li><li><a href=#and-on-jun-25th-2023-the-blogpost2023-06-25-everybody-loves-reynauds>and on [[Jun 25th, 2023]] the [[blogpost/2023-06-25-everybody-loves-reynauds]]</a></li><li><a href=#jul-6th-2023--can-i-do-a-supervised-fine-tuning-my-first->[[Jul 6th, 2023]] , can I do a #[[supervised fine-tuning]] #[[my first]] ,</a></li><li><a href=#jul-7th-2023-some-paraphrase-mining-hmm-how-can-i-build-my-dataset>[[Jul 7th, 2023]] some [[paraphrase-mining]] hmm how can I build my dataset</a></li><li><a href=#jul-10th-2023-looked-at-the-vocabulary-misses-and-job-titles>[[Jul 10th, 2023]] looked at the vocabulary misses and job titles</a></li><li><a href=#jul-11th-2023-refined-the-nohits-per-the-vocabulary-of-the-model-and-used-it-to-tokenize-to-verify-they-are-unknown>[[Jul 11th, 2023]] refined the nohits per the vocabulary of the model and used it to tokenize to verify they are unknown</a></li><li><a href=#jul-12th-2023-ok-started-building-up-code-to-capture-a-mini-corpus-of-the-sentences-which-have-words-that-are-not-part-of-the-vocabulary>[[Jul 12th, 2023]] ok started building up code to capture a mini corpus, of the sentences, which have words that are not part of the vocabulary,</a></li><li><a href=#jul-15th-2023-finally-tried-the-supervised-fine-tuning-but-didnt-seem-to-add-to-the-vocabulary>[[Jul 15th, 2023]] finally tried the [[supervised fine-tuning]] but didn&rsquo;t seem to add to the vocabulary</a></li><li><a href=#then-created-a-dataset-from-that-and-ran-fit-with-the-out-of-the-box-all-minilm-l6-v2-sentence-transformer-model>Then created a dataset from that, and ran fit with the out of the box &lsquo;all-MiniLM-L6-v2&rsquo; sentence transformer model</a></li><li><a href=#hmm-but-new-vocabulary-does-not-seem-to-reflect-new-terms-somehow>hmm but new vocabulary does not seem to reflect new terms somehow</a></li><li><a href=#jul-16th-2023-yea-tried-a-different-take-on-adding-tokens-to-a-tokenizer-and-that-seemed-to-do-it>[[Jul 16th, 2023]] yea tried a different take on adding tokens to a tokenizer and that seemed to do it.</a></li><li><a href=#jul-17th-2023-reading-more-i-learn-you-do-likely-need-to-train-a-new-tokenizer-and-you-cant-just-simply-update-its-vocabulary>[[Jul 17th, 2023]] Reading more, I learn you do likely need to train a new tokenizer and you can&rsquo;t just simply update its vocabulary</a></li><li><a href=#jul-18th-2023-reading-more-about-subword-tokenization-and-purpose-of-tokenizer-tuning>[[Jul 18th, 2023]] reading more about subword tokenization and purpose of tokenizer tuning</a></li><li><a href=#jul-19th-2023-ran-the-tokenizer-fine-tuning-with-a-small-dataset>[[Jul 19th, 2023]] Ran the tokenizer fine tuning with a small dataset</a></li><li><a href=#jul-20th-2023-did-bit-of-reading-learning--research-mode>[[Jul 20th, 2023]] did bit of reading learning , research mode</a></li><li><a href=#jul-21st-2023-mainly-just-brief-thoughts-about-building-a-new-dataset>[[Jul 21st, 2023]] mainly just brief thoughts about building a new dataset</a></li><li><a href=#jul-22nd-2023-one---brief-look-model-card-for-sentence-transformersall-minilm-l6-v2>[[Jul 22nd, 2023]] one brief look model card for, <code>'sentence-transformers/all-MiniLM-L6-v2'</code></a></li><li><a href=#jul-23rd-2023-more-practical-dive-today>[[Jul 23rd, 2023]] More practical dive today</a></li><li><a href=#jul-24th-2023-started-a-nice-debug-session-today--false-negative-analysis-here>[[Jul 24th, 2023]] started a nice debug session today , false negative analysis here,</a></li><li><a href=#jul-25th-2023-stop-word-removal-experiment>[[Jul 25th, 2023]] Stop word removal experiment</a></li><li><a href=#jul-26th-2023-some-more-experimentation-cosine-similarity-and-stop-words>[[Jul 26th, 2023]] some more experimentation, cosine similarity and stop words</a></li><li><a href=#jul-27th-2023-hmm-what-is-the-subword-tokenization--multiple--when-thinking-about-truncation>[[Jul 27th, 2023]] hmm what is the subword tokenization multiple , when thinking about truncation</a></li><li><a href=#jul-28th-2023-looking-more-closely-on-how-sentence-transformer-model-pools>[[Jul 28th, 2023]] looking more closely on how sentence transformer model pools</a></li><li><a href=#jul-29th-2023-hmm>[[Jul 29th, 2023]] hmm</a></li><li><a href=#jul-31st-2023-ok-interesting-removing-the-special-tokens-has-no-effect-on-the-cosine-similarity>[[Jul 31st, 2023]] ok interesting, removing the special tokens has no effect on the cosine similarity</a></li><li><a href=#aug-1st-2023-trying-a-few-more-things-wondering-why-weird-cosine-similarity-1-w-single-words-encoded-but-different-words>[[Aug 1st, 2023]] trying a few more things wondering why weird cosine similarity 1 w/ single words encoded but different words</a></li><li><a href=#aug-2nd-2023-interesting-attempts-around-single-and-multiword-embeddings>[[Aug 2nd, 2023]] interesting attempts around single and multiword embeddings</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>type:: #project-type
status:: #in-progress-status
blogDate:: 2023-02-18</p><h2 id=note>Note<a hidden class=anchor aria-hidden=true href=#note>#</a></h2><p>This is not a blog post but kind of a landing page I&rsquo;m using to aggregate on-going project notes here</p><h2 id=vision>Vision<a hidden class=anchor aria-hidden=true href=#vision>#</a></h2><p>Broadly would like to do here something like the following</p><h3 id=compare-against-arbitrary-job-listings--job-description->compare against arbitrary #job-listings , #job-description ,<a hidden class=anchor aria-hidden=true href=#compare-against-arbitrary-job-listings--job-description->#</a></h3><p>collapsed:: true
And [[my projects/personal/langchain-interview-me-2023-feb]] , also now the repo usable by anyone who wants to compare their #brag-document to #job-listings [[job-description]] out there , get a delta , and more broadly , understand say , their industry posture , since that‚Äôs a moving target . And you can interview yourself too haha .</p><p>I can use the [[my projects/personal/langchain-interview-me-2023-feb]] stuff concepts to see , what roles online do I align with and am I progressing towards them at #Humana or stagnating?</p><h3 id=making-updating-your-brag-document-like-a-fun-factor-having-fun-experience->Making updating your #brag-document like a #fun-factor #[[having fun]] experience üòÄ<a hidden class=anchor aria-hidden=true href=#making-updating-your-brag-document-like-a-fun-factor-having-fun-experience->#</a></h3><h3 id=and-original-intent-was-a-ui-to-actually-ask-questions>And original intent was a UI to actually ask questions<a hidden class=anchor aria-hidden=true href=#and-original-intent-was-a-ui-to-actually-ask-questions>#</a></h3><h3 id=also-better-tellmeaboutyourself--tell-a-story--since-the-brag-document-has-lots-of-cool-stories-and-also-chronological-story--this-could-be-a-cool-way-to-weave-together-the-personal-story>Also better #TellMeAboutYourself , #[[tell a story]] . Since the #brag-document has lots of cool stories, and also #chronological-story , this could be a cool way to weave together the personal story.<a hidden class=anchor aria-hidden=true href=#also-better-tellmeaboutyourself--tell-a-story--since-the-brag-document-has-lots-of-cool-stories-and-also-chronological-story--this-could-be-a-cool-way-to-weave-together-the-personal-story>#</a></h3><p>collapsed:: true
And for [[my projects/personal/langchain-interview-me-2023-feb]] thing, so I was in this [[May 28th, 2023]] too. Would be cool to make it easier for an individual to construct their [[TellMeAboutYourself]] since this is so important and at least to myself cannot rely on my memory haha</p><h3 id=getting-feedback-about-your-text-corpus-of-your-experience>Getting feedback about your text corpus of your experience<a hidden class=anchor aria-hidden=true href=#getting-feedback-about-your-text-corpus-of-your-experience>#</a></h3><p>Maybe the documents out there can help inform you, of other relevant terms that you forgot to discuss.
Also maybe there is low-information density in your corpus. Take out the stop words haha.</p><h2 id=my-blog-posts>my blog posts<a hidden class=anchor aria-hidden=true href=#my-blog-posts>#</a></h2><h3 id=initial-post-with-the-question-answer-task>initial post with the #question-answer-task<a hidden class=anchor aria-hidden=true href=#initial-post-with-the-question-answer-task>#</a></h3><p>20:55 So I have the #blog-post from [[Feb 18th, 2023]] <a href=https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/>here</a>, where I put together my technical background , create embeddings from them and run a #question-answer-task #langchain , with one of the chains called &ldquo;load_qa_with_sources_chain&rdquo; that gives intermediate source text results too.</p><h3 id=also-this-one>Also this one<a hidden class=anchor aria-hidden=true href=#also-this-one>#</a></h3><p>collapsed:: true
[[blogpost/2023-06-25-everybody-loves-reynauds]] <a href=https://michal.piekarczyk.xyz/post/2023-06-25-everybody-loves-reynauds>https://michal.piekarczyk.xyz/post/2023-06-25-everybody-loves-reynauds</a> with a comparison across a few embedding models, to suss out which of them do or do not have medical vocabulary</p><h3 id=also--applying-sentence-transformers-to-code-search>Also applying sentence transformers to code search<a hidden class=anchor aria-hidden=true href=#also--applying-sentence-transformers-to-code-search>#</a></h3><p><a href=https://michal.piekarczyk.xyz/post/2023-06-11-semantic-code-search-first-stab/>part one</a> and <a href=https://michal.piekarczyk.xyz/post/2023-06-13-semantic-code-search-part-2/>part two</a></p><h2 id=research>research<a hidden class=anchor aria-hidden=true href=#research>#</a></h2><h3 id=went-through-that-articlegetting-started-with-embeddings--which-was-useful-to-start-learning-about-sentence-transformers-library>went through that [[article/Getting Started With Embeddings]] , which was useful to start learning about #sentence-transformers library<a hidden class=anchor aria-hidden=true href=#went-through-that-articlegetting-started-with-embeddings--which-was-useful-to-start-learning-about-sentence-transformers-library>#</a></h3><p>collapsed:: true
And more recently, I went through the #[[hugging face]] example around #Medicare and with the #article-type , [[article/Getting Started With Embeddings]] , <a href=https://huggingface.co/blog/getting-started-with-embeddings>link</a>,</p><p>And used the &ldquo;langchainz&rdquo; virtual env I have, and I used the <a href=https://api-inference.huggingface.co>https://api-inference.huggingface.co</a> REST API specifying to use the &ldquo;sentence-transformers/all-MiniLM-L6-v2&rdquo; model to produce embeddings , and then the #sentence-transformers library, <code>semantic_search</code> , ( <code>from sentence_transformers.util import semantic_search</code> ) , to a question to a set of frequently asked questions</p><h3 id=i-have-this-question-is-the-sentence-transformers-average-pooling-noisy>I have this question, is the #sentence-transformers #average-pooling noisy?<a hidden class=anchor aria-hidden=true href=#i-have-this-question-is-the-sentence-transformers-average-pooling-noisy>#</a></h3><h3 id=can-i-use-better-ner-named-entity-recognition-ner->Can I use better #NER [[Named Entity Recognition NER]] ?<a hidden class=anchor aria-hidden=true href=#can-i-use-better-ner-named-entity-recognition-ner->#</a></h3><p>Maybe help from <a href=https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da>https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da</a> ?</p><h2 id=attempts>attempts<a hidden class=anchor aria-hidden=true href=#attempts>#</a></h2><h3 id=on-may-28th-2023-i-started-defining-the-job-description-comparison-concept-and-i-ran-a-comparison-of-my-blurb-2023-02-19t011846-the-story-blurbtxt-against-2023-05-28-enigma-mletxt--the-results-were-maybe-somewhat-not-easy-to-read-perhaps-a-lot-of-text-maybe-i-need-shorter-sentences>on [[May 28th, 2023]], I started defining the #job-description comparison concept, and I ran a comparison of my blurb &ldquo;2023-02-19T011846-the-story-blurb.txt&rdquo; against &ldquo;2023-05-28-enigma-mle.txt&rdquo; . The results were maybe somewhat not easy to read. Perhaps a lot of text. Maybe I need shorter sentences?<a hidden class=anchor aria-hidden=true href=#on-may-28th-2023-i-started-defining-the-job-description-comparison-concept-and-i-ran-a-comparison-of-my-blurb-2023-02-19t011846-the-story-blurbtxt-against-2023-05-28-enigma-mletxt--the-results-were-maybe-somewhat-not-easy-to-read-perhaps-a-lot-of-text-maybe-i-need-shorter-sentences>#</a></h3><p>collapsed:: true</p><h4 id=motivation--plan>Motivation / plan<a hidden class=anchor aria-hidden=true href=#motivation--plan>#</a></h4><p>So, now , let me create a quick tool, to cross the sentences of a brag document, against like 10 job description embeddings, and help match them, to understand say, two kinds of problems,</p><p>(1) Which job descriptions match the best,
(2) but then also, for a specific job description, which sentences are matched and which are not matched.
(3) So help you know, say even if you are not necessarily looking for something right now, can you know do you align with recent postings in your field?</p><h4 id=outcome>Outcome<a hidden class=anchor aria-hidden=true href=#outcome>#</a></h4><p>21:50 okay here&rsquo;s a quick example,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers.util <span style=color:#f92672>import</span> semantic_search
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>api_url <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;https://api-inference.huggingface.co/pipeline/feature-extraction/</span><span style=color:#e6db74>{</span>model_id<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>headers <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;Authorization&#34;</span>: <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Bearer </span><span style=color:#e6db74>{</span>hf_token<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>query</span>(texts):
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>post(api_url, headers<span style=color:#f92672>=</span>headers, json<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;inputs&#34;</span>: texts, <span style=color:#e6db74>&#34;options&#34;</span>:{<span style=color:#e6db74>&#34;wait_for_model&#34;</span>:<span style=color:#66d9ef>True</span>}})
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> response<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>repos_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> str(Path(repos_dir) <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-interview-me&#34;</span>  )
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> str(Path(repos_dir) 
</span></span><span style=display:flex><span>          <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-interview-me/2023-02-19T011846-the-story-blurb.txt&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_story_vec <span style=color:#f92672>=</span> Path(loc)<span style=color:#f92672>.</span>read_text()<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>folder <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;job_descriptions&#34;</span>
</span></span><span style=display:flex><span>jd1 <span style=color:#f92672>=</span> Path(folder) <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;2023-05-28-enigma-mle.txt&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>texts <span style=color:#f92672>=</span> jd1<span style=color:#f92672>.</span>read_text()<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> query(my_story_vec)
</span></span><span style=display:flex><span>my_story_embeddings <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>FloatTensor(output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> query(texts)
</span></span><span style=display:flex><span>jd_embeddings <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>FloatTensor(output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>hits <span style=color:#f92672>=</span> semantic_search(my_story_embeddings, jd_embeddings, top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><p>22:24 Ah interesting, so since unlike the #Medicare #faq tutorial, where one question was given, I am passing an array now so my output is now also multi-dimensional</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># [[texts[x[&#34;corpus_id&#34;]], x[&#34;corpus_id&#34;]] for x in hits[0] ]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, row <span style=color:#f92672>in</span> enumerate(hits):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;(</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>, <span style=color:#e6db74>&#34;matching,&#34;</span>, my_story_vec[i], <span style=color:#e6db74>&#34;:&#34;</span>)
</span></span><span style=display:flex><span>    hmm <span style=color:#f92672>=</span> [[texts[x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>]], x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>], x[<span style=color:#e6db74>&#34;score&#34;</span>]] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> row[:<span style=color:#ae81ff>3</span>] ]
</span></span><span style=display:flex><span>    print(hmm, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>1</span>) matching,  :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#ae81ff>34</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.9999997615814209</span>], [<span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>0.9999997615814209</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>2</span>) matching, When I worked at zibby1, there was a project <span style=color:#f92672>in</span> <span style=color:#ae81ff>2015</span>, various earlier projects<span style=color:#f92672>.</span>Created a Vagrant virtual machine based staging environment that developers can quickly use to stage code, to help us transition <span style=color:#f92672>from</span> personalized AWS staging environments which can potentially help us save several hundreds of dollars a month<span style=color:#f92672>..</span>  :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;‚Ä¢ Has experience working with distributed computing and building CI/CD tools.&#39;</span>, <span style=color:#ae81ff>26</span>, <span style=color:#ae81ff>0.34121203422546387</span>], [<span style=color:#e6db74>&#39;‚Ä¢ Engineers best-in-class solutions that enables data scientists to develop, test, explain, deploy and monitor statistical models to production environments (we use PySpark)&#39;</span>, <span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>0.3387572765350342</span>], [<span style=color:#e6db74>&#39;As a member of Machine Learning team, you will build the ML systems and infrastructure at the core of our small business data product. Your impact will be measured by the performance, testability and reliability of our ML systems.&#39;</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>0.28739088773727417</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>3</span>) matching, Implemented the retailer lead list reporting, so that big data heavy retailers like Sears could <span style=color:#66d9ef>finally</span> be more involved <span style=color:#f92672>in</span> following up <span style=color:#66d9ef>with</span> customers who were <span style=color:#f92672>not</span> originating their preapprovals<span style=color:#f92672>..</span>  :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;‚Ä¢ Is driven to work with customers to have an impact on the real world&#39;</span>, <span style=color:#ae81ff>29</span>, <span style=color:#ae81ff>0.3841177821159363</span>], [<span style=color:#e6db74>&#39;‚Ä¢ Impact: your work product will have a direct impact on hundreds of millions of significant decisions within the massive small business economy&#39;</span>, <span style=color:#ae81ff>21</span>, <span style=color:#ae81ff>0.28213953971862793</span>], [<span style=color:#e6db74>&#39;This is a critical and exciting time at Enigma. We are hearing from repeated customers that our product is creating tremendous value for them and is aligned perfectly with their needs. This creates an urgent need to accelerate the build out of our machine learning capabilities&#39;</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>0.2662915289402008</span>]] 
</span></span></code></pre></div><p>Okay there is some beginnings of something here. Got to do some more preprocessing on this data though, do get way more cleaner comparisons .</p><h3 id=and-on-jun-18th-2023--how-about-spacy-and-named-entity-recognition-ner->and on [[Jun 18th, 2023]] , how about #spacy and #[[Named Entity Recognition NER]] ,<a hidden class=anchor aria-hidden=true href=#and-on-jun-18th-2023--how-about-spacy-and-named-entity-recognition-ner->#</a></h3><p>collapsed:: true
Think because yea I saw that #sentence-transformers #[[cosine similarity]] between my #brag-document sentences and #job-description was super low, so thinking hey how about extract entities and then attempt matches using that instead,
Initially I saw that the first extraction was pulling only very few entities for this job description for instance,
19:33 hmm ok but , this is not capturing all the entities, hmm weird,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> spacy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nlp <span style=color:#f92672>=</span> spacy<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;en_core_web_sm&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>3</span>]: jd <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: Google&#39;s software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our pro
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: ducts need to handle information at massive scale, and extend well beyond web search. We&#39;re looking for engineers who bring fresh ideas from all areas, including information r
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: etrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google‚Äôs needs with opportunities to switch teams and projec
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: ts as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across t
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: he full-stack as we continue to push technology forward.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: .
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: The web is what you make of it and our team is helping the world make more of the web. From open-source pros to user-experience extraordinaires, we develop products that help 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: users connect, communicate and collaborate with others. Our consumer products and cloud platforms are giving millions of users at homes, businesses, universities and nonprofit
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: s around the world the tools that shape their web experience -- and changing the way they think about computing.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   ...: &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>4</span>]: doc <span style=color:#f92672>=</span> nlp(jd)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>5</span>]: <span style=color:#66d9ef>for</span> ent <span style=color:#f92672>in</span> doc<span style=color:#f92672>.</span>ents:
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>:     print(ent<span style=color:#f92672>.</span>text, ent<span style=color:#f92672>.</span>start_char, ent<span style=color:#f92672>.</span>end_char, ent<span style=color:#f92672>.</span>label_)
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>Google <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>7</span> ORG
</span></span><span style=display:flex><span>billions <span style=color:#ae81ff>86</span> <span style=color:#ae81ff>94</span> CARDINAL
</span></span><span style=display:flex><span>UI <span style=color:#ae81ff>504</span> <span style=color:#ae81ff>506</span> GPE
</span></span><span style=display:flex><span>Google<span style=color:#960050;background-color:#1e0010>‚Äô</span>s <span style=color:#ae81ff>641</span> <span style=color:#ae81ff>649</span> ORG
</span></span><span style=display:flex><span>millions <span style=color:#ae81ff>1396</span> <span style=color:#ae81ff>1404</span> CARDINAL
</span></span></code></pre></div><h3 id=and-on-jun-25th-2023-the-blogpost2023-06-25-everybody-loves-reynauds>and on [[Jun 25th, 2023]] the [[blogpost/2023-06-25-everybody-loves-reynauds]]<a hidden class=anchor aria-hidden=true href=#and-on-jun-25th-2023-the-blogpost2023-06-25-everybody-loves-reynauds>#</a></h3><p>So in that mini blogpost, I tried out multiple #[[embedding space]] using different embedding models. And it looked like only <code>‚Äúall-MiniLM-L12-v2‚Äù</code> appeared to have some kind of [[medical-condition]] knowledge .</p><h3 id=jul-6th-2023--can-i-do-a-supervised-fine-tuning-my-first->[[Jul 6th, 2023]] , can I do a #[[supervised fine-tuning]] #[[my first]] ,<a hidden class=anchor aria-hidden=true href=#jul-6th-2023--can-i-do-a-supervised-fine-tuning-my-first->#</a></h3><p>collapsed:: true
yea so just starting , going through , between <a href=https://www.sbert.net/docs/training/overview.html>https://www.sbert.net/docs/training/overview.html</a> and [[article/Train and Fine-Tune Sentence Transformers Models]]</p><p>08:35 so yea if a particular out of the box model uses [[average-pooling]] then for sure that yells at me that [[stop-words]] should be removed hmm
08:39 ACtually looking at <a href="https://www.kaggle.com/datasets?search=job">https://www.kaggle.com/datasets?search=job</a> and hmm I do see job related datasets. Maybe there are some relevant ones !?
How about, say, <a href=https://www.kaggle.com/datasets/niyamatalmass/google-job-skills>https://www.kaggle.com/datasets/niyamatalmass/google-job-skills</a> , obtained by way of #selenium . Ok cool so this gives me hope that maybe in the future I can pull some more posts in the future, hopefully [[web-scrape]] is still possible later.
08:55 ok that is actually pretty decent, looking at the &ldquo;job_skills.csv&rdquo; . Some nice jargon in there !
09:04 ok so of the 4 dataset cases in <a href=https://huggingface.co/blog/how-to-train-sentence-transformers>https://huggingface.co/blog/how-to-train-sentence-transformers</a> , I think makes most sense to use Case 2, where instead of assigning a number from 0 to 1 for similarity, I can just choose sentences that. I feel are similar to beuild a dataset. These are &ldquo;positive pairs&rdquo; [[positive pair]]
So <a href=https://huggingface.co/datasets/embedding-data/sentence-compression>https://huggingface.co/datasets/embedding-data/sentence-compression</a> here is a reference example that uses this. #[[Lossy compression]] perhaps . Kind of cool since yea #summarization-task is kind of this. Some details are missed yes but get the main idea #TLDR .
I see pretty simple, each row is a json looking pair. Ah ok [[json lines]] right. Learned about this from [[Michael Light]]. <a href=https://jsonlines.org/examples/>https://jsonlines.org/examples/</a> nice.
09:13 ok so I can write some dataset building code like this,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> InputExample
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_examples <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> read_json_lines(<span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, x <span style=color:#f92672>in</span> enumerate(dataset):
</span></span><span style=display:flex><span>    s1, s2 <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>    train_examples<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>        InputExample(texts<span style=color:#f92672>=</span>[s1, s2]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_dataloader <span style=color:#f92672>=</span> DataLoader(
</span></span><span style=display:flex><span>    train_examples, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>)
</span></span></code></pre></div><p>feels like I should use my [[my projects/personal/manage-my-photos]] labeler to help me kind of somewhat quickly build some labels.
Ok and for case 2 of [[positive pair]] looks like <a href=https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss>https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss</a> [[Multiple negatives ranking loss]] #[[loss function]] should be used</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> losses
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> SentenceTransformer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loss <span style=color:#f92672>=</span> losses<span style=color:#f92672>.</span>MultipleNegativesRankingLoss(model<span style=color:#f92672>=</span>model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># fine tune </span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(train_objectives<span style=color:#f92672>=</span>[(train_dataloader, train_loss)], epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>) 
</span></span></code></pre></div><p>just try for a handful then?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/job_skills.csv&#34;</span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(loc)
</span></span></code></pre></div><p>09:30 ok stopping here. next can continue to try out the first example of this fine tuning.</p><h3 id=jul-7th-2023-some-paraphrase-mining-hmm-how-can-i-build-my-dataset>[[Jul 7th, 2023]] some [[paraphrase-mining]] hmm how can I build my dataset<a hidden class=anchor aria-hidden=true href=#jul-7th-2023-some-paraphrase-mining-hmm-how-can-i-build-my-dataset>#</a></h3><p>08:40 [[my projects/personal/langchain-interview-me-2023-feb]]</p><p>collapsed:: true
so yea next was going to write that csv data, see can I do a fine tune , first try haha,
Wonder if I can dump out all the sentences , use out of the box similarity to see what looks like might be related, and maybe I can use the labeling annotation system I have to refine?
Ok, what are the closest right now?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> functools <span style=color:#f92672>import</span> reduce
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> Counter
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/job_skills.csv&#34;</span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>raw_sentences <span style=color:#f92672>=</span> reduce(<span style=color:#66d9ef>lambda</span> x, y: x <span style=color:#f92672>+</span> y,
</span></span><span style=display:flex><span>	[re<span style=color:#f92672>.</span>split(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#34;[\n\.]&#34;</span>, df<span style=color:#f92672>.</span>iloc[i][col])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(df<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;Responsibilities&#34;</span>, 
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#39;Minimum Qualifications&#39;</span>, 
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#39;Preferred Qualifications&#39;</span>]
</span></span><span style=display:flex><span>             <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> pd<span style=color:#f92672>.</span>isnull(df<span style=color:#f92672>.</span>iloc[i][col])
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> list(set(raw_sentences))
</span></span></code></pre></div><p>hmm ok</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>101</span>]: dict(Counter(raw_sentences)<span style=color:#f92672>.</span>most_common(<span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>101</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;&#39;</span>: <span style=color:#ae81ff>14038</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;BA/BS degree or equivalent practical experience&#39;</span>: <span style=color:#ae81ff>521</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;g&#39;</span>: <span style=color:#ae81ff>261</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#34;Bachelor&#39;s degree or equivalent practical experience&#34;</span>: <span style=color:#ae81ff>71</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39; Specific responsibilities are assigned to interns at the start of the program&#39;</span>: <span style=color:#ae81ff>69</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>102</span>]: len(raw_sentences), len(sentences)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>102</span>]: (<span style=color:#ae81ff>31424</span>, <span style=color:#ae81ff>9421</span>)
</span></span></code></pre></div><p>09:13 ok and similarities , [[paraphrase-mining]]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> SentenceTransformer, util
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>paraphrases <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>paraphrase_mining(model, sentences)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> paraphrase <span style=color:#f92672>in</span> paraphrases[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>10</span>]:
</span></span><span style=display:flex><span>    score, i, j <span style=color:#f92672>=</span> paraphrase
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74> </span><span style=color:#ae81ff>\t\t</span><span style=color:#e6db74> </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> </span><span style=color:#ae81ff>\t\t</span><span style=color:#e6db74> Score: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(sentences[i], sentences[j], score))
</span></span></code></pre></div><p>09:21 wow that was pretty fast .</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span> Work <span style=color:#66d9ef>with</span> Google Cloud Platform Partners to develop campaigns 		  Work <span style=color:#66d9ef>with</span> Google Cloud Platform partners to develop campaigns 	 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>Collect customer support data <span style=color:#f92672>from</span> partners <span style=color:#f92672>and</span> derive insights <span style=color:#66d9ef>for</span> cross<span style=color:#f92672>-</span>functional teams 		  Collect customer support data <span style=color:#f92672>from</span> partners <span style=color:#f92672>and</span> derive insights <span style=color:#66d9ef>for</span> cross<span style=color:#f92672>-</span>functional teams 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>10</span> years of partner programs experience at an enterprise software (<span style=color:#f92672>or</span> Cloud) company <span style=color:#f92672>and</span> experience <span style=color:#66d9ef>with</span> competitive partner programs 		 <span style=color:#ae81ff>10</span> years of partner programs experience at an Enterprise Software (<span style=color:#f92672>or</span> Cloud) company <span style=color:#f92672>and</span> experience <span style=color:#66d9ef>with</span> competitive partner programs 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>9</span> years of experience serving <span style=color:#f92672>in</span> the capacity of a technical sales engineer <span style=color:#f92672>in</span> a cloud computing environment <span style=color:#f92672>or</span> equivalent experience <span style=color:#f92672>in</span> a customer facing role (including working <span style=color:#66d9ef>as</span> a member of a professional services <span style=color:#f92672>or</span> systems engineering team) 		 <span style=color:#ae81ff>9</span> years of experience serving <span style=color:#f92672>in</span> the capacity of a Technical Sales Engineer <span style=color:#f92672>in</span> a cloud computing environment <span style=color:#f92672>or</span> equivalent experience <span style=color:#f92672>in</span> a customer facing role (including working <span style=color:#66d9ef>as</span> a member of a professional services <span style=color:#f92672>or</span> systems engineering team) 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span> Identify, engage, <span style=color:#f92672>and</span> advise Google<span style=color:#f92672>-</span>caliber talent <span style=color:#66d9ef>with</span> a focus on creating a great experience <span style=color:#66d9ef>for</span> each candidate 		 Identify, engage, <span style=color:#f92672>and</span> advise Google<span style=color:#f92672>-</span>caliber talent <span style=color:#66d9ef>with</span> a focus on creating a great experience <span style=color:#66d9ef>for</span> each candidate 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>Manage a team of software engineers, including task planning <span style=color:#f92672>and</span> code reviews 		 Manage a team of Software Engineers, including task planning <span style=color:#f92672>and</span> code reviews 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>Perform an array of administrative tasks (Manage calendars, book travel, <span style=color:#f92672>and</span> schedule facilities <span style=color:#f92672>and</span> equipment) 		 Perform an array of administrative tasks (manage calendars, book travel, <span style=color:#f92672>and</span> schedule facilities <span style=color:#f92672>and</span> equipment) 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>Understanding of solution architecture within web <span style=color:#f92672>and</span> mobile environments <span style=color:#f92672>and</span> technical experience of web<span style=color:#f92672>/</span>internet related technologies, architecture across SAAS, PAAS <span style=color:#f92672>and</span> IAAS <span style=color:#f92672>and</span> competitive cloud productivity suites 		 Understanding of solution architecture within web <span style=color:#f92672>and</span> mobile environments <span style=color:#f92672>and</span> technical experience of web<span style=color:#f92672>/</span>internet related technologies, architecture across SaaS, PaaS <span style=color:#f92672>and</span> IaaS <span style=color:#f92672>and</span> competitive cloud productivity suites 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>Extensive knowledge of UNIX<span style=color:#f92672>/</span>Linux environments 		 Extensive knowledge of Unix<span style=color:#f92672>/</span>Linux environments 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span> Experience working towards strategic business goals 		 Experience working towards strategic business goals 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>CPU times: user <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>41</span>s, sys: <span style=color:#ae81ff>3.35</span> s, total: <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>44</span>s
</span></span><span style=display:flex><span>Wall time: <span style=color:#ae81ff>57.3</span> s
</span></span></code></pre></div><p>Ok seeing even though I used &ldquo;set&rdquo; I still have dupes . Ok seeing , should also use strip and lower case too
09:25 ok</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>stripped_raw_sentences <span style=color:#f92672>=</span> [x<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>lower() <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> raw_sentences]
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> list(set(stripped_raw_sentences))
</span></span><span style=display:flex><span>print(len(raw_sentences), len(set(raw_sentences)), len(sentences))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 31424 9421 9325</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> SentenceTransformer, util
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>paraphrases <span style=color:#f92672>=</span> util<span style=color:#f92672>.</span>paraphrase_mining(model, sentences)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> paraphrase <span style=color:#f92672>in</span> paraphrases[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>10</span>]:
</span></span><span style=display:flex><span>    score, i, j <span style=color:#f92672>=</span> paraphrase
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74> </span><span style=color:#ae81ff>\t\t</span><span style=color:#e6db74> </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> </span><span style=color:#ae81ff>\t\t</span><span style=color:#e6db74> Score: </span><span style=color:#e6db74>{:.4f}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(sentences[i], sentences[j], score))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cpa <span style=color:#f92672>/</span> ca <span style=color:#f92672>or</span> other professional accounting accreditation 		 cpa<span style=color:#f92672>/</span>ca <span style=color:#f92672>or</span> other professional accounting accreditation 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>10</span> years of partner programs experience at a enterprise software (<span style=color:#f92672>or</span> cloud) company <span style=color:#f92672>and</span> experience <span style=color:#66d9ef>with</span> competitive partner programs 		 <span style=color:#ae81ff>10</span> years of partner programs experience at an enterprise software (<span style=color:#f92672>or</span> cloud) company <span style=color:#f92672>and</span> experience <span style=color:#66d9ef>with</span> competitive partner programs 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span> years of partner programs experience at a enterprise software (<span style=color:#f92672>or</span> cloud) company 		 <span style=color:#ae81ff>5</span> years of partner programs experience at an enterprise software (<span style=color:#f92672>or</span> cloud) company 		 Score: <span style=color:#ae81ff>1.0000</span>
</span></span><span style=display:flex><span>technically minded, <span style=color:#66d9ef>with</span> an understanding of the technology <span style=color:#f92672>and</span> cloud computing market, <span style=color:#f92672>and</span> a passion <span style=color:#66d9ef>for</span> google cloud products (g<span style=color:#f92672>-</span>suite, google cloud platform) 		 technically minded, <span style=color:#66d9ef>with</span> a understanding of the technology <span style=color:#f92672>and</span> cloud computing market, <span style=color:#f92672>and</span> a passion <span style=color:#66d9ef>for</span> google cloud products (g<span style=color:#f92672>-</span>suite, google cloud platform) 		 Score: <span style=color:#ae81ff>0.9999</span>
</span></span><span style=display:flex><span>shape google<span style=color:#960050;background-color:#1e0010>‚Äô</span>s approach to partnership strategy <span style=color:#66d9ef>with</span> stakeholders <span style=color:#f92672>in</span> partner programs, product management, engineering, sales, <span style=color:#f92672>and</span> marketing; support regional engagement <span style=color:#66d9ef>with</span> strategic <span style=color:#66d9ef>global</span> <span style=color:#f92672>and</span> regional partners 		 shape google<span style=color:#960050;background-color:#1e0010>‚Äô</span>s approach to partnership strategy <span style=color:#66d9ef>with</span> stakeholders <span style=color:#f92672>in</span> partner programs, product management, engineering, sales <span style=color:#f92672>and</span> marketing; support regional engagement <span style=color:#66d9ef>with</span> strategic <span style=color:#66d9ef>global</span> <span style=color:#f92672>and</span> regional partners 		 Score: <span style=color:#ae81ff>0.9998</span>
</span></span><span style=display:flex><span>a combination of hr experience <span style=color:#f92672>in</span> the following areas: organizational design, succession planning, performance management, diversity <span style=color:#f92672>and</span> inclusion, business consulting, coaching <span style=color:#f92672>and</span> development, talent management, data analysis <span style=color:#f92672>and</span> employee relations 		 a combination of hr experience <span style=color:#f92672>in</span> the following areas: organizational design, succession planning, performance management, diversity <span style=color:#f92672>and</span> inclusion, business consulting, coaching <span style=color:#f92672>and</span> development, talent management, data analysis, <span style=color:#f92672>and</span> employee relations 		 Score: <span style=color:#ae81ff>0.9998</span>
</span></span><span style=display:flex><span>assist clients <span style=color:#f92672>in</span> the adoption of new products via upgrades <span style=color:#f92672>and</span> migrations to develop their long term success <span style=color:#f92672>and</span> improve product offerings by providing client feedback on features to product management <span style=color:#f92672>and</span> engineering 		 assist clients <span style=color:#f92672>in</span> the adoption of new products via upgrades <span style=color:#f92672>and</span> migrations to develop their long<span style=color:#f92672>-</span>term success <span style=color:#f92672>and</span> improve product offerings by providing client feedback on features to product management <span style=color:#f92672>and</span> engineering 		 Score: <span style=color:#ae81ff>0.9995</span>
</span></span><span style=display:flex><span>build strong relationships <span style=color:#f92672>and</span> operating rhythms <span style=color:#66d9ef>with</span> leaders inside <span style=color:#f92672>and</span> outside their core product team to efficiently implement user experiences that are cohesive, inclusive <span style=color:#f92672>and</span> well<span style=color:#f92672>-</span>informed 		 build strong relationships <span style=color:#f92672>and</span> operating rhythms <span style=color:#66d9ef>with</span> leaders inside <span style=color:#f92672>and</span> outside their core product team to efficiently implement user experiences that are cohesive, inclusive, <span style=color:#f92672>and</span> well<span style=color:#f92672>-</span>informed 		 Score: <span style=color:#ae81ff>0.9995</span>
</span></span><span style=display:flex><span>take responsibility <span style=color:#66d9ef>for</span> technical aspects of solutions to include such activities <span style=color:#66d9ef>as</span> supporting bid responses, product <span style=color:#f92672>and</span> solution briefings, proof<span style=color:#f92672>-</span>of<span style=color:#f92672>-</span>concept work <span style=color:#f92672>and</span> the coordination of supporting technical resources 		 take responsibility <span style=color:#66d9ef>for</span> technical aspects of solutions to include such activities <span style=color:#66d9ef>as</span> supporting bid responses, product <span style=color:#f92672>and</span> solution briefings, proof<span style=color:#f92672>-</span>of<span style=color:#f92672>-</span>concept work, <span style=color:#f92672>and</span> the coordination of supporting technical resources 		 Score: <span style=color:#ae81ff>0.9994</span>
</span></span><span style=display:flex><span>experience serving <span style=color:#f92672>in</span> the capacity of a technical sales engineer <span style=color:#f92672>in</span> a cloud computing environment <span style=color:#f92672>or</span> equivalent experience <span style=color:#f92672>in</span> a customer facing role (including working <span style=color:#66d9ef>as</span> a member of a professional services <span style=color:#f92672>or</span> systems engineering team) 		 experience serving <span style=color:#f92672>in</span> the capacity of a technical sales engineer <span style=color:#f92672>in</span> a cloud computing environment <span style=color:#f92672>or</span> equivalent experience <span style=color:#f92672>in</span> a customer<span style=color:#f92672>-</span>facing role (including working <span style=color:#66d9ef>as</span> a member of a professional services <span style=color:#f92672>or</span> systems engineering team) 		 Score: <span style=color:#ae81ff>0.9994</span>
</span></span><span style=display:flex><span>CPU times: user <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>45</span>s, sys: <span style=color:#ae81ff>2.84</span> s, total: <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>48</span>s
</span></span><span style=display:flex><span>Wall time: <span style=color:#ae81ff>1</span>min
</span></span></code></pre></div><p>09:41 ok haha I can see there are some arbitrary internal space differences as well haha
08:49 ok lets find 10 [[positive pair]] , and use that ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pairs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  []
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><h3 id=jul-10th-2023-looked-at-the-vocabulary-misses-and-job-titles>[[Jul 10th, 2023]] looked at the vocabulary misses and job titles<a hidden class=anchor aria-hidden=true href=#jul-10th-2023-looked-at-the-vocabulary-misses-and-job-titles>#</a></h3><p>collapsed:: true</p><h4 id=noticed-that-the-paraphrase-mining-output-is-not-full>noticed that the paraphrase mining output is not full<a hidden class=anchor aria-hidden=true href=#noticed-that-the-paraphrase-mining-output-is-not-full>#</a></h4><p>looks like more or less we get the better matches first</p><h4 id=the-model-im-testing-with-does-have-technical-data-sources>the model I&rsquo;m testing with does have technical data sources<a hidden class=anchor aria-hidden=true href=#the-model-im-testing-with-does-have-technical-data-sources>#</a></h4><p>08:56 haha this is not simple, so many sentences, is there any way of getting around hand labeling?</p><p>Maybe I can look for technical terms which I suspect are not part of the #vocabulary , hmm
So <a href=https://huggingface.co/datasets/code_search_net>https://huggingface.co/datasets/code_search_net</a> and [[stack exchange]] duplicate questions and actually many other technical datasets are used per <a href=https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2>https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a> ,</p><h4 id=hmm-oh-the-autotokenizer-is-a-way-to-get-tokens-and-vocabulary-in-the-model>hmm oh the AutoTokenizer is a way to get tokens and vocabulary in the model<a hidden class=anchor aria-hidden=true href=#hmm-oh-the-autotokenizer-is-a-way-to-get-tokens-and-vocabulary-in-the-model>#</a></h4><p>collapsed:: true
09:14 tokenizer?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModel
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Downloading (<span style=color:#960050;background-color:#1e0010>‚Ä¶</span>)okenizer_config<span style=color:#f92672>.</span>json: <span style=color:#ae81ff>100</span><span style=color:#f92672>%|</span><span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>350</span><span style=color:#f92672>/</span><span style=color:#ae81ff>350</span> [<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span><span style=color:#f92672>&lt;</span><span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>, <span style=color:#ae81ff>36.5</span>kB<span style=color:#f92672>/</span>s]
</span></span><span style=display:flex><span>Downloading (<span style=color:#960050;background-color:#1e0010>‚Ä¶</span>)solve<span style=color:#f92672>/</span>main<span style=color:#f92672>/</span>vocab<span style=color:#f92672>.</span>txt: <span style=color:#ae81ff>100</span><span style=color:#f92672>%|</span><span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>232</span>k<span style=color:#f92672>/</span><span style=color:#ae81ff>232</span>k [<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span><span style=color:#f92672>&lt;</span><span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>, <span style=color:#ae81ff>7.02</span>MB<span style=color:#f92672>/</span>s]
</span></span><span style=display:flex><span>Downloading (<span style=color:#960050;background-color:#1e0010>‚Ä¶</span>)<span style=color:#f92672>/</span>main<span style=color:#f92672>/</span>tokenizer<span style=color:#f92672>.</span>json: <span style=color:#ae81ff>100</span><span style=color:#f92672>%|</span><span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>466</span>k<span style=color:#f92672>/</span><span style=color:#ae81ff>466</span>k [<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span><span style=color:#f92672>&lt;</span><span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>, <span style=color:#ae81ff>8.44</span>MB<span style=color:#f92672>/</span>s]
</span></span><span style=display:flex><span>Downloading (<span style=color:#960050;background-color:#1e0010>‚Ä¶</span>)cial_tokens_map<span style=color:#f92672>.</span>json: <span style=color:#ae81ff>100</span><span style=color:#f92672>%|</span><span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>112</span><span style=color:#f92672>/</span><span style=color:#ae81ff>112</span> [<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span><span style=color:#f92672>&lt;</span><span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>, <span style=color:#ae81ff>28.4</span>kB<span style=color:#f92672>/</span>s]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>129</span>]: tokenizer<span style=color:#f92672>.</span>vocab_files_names
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>129</span>]: {<span style=color:#e6db74>&#39;vocab_file&#39;</span>: <span style=color:#e6db74>&#39;vocab.txt&#39;</span>, <span style=color:#e6db74>&#39;tokenizer_file&#39;</span>: <span style=color:#e6db74>&#39;tokenizer.json&#39;</span>}
</span></span></code></pre></div><p>well that looks good ! Like a nice way perhaps to see the vocabulary,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>131</span>]: vocabulary <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>get_vocab()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>133</span>]: len(vocabulary)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>133</span>]: <span style=color:#ae81ff>30522</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>135</span>]: print(list(vocabulary<span style=color:#f92672>.</span>keys())[:<span style=color:#ae81ff>30</span>])
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;##iq&#39;</span>, <span style=color:#e6db74>&#34;##&#39;&#34;</span>, <span style=color:#e6db74>&#39;1723&#39;</span>, <span style=color:#e6db74>&#39;italians&#39;</span>, <span style=color:#e6db74>&#39;caretaker&#39;</span>, <span style=color:#e6db74>&#39;debbie&#39;</span>, <span style=color:#e6db74>&#39;bloomberg&#39;</span>, <span style=color:#e6db74>&#39;enforcing&#39;</span>, <span style=color:#e6db74>&#39;sex&#39;</span>, <span style=color:#e6db74>&#39;flicking&#39;</span>, <span style=color:#e6db74>&#39;likes&#39;</span>, <span style=color:#e6db74>&#39;glimpse&#39;</span>, <span style=color:#e6db74>&#39;relax&#39;</span>, <span style=color:#e6db74>&#39;coward&#39;</span>, <span style=color:#e6db74>&#39;eyelids&#39;</span>, <span style=color:#e6db74>&#39;worth&#39;</span>, <span style=color:#e6db74>&#39;dynamics&#39;</span>, <span style=color:#e6db74>&#39;##¬π&#39;</span>, <span style=color:#e6db74>&#39;recognizes&#39;</span>, <span style=color:#e6db74>&#39;arcadia&#39;</span>, <span style=color:#e6db74>&#39;deportivo&#39;</span>, <span style=color:#e6db74>&#39;pointedly&#39;</span>, <span style=color:#e6db74>&#39;iowa&#39;</span>, <span style=color:#e6db74>&#39;##rio&#39;</span>, <span style=color:#e6db74>&#39;moved&#39;</span>, <span style=color:#e6db74>&#39;—è&#39;</span>, <span style=color:#e6db74>&#39;news&#39;</span>, <span style=color:#e6db74>&#39;whoever&#39;</span>, <span style=color:#e6db74>&#39;blossom&#39;</span>, <span style=color:#e6db74>&#39;preserved&#39;</span>]
</span></span></code></pre></div><p>09:21 Okay let me look for like if a few vocabulary terms in job descriptions are there,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>job_terms <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;html&#34;</span>, <span style=color:#e6db74>&#34;databricks&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;css&#34;</span>, <span style=color:#e6db74>&#34;api&#34;</span>, <span style=color:#e6db74>&#34;postgresql&#34;</span>, <span style=color:#e6db74>&#34;database&#34;</span>, 
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;mysql&#34;</span>, <span style=color:#e6db74>&#34;clojure&#34;</span>, <span style=color:#e6db74>&#34;java&#34;</span>, <span style=color:#e6db74>&#34;javascript&#34;</span>, <span style=color:#e6db74>&#34;angular&#34;</span>, <span style=color:#e6db74>&#34;idempotent&#34;</span>, <span style=color:#e6db74>&#34;azure&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;github&#34;</span>, <span style=color:#e6db74>&#34;git&#34;</span>, <span style=color:#e6db74>&#34;concurrency&#34;</span>, <span style=color:#e6db74>&#34;asyncio&#34;</span>, <span style=color:#e6db74>&#34;dbutils&#34;</span>, <span style=color:#e6db74>&#34;ipython&#34;</span>, <span style=color:#e6db74>&#34;docker&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;pipeline&#34;</span>, <span style=color:#e6db74>&#34;sklearn&#34;</span>, <span style=color:#e6db74>&#34;tensorflow&#34;</span>, <span style=color:#e6db74>&#34;pytorch&#34;</span>, <span style=color:#e6db74>&#34;numpy&#34;</span>, <span style=color:#e6db74>&#34;pandas&#34;</span>, <span style=color:#e6db74>&#34;ec2&#34;</span>, <span style=color:#e6db74>&#34;ecs&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;aws&#34;</span>, <span style=color:#e6db74>&#34;sagemaker&#34;</span>, <span style=color:#e6db74>&#34;nginx&#34;</span>, <span style=color:#e6db74>&#34;redis&#34;</span>, <span style=color:#e6db74>&#34;cli&#34;</span>, <span style=color:#e6db74>&#34;auc&#34;</span>, <span style=color:#e6db74>&#34;xgboost&#34;</span>, <span style=color:#e6db74>&#34;repository&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span>hits <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>no_hits <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> term <span style=color:#f92672>in</span> job_terms:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tqdm(vocabulary<span style=color:#f92672>.</span>keys()):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> term <span style=color:#f92672>in</span> token:
</span></span><span style=display:flex><span>            hits<span style=color:#f92672>.</span>append([term, token])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>no_hits <span style=color:#f92672>=</span> list(set(job_terms) <span style=color:#f92672>-</span> set([x[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> hits]))
</span></span></code></pre></div><pre tabindex=0><code>In [137]: len(hits)
Out[137]: 117

In [138]: hits
Out[138]: 
[[&#39;api&#39;, &#39;rapidly&#39;],
 [&#39;api&#39;, &#39;shapiro&#39;],
 [&#39;api&#39;, &#39;shaping&#39;],
 [&#39;database&#39;, &#39;database&#39;],
 [&#39;ecs&#39;, &#39;ecstasy&#39;],
 [&#39;git&#39;, &#39;illegitimate&#39;],
 [&#39;angular&#39;, &#39;triangular&#39;],
 [&#39;git&#39;, &#39;digits&#39;],
 [&#39;cli&#39;, &#39;clicks&#39;],
 [&#39;cli&#39;, &#39;inclination&#39;],
 [&#39;api&#39;, &#39;apical&#39;],
 [&#39;java&#39;, &#39;java&#39;],
 [&#39;cli&#39;, &#39;cycling&#39;],
 [&#39;cli&#39;, &#39;clip&#39;],
 [&#39;cli&#39;, &#39;clit&#39;],
 [&#39;api&#39;, &#39;capitals&#39;],
 [&#39;cli&#39;, &#39;clicked&#39;],
 [&#39;cli&#39;, &#39;cliff&#39;],
 [&#39;concurrency&#39;, &#39;concurrency&#39;],
 [&#39;auc&#39;, &#39;caucus&#39;],
 [&#39;java&#39;, &#39;javanese&#39;],
 [&#39;cli&#39;, &#39;clifton&#39;],
 [&#39;cli&#39;, &#39;client&#39;],
 [&#39;git&#39;, &#39;legitimacy&#39;],
 [&#39;api&#39;, &#39;capita&#39;],
 [&#39;cli&#39;, &#39;clicking&#39;],
 [&#39;git&#39;, &#39;digit&#39;],
 [&#39;api&#39;, &#39;capitalist&#39;],
 [&#39;aws&#39;, &#39;flaws&#39;],
 [&#39;cli&#39;, &#39;incline&#39;],
 [&#39;cli&#39;, &#39;climbs&#39;],
 [&#39;cli&#39;, &#39;inclined&#39;],
 [&#39;git&#39;, &#39;digitally&#39;],
 [&#39;git&#39;, &#39;legitimate&#39;],
 [&#39;cli&#39;, &#39;decline&#39;],
 [&#39;cli&#39;, &#39;clinical&#39;],
 [&#39;git&#39;, &#39;longitude&#39;],
 [&#39;cli&#39;, &#39;declining&#39;],
 [&#39;pipeline&#39;, &#39;pipeline&#39;],
 [&#39;cli&#39;, &#39;climax&#39;],
 [&#39;cli&#39;, &#39;clinics&#39;],
 [&#39;api&#39;, &#39;capitol&#39;],
 [&#39;aws&#39;, &#39;laws&#39;],
 [&#39;aws&#39;, &#39;claws&#39;],
 [&#39;api&#39;, &#39;rapid&#39;],
 [&#39;azure&#39;, &#39;azure&#39;],
 [&#39;api&#39;, &#39;dilapidated&#39;],
 [&#39;angular&#39;, &#39;rectangular&#39;],
 [&#39;api&#39;, &#39;api&#39;],
 [&#39;api&#39;, &#39;gaping&#39;],
 [&#39;auc&#39;, &#39;caucasus&#39;],
 [&#39;redis&#39;, &#39;rediscovered&#39;],
 [&#39;cli&#39;, &#39;declines&#39;],
 [&#39;cli&#39;, &#39;eclipse&#39;],
 [&#39;git&#39;, &#39;agitated&#39;],
 [&#39;auc&#39;, &#39;bureaucracy&#39;],
 [&#39;api&#39;, &#39;scraping&#39;],
 [&#39;cli&#39;, &#39;clive&#39;],
 [&#39;database&#39;, &#39;databases&#39;],
 [&#39;api&#39;, &#39;therapist&#39;],
 [&#39;git&#39;, &#39;longitudinal&#39;],
 [&#39;cli&#39;, &#39;cyclist&#39;],
 [&#39;cli&#39;, &#39;climates&#39;],
 [&#39;cli&#39;, &#39;clinging&#39;],
 [&#39;auc&#39;, &#39;caucasian&#39;],
 [&#39;angular&#39;, &#39;angular&#39;],
 [&#39;cli&#39;, &#39;radcliffe&#39;],
 [&#39;cli&#39;, &#39;clinched&#39;],
 [&#39;git&#39;, &#39;agitation&#39;],
 [&#39;api&#39;, &#39;capitalism&#39;],
 [&#39;cli&#39;, &#39;recycling&#39;],
 [&#39;aws&#39;, &#39;lawson&#39;],
 [&#39;git&#39;, &#39;fugitive&#39;],
 [&#39;cli&#39;, &#39;cyclists&#39;],
 [&#39;api&#39;, &#39;capital&#39;],
 [&#39;python&#39;, &#39;python&#39;],
 [&#39;aws&#39;, &#39;paws&#39;],
 [&#39;cli&#39;, &#39;clint&#39;],
 [&#39;cli&#39;, &#39;clifford&#39;],
 [&#39;cli&#39;, &#39;##cliff&#39;],
 [&#39;auc&#39;, &#39;auction&#39;],
 [&#39;cli&#39;, &#39;circling&#39;],
 [&#39;repository&#39;, &#39;repository&#39;],
 [&#39;auc&#39;, &#39;sauce&#39;],
 [&#39;html&#39;, &#39;html&#39;],
 [&#39;cli&#39;, &#39;clips&#39;],
 [&#39;aws&#39;, &#39;outlaws&#39;],
 [&#39;cli&#39;, &#39;##cliffe&#39;],
 [&#39;api&#39;, &#39;escaping&#39;],
 [&#39;aws&#39;, &#39;lawsuit&#39;],
 [&#39;cli&#39;, &#39;clinch&#39;],
 [&#39;api&#39;, &#39;leaping&#39;],
 [&#39;api&#39;, &#39;rapids&#39;],
 [&#39;cli&#39;, &#39;clients&#39;],
 [&#39;auc&#39;, &#39;auckland&#39;],
 [&#39;cli&#39;, &#39;climb&#39;],
 [&#39;cli&#39;, &#39;climate&#39;],
 [&#39;cli&#39;, &#39;cyclic&#39;],
 [&#39;aws&#39;, &#39;dawson&#39;],
 [&#39;cli&#39;, &#39;declined&#39;],
 [&#39;cli&#39;, &#39;click&#39;],
 [&#39;cli&#39;, &#39;climatic&#39;],
 [&#39;cli&#39;, &#39;clinic&#39;],
 [&#39;aws&#39;, &#39;lawsuits&#39;],
 [&#39;cli&#39;, &#39;climbing&#39;],
 [&#39;api&#39;, &#39;napier&#39;],
 [&#39;aws&#39;, &#39;draws&#39;],
 [&#39;api&#39;, &#39;landscaping&#39;],
 [&#39;aws&#39;, &#39;jaws&#39;],
 [&#39;git&#39;, &#39;digital&#39;],
 [&#39;cli&#39;, &#39;clinton&#39;],
 [&#39;redis&#39;, &#39;redistribution&#39;],
 [&#39;git&#39;, &#39;##git&#39;],
 [&#39;cli&#39;, &#39;climbed&#39;],
 [&#39;cli&#39;, &#39;clipped&#39;],
 [&#39;cli&#39;, &#39;cliffs&#39;],
 [&#39;cli&#39;, &#39;euclidean&#39;]]
</code></pre><p>ok this is interesting then</p><pre tabindex=0><code>In [140]: no_hits
Out[140]: 
[&#39;github&#39;,
 &#39;databricks&#39;,
 &#39;mysql&#39;,
 &#39;pytorch&#39;,
 &#39;sklearn&#39;,
 &#39;postgresql&#39;,
 &#39;docker&#39;,
 &#39;nginx&#39;,
 &#39;idempotent&#39;,
 &#39;sagemaker&#39;,
 &#39;xgboost&#39;,
 &#39;css&#39;,
 &#39;clojure&#39;,
 &#39;dbutils&#39;,
 &#39;tensorflow&#39;,
 &#39;asyncio&#39;,
 &#39;pandas&#39;,
 &#39;numpy&#39;,
 &#39;ec2&#39;,
 &#39;ipython&#39;,
 &#39;javascript&#39;]
</code></pre><h4 id=and-job-titles-maybe-i-could-group-along-that-first>and job titles, maybe I could group along that first<a hidden class=anchor aria-hidden=true href=#and-job-titles-maybe-i-could-group-along-that-first>#</a></h4><p>collapsed:: true
09:36 also another thing for trying next is I should also cut up and do paraphrase mining perhaps within the particular job title, (printing just a sample below )</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>144</span>]: print(df[<span style=color:#e6db74>&#34;Title&#34;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#f92672>.</span>tolist()[:<span style=color:#ae81ff>20</span>])
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;Google Cloud Program Manager&#39;</span>, <span style=color:#e6db74>&#39;Supplier Development Engineer (SDE), Cable/Connector&#39;</span>, <span style=color:#e6db74>&#39;Data Analyst, Product and Tools Operations, Google Technical Services&#39;</span>, <span style=color:#e6db74>&#39;Developer Advocate, Partner Engineering&#39;</span>, <span style=color:#e6db74>&#39;Program Manager, Audio Visual (AV) Deployments&#39;</span>, <span style=color:#e6db74>&#39;Associate Account Strategist (Czech/Slovak), Global Customer Experience&#39;</span>, <span style=color:#e6db74>&#39;Supplier Development Engineer, Camera, Consumer Hardware&#39;</span>, <span style=color:#e6db74>&#39;Strategic Technology Partner Manager, Healthcare and Life Sciences&#39;</span>, <span style=color:#e6db74>&#39;Manufacturing Business Manager, Google Hardware&#39;</span>, <span style=color:#e6db74>&#39;Solutions Architect, Healthcare and Life Sciences, Google Cloud&#39;</span>, <span style=color:#e6db74>&#39;Data Analyst, Consumer Hardware&#39;</span>, <span style=color:#e6db74>&#39;Partner Onboarding Manager (Americas)&#39;</span>, <span style=color:#e6db74>&#39;Associate Account Strategist (Ukrainian), GMS Sales&#39;</span>, <span style=color:#e6db74>&#39;Survey Lead, Google Cloud Support&#39;</span>, <span style=color:#e6db74>&#39;Solution Architect, Google Cloud Platform (Apigee)&#39;</span>, <span style=color:#e6db74>&#39;Manufacturing Test Engineer&#39;</span>, <span style=color:#e6db74>&#39;Machine Learning Product Specialist, Google Cloud (EMEA)&#39;</span>, <span style=color:#e6db74>&#39;Software Engineering Manager, Cloud Storage, Site Reliability Engineering&#39;</span>, <span style=color:#e6db74>&#39;Global Supply Chain Manager, Display/Touch, Consumer Hardware&#39;</span>, <span style=color:#e6db74>&#39;Technical Program Manager, ASIC Development&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>145</span>]: len(df[<span style=color:#e6db74>&#34;Title&#34;</span>]<span style=color:#f92672>.</span>unique()<span style=color:#f92672>.</span>tolist())
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>145</span>]: <span style=color:#ae81ff>794</span>
</span></span></code></pre></div><p>this list of titles is pretty extensive and might have duplicates also</p><h4 id=thoughts-for-later>thoughts for later<a hidden class=anchor aria-hidden=true href=#thoughts-for-later>#</a></h4><p>use vocabulary misses maybe to figure out what to fine tune with</p><h3 id=jul-11th-2023-refined-the-nohits-per-the-vocabulary-of-the-model-and-used-it-to-tokenize-to-verify-they-are-unknown>[[Jul 11th, 2023]] refined the nohits per the vocabulary of the model and used it to tokenize to verify they are unknown<a hidden class=anchor aria-hidden=true href=#jul-11th-2023-refined-the-nohits-per-the-vocabulary-of-the-model-and-used-it-to-tokenize-to-verify-they-are-unknown>#</a></h3><p>collapsed:: true</p><h4 id=yea-no-hits>yea no hits<a hidden class=anchor aria-hidden=true href=#yea-no-hits>#</a></h4><p>first let me use more precise way of looking for hits,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>job_terms <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;html&#34;</span>, <span style=color:#e6db74>&#34;databricks&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;css&#34;</span>, <span style=color:#e6db74>&#34;api&#34;</span>, <span style=color:#e6db74>&#34;postgresql&#34;</span>, <span style=color:#e6db74>&#34;database&#34;</span>, 
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;mysql&#34;</span>, <span style=color:#e6db74>&#34;clojure&#34;</span>, <span style=color:#e6db74>&#34;java&#34;</span>, <span style=color:#e6db74>&#34;javascript&#34;</span>, <span style=color:#e6db74>&#34;angular&#34;</span>, <span style=color:#e6db74>&#34;idempotent&#34;</span>, <span style=color:#e6db74>&#34;azure&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;github&#34;</span>, <span style=color:#e6db74>&#34;git&#34;</span>, <span style=color:#e6db74>&#34;concurrency&#34;</span>, <span style=color:#e6db74>&#34;asyncio&#34;</span>, <span style=color:#e6db74>&#34;dbutils&#34;</span>, <span style=color:#e6db74>&#34;ipython&#34;</span>, <span style=color:#e6db74>&#34;docker&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;pipeline&#34;</span>, <span style=color:#e6db74>&#34;sklearn&#34;</span>, <span style=color:#e6db74>&#34;tensorflow&#34;</span>, <span style=color:#e6db74>&#34;pytorch&#34;</span>, <span style=color:#e6db74>&#34;numpy&#34;</span>, <span style=color:#e6db74>&#34;pandas&#34;</span>, <span style=color:#e6db74>&#34;ec2&#34;</span>, <span style=color:#e6db74>&#34;ecs&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;aws&#34;</span>, <span style=color:#e6db74>&#34;sagemaker&#34;</span>, <span style=color:#e6db74>&#34;nginx&#34;</span>, <span style=color:#e6db74>&#34;redis&#34;</span>, <span style=color:#e6db74>&#34;cli&#34;</span>, <span style=color:#e6db74>&#34;auc&#34;</span>, <span style=color:#e6db74>&#34;xgboost&#34;</span>, <span style=color:#e6db74>&#34;repository&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span>hits <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>no_hits <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> term <span style=color:#f92672>in</span> job_terms:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tqdm(vocabulary<span style=color:#f92672>.</span>keys()):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> term <span style=color:#f92672>==</span> token<span style=color:#f92672>.</span>strip(<span style=color:#e6db74>&#34;#&#34;</span>):
</span></span><span style=display:flex><span>            hits<span style=color:#f92672>.</span>append([term, token])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>no_hits <span style=color:#f92672>=</span> list(set(job_terms) <span style=color:#f92672>-</span> set([x[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> hits]))
</span></span></code></pre></div><p>08:58 yea thats a lot of nohits,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>151</span>]: hits
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>151</span>]: 
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;html&#39;</span>, <span style=color:#e6db74>&#39;html&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;python&#39;</span>, <span style=color:#e6db74>&#39;python&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;api&#39;</span>, <span style=color:#e6db74>&#39;api&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;database&#39;</span>, <span style=color:#e6db74>&#39;database&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;java&#39;</span>, <span style=color:#e6db74>&#39;java&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;angular&#39;</span>, <span style=color:#e6db74>&#39;angular&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;azure&#39;</span>, <span style=color:#e6db74>&#39;azure&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;git&#39;</span>, <span style=color:#e6db74>&#39;##git&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;concurrency&#39;</span>, <span style=color:#e6db74>&#39;concurrency&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;pipeline&#39;</span>, <span style=color:#e6db74>&#39;pipeline&#39;</span>],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;repository&#39;</span>, <span style=color:#e6db74>&#39;repository&#39;</span>]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>152</span>]: no_hits
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>152</span>]: 
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;github&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;databricks&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;ecs&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;mysql&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;pytorch&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sklearn&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;auc&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;aws&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;postgresql&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;docker&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;nginx&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;idempotent&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sagemaker&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;cli&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;xgboost&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;css&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;clojure&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;dbutils&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;tensorflow&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;asyncio&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;pandas&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;redis&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;numpy&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;ec2&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;ipython&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;javascript&#39;</span>]
</span></span></code></pre></div><h4 id=and-drafting-looking-for-the-nohits-in-the-dataset>and drafting looking for the nohits in the dataset,<a hidden class=anchor aria-hidden=true href=#and-drafting-looking-for-the-nohits-in-the-dataset>#</a></h4><p>So do I see job descriptions that have those job terms I did not find vocabulary hits for?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/job_skills.csv&#34;</span>)
</span></span><span style=display:flex><span>jobsdf <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Responsibilities&#34;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Minimum Qualifications&#39;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Preferred Qualifications&#39;</span>]
</span></span><span style=display:flex><span>raw_sentences <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>extract_raw_sentences(jobsdf, columns)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sentences_with_oov_tokens <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> sentence <span style=color:#f92672>in</span> raw_sentences:
</span></span><span style=display:flex><span>    words <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>split(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#34;[^a-zA-Z0-9]&#34;</span>, sentence)
</span></span><span style=display:flex><span>    words <span style=color:#f92672>=</span> [x <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> words <span style=color:#66d9ef>if</span> x]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># for token in no_hits:</span>
</span></span><span style=display:flex><span>  
</span></span></code></pre></div><h4 id=realized-how-to-tokenize-an-arbitrary-sentence>realized how to tokenize an arbitrary sentence<a hidden class=anchor aria-hidden=true href=#realized-how-to-tokenize-an-arbitrary-sentence>#</a></h4><p>and therefore I can see this model indeed does not know about that vocabulary !
09:23 let me just try to tokenize a fabricated sentence that has the no hit tokens,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModel
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;familiar with xgboost pandas and tensorflow including docker and other technologies&#34;</span>
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [sentence]
</span></span><span style=display:flex><span>encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>  sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;input_ids&#39;</span>: tensor([[  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>5220</span>,  <span style=color:#ae81ff>2007</span>,  <span style=color:#ae81ff>1060</span>, <span style=color:#ae81ff>18259</span>,  <span style=color:#ae81ff>9541</span>,  <span style=color:#ae81ff>3367</span>, <span style=color:#ae81ff>25462</span>,  <span style=color:#ae81ff>2015</span>,  <span style=color:#ae81ff>1998</span>,
</span></span><span style=display:flex><span>         <span style=color:#ae81ff>23435</span>, <span style=color:#ae81ff>12314</span>,  <span style=color:#ae81ff>2164</span>,  <span style=color:#ae81ff>8946</span>,  <span style=color:#ae81ff>2121</span>,  <span style=color:#ae81ff>1998</span>,  <span style=color:#ae81ff>2060</span>,  <span style=color:#ae81ff>6786</span>,   <span style=color:#ae81ff>102</span>]]), <span style=color:#e6db74>&#39;token_type_ids&#39;</span>: tensor([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]]), <span style=color:#e6db74>&#39;attention_mask&#39;</span>: tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>]])}
</span></span></code></pre></div><p>hmm not really clear since these are numeric encodings, how to get the vocabulary debugging part here.
09:38 ah ok never mind found it in the docs here <a href="https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization">https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization</a>
09:38 ok so here is how to #debugging #tokenizer #tokenization</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;familiar with xgboost pandas and tensorflow including docker and other technologies&#34;</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>tokenize(sentence)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>178</span>]: print(tokenizer<span style=color:#f92672>.</span>tokenize(sentence))
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;familiar&#39;</span>, <span style=color:#e6db74>&#39;with&#39;</span>, <span style=color:#e6db74>&#39;x&#39;</span>, <span style=color:#e6db74>&#39;##gb&#39;</span>, <span style=color:#e6db74>&#39;##oo&#39;</span>, <span style=color:#e6db74>&#39;##st&#39;</span>, <span style=color:#e6db74>&#39;panda&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;tensor&#39;</span>, <span style=color:#e6db74>&#39;##flow&#39;</span>, <span style=color:#e6db74>&#39;including&#39;</span>, <span style=color:#e6db74>&#39;dock&#39;</span>, <span style=color:#e6db74>&#39;##er&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;other&#39;</span>, <span style=color:#e6db74>&#39;technologies&#39;</span>]
</span></span></code></pre></div><p>so yea super interesting ! if a particular word is not recognized in the vocabulary, it just gets split up into stuff that is known or the <code>##</code> is used perhaps to create some kinds of smaller #subword-tokenization .
I was reading the documentation of that tokenizer and found this section,
#+BEGIN_QUOTE</p><pre><code>is_split_into_words (`bool`, *optional*, defaults to `False`):
    Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the
    tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)
    which it will tokenize. This is useful for NER or token classification.
</code></pre><p>#+END_QUOTE
which I think is pretty cool, referring to #[[Named Entity Recognition NER]] , used with this,
09:41 next ok yea thinking would love to inform this model of the entities, vocabulary t hat is missing.</p><h3 id=jul-12th-2023-ok-started-building-up-code-to-capture-a-mini-corpus-of-the-sentences-which-have-words-that-are-not-part-of-the-vocabulary>[[Jul 12th, 2023]] ok started building up code to capture a mini corpus, of the sentences, which have words that are not part of the vocabulary,<a hidden class=anchor aria-hidden=true href=#jul-12th-2023-ok-started-building-up-code-to-capture-a-mini-corpus-of-the-sentences-which-have-words-that-are-not-part-of-the-vocabulary>#</a></h3><p>collapsed:: true
08:17 [[my projects/personal/langchain-interview-me-2023-feb]]</p><p>09:05 ok wow organized earlier notes a bit !
So should I therefore, collect the sentences that have the no hits and at least see what happens if I fine tune with those, if the new #sentence-transformers model has the new vocabulary ?
ok, so to build a corpus, thinking for each sentence in this dataset I am working with right now, if I tokenize it using the AutoTokenizer from <code>'sentence-transformers/all-MiniLM-L6-v2'</code> , and the output does not include my desired tokens or tokens prefixed with <code>##</code> but the sentence does have the words in question visible in plain text, then that sentence is a candidate for the fine tuning set I think !
09:16 Also I just glanced through what this out put, looks like,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModel
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>vocabulary <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>get_vocab()
</span></span><span style=display:flex><span>print(vocabulary<span style=color:#f92672>.</span>keys())
</span></span></code></pre></div><p>And I don&rsquo;t see anything upper case so pretty sure I can stick to lower case !
So first the slower way, and maybe I can find a faster #PyTorch way later,
09:34 ok drafting this on the side still. but high level concept yea,
find sentences that have the one or more of the desired terms in plain text,
that actually might be good enough, as long as I have checked indeed the words are no hits against the model vocabulary
but can also tokenize such sentences and verify that the expected tokens do not exist
09:36 have a high level #question though, per #subword-tokenization how do you contain #[[Named Entity Recognition NER]] concepts if they can end up being broken up? #card
Like even in the example in <a href="https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization">https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenization</a> , somehow &ldquo;transformer&rdquo; is not in</p><pre tabindex=0><code>&#34;bert-base-cased&#34;
</code></pre><p>isn&rsquo;t that kind of silly?</p><p>and [[Jul 13th, 2023]] , got a bunch of the no hit sentences, at least for some definition,
08:41 [[my projects/personal/langchain-interview-me-2023-feb]]</p><p>collapsed:: true
hm ok,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>nohit_list <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>current_nohit_list(<span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>raw_sentences <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>extract_raw_sentences(jobsdf, columns)
</span></span></code></pre></div><p>09:02 ok will filter oov words like this</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>191</span>]: <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> raw_sentences[:<span style=color:#ae81ff>4</span>]:
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     print(<span style=color:#e6db74>&#34;=============&#34;</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     print(x, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, ut<span style=color:#f92672>.</span>sequence_from_sentence(x), <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span><span style=color:#f92672>=============</span>
</span></span><span style=display:flex><span>lead projects <span style=color:#f92672>from</span> start to finish <span style=color:#f92672>and</span> manage all issues that impact design 
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;lead&#39;</span>, <span style=color:#e6db74>&#39;projects&#39;</span>, <span style=color:#e6db74>&#39;from&#39;</span>, <span style=color:#e6db74>&#39;start&#39;</span>, <span style=color:#e6db74>&#39;to&#39;</span>, <span style=color:#e6db74>&#39;finish&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;manage&#39;</span>, <span style=color:#e6db74>&#39;all&#39;</span>, <span style=color:#e6db74>&#39;issues&#39;</span>, <span style=color:#e6db74>&#39;that&#39;</span>, <span style=color:#e6db74>&#39;impact&#39;</span>, <span style=color:#e6db74>&#39;design&#39;</span>] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>=============</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>break</span> the mold, <span style=color:#f92672>and</span> bring creativity <span style=color:#f92672>and</span> innovation <span style=color:#f92672>in</span> strategy <span style=color:#f92672>and</span> tactics 
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;break&#39;</span>, <span style=color:#e6db74>&#39;the&#39;</span>, <span style=color:#e6db74>&#39;mold&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;bring&#39;</span>, <span style=color:#e6db74>&#39;creativity&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;innovation&#39;</span>, <span style=color:#e6db74>&#39;in&#39;</span>, <span style=color:#e6db74>&#39;strategy&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;tactics&#39;</span>] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>=============</span>
</span></span><span style=display:flex><span>become a brand advocate; engage <span style=color:#f92672>and</span> influence internal <span style=color:#f92672>and</span> external relationships; build, customize <span style=color:#f92672>and</span> deliver solutions through forums to achieve outcomes <span style=color:#f92672>in</span> support of the brand advertising annual plan 
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;become&#39;</span>, <span style=color:#e6db74>&#39;a&#39;</span>, <span style=color:#e6db74>&#39;brand&#39;</span>, <span style=color:#e6db74>&#39;advocate&#39;</span>, <span style=color:#e6db74>&#39;engage&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;influence&#39;</span>, <span style=color:#e6db74>&#39;internal&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;external&#39;</span>, <span style=color:#e6db74>&#39;relationships&#39;</span>, <span style=color:#e6db74>&#39;build&#39;</span>, <span style=color:#e6db74>&#39;customize&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;deliver&#39;</span>, <span style=color:#e6db74>&#39;solutions&#39;</span>, <span style=color:#e6db74>&#39;through&#39;</span>, <span style=color:#e6db74>&#39;forums&#39;</span>, <span style=color:#e6db74>&#39;to&#39;</span>, <span style=color:#e6db74>&#39;achieve&#39;</span>, <span style=color:#e6db74>&#39;outcomes&#39;</span>, <span style=color:#e6db74>&#39;in&#39;</span>, <span style=color:#e6db74>&#39;support&#39;</span>, <span style=color:#e6db74>&#39;of&#39;</span>, <span style=color:#e6db74>&#39;the&#39;</span>, <span style=color:#e6db74>&#39;brand&#39;</span>, <span style=color:#e6db74>&#39;advertising&#39;</span>, <span style=color:#e6db74>&#39;annual&#39;</span>, <span style=color:#e6db74>&#39;plan&#39;</span>] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>=============</span>
</span></span><span style=display:flex><span>experience <span style=color:#f92672>in</span> java <span style=color:#f92672>and</span><span style=color:#f92672>/</span><span style=color:#f92672>or</span> python development 
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience&#39;</span>, <span style=color:#e6db74>&#39;in&#39;</span>, <span style=color:#e6db74>&#39;java&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;or&#39;</span>, <span style=color:#e6db74>&#39;python&#39;</span>, <span style=color:#e6db74>&#39;development&#39;</span>] 
</span></span></code></pre></div><p>09:08 only search some of the technical roles maybe, to try to get faster results,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>raw_titles <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>extract_raw_sentences(jobsdf, [<span style=color:#e6db74>&#34;Title&#34;</span>])
</span></span><span style=display:flex><span>title_vocab <span style=color:#f92672>=</span> reduce(<span style=color:#66d9ef>lambda</span> x, y: x <span style=color:#f92672>+</span> y, 
</span></span><span style=display:flex><span>                     [ut<span style=color:#f92672>.</span>sequence_from_sentence(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> raw_titles]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(Counter(title_vocab)<span style=color:#f92672>.</span>most_common(<span style=color:#ae81ff>25</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[(<span style=color:#e6db74>&#39;manager&#39;</span>, <span style=color:#ae81ff>300</span>), (<span style=color:#e6db74>&#39;google&#39;</span>, <span style=color:#ae81ff>237</span>), (<span style=color:#e6db74>&#39;cloud&#39;</span>, <span style=color:#ae81ff>167</span>), (<span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#ae81ff>127</span>), (<span style=color:#e6db74>&#39;sales&#39;</span>, <span style=color:#ae81ff>89</span>), (<span style=color:#e6db74>&#39;marketing&#39;</span>, <span style=color:#ae81ff>87</span>), (<span style=color:#e6db74>&#39;engineer&#39;</span>, <span style=color:#ae81ff>79</span>), (<span style=color:#e6db74>&#39;technical&#39;</span>, <span style=color:#ae81ff>71</span>), (<span style=color:#e6db74>&#39;account&#39;</span>, <span style=color:#ae81ff>64</span>), (<span style=color:#e6db74>&#39;lead&#39;</span>, <span style=color:#ae81ff>64</span>), (<span style=color:#e6db74>&#39;business&#39;</span>, <span style=color:#ae81ff>63</span>), (<span style=color:#e6db74>&#39;partner&#39;</span>, <span style=color:#ae81ff>62</span>), (<span style=color:#e6db74>&#39;solutions&#39;</span>, <span style=color:#ae81ff>61</span>), (<span style=color:#e6db74>&#39;operations&#39;</span>, <span style=color:#ae81ff>59</span>), (<span style=color:#e6db74>&#39;product&#39;</span>, <span style=color:#ae81ff>57</span>), (<span style=color:#e6db74>&#39;specialist&#39;</span>, <span style=color:#ae81ff>57</span>), (<span style=color:#e6db74>&#39;services&#39;</span>, <span style=color:#ae81ff>53</span>), (<span style=color:#e6db74>&#39;english&#39;</span>, <span style=color:#ae81ff>52</span>), (<span style=color:#e6db74>&#39;analyst&#39;</span>, <span style=color:#ae81ff>52</span>), (<span style=color:#e6db74>&#39;hardware&#39;</span>, <span style=color:#ae81ff>51</span>), (<span style=color:#e6db74>&#39;associate&#39;</span>, <span style=color:#ae81ff>48</span>), (<span style=color:#e6db74>&#39;global&#39;</span>, <span style=color:#ae81ff>46</span>), (<span style=color:#e6db74>&#39;program&#39;</span>, <span style=color:#ae81ff>44</span>), (<span style=color:#e6db74>&#39;customer&#39;</span>, <span style=color:#ae81ff>43</span>), (<span style=color:#e6db74>&#39;development&#39;</span>, <span style=color:#ae81ff>42</span>)]
</span></span></code></pre></div><p>ok based off of that, &ldquo;engineer&rdquo; feels like a safe assumption here,
09:17 ok so just the engineer sentences then,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>203</span>]: jobsdf[jobsdf[<span style=color:#e6db74>&#34;Title&#34;</span>]<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>contains(<span style=color:#e6db74>&#34;engineer&#34;</span>)]<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>203</span>]: (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>204</span>]: jobsdf[jobsdf[<span style=color:#e6db74>&#34;Title&#34;</span>]<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>contains(<span style=color:#e6db74>&#34;Engineer&#34;</span>)]<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>204</span>]: (<span style=color:#ae81ff>140</span>, <span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>205</span>]: jobsdf<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>205</span>]: (<span style=color:#ae81ff>1250</span>, <span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Responsibilities&#34;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Minimum Qualifications&#39;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Preferred Qualifications&#39;</span>]
</span></span><span style=display:flex><span>engineer_df <span style=color:#f92672>=</span> jobsdf[jobsdf[<span style=color:#e6db74>&#34;Title&#34;</span>]<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>contains(<span style=color:#e6db74>&#34;Engineer&#34;</span>)]<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>raw_sentences <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>extract_raw_sentences(engineer_df, columns)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nohit_sentences <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>find_nohit_sentences(raw_sentences, nohit_list)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>211</span>]: len(raw_sentences), len(nohit_sentences)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>211</span>]: (<span style=color:#ae81ff>1217</span>, <span style=color:#ae81ff>38</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>212</span>]: nohit_sentences
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>212</span>]: 
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with java for android, objective-c for ios, html, javascript&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;tensorflow&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;familiarity in one or more common web or mobile development language such as java, python, go, php, javascript, etc&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with front-end web technologies (html5, css3, and javascript)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;mysql&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;tensorflow&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;html5, css3, and javascript development experience&#39;</span>, [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;java, c/c++, c#, python, javascript, or go)&#39;</span>, [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with web technologies (object-oriented javascript, html, css), and experience with the latest web standards including html5 and css3&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience programming in one of the following: java, javascript and/or c++&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;4 years of relevant work experience, including web application experience or skills using ajax, html, css or javascript&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;, sql, mysql, mapreduce, hadoop)&#39;</span>, [<span style=color:#e6db74>&#39;mysql&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience working with deployment and orchestration technologies (such as pxe, docker, kubernetes, puppet, chef, salt, ansible, jenkins)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;docker&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;development experience in c, c++ or java and experience designing modular, object-oriented javascript&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;expert html and css skills&#39;</span>, [<span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;, unit, functional, integration, stress testing) for your code, using one or more of the following: c, c++, c#, java, javascript, go, or python&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience in writing software in one or more languages such as java, c++, python, go, javascript&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with one or more general purpose programming languages including but not limited to: c/c++, c#, python, javascript, go, objective-c, swift&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;fluency in one or more of the following: python, javascript, java, php, perl, or c++&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;previous tech internships or relevant work experience programming in c, c++, c#, java, javascript, go or python&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;, object-oriented javascript, html, css)&#39;</span>, [<span style=color:#e6db74>&#39;javascript&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;restful, soap, etc), and javascript&#39;</span>, [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience in backend development and using one or more cloud platform services (aws, azure, gcp)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;aws&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;1 year of experience in software engineering and coding, working with two or more of the following languages: java, c/c++, c#, objective-c, python, javascript, php, ruby and/or go&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;4 years of experience working with front end languages such as html5, css, javascript (angularjs)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with web technologies such as html, css, javascript, and http&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;software development platforms and solutions to include j2ee, java servlets, javascript, python, go, php, asp, cgi, ajax&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;, r, python, matlab, pandas) and database languages (e&#39;</span>, [<span style=color:#e6db74>&#39;pandas&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with modern javascript frameworks (such as backbone, angular, or ember) and css pre-processing frameworks (such as sass or less)&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience in writing code fixes and tools to solve problems in c, c++, c#, java, javascript, go or python (e&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;net, python, shell, perl, javascript)&#39;</span>, [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;programming experience in one or more of the following languages/platforms: android, java, kotlin, ios, javascript&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with one or more general purpose programming languages including but not limited to: java, c/c++, c#, objective c, python, javascript, or go&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience in writing software in one or more languages such as java, python, go, javascript, c++, or similar&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;experience with java for android, and objective-c for ios, html and javascript&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;javascript&#39;</span>]]]
</span></span></code></pre></div><p>09:25 hmm also actually seeing sometimes splitting on a <code>"."</code> is not quite accurate.
okay so next, since this is not looking terribly like a whole lot of sentences, can manually assign the ones that are similar, say, and try a fit.</p><h3 id=jul-15th-2023-finally-tried-the-supervised-fine-tuning-but-didnt-seem-to-add-to-the-vocabulary>[[Jul 15th, 2023]] finally tried the [[supervised fine-tuning]] but didn&rsquo;t seem to add to the vocabulary<a hidden class=anchor aria-hidden=true href=#jul-15th-2023-finally-tried-the-supervised-fine-tuning-but-didnt-seem-to-add-to-the-vocabulary>#</a></h3><p>collapsed:: true</p><h4 id=created-clusters-manually-by-looking-at-my-no-hit-list-from-earlier-of-sentences-containing-words-that-were-not-in-the-vocabulary>created clusters manually, by looking at my no hit list from earlier, of sentences containing words that were not in the vocabulary,<a hidden class=anchor aria-hidden=true href=#created-clusters-manually-by-looking-at-my-no-hit-list-from-earlier-of-sentences-containing-words-that-were-not-in-the-vocabulary>#</a></h4><p>collapsed:: true
20:07 going to just manually create some groups,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># web stuff, front end leaning</span>
</span></span><span style=display:flex><span>group1 <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;programming experience in one or more of the following: java, python, javascript, nodejs, c#, net, ruby&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with java, javascript, html5, and sap technologies like sap hana, sap fiori, netweaver&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;software development platforms and solutions experience (java servlets, javascript, php, asp, cgi, ajax, flash, cookies and xml)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with front-end web technologies (html5, css3, and javascript)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;html5, css3, and javascript development experience&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with web technologies (object-oriented javascript, html, css), and experience with the latest web standards including html5 and css3&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;4 years of relevant work experience, including web application experience or skills using ajax, html, css or javascript&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;expert html and css skills&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;restful, soap, etc), and javascript&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;4 years of experience working with front end languages such as html5, css, javascript (angularjs)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with web technologies such as html, css, javascript, and http&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;software development platforms and solutions to include j2ee, java servlets, javascript, python, go, php, asp, cgi, ajax&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with modern javascript frameworks (such as backbone, angular, or ember) and css pre-processing frameworks (such as sass or less)&#39;</span>,
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># feeling more back end mle ish, </span>
</span></span><span style=display:flex><span>group3 <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;, sql, mysql, mapreduce, hadoop)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience working with deployment and orchestration technologies (such as pxe, docker, kubernetes, puppet, chef, salt, ansible, jenkins)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience in backend development and using one or more cloud platform services (aws, azure, gcp)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;, r, python, matlab, pandas) and database languages &#39;</span>,
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># mobile dev </span>
</span></span><span style=display:flex><span>group2 <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with java for android, objective-c for ios, html, javascript&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;familiarity in one or more common web or mobile development language such as java, python, go, php, javascript, etc&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;java, c/c++, c#, python, javascript, or go)&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience programming in one of the following: java, javascript and/or c++&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;development experience in c, c++ or java and experience designing modular, object-oriented javascript&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;, unit, functional, integration, stress testing) for your code, using one or more of the following: c, c++, c#, java, javascript, go, or python&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience in writing software in one or more languages such as java, c++, python, go, javascript&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with one or more general purpose programming languages including but not limited to: c/c++, c#, python, javascript, go, objective-c, swift&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;fluency in one or more of the following: python, javascript, java, php, perl, or c++&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;1 year of experience in software engineering and coding, working with two or more of the following languages: java, c/c++, c#, objective-c, python, javascript, php, ruby and/or go&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience in writing code fixes and tools to solve problems in c, c++, c#, java, javascript, go or python &#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;programming experience in one or more of the following languages/platforms: android, java, kotlin, ios, javascript&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with one or more general purpose programming languages including but not limited to: java, c/c++, c#, objective c, python, javascript, or go&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience in writing software in one or more languages such as java, python, go, javascript, c++, or similar&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;experience with java for android, and objective-c for ios, html and javascript&#39;</span>,
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><h3 id=then-created-a-dataset-from-that-and-ran-fit-with-the-out-of-the-box-all-minilm-l6-v2-sentence-transformer-model>Then created a dataset from that, and ran fit with the out of the box &lsquo;all-MiniLM-L6-v2&rsquo; sentence transformer model<a hidden class=anchor aria-hidden=true href=#then-created-a-dataset-from-that-and-ran-fit-with-the-out-of-the-box-all-minilm-l6-v2-sentence-transformer-model>#</a></h3><p>collapsed:: true
20:36 since <a href=https://huggingface.co/datasets/embedding-data/sentence-compression/tree/main>https://huggingface.co/datasets/embedding-data/sentence-compression/tree/main</a> is given as the example and since I see those [[json lines]] , but it is with [[git-lfs]] , let me try pull it as appropriate,</p><p>ok file was &ldquo;sentence-compression_compressed.jsonl.gz&rdquo;, internally looks like this</p><pre tabindex=0><code>$ head data/kaggle-google-job-skills/sentence-compression_compressed.jsonl 
{&#34;set&#34;: [&#34;The USHL completed an expansion draft on Monday as 10 players who were on the rosters of USHL teams during the 2009-10 season were selected by the League&#39;s two newest entries, the Muskegon Lumberjacks and Dubuque Fighting Saints.&#34;, &#34;USHL completes expansion draft&#34;]}
{&#34;set&#34;: [&#34;Major League Baseball Commissioner Bud Selig will be speaking at St. Norbert College next month.&#34;, &#34;Bud Selig to speak at St. Norbert College&#34;]}
{&#34;set&#34;: [&#34;It&#39;s fresh cherry time in Michigan and the best time to enjoy this delicious and nutritious fruit.&#34;, &#34;It&#39;s cherry time&#34;]}
{&#34;set&#34;: [&#34;An Evesham man is facing charges in Pennsylvania after he allegedly dragged his girlfriend from the side of his pickup truck on the campus of Kutztown University in the early morning hours of Dec. 5, police said.&#34;, &#34;Evesham man faces charges for Pa.&#34;]}
{&#34;set&#34;: [&#34;NRT LLC, one of the nation&#39;s largest residential real estate brokerage companies, announced several executive appointments within its Coldwell Banker Residential Brokerage operations in Southern California.&#34;, &#34;NRT announces executive appointments at its Coldwell Banker operations in Southern California&#34;]}
{&#34;set&#34;: [&#34;THE JSE kept toying with an all time high by midday today as resources continued to fuel the bourse.&#34;, &#34;JSE keeps toying with all time high&#34;]}
{&#34;set&#34;: [&#34;The government is defending the latest police crime statistics despite a worrying rise in the recorded amount of violent offending.&#34;, &#34;Government defends crime statistics&#34;]}
{&#34;set&#34;: [&#34;The renovated Marappalam bridge, which had been opened for two-wheelers last week, was opened for other vehicles also on Friday.&#34;, &#34;Marappalam bridge opened&#34;]}
{&#34;set&#34;: [&#34;A new survey shows 30 percent of Californians use Twitter, and more and more of us are using our smart phones to go online.&#34;, &#34;Survey: 30 percent of Californians use Twitter&#34;]}
{&#34;set&#34;: [&#34;Brightpoint ,a provider of logistic services to the mobile industry, has started operations in the Turkish market.&#34;, &#34;Brightpoint starts operations on Turkish market&#34;]}
</code></pre><p>20:50 ah ok the literal word &ldquo;set&rdquo; really is in there okay !</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> u
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>make_positive_pairs_from_groups(group1, group2, group3)
</span></span><span style=display:flex><span>path<span style=color:#f92672>.</span>write_text(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>join([json<span style=color:#f92672>.</span>dumps(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> dataset]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> InputExample
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_examples <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, x <span style=color:#f92672>in</span> enumerate(dataset):
</span></span><span style=display:flex><span>    train_examples<span style=color:#f92672>.</span>append(
</span></span><span style=display:flex><span>        InputExample(texts<span style=color:#f92672>=</span>[x[<span style=color:#e6db74>&#34;set&#34;</span>][<span style=color:#ae81ff>0</span>], x[<span style=color:#e6db74>&#34;set&#34;</span>][<span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>train_dataloader <span style=color:#f92672>=</span> DataLoader(train_examples, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># MultipleNegativesRankingLoss </span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> losses
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>train_loss <span style=color:#f92672>=</span> losses<span style=color:#f92672>.</span>MultipleNegativesRankingLoss(model<span style=color:#f92672>=</span>model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(train_objectives<span style=color:#f92672>=</span>[(train_dataloader, train_loss)], epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>) 
</span></span></code></pre></div><p>21:31 ok started that . Actually going pretty fast as I expected since yea my dataset is small for a proof of concept ,</p><pre tabindex=0><code>Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:16&lt;00:00,  1.26s/it]
Epoch:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                             | 1/10 [00:16&lt;02:27, 16.36s/it]
Iteration:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 12/13 [00:19&lt;00:01,  1.64s/it]
...
...
CPU times: user 4min 8s, sys: 38.8 s, total: 4min 47s
Wall time: 2min 43s
</code></pre><h3 id=hmm-but-new-vocabulary-does-not-seem-to-reflect-new-terms-somehow>hmm but new vocabulary does not seem to reflect new terms somehow<a hidden class=anchor aria-hidden=true href=#hmm-but-new-vocabulary-does-not-seem-to-reflect-new-terms-somehow>#</a></h3><p>And yea curious if I can see the vocabulary now as different,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/2023-07-15-fine-tuned-on-pairs.foo&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>save(path)
</span></span></code></pre></div><p>21:42 oh nice, I see the vocab.txt got saved, in that folder,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 2023-07-15-fine-tuned-on-pairs.foo/vocab.txt&#34;</span>
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/2023-07-15-fine-tuned-on-pairs.foo&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;vocab.txt&#34;</span>)
</span></span><span style=display:flex><span>vocab <span style=color:#f92672>=</span> path<span style=color:#f92672>.</span>read_text()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>250</span>]: set(nohit_list) <span style=color:#f92672>&amp;</span> set(vocab)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>250</span>]: set()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>251</span>]: set([<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;##</span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> nohit_list]) <span style=color:#f92672>&amp;</span> set(vocab)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>251</span>]: set()
</span></span></code></pre></div><p>21:48 hmm not seeing any words from the nohit list in the dumped out vocab though. hmm ok back to the drawing board then? haha</p><h3 id=jul-16th-2023-yea-tried-a-different-take-on-adding-tokens-to-a-tokenizer-and-that-seemed-to-do-it>[[Jul 16th, 2023]] yea tried a different take on adding tokens to a tokenizer and that seemed to do it.<a hidden class=anchor aria-hidden=true href=#jul-16th-2023-yea-tried-a-different-take-on-adding-tokens-to-a-tokenizer-and-that-seemed-to-do-it>#</a></h3><p>collapsed:: true</p><h4 id=yea-it-was-not-tokenizerjson>yea it was not &ldquo;tokenizer.json&rdquo;<a hidden class=anchor aria-hidden=true href=#yea-it-was-not-tokenizerjson>#</a></h4><p>11:17 ok so next though,</p><p>11:23 hmm interesting, I also looked at the &ldquo;tokenizer.json&rdquo; file that got created when doing <code>model.save()</code>, next to the &ldquo;vocab.txt&rdquo;. They have the same tokens looks like except &ldquo;tokenizer.json&rdquo; also refers to the input ids [[tokenized-input-ids]] ,
11:30 hmm but maybe fine tuning simply does not update the vocabulary?</p><h4 id=but-add_tokens>but &ldquo;add_tokens&rdquo;<a hidden class=anchor aria-hidden=true href=#but-add_tokens>#</a></h4><p>collapsed:: true
12:01 ok super interesting, reading <a href=https://angelina-yang.medium.com/how-to-add-new-tokens-to-a-transformer-model-vocabulary-da778f99f910>here on medium</a> someone kind of confirming that to expand the vocabulary [[add to transformer vocabulary]], and prevent [[out-of-vocabulary-words-OOV]], you need another approach,</p><p>12:32 lets try their recommendation, just took out the for-loop since yea looking at current documentation for <code>add_tokens</code> function, you can add a list instead. Incorporating w/ a check of what is my hit list and no hit list ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nohit_list <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>current_nohit_list(<span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Before</span>
</span></span><span style=display:flex><span>vocabulary_before <span style=color:#f92672>=</span> list(tokenizer<span style=color:#f92672>.</span>get_vocab()<span style=color:#f92672>.</span>keys())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>add_tokens(nohit_list)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add new embeddings to the embedding matrix of the transformer model</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>resize_token_embeddings(len(tokenizer))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># After </span>
</span></span><span style=display:flex><span>vocabulary_after <span style=color:#f92672>=</span> list(tokenizer<span style=color:#f92672>.</span>get_vocab()<span style=color:#f92672>.</span>keys())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Did it work?</span>
</span></span></code></pre></div><p>14:09 hmm got an error,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>AttributeError</span>: <span style=color:#e6db74>&#39;SentenceTransformer&#39;</span> object has no attribute <span style=color:#e6db74>&#39;resize_token_embeddings&#39;</span>
</span></span></code></pre></div><p>when trying to resize .
Maybe need to go one level down, to the lower layer.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> child <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>children():
</span></span><span style=display:flex><span>    print(child, hasattr(child, <span style=color:#e6db74>&#34;resize_token_embeddings&#34;</span>), <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Transformer({<span style=color:#e6db74>&#39;max_seq_length&#39;</span>: <span style=color:#ae81ff>256</span>, <span style=color:#e6db74>&#39;do_lower_case&#39;</span>: <span style=color:#66d9ef>False</span>}) <span style=color:#66d9ef>with</span> Transformer model: BertModel  <span style=color:#66d9ef>False</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Pooling({<span style=color:#e6db74>&#39;word_embedding_dimension&#39;</span>: <span style=color:#ae81ff>384</span>, <span style=color:#e6db74>&#39;pooling_mode_cls_token&#39;</span>: <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;pooling_mode_mean_tokens&#39;</span>: <span style=color:#66d9ef>True</span>, <span style=color:#e6db74>&#39;pooling_mode_max_tokens&#39;</span>: <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;pooling_mode_mean_sqrt_len_tokens&#39;</span>: <span style=color:#66d9ef>False</span>}) <span style=color:#66d9ef>False</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Normalize() <span style=color:#66d9ef>False</span> 
</span></span></code></pre></div><p>14:13 hmm nope, weird.
But ok looks like this part worked,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>263</span>]: print(set(vocabulary_after) <span style=color:#f92672>-</span> set(vocabulary_before))
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;github&#39;</span>, <span style=color:#e6db74>&#39;databricks&#39;</span>, <span style=color:#e6db74>&#39;nlp&#39;</span>, <span style=color:#e6db74>&#39;ecs&#39;</span>, <span style=color:#e6db74>&#39;pytorch&#39;</span>, <span style=color:#e6db74>&#39;pyspark&#39;</span>, <span style=color:#e6db74>&#39;sklearn&#39;</span>, <span style=color:#e6db74>&#39;auc&#39;</span>, <span style=color:#e6db74>&#39;aws&#39;</span>, <span style=color:#e6db74>&#39;postgresql&#39;</span>, <span style=color:#e6db74>&#39;docker&#39;</span>, <span style=color:#e6db74>&#39;nginx&#39;</span>, <span style=color:#e6db74>&#39;idempotent&#39;</span>, <span style=color:#e6db74>&#39;sagemaker&#39;</span>, <span style=color:#e6db74>&#39;xgboost&#39;</span>, <span style=color:#e6db74>&#39;cli&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>, <span style=color:#e6db74>&#39;clojure&#39;</span>, <span style=color:#e6db74>&#39;spacy&#39;</span>, <span style=color:#e6db74>&#39;ipython&#39;</span>, <span style=color:#e6db74>&#39;dbutils&#39;</span>, <span style=color:#e6db74>&#39;tensorflow&#39;</span>, <span style=color:#e6db74>&#39;asyncio&#39;</span>, <span style=color:#e6db74>&#39;redis&#39;</span>, <span style=color:#e6db74>&#39;pandas&#39;</span>, <span style=color:#e6db74>&#39;numpy&#39;</span>, <span style=color:#e6db74>&#39;ec2&#39;</span>, <span style=color:#e6db74>&#39;mysql&#39;</span>, <span style=color:#e6db74>&#39;javascript&#39;</span>}
</span></span></code></pre></div><p>How about the tokenize command?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;familiar with xgboost pandas and tensorflow including docker and other technologies&#34;</span>
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>tokenize(sentence))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;familiar&#39;</span>, <span style=color:#e6db74>&#39;with&#39;</span>, <span style=color:#e6db74>&#39;xgboost&#39;</span>, <span style=color:#e6db74>&#39;pandas&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;tensorflow&#39;</span>, <span style=color:#e6db74>&#39;including&#39;</span>, <span style=color:#e6db74>&#39;docker&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;other&#39;</span>, <span style=color:#e6db74>&#39;technologies&#39;</span>]
</span></span></code></pre></div><p>14:18 ok nice #moment/satisfaction well that does seem to work.
so next, question is then, I should attempt to do some #[[cosine similarity]] , before and after, to understand did this really help üòÄ</p><h3 id=jul-17th-2023-reading-more-i-learn-you-do-likely-need-to-train-a-new-tokenizer-and-you-cant-just-simply-update-its-vocabulary>[[Jul 17th, 2023]] Reading more, I learn you do likely need to train a new tokenizer and you can&rsquo;t just simply update its vocabulary<a hidden class=anchor aria-hidden=true href=#jul-17th-2023-reading-more-i-learn-you-do-likely-need-to-train-a-new-tokenizer-and-you-cant-just-simply-update-its-vocabulary>#</a></h3><p>collapsed:: true</p><h4 id=quick-side-question-i-had-about-this-last-tokenizer-and-its-case-awareness>Quick side question I had about this last tokenizer and its case awareness,<a hidden class=anchor aria-hidden=true href=#quick-side-question-i-had-about-this-last-tokenizer-and-its-case-awareness>#</a></h4><p>out of curiosity, does tokenize now show this for upper case too now? Should be yes right since this is a uncased model</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;familiar with XGBoost pandas and TensorFlow including Docker and other technologies&#34;</span>
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>tokenize(sentence))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;familiar&#39;</span>, <span style=color:#e6db74>&#39;with&#39;</span>, <span style=color:#e6db74>&#39;xgboost&#39;</span>, <span style=color:#e6db74>&#39;pandas&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;tensorflow&#39;</span>, <span style=color:#e6db74>&#39;including&#39;</span>, <span style=color:#e6db74>&#39;docker&#39;</span>, <span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#e6db74>&#39;other&#39;</span>, <span style=color:#e6db74>&#39;technologies&#39;</span>]
</span></span></code></pre></div><p>08:47 nice . answer is yes.</p><h4 id=yea-hugging-face-docs>yea hugging face docs,<a hidden class=anchor aria-hidden=true href=#yea-hugging-face-docs>#</a></h4><p>collapsed:: true
So I suppose that now okay this is how you add tokens to this tokenizer, but two problems still.</p><p>Well one obvious problem is the tokenizer now needs to be thrown back into the model,
But also, so what if the tokenizer now has this vocabulary, I think now the [[supervised fine-tuning]] step next can help tell this model what is the association of these new tokens in the [[embedding space]] right?
Otherwise, without that, I&rsquo;m curious what would the output vector , embedding, even look like for sentences with those new words? Like a undefined error? or like a equivalent of a zero vector ?
09:03 ok wow so the answer is in their nice course here, <a href="https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt">chapter 6 </a>, on training [[tokenizer]] [[train new tokenizer from an old one]]
So funny enough, the example being used here is [[code understanding]] , [[source code embedding]]
and so this dataset is used, to update the tokenizer of <code>gpt-2</code>,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>raw_datasets <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;code_search_net&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>old_tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;gpt2&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> old_tokenizer<span style=color:#f92672>.</span>train_new_from_iterator(training_corpus, <span style=color:#ae81ff>52000</span>)
</span></span></code></pre></div><p>fastinating side note mentioned here is that there are tokenizers that can be written in python, which are slow and also can be written in #Rust-lang and also #cuda .
hmm so ok then you can save that tokenizer,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>save_pretrained(<span style=color:#e6db74>&#34;code-search-net-tokenizer&#34;</span>)
</span></span></code></pre></div><p>but how about updating the original model then ?
09:21 ok well conceptually, skipping ahead in the [[hugging face]] course there, I see <a href="https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#fine-tuning-the-model">here in chapter 7</a>, that you can use a <code>Trainer</code> from</p><pre tabindex=0><code>from transformers import Trainer
</code></pre><p>in order to fine tune a model and pass a tokenizer as an input,
So per above I suspect that is the answer to my question!</p><h4 id=so-thinking-about-next-steps>So thinking about next steps<a hidden class=anchor aria-hidden=true href=#so-thinking-about-next-steps>#</a></h4><p>Ok so a conceptual update here, I think maybe I need to hunt down some datasets or build a dataset which has additional technical language, and then use that to fine tune a tokenizer, and not just add vocabulary to it with <code>tokenizer.add_tokens</code> haha that was not a full answer. Yea and then I would need to use some of the tips in chapter 6 and 7 of the [[hugging face]] course to fine tune a model but a sentence transformer model say, with the tokenizer that I updated.</p><h3 id=jul-18th-2023-reading-more-about-subword-tokenization-and-purpose-of-tokenizer-tuning>[[Jul 18th, 2023]] reading more about subword tokenization and purpose of tokenizer tuning<a hidden class=anchor aria-hidden=true href=#jul-18th-2023-reading-more-about-subword-tokenization-and-purpose-of-tokenizer-tuning>#</a></h3><p>collapsed:: true
not sure if my use case where technical terms are lacking from a tokenizer warrants this but maybe
09:07 So then I&rsquo;m not sure why the <a href=https://huggingface.co/blog/how-to-train-sentence-transformers>https://huggingface.co/blog/how-to-train-sentence-transformers</a> method I used earlier for fine tuning, earlier <a href=https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/#then-created-a-dataset-from-that-and-ran-fit-with-the-out-of-the-box-all-minilm-l6-v2-sentence-transformer-model>here the other day</a> , did not update the vocabulary. ,</p><p>09:13 Hmm ok maybe the explanation they give up front <a href="https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt">here</a> helps to confirm that fine tuning a transformer model will not update the tokenizer. My impression is that fine tuning a transformer model is really just going to update the weights and since we know that many transformer models use [[subword-tokenization]], although you have new words, the fine tuning weight adjustments are made off of the subword tokens that are less meaningful than if they had concepts mapped out for those new words
Although I am also slightly getting the impression that training a tokenizer is less about vocabulary and more about like #grammar because in their phrasing they refer not to the differences in the vocabulary between #English and #Japanese but to the differences in punctuation .
And they are fine tuning a tokenizer on [[source code]] which definitely has different #grammar .
09:26 yea and additionally <a href="https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt">here</a> they spell out that [[train new tokenizer from an old one]] is really about deciding what are good [[subword-tokenization]] to use, actually what sub-words. hmm
But a tokenizer does indeed have a vocabulary so hmm, is my problem a tokenizer vocabulary problem or is it really I should be looking at this as a [[Named Entity Recognition NER]] problem ?</p><h4 id=but-yea-next-should-try>but yea next should try,<a hidden class=anchor aria-hidden=true href=#but-yea-next-should-try>#</a></h4><p>09:32 anyway I should still just try this,</p><p>so next let me apply the mini corpus I had from last time, to this, and lets see what the new tokenizer does</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> old_tokenizer<span style=color:#f92672>.</span>train_new_from_iterator(training_corpus, <span style=color:#ae81ff>52000</span>)
</span></span></code></pre></div><h3 id=jul-19th-2023-ran-the-tokenizer-fine-tuning-with-a-small-dataset>[[Jul 19th, 2023]] Ran the tokenizer fine tuning with a small dataset<a hidden class=anchor aria-hidden=true href=#jul-19th-2023-ran-the-tokenizer-fine-tuning-with-a-small-dataset>#</a></h3><p>The vocabulary of the output of <code>train_new_from_iterator</code> had only the new data. So &ldquo;mission accomplished&rdquo; haha I got the new vocabulary in there but at the cost of missing the original vocabulary üòÖ</p><h4 id=trying-this-out>Trying this out<a hidden class=anchor aria-hidden=true href=#trying-this-out>#</a></h4><p>collapsed:: true
so let&rsquo;s follow along per <a href="https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt">hugging face nlp chapter 6 </a>,</p><p>09:23 what does the data they are passing in for their use case look like?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># This can take a few minutes to load, so grab a coffee or tea while you wait!</span>
</span></span><span style=display:flex><span>raw_datasets <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;code_search_net&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>training_corpus <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>    raw_datasets[<span style=color:#e6db74>&#34;train&#34;</span>][i : i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1000</span>][<span style=color:#e6db74>&#34;whole_func_string&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, len(raw_datasets[<span style=color:#e6db74>&#34;train&#34;</span>]), <span style=color:#ae81ff>1000</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>ok haha I don&rsquo;t want to download the whole thing because I am using a laptop tethered to my phone [[phone-tether]] #moment/haha . But I can just look at it online, <a href=https://huggingface.co/datasets/code_search_net>https://huggingface.co/datasets/code_search_net</a> ,
per the above, each row is a record and &ldquo;whole_func_string&rdquo; is a string of the function definition.
So their training_corpus data would look like a list of strings basically. Ok let me repurpose my mini [[positive pair]] dataset from earlier for this then,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> u
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> functools <span style=color:#f92672>import</span> reduce
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>path <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/2023-07-15-positive-pairs.jsonl&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;2023-07-15-positive-pairs.jsonl&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>training_corpus <span style=color:#f92672>=</span> list(set(
</span></span><span style=display:flex><span>    reduce(
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>lambda</span> x, y: x <span style=color:#f92672>+</span> y,
</span></span><span style=display:flex><span>        [json<span style=color:#f92672>.</span>loads(x)[<span style=color:#e6db74>&#34;set&#34;</span>] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> Path(path)<span style=color:#f92672>.</span>read_text()<span style=color:#f92672>.</span>splitlines()]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>))
</span></span></code></pre></div><p>09:40 ok so the second argument to</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_new_from_iterator
</span></span></code></pre></div><p>is the vocab_size . And currently it is,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>old_tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>old_tokenizer<span style=color:#f92672>.</span>vocab_size
</span></span><span style=display:flex><span><span style=color:#75715e># Out[275]: 30522</span>
</span></span></code></pre></div><p>ok the 52,000 in that doc is a lot more. Curious what happens ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> old_tokenizer<span style=color:#f92672>.</span>train_new_from_iterator(training_corpus, <span style=color:#ae81ff>52000</span>)
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>] Pre<span style=color:#f92672>-</span>processing sequences                 <span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span> <span style=color:#ae81ff>0</span>        <span style=color:#f92672>/</span>        <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>] Tokenize words                           <span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span> <span style=color:#ae81ff>182</span>      <span style=color:#f92672>/</span>      <span style=color:#ae81ff>182</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>] Count pairs                              <span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span> <span style=color:#ae81ff>182</span>      <span style=color:#f92672>/</span>      <span style=color:#ae81ff>182</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>] Compute merges                           <span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span> <span style=color:#ae81ff>527</span>      <span style=color:#f92672>/</span>      <span style=color:#ae81ff>527</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>278</span>]: tokenizer<span style=color:#f92672>.</span>vocab_size
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>278</span>]: <span style=color:#ae81ff>601</span>
</span></span></code></pre></div><p>09:44 hmm interesting, so perhaps for this to work properly, I would have needed all the original data also, concatenated with the new data? Let me just last thing, look at the vocab and hit or no hit with my terms at least,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>old_vocabulary <span style=color:#f92672>=</span> old_tokenizer<span style=color:#f92672>.</span>get_vocab()
</span></span><span style=display:flex><span>vocabulary <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>get_vocab()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>len(old_vocabulary), len(vocabulary)
</span></span><span style=display:flex><span><span style=color:#75715e># Out[281]: (30522, 601)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>job_terms <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>get_nohit_job_terms()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(job_terms)
</span></span><span style=display:flex><span><span style=color:#75715e># [&#39;html&#39;, &#39;databricks&#39;, &#39;python&#39;, &#39;css&#39;, &#39;api&#39;, &#39;postgresql&#39;, &#39;database&#39;, &#39;mysql&#39;, &#39;clojure&#39;, &#39;java&#39;, &#39;javascript&#39;, &#39;angular&#39;, &#39;idempotent&#39;, &#39;azure&#39;, &#39;github&#39;, &#39;git&#39;, &#39;concurrency&#39;, &#39;asyncio&#39;, &#39;dbutils&#39;, &#39;ipython&#39;, &#39;docker&#39;, &#39;pipeline&#39;, &#39;sklearn&#39;, &#39;tensorflow&#39;, &#39;pytorch&#39;, &#39;numpy&#39;, &#39;pandas&#39;, &#39;ec2&#39;, &#39;ecs&#39;, &#39;aws&#39;, &#39;sagemaker&#39;, &#39;nginx&#39;, &#39;redis&#39;, &#39;cli&#39;, &#39;auc&#39;, &#39;xgboost&#39;, &#39;repository&#39;, &#39;pyspark&#39;, &#39;nlp&#39;, &#39;spacy&#39;]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>285</span>]: set(job_terms) <span style=color:#f92672>&amp;</span> set(vocabulary)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>285</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;angular&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;aws&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;azure&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;css&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;database&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;docker&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;html&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;java&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;javascript&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;mysql&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;pandas&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;python&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;tensorflow&#39;</span>}
</span></span></code></pre></div><p>ok haha well good to know at least this does indeed update the vocabulary for at least the new stuff. But since vocab is super small, makes me think yea this needs to be built using original and new data , from scratch perhaps.</p><h4 id=thoughts>Thoughts<a hidden class=anchor aria-hidden=true href=#thoughts>#</a></h4><p>summary thinkings here</p><p>I think I should go back to the original task of evaluating how good is a model at matching a brag document to job description texts, perhaps also building my own dataset that can be used for this evaluation on a out of the box model, comparing then to something new</p><h3 id=jul-20th-2023-did-bit-of-reading-learning--research-mode>[[Jul 20th, 2023]] did bit of reading learning , research mode<a hidden class=anchor aria-hidden=true href=#jul-20th-2023-did-bit-of-reading-learning--research-mode>#</a></h3><p>Think I am now convinced that yes, having new terminology is a good reason for a new tokenizer, because otherwise a tokenizer that does not have the new words, will do excessive splitting and am embedding model will be less likely to get useful signal from them,</p><h4 id=main-benefit-of-train_new_from_iterator--is-lets-you-quickly-use-the-same-class-as-an-earlier-tokenizer-but-yea-this-is-not-a-fine-tuning-step-like-i-thought-before>main benefit of train_new_from_iterator , is lets you quickly use the same class as an earlier tokenizer, but yea this is not a fine tuning step like I thought before<a hidden class=anchor aria-hidden=true href=#main-benefit-of-train_new_from_iterator--is-lets-you-quickly-use-the-same-class-as-an-earlier-tokenizer-but-yea-this-is-not-a-fine-tuning-step-like-i-thought-before>#</a></h4><p>Looking at notes from yesterday, and that new tokenizer, yea it has some of the new vocabulary,</p><p>Some of it is not sub-worded, remaining intact</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(set(job_terms) <span style=color:#f92672>&amp;</span> set(vocabulary))
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;aws&#39;</span>, <span style=color:#e6db74>&#39;tensorflow&#39;</span>, <span style=color:#e6db74>&#39;angular&#39;</span>, <span style=color:#e6db74>&#39;docker&#39;</span>, <span style=color:#e6db74>&#39;pandas&#39;</span>, <span style=color:#e6db74>&#39;database&#39;</span>, <span style=color:#e6db74>&#39;java&#39;</span>, <span style=color:#e6db74>&#39;mysql&#39;</span>, <span style=color:#e6db74>&#39;azure&#39;</span>, <span style=color:#e6db74>&#39;css&#39;</span>, <span style=color:#e6db74>&#39;python&#39;</span>, <span style=color:#e6db74>&#39;html&#39;</span>, <span style=color:#e6db74>&#39;javascript&#39;</span>}
</span></span></code></pre></div><p>And yea some of it underwent [[subword-tokenization]]</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(set(job_terms) <span style=color:#f92672>-</span> set(vocabulary))
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;databricks&#39;</span>, <span style=color:#e6db74>&#39;api&#39;</span>, <span style=color:#e6db74>&#39;nlp&#39;</span>, <span style=color:#e6db74>&#39;ecs&#39;</span>, <span style=color:#e6db74>&#39;concurrency&#39;</span>, <span style=color:#e6db74>&#39;pyspark&#39;</span>, <span style=color:#e6db74>&#39;pytorch&#39;</span>, <span style=color:#e6db74>&#39;sklearn&#39;</span>, <span style=color:#e6db74>&#39;auc&#39;</span>, <span style=color:#e6db74>&#39;pipeline&#39;</span>, <span style=color:#e6db74>&#39;postgresql&#39;</span>, <span style=color:#e6db74>&#39;nginx&#39;</span>, <span style=color:#e6db74>&#39;idempotent&#39;</span>, <span style=color:#e6db74>&#39;sagemaker&#39;</span>, <span style=color:#e6db74>&#39;cli&#39;</span>, <span style=color:#e6db74>&#39;xgboost&#39;</span>, <span style=color:#e6db74>&#39;git&#39;</span>, <span style=color:#e6db74>&#39;repository&#39;</span>, <span style=color:#e6db74>&#39;clojure&#39;</span>, <span style=color:#e6db74>&#39;spacy&#39;</span>, <span style=color:#e6db74>&#39;dbutils&#39;</span>, <span style=color:#e6db74>&#39;asyncio&#39;</span>, <span style=color:#e6db74>&#39;redis&#39;</span>, <span style=color:#e6db74>&#39;numpy&#39;</span>, <span style=color:#e6db74>&#39;ec2&#39;</span>, <span style=color:#e6db74>&#39;ipython&#39;</span>, <span style=color:#e6db74>&#39;github&#39;</span>}
</span></span></code></pre></div><p>09:11 So this sentence become,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>tokenize(
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;Within databricks, you can use pyspark or scala, but to use tensorflow or pytorch in databricks, you need to stick to pyspark.&#34;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;with&#39;</span>, <span style=color:#e6db74>&#39;##i&#39;</span>, <span style=color:#e6db74>&#39;##n&#39;</span>, <span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;##b&#39;</span>, <span style=color:#e6db74>&#39;##ri&#39;</span>, <span style=color:#e6db74>&#39;##ck&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;,&#39;</span>, <span style=color:#e6db74>&#39;yo&#39;</span>, <span style=color:#e6db74>&#39;##u&#39;</span>, <span style=color:#e6db74>&#39;c&#39;</span>, <span style=color:#e6db74>&#39;##a&#39;</span>, <span style=color:#e6db74>&#39;##n&#39;</span>, <span style=color:#e6db74>&#39;us&#39;</span>, <span style=color:#e6db74>&#39;##e&#39;</span>, <span style=color:#e6db74>&#39;py&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;##p&#39;</span>, <span style=color:#e6db74>&#39;##ark&#39;</span>, <span style=color:#e6db74>&#39;or&#39;</span>, <span style=color:#e6db74>&#39;sca&#39;</span>, <span style=color:#e6db74>&#39;##la&#39;</span>, <span style=color:#e6db74>&#39;,&#39;</span>, <span style=color:#e6db74>&#39;but&#39;</span>, <span style=color:#e6db74>&#39;to&#39;</span>, <span style=color:#e6db74>&#39;us&#39;</span>, <span style=color:#e6db74>&#39;##e&#39;</span>, <span style=color:#e6db74>&#39;tensorflow&#39;</span>, <span style=color:#e6db74>&#39;or&#39;</span>, <span style=color:#e6db74>&#39;py&#39;</span>, <span style=color:#e6db74>&#39;##t&#39;</span>, <span style=color:#e6db74>&#39;##or&#39;</span>, <span style=color:#e6db74>&#39;##ch&#39;</span>, <span style=color:#e6db74>&#39;in&#39;</span>, <span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;##b&#39;</span>, <span style=color:#e6db74>&#39;##ri&#39;</span>, <span style=color:#e6db74>&#39;##ck&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;,&#39;</span>, <span style=color:#e6db74>&#39;yo&#39;</span>, <span style=color:#e6db74>&#39;##u&#39;</span>, <span style=color:#e6db74>&#39;ne&#39;</span>, <span style=color:#e6db74>&#39;##e&#39;</span>, <span style=color:#e6db74>&#39;##d&#39;</span>, <span style=color:#e6db74>&#39;to&#39;</span>, <span style=color:#e6db74>&#39;st&#39;</span>, <span style=color:#e6db74>&#39;##ic&#39;</span>, <span style=color:#e6db74>&#39;##k&#39;</span>, <span style=color:#e6db74>&#39;to&#39;</span>, <span style=color:#e6db74>&#39;py&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;##p&#39;</span>, <span style=color:#e6db74>&#39;##ark&#39;</span>, <span style=color:#e6db74>&#39;[UNK]&#39;</span>]
</span></span></code></pre></div><p>Ok so I suppose the advantage of the form,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> old_tokenizer<span style=color:#f92672>.</span>train_new_from_iterator(training_corpus, <span style=color:#ae81ff>52000</span>)
</span></span></code></pre></div><p>is you are using the specific class of that earlier tokenizer,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(old_tokenizer<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__, tokenizer<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__)
</span></span><span style=display:flex><span>BertTokenizerFast BertTokenizerFast
</span></span></code></pre></div><p>and training a tokenizer from scratch, I can find,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tokenizers <span style=color:#f92672>import</span> BertWordPieceTokenizer
</span></span></code></pre></div><p>but I don&rsquo;t see <code>BertTokenizerFast</code> in that same namespace</p><h4 id=and-learned-more-precisely-why--why-a-custom-tokenizer--and-also-actually-how-a-typical-byte-pair-encoding-tokenizer-algorithm-relies-on-sub-word-frequencies-when-building-a-vocabulary>and learned more precisely why , [[why a custom tokenizer]] , and also actually how a typical [[byte-pair encoding]] [[tokenizer]] algorithm relies on sub word frequencies when building a vocabulary<a hidden class=anchor aria-hidden=true href=#and-learned-more-precisely-why--why-a-custom-tokenizer--and-also-actually-how-a-typical-byte-pair-encoding-tokenizer-algorithm-relies-on-sub-word-frequencies-when-building-a-vocabulary>#</a></h4><p>09:56 ok so then I am missing the primary reason maybe also of the custom tokenizer? [[why a custom tokenizer]]</p><p>Person <a href=https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c>with this medium post</a>, is also under the impression that a custom tokenizer [[tokenizer]] is useful as a way of focusing on your unique vocabulary,</p><blockquote><p>our domain is very specific, words and concepts about clothes, shapes,
colors, ‚Ä¶ Therefore, we are interested in defining our own tokenizer
created from our specific vocabulary, avoiding including more common
words from other domains or use cases that are irrelevant for our final
purpose.
10:13 ok reading, <a href=https://huggingface.co/docs/transformers/tokenizer_summary>here</a> for some more detail,
So the focus around tokenizing</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;Don&#39;t you love ü§ó Transformers? We sure do.&#34;</span>
</span></span></code></pre></div><p>w.r.t. <code>["Don't", "you",]</code> versus <code>["Don", "'", "t"]</code> versus <code>["Do", "n't"]</code>
does hint that yes you do want to help extract units of meaning,
And they say that yes you ideally want a smaller vocabulary size to help constrain computation, but using say [[character tokenization]] although ends up w/ a small vocabulary, will be less expressive and capturing the meaning of words will be more difficult.
Feels like in the case of jargon words, like #acronym or just brand new words like &ldquo;Tensorflow&rdquo; or &ldquo;pytorch&rdquo; , I can see sometimes subword tokenization will be helpful, since say it would be great if the word &ldquo;tensor&rdquo; was tokenized as meaningfully related to &ldquo;tensorflow&rdquo;, similarly between &ldquo;pytorch&rdquo; and &ldquo;python&rdquo;. And I get that if you are using a subword tokenizer with a good bit of language as a input, then you should have enough to prevent the [[out-of-vocabulary-words-OOV]] misses ,
10:51 Also they describe the example of <code>BertTokenizer</code> tokenizing an acronym it has not seen,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>)
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>tokenize(<span style=color:#e6db74>&#34;I have a new GPU!&#34;</span>)
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#34;i&#34;</span>, <span style=color:#e6db74>&#34;have&#34;</span>, <span style=color:#e6db74>&#34;a&#34;</span>, <span style=color:#e6db74>&#34;new&#34;</span>, <span style=color:#e6db74>&#34;gp&#34;</span>, <span style=color:#e6db74>&#34;##u&#34;</span>, <span style=color:#e6db74>&#34;!&#34;</span>]
</span></span></code></pre></div><p>And they only mention the benefit, of not having the vocabulary miss, but no mention of the meaning of the concept of #GPU getting missed .
Oh and they explain that the #double-hashtag allows tokenization to be reversible since now you know to re-attach the subwords. But not every tokenizer is reversible I think. Or at least not every tokenizer has the same syntax, since they point out , the use of the underscore <code>_</code> instead, in <code>XLNetTokenizer</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> XLNetTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> XLNetTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;xlnet-base-cased&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>tokenize(<span style=color:#e6db74>&#34;Don&#39;t you love ü§ó Transformers? We sure do.&#34;</span>)
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#34;‚ñÅDon&#34;</span>, <span style=color:#e6db74>&#34;&#39;&#34;</span>, <span style=color:#e6db74>&#34;t&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅyou&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅlove&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅ&#34;</span>, <span style=color:#e6db74>&#34;ü§ó&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅ&#34;</span>, <span style=color:#e6db74>&#34;Transform&#34;</span>, <span style=color:#e6db74>&#34;ers&#34;</span>, <span style=color:#e6db74>&#34;?&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅWe&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅsure&#34;</span>, <span style=color:#e6db74>&#34;‚ñÅdo&#34;</span>, <span style=color:#e6db74>&#34;.&#34;</span>]
</span></span></code></pre></div><p>11:10 going back to <a href=https://huggingface.co/learn/nlp-course/chapter6/2>chapter 6 here</a>, their statement helps with [[why a custom tokenizer]] ,
They highlight extreme reasons like your language is different than the original languages used in a model, or that your corpus is &ldquo;very different&rdquo; üòÄ.
So I am leaning more that this is about the statistics of your text data as it relates to allowing language models to extract meaning , but without being a performance burden.
12:36 continue reading there, see if I missed something,
collapsed:: true
Ah interesting so per <a href=https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe>here</a>, the [[vocabulary size]] is a hyper parameter, that is like a ceiling for splitting words, so if when just say space-splitting, we have too many words, then we split until the vocabulary size is under the input there.
collapsed:: true
But I know per my earlier attempt, yesterday, where I provided ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> old_tokenizer<span style=color:#f92672>.</span>train_new_from_iterator(training_corpus, <span style=color:#ae81ff>52000</span>)
</span></span></code></pre></div><p>I ended up with
<code>python In [278]: tokenizer.vocab_size Out[278]: 601</code>
and still I had quite a lot of [[subword-tokenization]] going on, so there are other details looks like.
So at least the [[byte-pair encoding]] #algorithm-type , will start with a base vocabulary which consists of just the characters.
collapsed:: true
12:51 ok actually glad I kept reading this is interesting, so this algo, will iteratively, find the most frequent symbol pair, adding the merger to its vocabulary, then finding the next most frequent symbol pair after that.
So symbols start out as just letters, but after one iteration, a symbol would consist of two characters together. And larger clumps can form after that.
So their example, started out with space split words with frequencies,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(<span style=color:#e6db74>&#34;hug&#34;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#34;pug&#34;</span>, <span style=color:#ae81ff>5</span>), (<span style=color:#e6db74>&#34;pun&#34;</span>, <span style=color:#ae81ff>12</span>), (<span style=color:#e6db74>&#34;bun&#34;</span>, <span style=color:#ae81ff>4</span>), (<span style=color:#e6db74>&#34;hugs&#34;</span>, <span style=color:#ae81ff>5</span>)
</span></span></code></pre></div><p>and starting with a base vocabulary of</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[<span style=color:#e6db74>&#34;b&#34;</span>, <span style=color:#e6db74>&#34;g&#34;</span>, <span style=color:#e6db74>&#34;h&#34;</span>, <span style=color:#e6db74>&#34;n&#34;</span>, <span style=color:#e6db74>&#34;p&#34;</span>, <span style=color:#e6db74>&#34;s&#34;</span>, <span style=color:#e6db74>&#34;u&#34;</span>]
</span></span></code></pre></div><p>and after performing three merges, having a vocabulary of</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[<span style=color:#e6db74>&#34;b&#34;</span>, <span style=color:#e6db74>&#34;g&#34;</span>, <span style=color:#e6db74>&#34;h&#34;</span>, <span style=color:#e6db74>&#34;n&#34;</span>, <span style=color:#e6db74>&#34;p&#34;</span>, <span style=color:#e6db74>&#34;s&#34;</span>, <span style=color:#e6db74>&#34;u&#34;</span>, <span style=color:#e6db74>&#34;ug&#34;</span>, <span style=color:#e6db74>&#34;un&#34;</span>, <span style=color:#e6db74>&#34;hug&#34;</span>]
</span></span></code></pre></div><p>and at that point they showed that the original set of words and their frequencies would be represented as</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(<span style=color:#e6db74>&#34;hug&#34;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#34;p&#34;</span>, <span style=color:#e6db74>&#34;ug&#34;</span>, <span style=color:#ae81ff>5</span>), (<span style=color:#e6db74>&#34;p&#34;</span>, <span style=color:#e6db74>&#34;un&#34;</span>, <span style=color:#ae81ff>12</span>), (<span style=color:#e6db74>&#34;b&#34;</span>, <span style=color:#e6db74>&#34;un&#34;</span>, <span style=color:#ae81ff>4</span>), 
</span></span><span style=display:flex><span>(<span style=color:#e6db74>&#34;hug&#34;</span>, <span style=color:#e6db74>&#34;s&#34;</span>, <span style=color:#ae81ff>5</span>)  
</span></span></code></pre></div><p>and that a new test word like <code>"mug"</code> say would end up getting represented as <code>["&lt;unk>", "ug"]</code>, since for this case <code>"m"</code> just was not part of the initial vocabulary.
13:10 and ultimately the size of the vocabulary for a tokenizer, [[vocabulary size]] will be the size of the base vocabulary plus the number of merges before it was decided to stop .
And clearly, haha we can have some absurd algorithm implementation that never stops and we end up with a vocabulary that includes all the full words that were encountered, and then therefore we would get fewer computational benefits .
13:21 ok so [[why a custom tokenizer]], in the <a href=https://youtu.be/DJimQynXZsQ>video link</a> #video-type, embedded in <a href="https://huggingface.co/learn/nlp-course/chapter6/2?fw=pt">chapter 6 link</a>, the lean is now to yes train a tokenizer from scratch if there is new jargon yes as in a new &ldquo;domain&rdquo; , #card
So high level four good reasons, for [[why a custom tokenizer]] [[derive-from-scratch]] #card
<img loading=lazy src=../assets/image_1689873985593_0.png alt=image.png>
new language,
new characters ( with accents)
new domain (medical, technical, legal)
new style (haha like [[Old English]] or [[Old French]] )
#take-away ohhh and #moment/aha a really good example is explained that a tokenizer unfamiliar with a #corpus will excessively split and that is not good because #[[input sequence]] is limited [[context-window]] [[maximum-context-size]] !
And so you will risk not capturing the full sentence you want to pass to a #LLM . Nice.
Excessive tokenizer splitting, can impact model performance, too, #question #card , why though?
Maybe the argument is similar to like &ldquo;&rdquo; those unknowns, in that there is less information being captured. My intuitive reasoning is that tiny subwords embedding representations will be likely meaningless . The #attention will get thrown off by basically letter chunks that will end up being as common as the word &ldquo;the&rdquo; , so perhaps you will have just #stop-words at that point with low information.
Example of this particular model tokenizer missing a lot of #unicode characters from [[Bangla]] #language-type .
<img loading=lazy src=../assets/image_1689874273026_0.png alt=image.png>
And yea [[out-of-vocabulary-words-OOV]] &ldquo;&rdquo; , has no useful information for the model to use there.
And [[excessive splitting by tokenizer]] , for say #[[biomedical Glossary]]
<img loading=lazy src=../assets/image_1689875286166_0.png alt=image.png>
And for the other example given, of using <code>code-search-net</code> python dataset to train a tokenizer, I like the question that gets asked is [[performance-lift]] at least eye-balling. And in below example, she does show it is desirable to capture a concept as one token, but I think this will ultimately only happen if that example is more frequent , relative to other patterns when doing merges per [[byte-pair encoding]]
<img loading=lazy src=../assets/image_1689875776287_0.png alt=image.png>
<img loading=lazy src=../assets/image_1689875808385_0.png alt=image.png></p><h3 id=jul-21st-2023-mainly-just-brief-thoughts-about-building-a-new-dataset>[[Jul 21st, 2023]] mainly just brief thoughts about building a new dataset<a hidden class=anchor aria-hidden=true href=#jul-21st-2023-mainly-just-brief-thoughts-about-building-a-new-dataset>#</a></h3><p>Thinking next I should look through various datasets out there , and choose which would have good english language breadth but also depth into the technical jargon world.</p><p>So for sure a dataset from job descriptions from that nice #Kaggle data. , <a href="https://www.kaggle.com/datasets/niyamatalmass/google-job-skills?resource=download">https://www.kaggle.com/datasets/niyamatalmass/google-job-skills?resource=download</a>
maybe there are others like that , but for sure I think this is something that would need to be updated at least once or twice a year because technical terms change frequently
09:29 side note this very much feels like a [[data-drift]] problem but feels like [[nlp-drift]]</p><h3 id=jul-22nd-2023-one---brief-look-model-card-for-sentence-transformersall-minilm-l6-v2>[[Jul 22nd, 2023]] one brief look model card for, <code>'sentence-transformers/all-MiniLM-L6-v2'</code><a hidden class=anchor aria-hidden=true href=#jul-22nd-2023-one---brief-look-model-card-for-sentence-transformersall-minilm-l6-v2>#</a></h3><p>Think I am seeing that this model did not update the tokenizer of the pretrained model that it fine tuned.
So what are the dataset sources in <code>'sentence-transformers/all-MiniLM-L6-v2'</code> which has been my goto model recently.</p><p>So per <a href=https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2>https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a> here, haha there are a billion sentence tuples in there haha. Not sure how easy it will be to perform the same fine tuning steps, but maybe the tokenizer step requires way less data.
13:31 So next want to answer yea what data was used for the tokenizer there.
Ok so looking at <a href=https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/blob/main/train_script.py>https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/blob/main/train_script.py</a> the <code>train_script.py</code> in there, it doesn&rsquo;t look like they modified the tokenizer that was from the pre-trained model, given as an input at the bottom,</p><pre tabindex=0><code>#python train_many_data_files_v2.py --steps 1000000 --batch_size 128 \
    --model nreimers/MiniLM-L6-H384-uncased train_data_configs/all_datasets_v4.json output/all_datasets_v4_MiniLM-L6-H384-uncased-batch128
</code></pre><p>ok so then perhaps I should look up the card for that then.</p><h3 id=jul-23rd-2023-more-practical-dive-today>[[Jul 23rd, 2023]] More practical dive today<a hidden class=anchor aria-hidden=true href=#jul-23rd-2023-more-practical-dive-today>#</a></h3><p>Refreshed my story blurb texts after several months. Pulled another kaggle job description dataset for analysis. And another run of cosine similarity for the corpori corpi (what is plural of corpus haha )</p><h4 id=wrapping-up-thoughts-on-tokenizers-from-other-day>wrapping up thoughts on tokenizers from other day<a hidden class=anchor aria-hidden=true href=#wrapping-up-thoughts-on-tokenizers-from-other-day>#</a></h4><p>09:53 ok two thoughts, so</p><p>collapsed:: true
I can look at <code>nreimers/MiniLM-L6-H384-uncased</code> and if a tokenizer is described there,
And I can also look at that <a href="https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#fine-tuning-the-model">chapter 7 fine tuning link</a> that takes a tokenizer passed in .
10:07 I also do want to think more high level, about the overall goal , task and reevaluate approaches.
But ok, quick look, of possible, at <a href=https://huggingface.co/nreimers/MiniLM-L6-H384-uncased>https://huggingface.co/nreimers/MiniLM-L6-H384-uncased</a> ,
10:13 only points to <a href=https://huggingface.co/microsoft/MiniLM-L12-H384-uncased>https://huggingface.co/microsoft/MiniLM-L12-H384-uncased</a> but not much else to go on
10:18 and that points to this paper, <a href=https://arxiv.org/abs/2002.10957>https://arxiv.org/abs/2002.10957</a> , hmm [[LLM distillation]] with a [[LLM distillation/teacher model and teacher assistant]]
10:28 well looking at their <a href=https://github.com/microsoft/unilm/blob/master/minilm/examples/run_xnli.py>train code in github</a> , it looks like for each teacher assistant model (where the MiniLM on hugging face is one of them) , they appear to use these tokenizers directly,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> (
</span></span><span style=display:flex><span>    BertTokenizer,
</span></span><span style=display:flex><span>    DistilBertTokenizer,
</span></span><span style=display:flex><span>    XLMTokenizer,
</span></span><span style=display:flex><span>    XLMRobertaTokenizer,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>without modification.</p><h4 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h4><p>10:31 ok hmm think I should define dataset and problem bit more now</p><p>collapsed:: true
ok going back to a core early example, from <a href=https://michal.piekarczyk.xyz/post/2023-06-18-my-projects-langchain-interview-me-2023-feb/#on-may-28th-2023-i-started-defining-the-job-description-comparison-concept-and-i-ran-a-comparison-of-my-blurb-2023-02-19t011846-the-story-blurbtxt-against-2023-05-28-enigma-mletxt--the-results-were-maybe-somewhat-not-easy-to-read-perhaps-a-lot-of-text-maybe-i-need-shorter-sentences>here</a> , I ran [[cosine similarity]] per [[sentence-transformers]] , let me try it again , ( [[May 28th, 2023]] )
This time with the new dataset I have and maybe I will add another one too.
So look at that dataset job titles again,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers.util <span style=color:#f92672>import</span> semantic_search
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> functools <span style=color:#f92672>import</span> reduce
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> Counter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/job_skills.csv&#34;</span>)
</span></span><span style=display:flex><span>jobsdf <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Responsibilities&#34;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Minimum Qualifications&#39;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Preferred Qualifications&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>raw_titles <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>extract_raw_sentences(jobsdf, [<span style=color:#e6db74>&#34;Title&#34;</span>])
</span></span><span style=display:flex><span>title_vocab <span style=color:#f92672>=</span> reduce(<span style=color:#66d9ef>lambda</span> x, y: x <span style=color:#f92672>+</span> y, 
</span></span><span style=display:flex><span>                     [ut<span style=color:#f92672>.</span>sequence_from_sentence(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> raw_titles]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(Counter(title_vocab)<span style=color:#f92672>.</span>most_common(<span style=color:#ae81ff>25</span>))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;total&#34;</span>, len(set(title_vocab)))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[(<span style=color:#e6db74>&#39;manager&#39;</span>, <span style=color:#ae81ff>300</span>), (<span style=color:#e6db74>&#39;google&#39;</span>, <span style=color:#ae81ff>237</span>), (<span style=color:#e6db74>&#39;cloud&#39;</span>, <span style=color:#ae81ff>167</span>), (<span style=color:#e6db74>&#39;and&#39;</span>, <span style=color:#ae81ff>127</span>), (<span style=color:#e6db74>&#39;sales&#39;</span>, <span style=color:#ae81ff>89</span>), (<span style=color:#e6db74>&#39;marketing&#39;</span>, <span style=color:#ae81ff>87</span>), (<span style=color:#e6db74>&#39;engineer&#39;</span>, <span style=color:#ae81ff>79</span>), (<span style=color:#e6db74>&#39;technical&#39;</span>, <span style=color:#ae81ff>71</span>), (<span style=color:#e6db74>&#39;account&#39;</span>, <span style=color:#ae81ff>64</span>), (<span style=color:#e6db74>&#39;lead&#39;</span>, <span style=color:#ae81ff>64</span>), (<span style=color:#e6db74>&#39;business&#39;</span>, <span style=color:#ae81ff>63</span>), (<span style=color:#e6db74>&#39;partner&#39;</span>, <span style=color:#ae81ff>62</span>), (<span style=color:#e6db74>&#39;solutions&#39;</span>, <span style=color:#ae81ff>61</span>), (<span style=color:#e6db74>&#39;operations&#39;</span>, <span style=color:#ae81ff>59</span>), (<span style=color:#e6db74>&#39;product&#39;</span>, <span style=color:#ae81ff>57</span>), (<span style=color:#e6db74>&#39;specialist&#39;</span>, <span style=color:#ae81ff>57</span>), (<span style=color:#e6db74>&#39;services&#39;</span>, <span style=color:#ae81ff>53</span>), (<span style=color:#e6db74>&#39;english&#39;</span>, <span style=color:#ae81ff>52</span>), (<span style=color:#e6db74>&#39;analyst&#39;</span>, <span style=color:#ae81ff>52</span>), (<span style=color:#e6db74>&#39;hardware&#39;</span>, <span style=color:#ae81ff>51</span>), (<span style=color:#e6db74>&#39;associate&#39;</span>, <span style=color:#ae81ff>48</span>), (<span style=color:#e6db74>&#39;global&#39;</span>, <span style=color:#ae81ff>46</span>), (<span style=color:#e6db74>&#39;program&#39;</span>, <span style=color:#ae81ff>44</span>), (<span style=color:#e6db74>&#39;customer&#39;</span>, <span style=color:#ae81ff>43</span>), (<span style=color:#e6db74>&#39;development&#39;</span>, <span style=color:#ae81ff>42</span>)]
</span></span><span style=display:flex><span>total <span style=color:#ae81ff>624</span>
</span></span></code></pre></div><p>Picking a few more that stand out,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>technical_job_title_terms <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;engineer&#34;</span>, <span style=color:#e6db74>&#34;developer&#34;</span>, <span style=color:#e6db74>&#34;research&#34;</span>, <span style=color:#e6db74>&#34;technical&#34;</span>, <span style=color:#e6db74>&#34;analyst&#34;</span>, <span style=color:#e6db74>&#34;engineering&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;data&#34;</span>, <span style=color:#e6db74>&#34;sciences&#34;</span>, <span style=color:#e6db74>&#34;ux&#34;</span>, <span style=color:#e6db74>&#34;analytics&#34;</span>, <span style=color:#e6db74>&#34;systems&#34;</span>, <span style=color:#e6db74>&#34;architect&#34;</span>, <span style=color:#e6db74>&#34;researcher&#34;</span>, <span style=color:#e6db74>&#34;web&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;infrastructure&#34;</span>, <span style=color:#e6db74>&#34;intelligence&#34;</span>, <span style=color:#e6db74>&#34;quantitative&#34;</span>, <span style=color:#e6db74>&#34;learning&#34;</span>, <span style=color:#e6db74>&#34;software&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;scientist&#34;</span>,
</span></span><span style=display:flex><span>        ]
</span></span></code></pre></div><p>11:48 Maybe can see if more #MLE jobs in <a href=https://www.kaggle.com/datasets/atahmasb/amazon-job-skills>this kaggle dataset</a> of #Amazon job descriptions.
12:47 ok so lets run cosine similarity, ranked, between my corpus and these, descriptions,
14:36 my last blurb was a few months ago, <a href=https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/>here</a> , so refreshing slightly,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> yaml
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tempfile
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pytz
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>utc_now</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> datetime<span style=color:#f92672>.</span>utcnow()<span style=color:#f92672>.</span>replace(tzinfo<span style=color:#f92672>=</span>pytz<span style=color:#f92672>.</span>UTC)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>utc_ts</span>(dt):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dt<span style=color:#f92672>.</span>strftime(<span style=color:#e6db74>&#34;%Y-%m-</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>T%H%M%S&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_yaml</span>(loc):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(loc) <span style=color:#66d9ef>as</span> fd:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> yaml<span style=color:#f92672>.</span>safe_load(fd)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>repos_dir <span style=color:#f92672>=</span> Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> repos_dir<span style=color:#f92672>.</span>is_dir()      
</span></span><span style=display:flex><span>experience_loc <span style=color:#f92672>=</span> repos_dir <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;my-challenges-and-accomplishments/experience.yaml&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>experiences_dict <span style=color:#f92672>=</span> read_yaml(experience_loc)[<span style=color:#e6db74>&#34;Descriptions&#34;</span>]
</span></span><span style=display:flex><span>my_sentences <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>build_my_blurb(experiences_dict)
</span></span></code></pre></div><p>15:44 ok and compare with those datasets,
Mini filter example,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vec <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#34;title&#34;</span>: <span style=color:#e6db74>&#34;Software Engineer yea&#34;</span>},
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#34;title&#34;</span>: <span style=color:#e6db74>&#34;Some Scientist&#34;</span>},
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#34;title&#34;</span>: <span style=color:#e6db74>&#34;Product Manager&#34;</span>},
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#34;title&#34;</span>: <span style=color:#e6db74>&#34;Industrial Designer&#34;</span>}
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame<span style=color:#f92672>.</span>from_records(vec)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>31</span>]: df
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>31</span>]: 
</span></span><span style=display:flex><span>                   title
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>  Software Engineer yea
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>         Some Scientist
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>        Product Manager
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>    Industrial Designer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>32</span>]: df<span style=color:#f92672>.</span>query(<span style=color:#e6db74>&#34;title.str.contains(&#39;software&#39;, case=False) or title.str.contains(&#39;scientist&#39;, case=False)&#34;</span>)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>32</span>]: 
</span></span><span style=display:flex><span>                   title
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>  Software Engineer yea
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>         Some Scientist
</span></span></code></pre></div><p>16:16 ok put that into a func, ( putting it <a href=https://github.com/namoopsoo/interview-me/blob/main/utils.py>here</a> )</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>filter_pandas_multiple_contains</span>(df, column, vec, <span style=color:#66d9ef>case</span><span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;filter dataframe for column containing any string from list vec given.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Example
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &gt;&gt;&gt; vec = [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ... {&#34;title&#34;: &#34;Software Engineer yea&#34;},
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ... {&#34;title&#34;: &#34;Some Scientist&#34;},
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ... {&#34;title&#34;: &#34;Product Manager&#34;},
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ... {&#34;title&#34;: &#34;Industrial Designer&#34;}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &gt;&gt;&gt; df = pd.DataFrame.from_records(vec)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &gt;&gt;&gt; df
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                       title
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    0  Software Engineer yea
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    1         Some Scientist
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    2        Product Manager
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    3    Industrial Designer
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &gt;&gt;&gt; import utils as ut
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &gt;&gt;&gt; ut.filter_pandas_multiple_contains(df, &#34;title&#34;, [&#34;engineer&#34;, &#34;scientist&#34;])
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                       title
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    0  Software Engineer yea
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    1         Some Scientist
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34; or &#34;</span><span style=color:#f92672>.</span>join(
</span></span><span style=display:flex><span>            [<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>column<span style=color:#e6db74>}</span><span style=color:#e6db74>.str.contains(&#39;</span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;, case=</span><span style=color:#e6db74>{</span>case<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>
</span></span><span style=display:flex><span>             <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> vec])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> df<span style=color:#f92672>.</span>query(query)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>39</span>]: ut<span style=color:#f92672>.</span>filter_pandas_multiple_contains(df, <span style=color:#e6db74>&#34;title&#34;</span>, [<span style=color:#e6db74>&#34;engineer&#34;</span>, <span style=color:#e6db74>&#34;scientist&#34;</span>])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>39</span>]: 
</span></span><span style=display:flex><span>                   title
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>  Software Engineer yea
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>         Some Scientist
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>raw_sentences <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>technical_job_title_terms <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;engineer&#34;</span>, <span style=color:#e6db74>&#34;developer&#34;</span>, <span style=color:#e6db74>&#34;research&#34;</span>, <span style=color:#e6db74>&#34;technical&#34;</span>, <span style=color:#e6db74>&#34;analyst&#34;</span>, <span style=color:#e6db74>&#34;engineering&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;data&#34;</span>, <span style=color:#e6db74>&#34;sciences&#34;</span>, <span style=color:#e6db74>&#34;ux&#34;</span>, <span style=color:#e6db74>&#34;analytics&#34;</span>, <span style=color:#e6db74>&#34;systems&#34;</span>, <span style=color:#e6db74>&#34;architect&#34;</span>, <span style=color:#e6db74>&#34;researcher&#34;</span>, <span style=color:#e6db74>&#34;web&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;infrastructure&#34;</span>, <span style=color:#e6db74>&#34;intelligence&#34;</span>, <span style=color:#e6db74>&#34;quantitative&#34;</span>, <span style=color:#e6db74>&#34;learning&#34;</span>, <span style=color:#e6db74>&#34;software&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;scientist&#34;</span>,
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-google-job-skills/job_skills.csv&#34;</span>)
</span></span><span style=display:flex><span>jobsdf <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>filter_pandas_multiple_contains(
</span></span><span style=display:flex><span>    pd<span style=color:#f92672>.</span>read_csv(loc), <span style=color:#e6db74>&#34;Title&#34;</span>, technical_job_title_terms)
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;Responsibilities&#34;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Minimum Qualifications&#39;</span>, 
</span></span><span style=display:flex><span>                            <span style=color:#e6db74>&#39;Preferred Qualifications&#39;</span>]
</span></span><span style=display:flex><span>raw_sentences<span style=color:#f92672>.</span>extend(ut<span style=color:#f92672>.</span>extract_raw_sentences(
</span></span><span style=display:flex><span>  jobsdf, columns))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;len raw_sentences after ingestion&#34;</span>, len(raw_sentences))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loc <span style=color:#f92672>=</span> (Path(os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;REPOS_DIR&#34;</span>)) 
</span></span><span style=display:flex><span>            <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;data&#34;</span> 
</span></span><span style=display:flex><span>			<span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;kaggle-amazon-job-skills/amazon_jobs_dataset.csv&#34;</span>)
</span></span><span style=display:flex><span>jobsdf <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>filter_pandas_multiple_contains(
</span></span><span style=display:flex><span>    pd<span style=color:#f92672>.</span>read_csv(loc), <span style=color:#e6db74>&#34;Title&#34;</span>, technical_job_title_terms)
</span></span><span style=display:flex><span>columns <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;DESCRIPTION&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;BASIC QUALIFICATIONS&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;PREFERRED QUALIFICATIONS&#39;</span>]
</span></span><span style=display:flex><span>raw_sentences<span style=color:#f92672>.</span>extend(ut<span style=color:#f92672>.</span>extract_raw_sentences(
</span></span><span style=display:flex><span>  jobsdf, columns))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;len raw_sentences after ingestion&#34;</span>, len(raw_sentences))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>len raw_sentences after ingestion <span style=color:#ae81ff>3259</span>
</span></span><span style=display:flex><span>len raw_sentences after ingestion <span style=color:#ae81ff>33081</span>
</span></span></code></pre></div><p>17:44 ok yea and doing that top k cosine similarity then</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers.util <span style=color:#f92672>import</span> semantic_search
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>hf_token <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;HF_TOKEN&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_story_embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, my_sentences)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>jd_embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, raw_sentences[:<span style=color:#ae81ff>1000</span>])
</span></span><span style=display:flex><span><span style=color:#75715e># this line took </span>
</span></span><span style=display:flex><span><span style=color:#75715e># Wall time: 7.05 s</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>hits <span style=color:#f92672>=</span> semantic_search(my_story_embeddings, jd_embeddings, top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, row <span style=color:#f92672>in</span> enumerate(hits[:<span style=color:#ae81ff>5</span>]):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;(</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>, <span style=color:#e6db74>&#34;matching,&#34;</span>, my_sentences[i], <span style=color:#e6db74>&#34;:&#34;</span>)
</span></span><span style=display:flex><span>    hmm <span style=color:#f92672>=</span> [[raw_sentences[x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>]], x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>], x[<span style=color:#e6db74>&#34;score&#34;</span>]] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> row[:<span style=color:#ae81ff>3</span>] ]
</span></span><span style=display:flex><span>    print(hmm, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>18:07</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>20</span>]: <span style=color:#66d9ef>for</span> i, row <span style=color:#f92672>in</span> enumerate(hits[:<span style=color:#ae81ff>5</span>]):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>:     print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;(</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>, <span style=color:#e6db74>&#34;matching,&#34;</span>, my_sentences[i], <span style=color:#e6db74>&#34;:&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>:     hmm <span style=color:#f92672>=</span> [[raw_sentences[x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>]], x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>], x[<span style=color:#e6db74>&#34;score&#34;</span>]] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> row[:<span style=color:#ae81ff>3</span>] ]
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>:     print(hmm, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>0</span>) matching, Built our initial method <span style=color:#66d9ef>for</span> model hosting, transitioning <span style=color:#f92672>from</span> a purely business rule based flow :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;experience in practical business modeling or financial modeling</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>distinctive problem solving and analytical skills, combined with impeccable business judgment&#39;</span>, <span style=color:#ae81ff>740</span>, <span style=color:#ae81ff>0.45160090923309326</span>], [<span style=color:#e6db74>&#39;review and fulfill managed software requests to ensure products meet business needs, while overseeing programmatic compliance with associated software licenses/other agreements, contractual terms, and policies&#39;</span>, <span style=color:#ae81ff>394</span>, <span style=color:#ae81ff>0.433631956577301</span>], [<span style=color:#e6db74>&#39;experience in business planning and/or financial modeling&#39;</span>, <span style=color:#ae81ff>425</span>, <span style=color:#ae81ff>0.4310281574726105</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>1</span>) matching, Created a Vagrant virtual machine based staging environment that developers can quickly use to stage code, to help us transition <span style=color:#f92672>from</span> personalized AWS staging environments which can potentially help us save several hundreds of dollars a month<span style=color:#f92672>.</span> :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;experience architecting, developing software, or internet scale production-grade big data solutions in virtualized environments such as google cloud platform&#39;</span>, <span style=color:#ae81ff>753</span>, <span style=color:#ae81ff>0.4532877206802368</span>], [<span style=color:#e6db74>&#39;establish and drive planning and execution steps towards production deployments&#39;</span>, <span style=color:#ae81ff>959</span>, <span style=color:#ae81ff>0.4100092053413391</span>], [<span style=color:#e6db74>&#39;experience in designing and implementing build automation, and configuration management for operating system platforms.&#39;</span>, <span style=color:#ae81ff>859</span>, <span style=color:#ae81ff>0.3592562973499298</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>2</span>) matching, 
</span></span><span style=display:flex><span>Implemented the retailer lead list reporting, so that big data heavy retailers like Sears could <span style=color:#66d9ef>finally</span> be more involved <span style=color:#f92672>in</span> following up <span style=color:#66d9ef>with</span> customers who were <span style=color:#f92672>not</span> originating their preapprovals<span style=color:#f92672>.</span> :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;create effective, scalable, and easy to understand reporting solutions (e.g&#39;</span>, <span style=color:#ae81ff>262</span>, <span style=color:#ae81ff>0.45110809803009033</span>], [<span style=color:#e6db74>&#39;lead global analysis of in-store demo device analytics&#39;</span>, <span style=color:#ae81ff>833</span>, <span style=color:#ae81ff>0.4380985200405121</span>], [<span style=color:#e6db74>&#39;demonstrated understanding of customer support verticals&#39;</span>, <span style=color:#ae81ff>961</span>, <span style=color:#ae81ff>0.42506536841392517</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>3</span>) matching, 
</span></span><span style=display:flex><span>Troubleshooted <span style=color:#f92672>and</span> fixed rare <span style=color:#f92672>and</span> difficult to detect buyout bugs<span style=color:#f92672>.</span> When customers had multiple payments being taken on a day when they also did a buyout, <span style=color:#66d9ef>for</span> example, there was a bug where we were incorrectly discounting the additional payments that they made that day<span style=color:#f92672>.</span> :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;serve as a central coordination point for customer bugs and issues escalated by internal sales teams&#39;</span>, <span style=color:#ae81ff>941</span>, <span style=color:#ae81ff>0.4794199466705322</span>], [<span style=color:#e6db74>&#39;author test plans with the goal of catching issues and fixing them at early design stage to improve the overall product quality and meet aggressive schedule&#39;</span>, <span style=color:#ae81ff>313</span>, <span style=color:#ae81ff>0.39714837074279785</span>], [<span style=color:#e6db74>&#39;support product implementation and help partners in their day to day challenges by delivering innovative and scalable solutions to their problems and troubleshooting their issues&#39;</span>, <span style=color:#ae81ff>796</span>, <span style=color:#ae81ff>0.3908129930496216</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>4</span>) matching, 
</span></span><span style=display:flex><span>Refactored payment processing to reflect a better interpretation of the law around customer suspense dollars<span style=color:#f92672>.</span> Previously, customers would pay down their next months payment<span style=color:#f92672>.</span> In the change, any payments that are made outside of the due date count towards the suspense account<span style=color:#f92672>.</span> This change required splitting out the plan shifting, away <span style=color:#f92672>from</span> the payment processing, into its own separate task, to simplify the new implementation of the payment law<span style=color:#f92672>.</span> :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;work with partner teams to re-engineer process workflows around demand planning, supply planning, ordering, and fulfillment&#39;</span>, <span style=color:#ae81ff>219</span>, <span style=color:#ae81ff>0.32384926080703735</span>], [<span style=color:#e6db74>&#39;experience in devising and implementing strategies and business improvements.&#39;</span>, <span style=color:#ae81ff>863</span>, <span style=color:#ae81ff>0.31515663862228394</span>], [<span style=color:#e6db74>&#39;consult with internal account management teams and customers to track the progress and impact&#39;</span>, <span style=color:#ae81ff>621</span>, <span style=color:#ae81ff>0.30947157740592957</span>]] 
</span></span></code></pre></div><p>Ok so definitely still not impressed with the hits I&rsquo;m getting here.</p><h4 id=thoughts-1>Thoughts<a hidden class=anchor aria-hidden=true href=#thoughts-1>#</a></h4><p>A few follow up items coming to mind,</p><p>collapsed:: true
I think per above experiment I ran, I want to find a few false negative matches , look at the scores they are producing, and then probably take a closer dive into the [[average-pooling]] . I want to really answer the question, do I need to do pre-processing, removing [[stop-words]] so the [[sentence-transformers]] [[cosine similarity]] after average pooling does not suffer?
And then after doing the preprocessing if necessary , excluding it as an issue or acting to remove stop words or fluff words, then lets run cosine similarity like that.
And then maybe a refined , more granular approach would be to think about using [[Named Entity Recognition NER]] maybe to better remove stop words , especially if I do not perhaps have the luxury of fine tuning.
but yea side note I think fine tuning would be really helpful to help with embedding these interesting jargon words close to each other if they are indeed related
And as a visual debugging I really should plot out or at least someone must have some nice tool to visualize embeddings</p><h3 id=jul-24th-2023-started-a-nice-debug-session-today--false-negative-analysis-here>[[Jul 24th, 2023]] started a nice debug session today , false negative analysis here,<a hidden class=anchor aria-hidden=true href=#jul-24th-2023-started-a-nice-debug-session-today--false-negative-analysis-here>#</a></h3><p>Ok per my notes from [[Jul 23rd, 2023]], let me hunt down one good [[false negative]],</p><p>For sure I am realizing yes a lot of my sentences in my personal corpus are lacking a succinctness and there is a lot of filler in there ideally I should cut out.
Actually it would be cool to have that kind of feedback actually as a tool, ranking sentences by fluff haha that should be improved. And even, how many sentences are used in describing each individual project/story, can be helpful to see analyzed too.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Using &#34;my_sentences&#34; defined last time</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, x <span style=color:#f92672>in</span> enumerate(random<span style=color:#f92672>.</span>choices(my_sentences, k<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)):
</span></span><span style=display:flex><span>    print(i, x)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span> So the behavior was changed <span style=color:#f92672>and</span> I had this now, erroneously haha, so I needed to now remove it
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span> Took Databricks Spark cluster <span style=color:#f92672>and</span> pyspark to answer a question about a CDC Covid dataset, what <span style=color:#f92672>is</span> the asymptomatic rate by age bin <span style=color:#66d9ef>as</span> well <span style=color:#66d9ef>as</span> hospitalization rate by presence of prior medical conditions
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span> Improved our sklearn  model training that was crashing on a laptop, by cutting up the data into chunks <span style=color:#f92672>and</span> using python multiprocessing
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span> (<span style=color:#ae81ff>7</span>) <span style=color:#e6db74>&#34;humana hackathon exploration of langchain against health plan documents&#34;</span> ( I have described this <span style=color:#f92672>in</span> more detail earlier above )
</span></span></code></pre></div><p>Ok in any case, so let me search the corpus I have, for terms, say, spark, cluster pyspark, databricks . Of course I am realizing Google and Amazon have their own options for clustering and these job descriptions might not mention pyspark, but let&rsquo;s see,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> utils <span style=color:#66d9ef>as</span> ut
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Using &#34;raw_sentences&#34; defined last time</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame({<span style=color:#e6db74>&#34;description&#34;</span>: raw_sentences})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>29</span>]: df<span style=color:#f92672>.</span>iloc[:<span style=color:#ae81ff>5</span>]
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>29</span>]: 
</span></span><span style=display:flex><span>                                         description
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>            app scripts, spreadsheet software, etc)
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>  leadership, problem solving <span style=color:#f92672>and</span> analysis exper<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>  hands<span style=color:#f92672>-</span>on experience using <span style=color:#f92672>and</span><span style=color:#f92672>/</span><span style=color:#f92672>or</span> managing data<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>  background <span style=color:#f92672>in</span> solving complex challenges <span style=color:#f92672>and</span> d<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>  travel frequently around emea <span style=color:#66d9ef>for</span> meetings, te<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>terms <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;pyspark&#34;</span>, <span style=color:#e6db74>&#34;spark&#34;</span>, <span style=color:#e6db74>&#34;databricks&#34;</span>, <span style=color:#e6db74>&#34;multiprocessing&#34;</span>, <span style=color:#e6db74>&#34;cluster&#34;</span>]
</span></span><span style=display:flex><span>resultsdf <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>filter_pandas_multiple_contains(df, <span style=color:#e6db74>&#34;description&#34;</span>, terms)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>34</span>]: resultsdf<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>34</span>]: (<span style=color:#ae81ff>154</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>36</span>]: resultsdf<span style=color:#f92672>.</span>iloc[:<span style=color:#ae81ff>10</span>][<span style=color:#e6db74>&#34;description&#34;</span>]<span style=color:#f92672>.</span>tolist()
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>36</span>]: 
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;hands-on experience using and/or managing databases, or cloud technologies such as sql, nosql, hadoop or spark&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;familiarity with architecture and operational aspects of large scale distributed systems; familiarity with the popular technologies in the machine learning/big data ecosystem (tensorflow, spark, etc)</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>technical experience in web technologies such as html, xml, json, oauth 2 along with experience in analysis of relational data in mysql, google bigquery or similar&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;regression, classification, clustering, etc)&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;experience using and/or managing databases, and with one or all of the following: mapreduce, hadoop, spark, flume, hive, impala, spark sql and/or bigquery&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;experience building multi-tier high availability applications with modern web technologies (such as nosql, mongodb, sparkml, tensorflow)&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;experience with data processing software (such as hadoop, spark, pig, hive) and data processing algorithms (mapreduce, flume)&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;technologies you will employ to solve these complex real-world business problems include natural language processing, machine learning, image recognition, elastic computing, spark, and a host of other state of the art aws technologies&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;in order to drive expansion of the amazon catalog, we use cluster-computing technologies like mapreduce, spark and hive to process billions of products and algorithmically find products not already sold on amazon&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;experience working with scala/python/java on spark to build and deploy ml models in production&#39;</span>]
</span></span></code></pre></div><p>09:19 Ok cool, so there are a few meaty sentences here that would be good to compare more directly with debugging eyes .
for example,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers.util <span style=color:#f92672>import</span> semantic_search, cos_sim
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModel
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>hf_token <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;HF_TOKEN&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>s1 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions&#34;</span>
</span></span><span style=display:flex><span>s2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>
</span></span><span style=display:flex><span>s3 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;experience working with scala/python/java on spark to build and deploy ml models in production&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> [s1, s2, s3]:
</span></span><span style=display:flex><span>    print(s, tokenizer<span style=color:#f92672>.</span>tokenize(s), <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, [s1, s2, s3])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>45</span>]: cos_sim(embeddings[<span style=color:#ae81ff>0</span>,:], embeddings[<span style=color:#ae81ff>1</span>, :])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>45</span>]: tensor([[<span style=color:#ae81ff>0.2323</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>46</span>]: cos_sim(embeddings[<span style=color:#ae81ff>0</span>,:], embeddings[<span style=color:#ae81ff>2</span>, :])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>46</span>]: tensor([[<span style=color:#ae81ff>0.2320</span>]])
</span></span></code></pre></div><p>also try the other way too ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>43</span>]: sentences <span style=color:#f92672>=</span> [s1, s2, s3]
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: hits <span style=color:#f92672>=</span> semantic_search(embeddings, embeddings, top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: <span style=color:#66d9ef>for</span> i, row <span style=color:#f92672>in</span> enumerate(hits[:<span style=color:#ae81ff>5</span>]):
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>:     print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;(</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>, <span style=color:#e6db74>&#34;matching,&#34;</span>, sentences[i], <span style=color:#e6db74>&#34;:&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>:     hmm <span style=color:#f92672>=</span> [[sentences[x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>]], x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>], x[<span style=color:#e6db74>&#34;score&#34;</span>]] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> row[:<span style=color:#ae81ff>3</span>] ]
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>:     print(hmm, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>0</span>) matching, Took Databricks Spark cluster <span style=color:#f92672>and</span> pyspark to answer a question about a CDC Covid dataset, what <span style=color:#f92672>is</span> the asymptomatic rate by age bin <span style=color:#66d9ef>as</span> well <span style=color:#66d9ef>as</span> hospitalization rate by presence of prior medical conditions :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#e6db74>&#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.23231448233127594</span>], [<span style=color:#e6db74>&#39;experience working with scala/python/java on spark to build and deploy ml models in production&#39;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0.2320040762424469</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>1</span>) matching, know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.9999999403953552</span>], [<span style=color:#e6db74>&#39;experience working with scala/python/java on spark to build and deploy ml models in production&#39;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0.4644332528114319</span>], [<span style=color:#e6db74>&#39;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.23231443762779236</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>2</span>) matching, experience working <span style=color:#66d9ef>with</span> scala<span style=color:#f92672>/</span>python<span style=color:#f92672>/</span>java on spark to build <span style=color:#f92672>and</span> deploy ml models <span style=color:#f92672>in</span> production :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;experience working with scala/python/java on spark to build and deploy ml models in production&#39;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1.000000238418579</span>], [<span style=color:#e6db74>&#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.4644332826137543</span>], [<span style=color:#e6db74>&#39;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.2320040762424469</span>]] 
</span></span></code></pre></div><p>Ok so that gave around same result. Ok cool, no matter which func was used,
Ok cool, so next want to keep diving deeper, probably ultimately looking at [[average-pooling]] here.</p><h3 id=jul-25th-2023-stop-word-removal-experiment>[[Jul 25th, 2023]] Stop word removal experiment<a hidden class=anchor aria-hidden=true href=#jul-25th-2023-stop-word-removal-experiment>#</a></h3><p>So for the question yesterday, why was it that two cosine similarity comparisons had basically same score, <code>0.2323</code> and <code>0.2320</code>, maybe that is a clue.</p><p>I think I have seen that completely unrelated sentences can have a zero score comparison right?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>s1 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;If a tree falls in the forest and there is no one there to hear it, then does it make a sound?&#34;</span>
</span></span><span style=display:flex><span>s2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;A bagel with cream cheese with some lox and some capers would go great with coffee.&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> [s1, s2]:
</span></span><span style=display:flex><span>    print(s, tokenizer<span style=color:#f92672>.</span>tokenize(s), <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>set(tokenizer<span style=color:#f92672>.</span>tokenize(s1)) <span style=color:#f92672>&amp;</span> set(tokenizer<span style=color:#f92672>.</span>tokenize(s2))
</span></span><span style=display:flex><span><span style=color:#75715e># {&#39;a&#39;, &#39;and&#39;}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, [s1, s2])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cos_sim(embeddings[<span style=color:#ae81ff>0</span>,:], embeddings[<span style=color:#ae81ff>1</span>, :])
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([[0.0240]])</span>
</span></span></code></pre></div><p>09:03 ok cool
So for example from yesterday, lets see what happens if we remove [[stop-words]] . ( Side note <a href=https://datascience.stackexchange.com/questions/86252/effect-of-stop-word-removal-on-transformers-for-text-classification>stacko link</a> someone suggests also trying to mask stop words ) .</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> nltk
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers.util <span style=color:#f92672>import</span> semantic_search, cos_sim
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModel
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>hf_token <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>getenv(<span style=color:#e6db74>&#34;HF_TOKEN&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>s1 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Took Databricks Spark cluster and pyspark to answer a question about a CDC Covid dataset, what is the asymptomatic rate by age bin as well as hospitalization rate by presence of prior medical conditions&#34;</span>
</span></span><span style=display:flex><span>s2 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>
</span></span><span style=display:flex><span>s3 <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;experience working with scala/python/java on spark to build and deploy ml models in production&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dont_stop</span>(s):
</span></span><span style=display:flex><span>    stop_words <span style=color:#f92672>=</span> stopwords<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#39;english&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join([x <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> s<span style=color:#f92672>.</span>split() <span style=color:#66d9ef>if</span> x<span style=color:#f92672>.</span>lower() <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> stop_words])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> [s1, s2, s3]:
</span></span><span style=display:flex><span>    print(s, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, dont_stop(s), <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Took Databricks Spark cluster <span style=color:#f92672>and</span> pyspark to answer a question about a CDC Covid dataset, what <span style=color:#f92672>is</span> the asymptomatic rate by age bin <span style=color:#66d9ef>as</span> well <span style=color:#66d9ef>as</span> hospitalization rate by presence of prior medical conditions 
</span></span><span style=display:flex><span> Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>know your way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery 
</span></span><span style=display:flex><span> know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>experience working <span style=color:#66d9ef>with</span> scala<span style=color:#f92672>/</span>python<span style=color:#f92672>/</span>java on spark to build <span style=color:#f92672>and</span> deploy ml models <span style=color:#f92672>in</span> production 
</span></span><span style=display:flex><span> experience working scala<span style=color:#f92672>/</span>python<span style=color:#f92672>/</span>java spark build deploy ml models production 
</span></span></code></pre></div><p>09:23 hmm not a whole lot of stop words there but lets try anyway,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [s1, s2, s3]
</span></span><span style=display:flex><span>stopped <span style=color:#f92672>=</span> [dont_stop(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> sentences]
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, stopped)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>hits <span style=color:#f92672>=</span> semantic_search(embeddings, embeddings, top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, row <span style=color:#f92672>in</span> enumerate(hits[:<span style=color:#ae81ff>5</span>]):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;(</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>, <span style=color:#e6db74>&#34;matching,&#34;</span>, stopped[i], <span style=color:#e6db74>&#34;:&#34;</span>)
</span></span><span style=display:flex><span>    hmm <span style=color:#f92672>=</span> [[stopped[x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>]], x[<span style=color:#e6db74>&#34;corpus_id&#34;</span>], x[<span style=color:#e6db74>&#34;score&#34;</span>]] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> row[:<span style=color:#ae81ff>3</span>] ]
</span></span><span style=display:flex><span>    print(hmm, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(<span style=color:#ae81ff>0</span>) matching, Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#e6db74>&#39;know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.3553411066532135</span>], [<span style=color:#e6db74>&#39;experience working scala/python/java spark build deploy ml models production&#39;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0.25089341402053833</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>1</span>) matching, know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1.0000001192092896</span>], [<span style=color:#e6db74>&#39;experience working scala/python/java spark build deploy ml models production&#39;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0.37388813495635986</span>], [<span style=color:#e6db74>&#39;Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.3553411364555359</span>]] 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>2</span>) matching, experience working scala<span style=color:#f92672>/</span>python<span style=color:#f92672>/</span>java spark build deploy ml models production :
</span></span><span style=display:flex><span>[[<span style=color:#e6db74>&#39;experience working scala/python/java spark build deploy ml models production&#39;</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1.0000001192092896</span>], [<span style=color:#e6db74>&#39;know way around map reduce, hadoop, spark, flume, hive, impala, sparksql, bigquery&#39;</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0.3738881051540375</span>], [<span style=color:#e6db74>&#39;Took Databricks Spark cluster pyspark answer question CDC Covid dataset, asymptomatic rate age bin well hospitalization rate presence prior medical conditions&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.25089341402053833</span>]] 
</span></span></code></pre></div><p>09:33 ok well I am seeing improvements undeniably here,</p><pre tabindex=0><code>0.3553 vs 0.2323
0.25089 vs 0.23200
</code></pre><p>so therefore [[sentence-transformers]] #take-away is not internally penalizing stop words as I had thought a bit that it might before.
Ok I think next I still want to just dissect the way [[sentence-transformers]] [[cosine similarity]] gets the result and reproduce it manually see if I get the same thing</p><h3 id=jul-26th-2023-some-more-experimentation-cosine-similarity-and-stop-words>[[Jul 26th, 2023]] some more experimentation, cosine similarity and stop words<a hidden class=anchor aria-hidden=true href=#jul-26th-2023-some-more-experimentation-cosine-similarity-and-stop-words>#</a></h3><p>ok next, yea let&rsquo;s try reproducing that [[cosine similarity]] ,</p><p>from last time, yea this model produces embeddings with this size,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>63</span>]: embeddings[<span style=color:#ae81ff>0</span>,:]<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>63</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>384</span>])
</span></span></code></pre></div><p>08:36 just double checking,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n [<span style=color:#ae81ff>68</span>]: np<span style=color:#f92672>.</span>dot(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>, :]), cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>, :])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>68</span>]: (<span style=color:#ae81ff>0.35534108</span>, tensor([[<span style=color:#ae81ff>0.3553</span>]]))
</span></span></code></pre></div><p>Ok next, just out of curiosity, since <a href=https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a>blog post</a> also mentions that #spacy has a longer #stop-words list,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords <span style=color:#66d9ef>as</span> sw_nltk
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> spacy
</span></span><span style=display:flex><span>en <span style=color:#f92672>=</span> spacy<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#39;en_core_web_sm&#39;</span>)
</span></span><span style=display:flex><span>sw_spacy <span style=color:#f92672>=</span> en<span style=color:#f92672>.</span>Defaults<span style=color:#f92672>.</span>stop_words
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dont_stop_both</span>(s):
</span></span><span style=display:flex><span>    en <span style=color:#f92672>=</span> spacy<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#39;en_core_web_sm&#39;</span>)
</span></span><span style=display:flex><span>    sw_spacy <span style=color:#f92672>=</span> en<span style=color:#f92672>.</span>Defaults<span style=color:#f92672>.</span>stop_words
</span></span><span style=display:flex><span>    stop_words <span style=color:#f92672>=</span> set(sw_nltk<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#39;english&#39;</span>) ) <span style=color:#f92672>|</span> set(sw_spacy)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join([x <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> s<span style=color:#f92672>.</span>split() <span style=color:#66d9ef>if</span> x<span style=color:#f92672>.</span>lower() <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> stop_words])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [s1, s2, s3]
</span></span><span style=display:flex><span>stopped <span style=color:#f92672>=</span> [dont_stop(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> sentences]
</span></span><span style=display:flex><span>stopped_both <span style=color:#f92672>=</span> [dont_stop_both(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> sentences]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[len(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> stopped], [len(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> stopped_both]
</span></span><span style=display:flex><span><span style=color:#75715e># ([158, 82, 76], [153, 75, 76])</span>
</span></span></code></pre></div><p>08:57 ok so compare all three ways, for completeness</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, a_list <span style=color:#f92672>in</span> [(<span style=color:#e6db74>&#34;original&#34;</span>, sentences), (<span style=color:#e6db74>&#34;nltk&#34;</span>, stopped), (<span style=color:#e6db74>&#34;nltk+spacy&#34;</span>, stopped_both)]:
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, a_list)
</span></span><span style=display:flex><span>    results<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;what&#34;</span>: name, 
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;one&#34;</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>, :]),
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;two&#34;</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>2</span>, :]),
</span></span><span style=display:flex><span>                   })
</span></span><span style=display:flex><span>pd<span style=color:#f92672>.</span>DataFrame<span style=color:#f92672>.</span>from_records(results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         what                 one                 two
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>    original  [[tensor(<span style=color:#ae81ff>0.2323</span>)]]  [[tensor(<span style=color:#ae81ff>0.2320</span>)]]
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>        nltk  [[tensor(<span style=color:#ae81ff>0.3553</span>)]]  [[tensor(<span style=color:#ae81ff>0.2509</span>)]]
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>  nltk<span style=color:#f92672>+</span>spacy  [[tensor(<span style=color:#ae81ff>0.3329</span>)]]  [[tensor(<span style=color:#ae81ff>0.2501</span>)]]
</span></span></code></pre></div><p>09:08 Ok haha can be slightly hit or miss then .
But if I did this other extreme stop removal, say specifically with information I know about these sentences,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>jargon <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;python&#34;</span>, <span style=color:#e6db74>&#34;databricks&#34;</span>, <span style=color:#e6db74>&#34;cluster&#34;</span>, <span style=color:#e6db74>&#34;pyspark&#34;</span>, <span style=color:#e6db74>&#34;scala&#34;</span>, 
</span></span><span style=display:flex><span>          <span style=color:#e6db74>&#34;answer&#34;</span>, <span style=color:#e6db74>&#34;question&#34;</span>, <span style=color:#e6db74>&#34;dataset&#34;</span>, <span style=color:#e6db74>&#34;map&#34;</span>, <span style=color:#e6db74>&#34;reduce&#34;</span>, 
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;hadoop&#34;</span>, <span style=color:#e6db74>&#34;spark&#34;</span>, <span style=color:#e6db74>&#34;flume&#34;</span>, <span style=color:#e6db74>&#34;hive&#34;</span>, <span style=color:#e6db74>&#34;impala&#34;</span>, <span style=color:#e6db74>&#34;sparksql&#34;</span>, <span style=color:#e6db74>&#34;bigquery&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:#e6db74>&#34;java&#34;</span>, <span style=color:#e6db74>&#34;build&#34;</span>, <span style=color:#e6db74>&#34;deploy&#34;</span>, <span style=color:#e6db74>&#34;ml&#34;</span>, <span style=color:#e6db74>&#34;models&#34;</span>, <span style=color:#e6db74>&#34;production&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>talk_jargon_to_me</span>(jargon, s):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join([
</span></span><span style=display:flex><span>      (x <span style=color:#66d9ef>if</span> x<span style=color:#f92672>.</span>lower() <span style=color:#f92672>in</span> jargon <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;blah&#34;</span>) 
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> s<span style=color:#f92672>.</span>split()])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>only_jargon_sentences <span style=color:#f92672>=</span> [talk_jargon_to_me(jargon, x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> sentences]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, a_list <span style=color:#f92672>in</span> [
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#34;original&#34;</span>, sentences), (<span style=color:#e6db74>&#34;nltk&#34;</span>, stopped), (<span style=color:#e6db74>&#34;nltk+spacy&#34;</span>, stopped_both),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#34;only_jargon&#34;</span>, only_jargon_sentences)
</span></span><span style=display:flex><span>]:
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, a_list)
</span></span><span style=display:flex><span>    results<span style=color:#f92672>.</span>append({<span style=color:#e6db74>&#34;what&#34;</span>: name, 
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;one&#34;</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>, :]),
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#34;two&#34;</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>2</span>, :]),
</span></span><span style=display:flex><span>                   })
</span></span><span style=display:flex><span>pd<span style=color:#f92672>.</span>DataFrame<span style=color:#f92672>.</span>from_records(results)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>          what                 one                 two
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>     original  [[tensor(<span style=color:#ae81ff>0.2323</span>)]]  [[tensor(<span style=color:#ae81ff>0.2320</span>)]]
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>         nltk  [[tensor(<span style=color:#ae81ff>0.3553</span>)]]  [[tensor(<span style=color:#ae81ff>0.2509</span>)]]
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>   nltk<span style=color:#f92672>+</span>spacy  [[tensor(<span style=color:#ae81ff>0.3329</span>)]]  [[tensor(<span style=color:#ae81ff>0.2501</span>)]]
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>  only_jargon  [[tensor(<span style=color:#ae81ff>0.3406</span>)]]  [[tensor(<span style=color:#ae81ff>0.5675</span>)]]
</span></span></code></pre></div><p>09:36 Ok so sentence transformers can be a bit of a blunt tool for sure but I think I am verifying that with some pre-processing , I can get better use from them . And improving the jargon game can be helpful too.</p><p>One more thing out of curiosity,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>vocabulary <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vocabulary_of_model(model_id)
</span></span><span style=display:flex><span>[(x, x <span style=color:#f92672>in</span> vocabulary) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;spark&#34;</span>, <span style=color:#e6db74>&#34;pyspark&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>]]
</span></span><span style=display:flex><span><span style=color:#75715e># [(&#39;spark&#39;, True), (&#39;pyspark&#39;, False), (&#39;python&#39;, True)]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[(x, tokenizer<span style=color:#f92672>.</span>tokenize(x)) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;spark&#34;</span>, <span style=color:#e6db74>&#34;pyspark&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>]]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[(<span style=color:#e6db74>&#39;spark&#39;</span>, [<span style=color:#e6db74>&#39;spark&#39;</span>]),
</span></span><span style=display:flex><span> (<span style=color:#e6db74>&#39;pyspark&#39;</span>, [<span style=color:#e6db74>&#39;p&#39;</span>, <span style=color:#e6db74>&#39;##ys&#39;</span>, <span style=color:#e6db74>&#39;##park&#39;</span>]),
</span></span><span style=display:flex><span> (<span style=color:#e6db74>&#39;python&#39;</span>, [<span style=color:#e6db74>&#39;python&#39;</span>])]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>terms <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;spark&#34;</span>, <span style=color:#e6db74>&#34;pyspark&#34;</span>, <span style=color:#e6db74>&#34;python&#34;</span>]
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, hf_token, terms)
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#34;spark, pyspark&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>, :])], 
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#34;spark, python&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>2</span>, :])],
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#34;pyspark, python&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>1</span>, :], embeddings[<span style=color:#ae81ff>2</span>, :])],
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[[<span style=color:#e6db74>&#39;spark, pyspark&#39;</span>, tensor([[<span style=color:#ae81ff>0.5201</span>]])],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;spark, python&#39;</span>, tensor([[<span style=color:#ae81ff>0.2316</span>]])],
</span></span><span style=display:flex><span> [<span style=color:#e6db74>&#39;pyspark, python&#39;</span>, tensor([[<span style=color:#ae81ff>0.4150</span>]])]]
</span></span></code></pre></div><p>09:47 might be worth poking at this a bit more, so if &ldquo;pyspark&rdquo; is not in the vocabulary here, but hmm maybe through all the fine tuning, on a billion sentences, maybe the embeddings still ended up being meaningfully close? Want to better understand this</p><h3 id=jul-27th-2023-hmm-what-is-the-subword-tokenization--multiple--when-thinking-about-truncation>[[Jul 27th, 2023]] hmm what is the subword tokenization multiple , when thinking about truncation<a hidden class=anchor aria-hidden=true href=#jul-27th-2023-hmm-what-is-the-subword-tokenization--multiple--when-thinking-about-truncation>#</a></h3><p>So it was cool to see you can get a better #[[cosine similarity]] score when taking out stop words and also when replacing the non-jargon words with blah words. Although that second part hmm might have actually artificially increased the score thre now that I think about it, ü§î</p><p>Maybe thinking about sentences is hmm not going to be as useful as thinking about the entire job description itself maybe.</p><p>So we have space only for 384 tokens but that can still be a good number of sentences, 10 mmaybe .
Ah yea and that is another good reason to get rid of the stop words
Like from my example from yesterday and day before, how many word-pieces in those?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[{<span style=color:#e6db74>&#34;num_words&#34;</span>: len(x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34; &#34;</span>)), <span style=color:#e6db74>&#34;num_tokens&#34;</span>: len(tokenizer<span style=color:#f92672>.</span>tokenize(x)), 
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#34;token_inflation_factor&#34;</span>: len(tokenizer<span style=color:#f92672>.</span>tokenize(x))<span style=color:#f92672>/</span>len(x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34; &#34;</span>))} 
</span></span><span style=display:flex><span> <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> [s1, s2, s3]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[{<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>34</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;num_tokens&#39;</span>: <span style=color:#ae81ff>46</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;token_inflation_factor&#39;</span>: <span style=color:#ae81ff>1.3529411764705883</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;num_tokens&#39;</span>: <span style=color:#ae81ff>27</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;token_inflation_factor&#39;</span>: <span style=color:#ae81ff>2.076923076923077</span>},
</span></span><span style=display:flex><span> {<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>14</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;num_tokens&#39;</span>: <span style=color:#ae81ff>18</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;token_inflation_factor&#39;</span>: <span style=color:#ae81ff>1.2857142857142858</span>}]
</span></span></code></pre></div><p>So yea if we have a 384 length input window then yea maybe 10 sentences or so on a good day. Not bad.
09:09 yea side note about that [[average-pooling]],</p><p>Reading the <a href=https://www.sbert.net/examples/applications/computing-embeddings/README.html>https://www.sbert.net/examples/applications/computing-embeddings/README.html</a> section here again for insight.
This is a good page
Interesting looking at that <code>mean_pooling</code> function, it takes the attention mask into accoount. kinda cool,</p><h3 id=jul-28th-2023-looking-more-closely-on-how-sentence-transformer-model-pools>[[Jul 28th, 2023]] looking more closely on how sentence transformer model pools<a hidden class=anchor aria-hidden=true href=#jul-28th-2023-looking-more-closely-on-how-sentence-transformer-model-pools>#</a></h3><p>ok thing I got it this time, per <a href=https://www.sbert.net/examples/applications/computing-embeddings/README.html>https://www.sbert.net/examples/applications/computing-embeddings/README.html</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>109</span>]: <span style=color:#75715e>#Sentences we want sentence embeddings for</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;This framework generates embeddings for each input sentence&#39;</span>,
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:              <span style=color:#e6db74>&#39;Sentences are passed as a list of string.&#39;</span>,
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:              <span style=color:#e6db74>&#39;The quick brown fox jumps over the lazy dog.&#39;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: <span style=color:#75715e>#Load AutoModel from huggingface model repository</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model <span style=color:#f92672>=</span> AutoModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: <span style=color:#75715e>#Tokenize sentences</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: encoded_input <span style=color:#f92672>=</span> tokenizer(sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: <span style=color:#75715e>#Compute token embeddings</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     model_output <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>encoded_input)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Downloading (<span style=color:#960050;background-color:#1e0010>‚Ä¶</span>)<span style=color:#e6db74>&#34;pytorch_model.bin&#34;</span>;: <span style=color:#ae81ff>100</span><span style=color:#f92672>%|</span><span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>90.9</span>M<span style=color:#f92672>/</span><span style=color:#ae81ff>90.9</span>M [<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>56</span><span style=color:#f92672>&lt;</span><span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>, <span style=color:#ae81ff>1.61</span>MB<span style=color:#f92672>/</span>s]
</span></span><span style=display:flex><span>Downloading (<span style=color:#960050;background-color:#1e0010>‚Ä¶</span>)<span style=color:#e6db74>&#34;pytorch_model.bin&#34;</span>;: <span style=color:#ae81ff>100</span><span style=color:#f92672>%|</span><span style=color:#960050;background-color:#1e0010>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</span><span style=color:#f92672>|</span> <span style=color:#ae81ff>90.9</span>M<span style=color:#f92672>/</span><span style=color:#ae81ff>90.9</span>M [<span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>56</span><span style=color:#f92672>&lt;</span><span style=color:#ae81ff>00</span>:<span style=color:#ae81ff>00</span>, <span style=color:#ae81ff>1.93</span>MB<span style=color:#f92672>/</span>s]
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>110</span>]: 
</span></span></code></pre></div><p>08:53 ok let&rsquo;s try to understand the function they show there,
So model_output,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>123</span>]: type(encoded_input)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>123</span>]: transformers<span style=color:#f92672>.</span>tokenization_utils_base<span style=color:#f92672>.</span>BatchEncoding
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>124</span>]: vars(encoded_input)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>124</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;data&#39;</span>: {<span style=color:#e6db74>&#39;input_ids&#39;</span>: tensor([[  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>2023</span>,  <span style=color:#ae81ff>7705</span>, <span style=color:#ae81ff>19421</span>,  <span style=color:#ae81ff>7861</span>,  <span style=color:#ae81ff>8270</span>,  <span style=color:#ae81ff>4667</span>,  <span style=color:#ae81ff>2015</span>,  <span style=color:#ae81ff>2005</span>,  <span style=color:#ae81ff>2169</span>,
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>7953</span>,  <span style=color:#ae81ff>6251</span>,   <span style=color:#ae81ff>102</span>],
</span></span><span style=display:flex><span>          [  <span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>11746</span>,  <span style=color:#ae81ff>2024</span>,  <span style=color:#ae81ff>2979</span>,  <span style=color:#ae81ff>2004</span>,  <span style=color:#ae81ff>1037</span>,  <span style=color:#ae81ff>2862</span>,  <span style=color:#ae81ff>1997</span>,  <span style=color:#ae81ff>5164</span>,  <span style=color:#ae81ff>1012</span>,
</span></span><span style=display:flex><span>             <span style=color:#ae81ff>102</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>          [  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>1996</span>,  <span style=color:#ae81ff>4248</span>,  <span style=color:#ae81ff>2829</span>,  <span style=color:#ae81ff>4419</span>, <span style=color:#ae81ff>14523</span>,  <span style=color:#ae81ff>2058</span>,  <span style=color:#ae81ff>1996</span>, <span style=color:#ae81ff>13971</span>,  <span style=color:#ae81ff>3899</span>,
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>1012</span>,   <span style=color:#ae81ff>102</span>,     <span style=color:#ae81ff>0</span>]]),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;token_type_ids&#39;</span>: tensor([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>          [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>          [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]]),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;attention_mask&#39;</span>: tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>          [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>          [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]])},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;_encodings&#39;</span>: [Encoding(num_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>13</span>, attributes<span style=color:#f92672>=</span>[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
</span></span><span style=display:flex><span>  Encoding(num_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>13</span>, attributes<span style=color:#f92672>=</span>[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
</span></span><span style=display:flex><span>  Encoding(num_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>13</span>, attributes<span style=color:#f92672>=</span>[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])],
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;_n_sequences&#39;</span>: <span style=color:#ae81ff>1</span>}
</span></span><span style=display:flex><span> 
</span></span></code></pre></div><p>09:16 ok so looks somewhat close, to doing it manually, below,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(<span style=color:#e6db74>&#34;what was encoded,&#34;</span>, encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>], )
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;back to tokens though,&#34;</span>, tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;original sentence,&#34;</span>, sentences[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>tokens <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>tokenize(sentences[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;sentence to tokens&#34;</span>, tokens)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;tokens to ids&#34;</span>, tokenizer<span style=color:#f92672>.</span>convert_tokens_to_ids(tokens))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>what was encoded, tensor([  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>2023</span>,  <span style=color:#ae81ff>7705</span>, <span style=color:#ae81ff>19421</span>,  <span style=color:#ae81ff>7861</span>,  <span style=color:#ae81ff>8270</span>,  <span style=color:#ae81ff>4667</span>,  <span style=color:#ae81ff>2015</span>,  <span style=color:#ae81ff>2005</span>,  <span style=color:#ae81ff>2169</span>,
</span></span><span style=display:flex><span>         <span style=color:#ae81ff>7953</span>,  <span style=color:#ae81ff>6251</span>,   <span style=color:#ae81ff>102</span>])
</span></span><span style=display:flex><span>back to tokens though, [<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;this&#39;</span>, <span style=color:#e6db74>&#39;framework&#39;</span>, <span style=color:#e6db74>&#39;generates&#39;</span>, <span style=color:#e6db74>&#39;em&#39;</span>, <span style=color:#e6db74>&#39;##bed&#39;</span>, <span style=color:#e6db74>&#39;##ding&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;for&#39;</span>, <span style=color:#e6db74>&#39;each&#39;</span>, <span style=color:#e6db74>&#39;input&#39;</span>, <span style=color:#e6db74>&#39;sentence&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>original sentence, This framework generates embeddings <span style=color:#66d9ef>for</span> each input sentence
</span></span><span style=display:flex><span>sentence to tokens [<span style=color:#e6db74>&#39;this&#39;</span>, <span style=color:#e6db74>&#39;framework&#39;</span>, <span style=color:#e6db74>&#39;generates&#39;</span>, <span style=color:#e6db74>&#39;em&#39;</span>, <span style=color:#e6db74>&#39;##bed&#39;</span>, <span style=color:#e6db74>&#39;##ding&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;for&#39;</span>, <span style=color:#e6db74>&#39;each&#39;</span>, <span style=color:#e6db74>&#39;input&#39;</span>, <span style=color:#e6db74>&#39;sentence&#39;</span>]
</span></span><span style=display:flex><span>tokens to ids [<span style=color:#ae81ff>2023</span>, <span style=color:#ae81ff>7705</span>, <span style=color:#ae81ff>19421</span>, <span style=color:#ae81ff>7861</span>, <span style=color:#ae81ff>8270</span>, <span style=color:#ae81ff>4667</span>, <span style=color:#ae81ff>2015</span>, <span style=color:#ae81ff>2005</span>, <span style=color:#ae81ff>2169</span>, <span style=color:#ae81ff>7953</span>, <span style=color:#ae81ff>6251</span>]
</span></span></code></pre></div><p>Only difference is I see when going back from input ids to tokens , there is an additional <code>[CLS]</code> at the start and a <code>[SEP]</code> at the end.
no pad ?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>encoded_input_no_pad <span style=color:#f92672>=</span> tokenizer(sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;without pad, tokens, &#34;</span>, tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(
</span></span><span style=display:flex><span>    encoded_input_no_pad<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ValueError</span>: Unable to create tensor, you should probably activate truncation <span style=color:#f92672>and</span><span style=color:#f92672>/</span><span style=color:#f92672>or</span> padding <span style=color:#66d9ef>with</span> <span style=color:#e6db74>&#39;padding=True&#39;</span> <span style=color:#e6db74>&#39;truncation=True&#39;</span> to have batched tensors <span style=color:#66d9ef>with</span> the same length<span style=color:#f92672>.</span> Perhaps your features (<span style=color:#960050;background-color:#1e0010>`</span>input_ids<span style=color:#960050;background-color:#1e0010>`</span> <span style=color:#f92672>in</span> this <span style=color:#66d9ef>case</span>) have excessive nesting (inputs type <span style=color:#960050;background-color:#1e0010>`</span>list<span style=color:#960050;background-color:#1e0010>`</span> where type <span style=color:#960050;background-color:#1e0010>`</span>int<span style=color:#960050;background-color:#1e0010>`</span> <span style=color:#f92672>is</span> expected)<span style=color:#f92672>.</span>
</span></span></code></pre></div><p>Ah ok so that doesn&rsquo;t even work then so padding required.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>([[{<span style=color:#e6db74>&#34;num_words&#34;</span>: len(x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34; &#34;</span>)), 
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;len_tokens&#34;</span>: len(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][i])),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;len_input_ids&#34;</span>: encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][i,:]<span style=color:#f92672>.</span>shape,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;len_mask&#34;</span>: encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;attention_mask&#34;</span>][i,:]<span style=color:#f92672>.</span>shape,
</span></span><span style=display:flex><span>   }, x, 
</span></span><span style=display:flex><span>   tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][i]),
</span></span><span style=display:flex><span>   encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][i,:], 
</span></span><span style=display:flex><span>   encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;attention_mask&#34;</span>][i,:]
</span></span><span style=display:flex><span>  ] 
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> i, x <span style=color:#f92672>in</span> enumerate(sentences)])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[[{<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_tokens&#39;</span>: <span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_input_ids&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>]),
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_mask&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>])},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;This framework generates embeddings for each input sentence&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;[CLS]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;this&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;framework&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;generates&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;em&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;##bed&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;##ding&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;##s&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;for&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;each&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;input&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;sentence&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;[SEP]&#39;</span>],
</span></span><span style=display:flex><span>  tensor([  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>2023</span>,  <span style=color:#ae81ff>7705</span>, <span style=color:#ae81ff>19421</span>,  <span style=color:#ae81ff>7861</span>,  <span style=color:#ae81ff>8270</span>,  <span style=color:#ae81ff>4667</span>,  <span style=color:#ae81ff>2015</span>,  <span style=color:#ae81ff>2005</span>,  <span style=color:#ae81ff>2169</span>,
</span></span><span style=display:flex><span>           <span style=color:#ae81ff>7953</span>,  <span style=color:#ae81ff>6251</span>,   <span style=color:#ae81ff>102</span>]),
</span></span><span style=display:flex><span>  tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>])],
</span></span><span style=display:flex><span> [{<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_tokens&#39;</span>: <span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_input_ids&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>]),
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_mask&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>])},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;Sentences are passed as a list of string.&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;[CLS]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;sentences&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;are&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;passed&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;as&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;a&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;list&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;of&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;string&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;.&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;[SEP]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;[PAD]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;[PAD]&#39;</span>],
</span></span><span style=display:flex><span>  tensor([  <span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>11746</span>,  <span style=color:#ae81ff>2024</span>,  <span style=color:#ae81ff>2979</span>,  <span style=color:#ae81ff>2004</span>,  <span style=color:#ae81ff>1037</span>,  <span style=color:#ae81ff>2862</span>,  <span style=color:#ae81ff>1997</span>,  <span style=color:#ae81ff>5164</span>,  <span style=color:#ae81ff>1012</span>,
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>102</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>  tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>])],
</span></span><span style=display:flex><span> [{<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>9</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_tokens&#39;</span>: <span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_input_ids&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>]),
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_mask&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>])},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;The quick brown fox jumps over the lazy dog.&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;[CLS]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_tokens&#39;</span>: <span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_input_ids&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>]),
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_mask&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>])},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;The quick brown fox jumps over the lazy dog.&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;[CLS]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;the&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;quick&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;brown&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;fox&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;jumps&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;over&#39;</span>,
</span></span><span style=display:flex><span>  tensor([  <span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>11746</span>,  <span style=color:#ae81ff>2024</span>,  <span style=color:#ae81ff>2979</span>,  <span style=color:#ae81ff>2004</span>,  <span style=color:#ae81ff>1037</span>,  <span style=color:#ae81ff>2862</span>,  <span style=color:#ae81ff>1997</span>,  <span style=color:#ae81ff>5164</span>,  <span style=color:#ae81ff>1012</span>,
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>102</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>  tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>])],
</span></span><span style=display:flex><span> [{<span style=color:#e6db74>&#39;num_words&#39;</span>: <span style=color:#ae81ff>9</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_tokens&#39;</span>: <span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_input_ids&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>]),
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;len_mask&#39;</span>: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>13</span>])},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;The quick brown fox jumps over the lazy dog.&#39;</span>,
</span></span><span style=display:flex><span>  [<span style=color:#e6db74>&#39;[CLS]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;the&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;quick&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;brown&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;fox&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;jumps&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;over&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;the&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;lazy&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;dog&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;.&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;[SEP]&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;[PAD]&#39;</span>],
</span></span><span style=display:flex><span>  tensor([  <span style=color:#ae81ff>101</span>,  <span style=color:#ae81ff>1996</span>,  <span style=color:#ae81ff>4248</span>,  <span style=color:#ae81ff>2829</span>,  <span style=color:#ae81ff>4419</span>, <span style=color:#ae81ff>14523</span>,  <span style=color:#ae81ff>2058</span>,  <span style=color:#ae81ff>1996</span>, <span style=color:#ae81ff>13971</span>,  <span style=color:#ae81ff>3899</span>,
</span></span><span style=display:flex><span>           <span style=color:#ae81ff>1012</span>,   <span style=color:#ae81ff>102</span>,     <span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>  tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>])]]
</span></span></code></pre></div><p>09:36 ah interesting so the <code>[PAD]</code> is separate actually, and that corresponds to the additional <code>0</code> in the [[attention-mask]]</p><h3 id=jul-29th-2023-hmm>[[Jul 29th, 2023]] hmm<a hidden class=anchor aria-hidden=true href=#jul-29th-2023-hmm>#</a></h3><p>so back to that cool document, <a href=https://www.sbert.net/examples/applications/computing-embeddings/README.html>https://www.sbert.net/examples/applications/computing-embeddings/README.html</a></p><p>it was cool I saw you can easily do <code>tokenizer.convert_ids_to_tokens</code> and <code>tokenizer.convert_tokens_to_ids</code>, because I was able to veriffy that running</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>  sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span></code></pre></div><p>will add some additional <code>'[CLS]'</code> and <code>'[SEP]'</code> tokens at the beginning and end. I noticed since the length of the output was weird especially looking at the [[attention-mask]] . So that attention mask basically was showing that the three sentences being encoded had a final length that was the same number of ids and tokens , both 13, but there were some 0s at the end of some of the sentences. So that ended up being just yet another <code>[PAD]</code> token.
Ok so <code>'[CLS]</code> means special #BERT token for start of sequence and <code>[SEP]</code> is the separator between sequences . Yea and <code>[PAD]</code> just says, nothing to do here.
And the <code>padding=True</code> option is not about padding up to like [[context-window]] limit, it just says if you are passing multiple sentences to be encoded, to make them all equal length.
Anyway <code>384</code> is the size of the embedding, in this case, and it is not yet clear to me what is the [[what is relationship between size of input token sequence and dimension of embedding]] , there might be some [[dimensionality reduction]] right.
12:58 so continuing along then, next step was</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#Compute token embeddings</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>    model_output <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>encoded_input)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>143</span>]: vars(model_output)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>143</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;last_hidden_state&#39;</span>: tensor([[[ <span style=color:#ae81ff>0.2913</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2685</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2250</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.4261</span>,  <span style=color:#ae81ff>0.0493</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2095</span>],
</span></span><span style=display:flex><span>          [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.6272</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0421</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2452</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5336</span>,  <span style=color:#ae81ff>1.3115</span>,  <span style=color:#ae81ff>0.5999</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.0023</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2805</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4198</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2900</span>,  <span style=color:#ae81ff>1.5808</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4912</span>],
</span></span><span style=display:flex><span>          <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.1802</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5567</span>,  <span style=color:#ae81ff>0.0146</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.9311</span>,  <span style=color:#ae81ff>0.5940</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3536</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.0603</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2502</span>,  <span style=color:#ae81ff>0.5959</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.9435</span>,  <span style=color:#ae81ff>0.9465</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0680</span>],
</span></span><span style=display:flex><span>          [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.3356</span>,  <span style=color:#ae81ff>0.0650</span>,  <span style=color:#ae81ff>0.1109</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.0801</span>,  <span style=color:#ae81ff>0.2653</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2762</span>]],
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>         [[ <span style=color:#ae81ff>0.0856</span>,  <span style=color:#ae81ff>0.1876</span>,  <span style=color:#ae81ff>0.0488</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1204</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0907</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1662</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.1291</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0266</span>,  <span style=color:#ae81ff>0.6318</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.7958</span>,  <span style=color:#ae81ff>0.1555</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.2737</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.0062</span>,  <span style=color:#ae81ff>0.2263</span>,  <span style=color:#ae81ff>0.1851</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.3981</span>,  <span style=color:#ae81ff>0.6461</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2192</span>],
</span></span><span style=display:flex><span>          <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.3036</span>,  <span style=color:#ae81ff>0.3740</span>,  <span style=color:#ae81ff>0.2523</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.6319</span>,  <span style=color:#ae81ff>0.5731</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2901</span>],
</span></span><span style=display:flex><span>          [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2124</span>,  <span style=color:#ae81ff>0.2626</span>,  <span style=color:#ae81ff>0.6867</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5504</span>,  <span style=color:#ae81ff>0.7065</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4728</span>],
</span></span><span style=display:flex><span>          [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2220</span>,  <span style=color:#ae81ff>0.2086</span>,  <span style=color:#ae81ff>0.6693</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5410</span>,  <span style=color:#ae81ff>0.5683</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3963</span>]],
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>         [[ <span style=color:#ae81ff>0.0464</span>,  <span style=color:#ae81ff>0.3381</span>,  <span style=color:#ae81ff>0.2082</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.2766</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0861</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0358</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.1162</span>,  <span style=color:#ae81ff>0.2264</span>,  <span style=color:#ae81ff>0.1021</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1858</span>,  <span style=color:#ae81ff>0.4895</span>,  <span style=color:#ae81ff>1.2175</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.1537</span>,  <span style=color:#ae81ff>0.1730</span>,  <span style=color:#ae81ff>0.5151</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.3720</span>,  <span style=color:#ae81ff>0.3621</span>,  <span style=color:#ae81ff>0.5758</span>],
</span></span><span style=display:flex><span>          <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.3883</span>,  <span style=color:#ae81ff>0.2813</span>,  <span style=color:#ae81ff>0.0309</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.3264</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1039</span>,  <span style=color:#ae81ff>0.5856</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.3477</span>,  <span style=color:#ae81ff>0.0940</span>,  <span style=color:#ae81ff>0.2564</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1463</span>,  <span style=color:#ae81ff>0.1743</span>,  <span style=color:#ae81ff>0.5586</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.1911</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0142</span>,  <span style=color:#ae81ff>0.3021</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1814</span>,  <span style=color:#ae81ff>0.2111</span>,  <span style=color:#ae81ff>0.2329</span>]]]),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;pooler_output&#39;</span>: tensor([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0417</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0041</span>,  <span style=color:#ae81ff>0.0332</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.0117</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0634</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0058</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0227</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0248</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0112</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.0482</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1108</span>,  <span style=color:#ae81ff>0.0122</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0663</span>,  <span style=color:#ae81ff>0.0281</span>,  <span style=color:#ae81ff>0.0706</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.0258</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0222</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0608</span>]]),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;hidden_states&#39;</span>: <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;past_key_values&#39;</span>: <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;attentions&#39;</span>: <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;cross_attentions&#39;</span>: <span style=color:#66d9ef>None</span>}
</span></span></code></pre></div><p>Ok cool, so seeing each input is embedded separately, we have size of 3 rows here, like 3 input encodings ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>146</span>]: [model_output<span style=color:#f92672>.</span>last_hidden_state<span style=color:#f92672>.</span>shape, model_output<span style=color:#f92672>.</span>pooler_output<span style=color:#f92672>.</span>shape]
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>146</span>]: [torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>384</span>]), torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>384</span>])]
</span></span></code></pre></div><p>Hmm and last hidden state, with the 13 wonder if those are 13 [[attention-head]] ? And then the <code>pooler_output</code> what combines these then ?
13:29 oh never mind #moment/doh #moment/duh that is 13 because thre are 13 tokens in each sentence ! [[moment/haha]] . üòÄ</p><p>13:14 ok so final [[average-pooling]] mean pooling step</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#Perform pooling. In this case, mean pooling</span>
</span></span><span style=display:flex><span>sentence_embeddings <span style=color:#f92672>=</span> mean_pooling(model_output, encoded_input[<span style=color:#e6db74>&#39;attention_mask&#39;</span>])
</span></span></code></pre></div><p>Ok so you can use key or index to access these,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>155</span>]: np<span style=color:#f92672>.</span>allclose(model_output<span style=color:#f92672>.</span>last_hidden_state, model_output[<span style=color:#ae81ff>0</span>]), np<span style=color:#f92672>.</span>allclose(model_output<span style=color:#f92672>.</span>pooler_output, model_output[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>155</span>]: (<span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>13:24 Not yet clear why this additional unsqueeze [[numpy unsqueeze ]] step is done</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>163</span>]: attention_mask <span style=color:#f92672>=</span> encoded_input[<span style=color:#e6db74>&#39;attention_mask&#39;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: attention_mask<span style=color:#f92672>.</span>shape, attention_mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>163</span>]: (torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>]), torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>1</span>]))
</span></span></code></pre></div><p>Ah ok interesting,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>166</span>]: token_embeddings <span style=color:#f92672>=</span> model_output[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>167</span>]:     input_mask_expanded <span style=color:#f92672>=</span> attention_mask<span style=color:#f92672>.</span>unsqueeze(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>expand(token_embeddings<span style=color:#f92672>.</span>size())<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>168</span>]: input_mask_expanded
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>168</span>]: 
</span></span><span style=display:flex><span>tensor([[[<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>,  <span style=color:#f92672>...</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.</span>]]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>169</span>]: input_mask_expanded<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>169</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>384</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>170</span>]: (token_embeddings<span style=color:#f92672>.</span>size())
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>170</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>384</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>171</span>]: input_mask_expanded[<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>,:]
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>171</span>]: 
</span></span><span style=display:flex><span>tensor([<span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>])
</span></span></code></pre></div><p>So this interesting unsqueeze then expand pattern , to the 384 size of the embedding dimension,
13:30 so yea literally the output is</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>172</span>]: token_embeddings<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>172</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>384</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>173</span>]: token_embeddings
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>173</span>]: 
</span></span><span style=display:flex><span>tensor([[[ <span style=color:#ae81ff>0.2913</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2685</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2250</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.4261</span>,  <span style=color:#ae81ff>0.0493</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2095</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.6272</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0421</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2452</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5336</span>,  <span style=color:#ae81ff>1.3115</span>,  <span style=color:#ae81ff>0.5999</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.0023</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2805</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4198</span>,  <span style=color:#f92672>...</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2900</span>,  <span style=color:#ae81ff>1.5808</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4912</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1802</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5567</span>,  <span style=color:#ae81ff>0.0146</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.9311</span>,  <span style=color:#ae81ff>0.5940</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3536</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.0603</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2502</span>,  <span style=color:#ae81ff>0.5959</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.9435</span>,  <span style=color:#ae81ff>0.9465</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0680</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.3356</span>,  <span style=color:#ae81ff>0.0650</span>,  <span style=color:#ae81ff>0.1109</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.0801</span>,  <span style=color:#ae81ff>0.2653</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2762</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>0.0856</span>,  <span style=color:#ae81ff>0.1876</span>,  <span style=color:#ae81ff>0.0488</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1204</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0907</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1662</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1291</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0266</span>,  <span style=color:#ae81ff>0.6318</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.7958</span>,  <span style=color:#ae81ff>0.1555</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.2737</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.0062</span>,  <span style=color:#ae81ff>0.2263</span>,  <span style=color:#ae81ff>0.1851</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.3981</span>,  <span style=color:#ae81ff>0.6461</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2192</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.3036</span>,  <span style=color:#ae81ff>0.3740</span>,  <span style=color:#ae81ff>0.2523</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.6319</span>,  <span style=color:#ae81ff>0.5731</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.2901</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2124</span>,  <span style=color:#ae81ff>0.2626</span>,  <span style=color:#ae81ff>0.6867</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5504</span>,  <span style=color:#ae81ff>0.7065</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.4728</span>],
</span></span><span style=display:flex><span>         [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2220</span>,  <span style=color:#ae81ff>0.2086</span>,  <span style=color:#ae81ff>0.6693</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.5410</span>,  <span style=color:#ae81ff>0.5683</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.3963</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>0.0464</span>,  <span style=color:#ae81ff>0.3381</span>,  <span style=color:#ae81ff>0.2082</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.2766</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0861</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0358</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1162</span>,  <span style=color:#ae81ff>0.2264</span>,  <span style=color:#ae81ff>0.1021</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1858</span>,  <span style=color:#ae81ff>0.4895</span>,  <span style=color:#ae81ff>1.2175</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1537</span>,  <span style=color:#ae81ff>0.1730</span>,  <span style=color:#ae81ff>0.5151</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>1.3720</span>,  <span style=color:#ae81ff>0.3621</span>,  <span style=color:#ae81ff>0.5758</span>],
</span></span><span style=display:flex><span>         <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.3883</span>,  <span style=color:#ae81ff>0.2813</span>,  <span style=color:#ae81ff>0.0309</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.3264</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1039</span>,  <span style=color:#ae81ff>0.5856</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.3477</span>,  <span style=color:#ae81ff>0.0940</span>,  <span style=color:#ae81ff>0.2564</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1463</span>,  <span style=color:#ae81ff>0.1743</span>,  <span style=color:#ae81ff>0.5586</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>0.1911</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0142</span>,  <span style=color:#ae81ff>0.3021</span>,  <span style=color:#f92672>...</span>,  <span style=color:#ae81ff>0.1814</span>,  <span style=color:#ae81ff>0.2111</span>,  <span style=color:#ae81ff>0.2329</span>]]])
</span></span></code></pre></div><p>at does the model_output.pooler_output mean then? Is that also doing a average of the 13 tokens?
13:33 The final step makes a bit more sense now</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    sum_embeddings <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(token_embeddings <span style=color:#f92672>*</span> input_mask_expanded, <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>because it is just saying, dont take into account the stuff that the mask masked away.
And we add them, and divide by length for each sentence,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>178</span>]:     sum_embeddings <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(token_embeddings <span style=color:#f92672>*</span> input_mask_expanded, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     sum_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>clamp(input_mask_expanded<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>), min<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-9</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>179</span>]: pooled <span style=color:#f92672>=</span> sum_embeddings <span style=color:#f92672>/</span> sum_mask
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>180</span>]: pooled<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>180</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>384</span>])
</span></span></code></pre></div><p>too of course.
16:34 Ok so since I now basically know, the [[average-pooling]] of [[sentence-transformers]] indeed is just literally averaging each word, yes in the 384 dimensions but yea any unimportant word should most certainly be removed before average pooling</p><p>And I still have that pretty critical two-part question, so if we are using [[subword-tokenization]] , and therefore a concept is going to be spread apart to multiple tokens, does that mean we are relying on this multi-dimensional averaging to somehow maintain the meaning of a word that was broken up into pieces?
So the [[subword-tokenization]] clearly I now understand is a statistical procedure unrelated to the #[[supervised fine-tuning]] step and yea likely that the more common sub-words will end up being longer subwords after [[why a custom tokenizer]] , but most likely a concept still gets broken apart, into multiple sub-words, so then ultimately does that not really matter, because upon computing [[cosine similarity]], another sentence which has the same exact [[jargon]] word will be split up in the same way.
And maybe even if a model is not #[[supervised fine-tuning]] with a new corpus, we may still benefit from at least words with a common word parts being close dimensionally, right,
18:20 hmm so a dead super simple test, of above sub-word theory,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pyspark&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>189</span>]: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>189</span>]: tensor([[<span style=color:#ae81ff>0.4150</span>]])
</span></span></code></pre></div><p>Ok kind of thought so.
18:47 added one more option for myself there,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>reload(ut)
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pyspark&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>    model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;python&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;p&#39;</span>, <span style=color:#e6db74>&#39;##ys&#39;</span>, <span style=color:#e6db74>&#39;##park&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span></code></pre></div><p>That&rsquo;s terrible haha ok no wonder the cosine similarity is so low, <code>0.415</code> haha.
18:51 one more example to try, hmm, this one should be obvious ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;postgresql&#34;</span>, <span style=color:#e6db74>&#34;database&#34;</span>]
</span></span><span style=display:flex><span>encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>    model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;post&#39;</span>, <span style=color:#e6db74>&#39;##gre&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;##q&#39;</span>, <span style=color:#e6db74>&#39;##l&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;database&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.5301</span>]])
</span></span></code></pre></div><p>hmm haha thats really bad I think
One more</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;postgresql&#34;</span>, <span style=color:#e6db74>&#34;sql&#34;</span>]
</span></span><span style=display:flex><span>encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>    model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;post&#39;</span>, <span style=color:#e6db74>&#39;##gre&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;##q&#39;</span>, <span style=color:#e6db74>&#39;##l&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;sql&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.5085</span>]])
</span></span></code></pre></div><p>hmm yea this is no good. Whatever this is, it is terrible I think
It should be as good as this,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>200</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;banana&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;banana&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;apple&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.4240</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>201</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;fruit&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;fruit&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;apple&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.5372</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>202</span>]: 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>202</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;macintosh&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;macintosh&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;apple&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.7044</span>]])
</span></span></code></pre></div><p>18:57 ok haha I&rsquo;m confused. these are also kind of bad.</p><h3 id=jul-31st-2023-ok-interesting-removing-the-special-tokens-has-no-effect-on-the-cosine-similarity>[[Jul 31st, 2023]] ok interesting, removing the special tokens has no effect on the cosine similarity<a hidden class=anchor aria-hidden=true href=#jul-31st-2023-ok-interesting-removing-the-special-tokens-has-no-effect-on-the-cosine-similarity>#</a></h3><p>so following from [[Jul 29th, 2023]] I was going to remove the special tokens try again,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>texts <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;macintosh&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>    texts, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>    cls_token<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    sep_token<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>    return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span></code></pre></div><p>hmm that didn&rsquo;t work,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>TypeError</span>: _batch_encode_plus() got an unexpected keyword argument <span style=color:#e6db74>&#39;cls_token&#39;</span>
</span></span></code></pre></div><p>how about,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>texts <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;macintosh&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>    texts, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>    add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span></code></pre></div><p>09:14 ok nice yes that did it !</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>206</span>]: encoded_input
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>206</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;input_ids&#39;</span>: tensor([[<span style=color:#ae81ff>22228</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>6207</span>]]), <span style=color:#e6db74>&#39;token_type_ids&#39;</span>: tensor([[<span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0</span>]]), <span style=color:#e6db74>&#39;attention_mask&#39;</span>: tensor([[<span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>1</span>]])}
</span></span></code></pre></div><p>Let&rsquo;s then compare cosine similarity between these, with and without the special tokens present.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>encoded_to_embeddings(encoded_input, model_id)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.7044</span>]])
</span></span></code></pre></div><p>09:27 ok looks like didn&rsquo;t make a difference. Try one more,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;fruit&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> add_special_tokens <span style=color:#f92672>in</span> [<span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>False</span>]:
</span></span><span style=display:flex><span>    encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>        sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>        add_special_tokens<span style=color:#f92672>=</span>add_special_tokens,
</span></span><span style=display:flex><span>        return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>encoded_to_embeddings(encoded_input, model_id)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;add_special_tokens:&#34;</span>, add_special_tokens, <span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>True</span> cosine similarity tensor([[<span style=color:#ae81ff>0.2258</span>]])
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>False</span> cosine similarity tensor([[<span style=color:#ae81ff>1.0000</span>]])
</span></span></code></pre></div><p>09:34 oops, have some kind of bug in the mean pooling code I think.</p><h3 id=aug-1st-2023-trying-a-few-more-things-wondering-why-weird-cosine-similarity-1-w-single-words-encoded-but-different-words>[[Aug 1st, 2023]] trying a few more things wondering why weird cosine similarity 1 w/ single words encoded but different words<a hidden class=anchor aria-hidden=true href=#aug-1st-2023-trying-a-few-more-things-wondering-why-weird-cosine-similarity-1-w-single-words-encoded-but-different-words>#</a></h3><p>ok what is bug from yesterday then ?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;fruit&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> add_special_tokens <span style=color:#f92672>in</span> [<span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>False</span>]:
</span></span><span style=display:flex><span>    encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>        sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>        add_special_tokens<span style=color:#f92672>=</span>add_special_tokens,
</span></span><span style=display:flex><span>        return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>encoded_to_embeddings(encoded_input, model_id)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;add_special_tokens:&#34;</span>, add_special_tokens, <span style=color:#e6db74>&#34;cosine similarity&#34;</span>, 
</span></span><span style=display:flex><span>          cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;all close&#34;</span>, np<span style=color:#f92672>.</span>allclose(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>True</span> cosine similarity tensor([[<span style=color:#ae81ff>0.5372</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>False</span> cosine similarity tensor([[<span style=color:#ae81ff>1.0000</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>217</span>]: embeddings<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>217</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>384</span>])
</span></span></code></pre></div><p>hmm weird, yea spot checked, they don&rsquo;t look identical actually, but similar,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>218</span>]: np<span style=color:#f92672>.</span>transpose(embeddings)<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>218</span>]: torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>384</span>, <span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>219</span>]: np<span style=color:#f92672>.</span>transpose(embeddings)[:<span style=color:#ae81ff>5</span>]
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>219</span>]: 
</span></span><span style=display:flex><span>tensor([[ <span style=color:#ae81ff>0.0025</span>,  <span style=color:#ae81ff>0.0021</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.0335</span>,  <span style=color:#ae81ff>0.0337</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.0014</span>,  <span style=color:#ae81ff>0.0013</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0084</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0081</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0206</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0208</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>n [<span style=color:#ae81ff>220</span>]: np<span style=color:#f92672>.</span>transpose(embeddings)[<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>:]
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>220</span>]: 
</span></span><span style=display:flex><span>tensor([[ <span style=color:#ae81ff>0.0678</span>,  <span style=color:#ae81ff>0.0675</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.1426</span>,  <span style=color:#ae81ff>0.1423</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0018</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0016</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.1755</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1752</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0967</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.0969</span>]])
</span></span></code></pre></div><p>but it is for sure a bug since, this happens for absurd cases,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;fruit&#34;</span>, <span style=color:#e6db74>&#34;couch&#34;</span>]
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> add_special_tokens <span style=color:#f92672>in</span> [<span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>False</span>]:
</span></span><span style=display:flex><span>    encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>        sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>        add_special_tokens<span style=color:#f92672>=</span>add_special_tokens,
</span></span><span style=display:flex><span>        return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>encoded_to_embeddings(encoded_input, model_id)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;add_special_tokens:&#34;</span>, add_special_tokens, <span style=color:#e6db74>&#34;cosine similarity&#34;</span>, 
</span></span><span style=display:flex><span>          cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;all close&#34;</span>, np<span style=color:#f92672>.</span>allclose(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>True</span> cosine similarity tensor([[<span style=color:#ae81ff>0.2795</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>False</span> cosine similarity tensor([[<span style=color:#ae81ff>1.0000</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>pdb trace,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>224</span>]: ipdb<span style=color:#f92672>.</span>runcall(ut<span style=color:#f92672>.</span>encoded_to_embeddings, encoded_input, model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ipdb<span style=color:#f92672>&gt;</span> p encoded_input
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;input_ids&#39;</span>: tensor([[<span style=color:#ae81ff>5909</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>6411</span>]]), <span style=color:#e6db74>&#39;token_type_ids&#39;</span>: tensor([[<span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0</span>]]), <span style=color:#e6db74>&#39;attention_mask&#39;</span>: tensor([[<span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>1</span>]])}
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>ipdb<span style=color:#f92672>&gt;</span> p model_output<span style=color:#f92672>.</span>last_hidden_state<span style=color:#f92672>.</span>shape, model_output<span style=color:#f92672>.</span>pooler_output<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>(torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>384</span>]), torch<span style=color:#f92672>.</span>Size([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>384</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cos_sim(model_output<span style=color:#f92672>.</span>last_hidden_state[<span style=color:#ae81ff>0</span>,:,:], model_output<span style=color:#f92672>.</span>last_hidden_state[<span style=color:#ae81ff>1</span>,:,:])
</span></span><span style=display:flex><span>tensor([[<span style=color:#ae81ff>1.0000</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cos_sim(model_output<span style=color:#f92672>.</span>pooler_output[<span style=color:#ae81ff>0</span>, :], model_output<span style=color:#f92672>.</span>pooler_output[<span style=color:#ae81ff>1</span>, :])
</span></span><span style=display:flex><span>tensor([[<span style=color:#ae81ff>1.0000</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>allclose(model_output<span style=color:#f92672>.</span>pooler_output[<span style=color:#ae81ff>0</span>, :], model_output<span style=color:#f92672>.</span>pooler_output[<span style=color:#ae81ff>1</span>, :])
</span></span><span style=display:flex><span><span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>weird ok so this happens before the <code>mean_pooling</code> func gets called. weird.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;the fruit is edible&#34;</span>, <span style=color:#e6db74>&#34;this couch is on sale&#34;</span>]
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> add_special_tokens <span style=color:#f92672>in</span> [<span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>False</span>]:
</span></span><span style=display:flex><span>    encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>        sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>        add_special_tokens<span style=color:#f92672>=</span>add_special_tokens,
</span></span><span style=display:flex><span>        return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>encoded_to_embeddings(encoded_input, model_id)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;add_special_tokens:&#34;</span>, add_special_tokens, <span style=color:#e6db74>&#34;cosine similarity&#34;</span>, 
</span></span><span style=display:flex><span>          cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;all close&#34;</span>, np<span style=color:#f92672>.</span>allclose(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>True</span> cosine similarity tensor([[<span style=color:#ae81ff>0.0687</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>False</span> cosine similarity tensor([[<span style=color:#ae81ff>0.0756</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>ok so then something weird going on w/ a single token hmm?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;there is fruit on the table&#34;</span>, <span style=color:#e6db74>&#34;look at the table there is fruit&#34;</span>]
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> add_special_tokens <span style=color:#f92672>in</span> [<span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>False</span>]:
</span></span><span style=display:flex><span>    encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>        sentences, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, 
</span></span><span style=display:flex><span>        add_special_tokens<span style=color:#f92672>=</span>add_special_tokens,
</span></span><span style=display:flex><span>        return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>encoded_to_embeddings(encoded_input, model_id)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;add_special_tokens:&#34;</span>, add_special_tokens, <span style=color:#e6db74>&#34;cosine similarity&#34;</span>, 
</span></span><span style=display:flex><span>          cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;all close&#34;</span>, np<span style=color:#f92672>.</span>allclose(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>True</span> cosine similarity tensor([[<span style=color:#ae81ff>0.9155</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>add_special_tokens: <span style=color:#66d9ef>False</span> cosine similarity tensor([[<span style=color:#ae81ff>0.9066</span>]])
</span></span><span style=display:flex><span>all close <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>09:34 dont know why the output from the model produces nearly same embedding, for a single word encoded, but multiple words, it seems to be working fine.
guess more multi-word experiments then next .</p><h3 id=aug-2nd-2023-interesting-attempts-around-single-and-multiword-embeddings>[[Aug 2nd, 2023]] interesting attempts around single and multiword embeddings<a hidden class=anchor aria-hidden=true href=#aug-2nd-2023-interesting-attempts-around-single-and-multiword-embeddings>#</a></h3><p>Since like I saw yesterday, I can get high <code>0.90s</code> score if I have a longer sentence, and for a single word, there is some kind of weird bug</p><p>So for just <code>["fruit", "apple"]</code> I had</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;fruit&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>229</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;fruit&#34;</span>, <span style=color:#e6db74>&#34;apple&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>229</span>]: tensor([[<span style=color:#ae81ff>0.5372</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>230</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;a fruit&#34;</span>, <span style=color:#e6db74>&#34;my apple&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>230</span>]: tensor([[<span style=color:#ae81ff>0.4532</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>231</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;its some fruit&#34;</span>, <span style=color:#e6db74>&#34;here my apple&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>231</span>]: tensor([[<span style=color:#ae81ff>0.3700</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>232</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;its some fruit juice&#34;</span>, <span style=color:#e6db74>&#34;here my apple sauce&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>232</span>]: tensor([[<span style=color:#ae81ff>0.4277</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>233</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;its some fruit juice home made&#34;</span>, <span style=color:#e6db74>&#34;here my apple sauce custom recipe&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>233</span>]: tensor([[<span style=color:#ae81ff>0.3862</span>]])
</span></span></code></pre></div><p>Ok I don&rsquo;t know haha, might not solve this mystery right now.
It might also be that hmm not all words are as close together as I thought?
09:05 ok yea haha, indeed I found some better single-word examples.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>234</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;couch&#34;</span>, <span style=color:#e6db74>&#34;sofa&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>234</span>]: tensor([[<span style=color:#ae81ff>0.8564</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>235</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;hammock&#34;</span>, <span style=color:#e6db74>&#34;bed&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>235</span>]: tensor([[<span style=color:#ae81ff>0.2903</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>236</span>]: sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;mattress&#34;</span>, <span style=color:#e6db74>&#34;bed&#34;</span>]
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(model_id, sentences)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>236</span>]: tensor([[<span style=color:#ae81ff>0.6860</span>]])
</span></span></code></pre></div><p>So yea putting the single-word-bug theory to rest, at least when using CLS, SEP <code>add_special_tokens=True</code> there is no problem. And without CLS, SEP, using <code>add_special_tokens=False</code> then yea, the embeddings for both inputs are nearly the same producing cosine similarity of 1. That&rsquo;s really weird. So I should stick to using <code>add_special_tokens=True</code> for now at least for this model.
Ok back to big picture then,</p><p>So I have observed this model has bad performance when I try throwing technical [[jargon]] at it,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>reload(ut)
</span></span><span style=display:flex><span>model_id <span style=color:#f92672>=</span>  <span style=color:#e6db74>&#34;sentence-transformers/all-MiniLM-L6-v2&#34;</span>
</span></span><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;pyspark&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>    model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;python&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;p&#39;</span>, <span style=color:#e6db74>&#39;##ys&#39;</span>, <span style=color:#e6db74>&#39;##park&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;postgresql&#34;</span>, <span style=color:#e6db74>&#34;database&#34;</span>]
</span></span><span style=display:flex><span>encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>    model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;post&#39;</span>, <span style=color:#e6db74>&#39;##gre&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;##q&#39;</span>, <span style=color:#e6db74>&#39;##l&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;database&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.5301</span>]])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sentences <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;postgresql&#34;</span>, <span style=color:#e6db74>&#34;sql&#34;</span>]
</span></span><span style=display:flex><span>encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>    model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;cosine similarity&#34;</span>, cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;post&#39;</span>, <span style=color:#e6db74>&#39;##gre&#39;</span>, <span style=color:#e6db74>&#39;##s&#39;</span>, <span style=color:#e6db74>&#39;##q&#39;</span>, <span style=color:#e6db74>&#39;##l&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;sql&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>cosine similarity tensor([[<span style=color:#ae81ff>0.5085</span>]])
</span></span></code></pre></div><p>I have tried building a tokenizer from this model&rsquo;s tokenizer, but that was problematic because it is not tokenizer fine tuning like model fine tuning, it just uses the same class.
So initially I was thinking building a new tokenizer means I need all billion examples earlier, hmm but that&rsquo;s just what went in to fine tuning the model. Maybe for a tokenizer, perhaps I just need to build a dataset that has a good sampling mix of English language and a healthy proportion of technical language.
But I think before doing that, I would like to take another stab at understanding, how to answer the more general question, about [[subword-tokenization]] , so for non jargon language, any tokenizer will still end up having plenty of subwords, but they will still end up with good embeddings right? So since subwords are split up into multiple embeddings, then maybe is it that the model just associates those subword embeddings appropriately then? So is it like you identify that multi-syllable words have the same #etymology roots perhaps, like
like, &ldquo;charismatic&rdquo; , &ldquo;charisma&rdquo; and &ldquo;character&rdquo; and &ldquo;characterization&rdquo; ,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>In [<span style=color:#ae81ff>242</span>]: sentences <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     <span style=color:#e6db74>&#34;charismatic&#34;</span> , <span style=color:#e6db74>&#34;charisma&#34;</span>, <span style=color:#e6db74>&#34;character&#34;</span>, <span style=color:#e6db74>&#34;characterization&#34;</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: ]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>243</span>]: encoded_input, embeddings <span style=color:#f92672>=</span> ut<span style=color:#f92672>.</span>vec_to_embeddings(
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>:     model_id, sentences, return_tokenizer_output<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_id)
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>0</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>1</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>2</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: print(tokenizer<span style=color:#f92672>.</span>convert_ids_to_tokens(encoded_input<span style=color:#f92672>.</span>data[<span style=color:#e6db74>&#34;input_ids&#34;</span>][<span style=color:#ae81ff>3</span>, :]))
</span></span><span style=display:flex><span>     <span style=color:#f92672>...</span>: 
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;charismatic&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;char&#39;</span>, <span style=color:#e6db74>&#39;##ism&#39;</span>, <span style=color:#e6db74>&#39;##a&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;character&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;[CLS]&#39;</span>, <span style=color:#e6db74>&#39;characterization&#39;</span>, <span style=color:#e6db74>&#39;[SEP]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[PAD]&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>244</span>]: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>1</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>244</span>]: tensor([[<span style=color:#ae81ff>0.5882</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>245</span>]: cos_sim(embeddings[<span style=color:#ae81ff>0</span>, :], embeddings[<span style=color:#ae81ff>2</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>245</span>]: tensor([[<span style=color:#ae81ff>0.4623</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>246</span>]: cos_sim(embeddings[<span style=color:#ae81ff>1</span>, :], embeddings[<span style=color:#ae81ff>2</span>,:])
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>246</span>]: tensor([[<span style=color:#ae81ff>0.6060</span>]])
</span></span></code></pre></div><p>yea maybe something like that happens with an embedding model then, it can still embed subwords well. Should try to do some reading on this .</p><p>ok</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2023-02-18-fact-checking-dark-humor/><span class=title>¬´ Prev</span><br><span>fact checking dark humor</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2023-02-18-first-stab-langchain-interview-me/><span class=title>Next ¬ª</span><br><span>Using langchain to interview myself about my skills</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>