<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Citibike Learn Project | michal.piekarczyk.xyz</title><meta name=keywords content><meta name=description content="Can your Bike Share Destination be Predicted"><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/project/2016-12-18-citibike-project/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Citibike Learn Project"><meta property="og:description" content="Can your Bike Share Destination be Predicted"><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/project/2016-12-18-citibike-project/"><meta property="og:image" content="https://my-blog-content.s3.amazonaws.com/2020/CartShare.jpeg"><meta property="article:section" content="project"><meta property="article:published_time" content="2016-12-18T18:33:28+00:00"><meta property="article:modified_time" content="2023-02-26T19:27:16-05:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-content.s3.amazonaws.com/2020/CartShare.jpeg"><meta name=twitter:title content="Citibike Learn Project"><meta name=twitter:description content="Can your Bike Share Destination be Predicted"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Side Projects","item":"https://michal.piekarczyk.xyz/project/"},{"@type":"ListItem","position":2,"name":"Citibike Learn Project","item":"https://michal.piekarczyk.xyz/project/2016-12-18-citibike-project/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Citibike Learn Project","name":"Citibike Learn Project","description":"Can your Bike Share Destination be Predicted","keywords":[],"articleBody":"Citibike Project: Can your Destination be Predicted ( https://github.com/namoopsoo/learn-citibike )\nMotivation I think sometimes the most interesting projects live behind ideas that sound impractical or even crazy. That’s why I thought it would be fun to use the Citibike bike share trip data to try and predict a person’s destination based on what we know.\nRoughly speaking trip data looks like this\n\"tripduration\",\"starttime\",\"stoptime\",\"start station id\",\"start station name\",\"start station latitude\",\"start station longitude\",\"end station id\",\"end station name\",\"end station latitude\",\"end station longitude\",\"bikeid\",\"usertype\",\"birth year\",\"gender\" \"171\",\"10/1/2015 00:00:02\",\"10/1/2015 00:02:54\",\"388\",\"W 26 St \u0026 10 Ave\",\"40.749717753\",\"-74.002950346\",\"494\",\"W 26 St \u0026 8 Ave\",\"40.74734825\",\"-73.99723551\",\"24302\",\"Subscriber\",\"1973\",\"1\" \"593\",\"10/1/2015 00:00:02\",\"10/1/2015 00:09:55\",\"518\",\"E 39 St \u0026 2 Ave\",\"40.74780373\",\"-73.9734419\",\"438\",\"St Marks Pl \u0026 1 Ave\",\"40.72779126\",\"-73.98564945\",\"19904\",\"Subscriber\",\"1990\",\"1\" \"233\",\"10/1/2015 00:00:11\",\"10/1/2015 00:04:05\",\"447\",\"8 Ave \u0026 W 52 St\",\"40.76370739\",\"-73.9851615\",\"447\",\"8 Ave \u0026 W 52 St\",\"40.76370739\",\"-73.9851615\",\"17797\",\"Subscriber\",\"1984\",\"1\" \"250\",\"10/1/2015 00:00:15\",\"10/1/2015 00:04:25\",\"336\",\"Sullivan St \u0026 Washington Sq\",\"40.73047747\",\"-73.99906065\",\"223\",\"W 13 St \u0026 7 Ave\",\"40.73781509\",\"-73.99994661\",\"23966\",\"Subscriber\",\"1984\",\"1\" \"528\",\"10/1/2015 00:00:17\",\"10/1/2015 00:09:05\",\"3107\",\"Bedford Ave \u0026 Nassau Ave\",\"40.72311651\",\"-73.95212324\",\"539\",\"Metropolitan Ave \u0026 Bedford Ave\",\"40.71534825\",\"-73.96024116\",\"16246\",\"Customer\",\"\",\"0\" \"440\",\"10/1/2015 00:00:17\",\"10/1/2015 00:07:37\",\"3107\",\"Bedford Ave \u0026 Nassau Ave\",\"40.72311651\",\"-73.95212324\",\"539\",\"Metropolitan Ave \u0026 Bedford Ave\",\"40.71534825\",\"-73.96024116\",\"23698\",\"Customer\",\"\",\"0\" The data if fairly clean and regular, so I thought this was a fun data set to sharpen my teeth on.\nQuick Bird’s Eye of my Journey First started just looking at this data. Just out of curiosity, as a first mini starter project I decided to look at the relationship between rider age and speed I realized pretty early that the bike station target was too small, so I started using the Google Geolocation API to get broader location data such as zip codes and neighborhoods . geolocation I also thought on a high level that knowing whether you got on your bike at 4:05 in the afternoon versus 4:06 shouldn’t influence my learning algorithm, so I added some more transformations. I compared the prediction accuracy of the new geolocation data as a first stab I ran through a couple of modeling scenarios here and finding some impoartant inconsistencies in how I was running my testing I ended up getting some good gains by binarizing my inputs I ended up using a different evaluation metric again to get a different perspective on this problem. Maybe a year or more later I came back to this problem from the point of view of experimenting with AWS approaches to training models , including Docker and SageMaker. Thoughts for future improvements More on this data When I started looking at this data, there were 400+ stations for docking your citibike. There is age, and some of the riders were actually born in the 1800s, which is kind of cool. df = load_data('data/201509_10-citibike-tripdata.csv.annotated.100000.06112016T1814.csv') In [6]: df['birth year'].describe() Out[6]: count 83171.000000 mean 1977.149680 std 11.400096 min 1885.000000 25% 1969.000000 50% 1980.000000 75% 1986.000000 max 1999.000000 Name: birth year, dtype: float64 Speed and Age Turns out that you need to know the miles per the longitude degree at a particular latitude on our planet. So for our particular location, at lat around 40.723 and using the earth’s radius of about 3958 miles , we have about 52.3 miles/longitude degree here in NYC.\nSo from there, looking at some of the speed data just involved looking at the tripdata trip time and calculating the cartesian distance.\n(More on the code here ) (Also more detail on this analysis in the main jupyter notebook)\nNeed additional location data With the 400+ stations, trying to predict a multi-class problem of this sort with basic machine learning algorithms would not be a way to get quick results to help keep the project going, so I decided to constrain the problem. I ended up turning to the Google Geocoding API. Using this data became its own side project, because parsing through the Google geolocation data can get pretty hairy! The meat of the output can look like this, for the docking station “1st Avenue \u0026 E 15th St”\n{ 'raw_result': [{u'address_components': [{u'long_name': u'1st Avenue', u'short_name': u'1st Avenue', u'types': [u'route']}, {u'long_name': u'Midtown', u'short_name': u'Midtown', u'types': [u'neighborhood', u'political']}, {u'long_name': u'Manhattan', u'short_name': u'Manhattan', u'types': [u'sublocality_level_1', u'sublocality', u'political']}, {u'long_name': u'New York', u'short_name': u'New York', u'types': [u'locality', u'political']}, {u'long_name': u'New York County', u'short_name': u'New York County', u'types': [u'administrative_area_level_2', u'political']}, {u'long_name': u'New York', u'short_name': u'NY', u'types': [u'administrative_area_level_1', u'political']}, {u'long_name': u'United States', u'short_name': u'US', u'types': [u'country', u'political']}, {u'long_name': u'10003', u'short_name': u'10003', u'types': [u'postal_code']}], u'formatted_address': u'1st Avenue \u0026 E 15th St, New York, NY 10003, USA', } Some of the challenges here were that the outputs from the API were not always consistent. The above output shows that the\n'neighborhood' is 'Midtown', but because there were 400+ stations, I did not initially notice that sometimes the neighborhood was missing or that the zip code was missing. That ended up throwing off my code a couple of times. It turned out that the street intersection was not an ideal clean data input to the API. For instance E 3 St \u0026 1 Ave, NY was understood by the API as just a street or a route as it is called, ( raw output ). Later on I ended up refactoring this to use the raw latitude and longitude . However, I eventually noticed that often times the docking stations were on the edge of neighborhoods. So I literally had edge cases! The Neighborhood would come back blank and I ended up having to fill in a lot of that data by hand anyhow! Also the data calls were not free and I ended up building a small caching layer with redis . The other reason I had done that was that I would often work out of cafes where the Wifi was spotty and I didn’t want an internet connection to hold me back. I think in hind-sight, I could have avoided some of the automation here and just decoupled the data fetch so that I wouldn’t have to worry about that internet connection. Of course every time I wanted to add additional data from Citibike, there would be new docking stations and I had to get back to making sure my station location data was correct, so that bad data did not impact predictions. Time bucketing In order to get better information from the source time, the source time was bucketted into 24 hour-buckets per day. That is since a ride starting at 1:04:23pm shouldn’t be treated as being too different from a ride departing at 1:05:24pm . There is more value in intuitively clustering the rides.\nComparing Geolocation Granularities There are about 463 stations found in the dataset, 28 neighborhoods, representing 49 postal codes and 3 out of 5 boroughs,\nSo using the (start time bucket, start station id, age, gender) as the inputs and with RandomizedLogisticRegression as a classifier , for about a months worth of trip data, I saw roughly the following comparison.\n{'end station id': OrderedDict([('training', OrderedDict([('accuracy_score', 0.041432771986099973), ('f1_score', 0.015138704086611844), ('recall_score', 0.041432771986099973), ('precision_score', 0.016942125433308568)])), ('holdout', OrderedDict([('accuracy_score', 0.031533939070016032), ('f1_score', 0.0093952628045424723), ('recall_score', 0.031533939070016032), ('precision_score', 0.0067157290264759353)]))]), 'end_neighborhood': OrderedDict([('training', OrderedDict([('accuracy_score', 0.39047231270358307), ('f1_score', 0.28885663229134789), ('recall_score', 0.39047231270358307), ('precision_score', 0.26445041603375502)])), ('holdout', OrderedDict([('accuracy_score', 0.39630836047774159), ('f1_score', 0.2935527390151364), ('recall_score', 0.39630836047774159), ('precision_score', 0.26579390443173939)]))]), 'end_postal_code': OrderedDict([('training', OrderedDict([('accuracy_score', 0.14129127122042506), ('f1_score', 0.068340173428106887), ('recall_score', 0.14129127122042506), ('precision_score', 0.068168259430770747)])), ('holdout', OrderedDict([('accuracy_score', 0.13361838588989844), ('f1_score', 0.064738931917963718), ('recall_score', 0.13361838588989844), ('precision_score', 0.067139580345228156)]))]), 'end_sublocality': OrderedDict([('training', OrderedDict([('accuracy_score', 0.95354786589470852), ('f1_score', 0.95209028150037733), ('recall_score', 0.95354786589470852), ('precision_score', 0.95217920885515972)])), ('holdout', OrderedDict([('accuracy_score', 0.9493807215939688), ('f1_score', 0.94990282092170586), ('recall_score', 0.9493807215939688), ('precision_score', 0.95132373575480056)]))])} The rough accuracies for prediction, end station id (~3%), postal code (~13%), neighborhood (~40%), and borough (~95%), using the small dataset, shows the rough differences in what happens when you reduce the number of possible outputs. Overall this gave me the motivation to focus on the neighborhood as a target to try to improve upon. More in the jupyter notebook Deeper into the weeds I compared the SGDClassifier with the LogisticRegression classifier (which I believe just uses Gradient Descent, while the SGDClassifier classifier is also a Logistic Regression classifier, but it uses Stochastic Gradient Descent).\nI also tried applying Standard Scaling to my input data after reading that scikit learn ’s SGDClassifier implementation is sensitive unless the input data has a mean=0 and variance=1 . Indeed per the below this helped a little.\nI also applied a GridSearch around the alpha parameter to the SGDClassifier, but this did not help at least the way I tried it,\nI next started varying the training set size, given that a month worth of trip data had about a million rows, I went from 10k to 1M,\nBut this didn’t look great. I realized a problem I had was that I was not randomly sampling my input data. Since a month-size dataset is around 1.2 Million rows, then a 10,000 large set just ends up barely dipping into the first day. So choosing the dataset sizes has to be done, by random sampling.\nAfter making the sampling randomized, the output below, feels like it has a better upward trend, but it is still not visible enough.\nI also realized I was being inconsistent in my assessment because I was not using a single holdout set to test. I was actually randomly generating a test set each time. That was really bad. So I created ten models on sizes 10,000 through 100,000 datasets, created from 09 and 10 2015, and testing on a single holdout dataset, taken from November 2015. In this approach, the accuracy results are found using the same holdout set instead of using a differently derived test set each time.\nAlthough the results were still pretty flat, at least I can trust the consistency of my test method more now.\nMore in the jupyter notebook\nBinarizing the inputs Another important modeling change to try was to do a better of job of preparing the input data to better expose the stratification across citibike trips across the sources. To do this, instead of using a source station column and source neighborhood columns, the source neighborhood column was binarized using the sklearn OneHotEncoder, to a column for each of the neighborhoods in the surface area of the city. The same experiment as earlier was conducted, comparing results across the default SGDClassifier and LogisticRegression classifiers and also across 100,000 to 1,000,000 size datasets used for a train/test split along with a 100,000 large holdout set. These were created from just the single 2015-09 dataset (201509-citibike-tripdata.csv). I found this to be very helpful. Here’s a summary graphic, More details in the notebook\nChanging my metric one more time Given that there are about 28 neighborhoods covered by Citibike (at least in the data end of 2015), a high accuracy is difficult to achieve especially because there are many output classes to choose from. Another idea that was explored was to create a Rank K Accuracy, such that a prediction is correct when the correct class is found in the top highest K probabilities. The overall reasoning I had here is two-fold. One, I think of the analogy of a search engine, where it is typically acceptable to show someone five results as opposed to the so called “I am feeling lucky” result. Of course not every machine learning use case will have the tolerance to take five results as opposed to five, but I think my particular problem of choice it might be fine. Or at least asking people would help to answer that question.\nBut I think the main reason I wanted to do this was to just better understand whether my classification approach was doing anything at all. So since, out of these 28 or so neighborhoods, if going from the top 1 result to the top 2 results, yields an additional 20 points of accuracy, then I feel a little better about the result making sense.\nMore detail in the notebook\nSageMaker approach I wanted to test drive AWS SageMaker ,\nTo see if I could more easily train models without relying on my laptop And make my training environment more reproducible And the prospect of hyper parameter tuning jobs seemed pretty neat too And I wanated to see just how simple serving models would be And in general I wanted to do all of these things to see if I could end up using this at my job (which I did). What ended up happening I managed to recreate my basic model training and evaluation setup using Docker and Sagemaker. With SageMaker, Docker, your model is more compartmentalized and I found this was helpful when iterating model code and boiler plate code. I also ended up serving the model on an endpoint. This is still a sort of a back burner project I would like to come back to at some point, but below I show one update I made to have a slightly better time debugging changes my model iterations And I go over several more data roadblocks I ran into. Changes to Model Iterations To make things slightly easier to understand, especially for debugging purposes, I now made models into json objects that are easier to display\nlocalfn = '/Users/michal/Downloads/2018-12-07-update-model/2018-12-07-update- ...: model/tree-foo-bundle-pensive-swirles.2018-12-04T210259ZUTC.pkl' In [5]: with open(localfn) as fd: bundle = cPickle.load(fd) In [6]: from bikelearn import classify as blc In [12]: blc.label_decode(bundle['label_encoders']['end_neighborhood'], range(40)) Out[12]: array(['-1', 'Alphabet City', 'Battery Park City', 'Bedford-Stuyvesant', 'Boerum Hill', 'Bowery', 'Broadway Triangle', 'Brooklyn Heights', 'Brooklyn Navy Yard', 'Central Park', 'Chelsea', 'Chinatown', 'Civic Center', 'Clinton Hill', 'Columbia Street Waterfront District', 'Downtown Brooklyn', 'Dumbo', 'East Village', 'Financial District', 'Flatiron District', 'Fort Greene', 'Fulton Ferry District', 'Garment District', 'Gramercy Park', 'Greenpoint', 'Greenwich Village', \"Hell's Kitchen\", 'Hudson Square', 'Hunters Point', 'Kips Bay', 'Korea Town', 'Lenox Hill', 'Lincoln Square', 'Little Italy', 'Long Island City', 'Lower East Side', 'Lower Manhattan', 'Meatpacking District', 'Midtown', 'Midtown East'], dtype=object) In [19]: len(bundle['label_encoders']['end_neighborhood'].classes_) Out[19]: 65 In [42]: bu.print_bundle(bundle) Out[42]: {'bundle_name': 'tree-foo-bundle-pensive-swirles', 'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False), 'clf_info': {'feature_importances': [('start_postal_code', 0.4118465753431595), ('start_sublocality', 0.2228924462201325), ('start_neighborhood', 0.28008403752725497), ('start_day', 0.006151427471547376), ('start_hour', 0.03971509075090292), ('age', 0.0180403831546044), ('gender', 0.008144851450140815), ('usertype', 0.013125188082257624)]}, 'evaluation': {'test_metrics': {'confusion_matrix': 64, 'f1_scores': {'macro': 0.041605116043552354, 'micro': 0.1599860211928701, 'weighted': 0.06355337311627869}, 'rank_k_proba_scores': {1: 0.1599860211928701, 2: 0.2536814721966162, 3: 0.3240640528912417, 4: 0.3873466833318772, 5: 0.443358194451626, 10: 0.629839760791229}}, 'validation_metrics': {'confusion_matrix': 64, 'f1_scores': {'macro': 0.04327900735885162, 'micro': 0.16284068269032595, 'weighted': 0.06549596580599053}, 'rank_k_proba_scores': {1: 0.16284068269032595, 2: 0.2563720053782964, 3: 0.3247784397051751, 4: 0.3873943724175835, 5: 0.4436898254016547, 10: 0.6304384096730821}}}, 'features': {'dtypes': {'age': float, 'end_neighborhood': str, 'start_neighborhood': str, 'start_postal_code': str, 'start_sublocality': str, 'usertype': str}, 'input': ['start_postal_code', 'start_sublocality', 'start_neighborhood', 'start_day', 'start_hour', 'age', 'gender', 'usertype'], 'output_label': 'end_neighborhood'}, 'label_encoders': {'age': LabelEncoder(), 'end_neighborhood': LabelEncoder(), 'start_neighborhood': LabelEncoder(), 'start_postal_code': LabelEncoder(), 'start_sublocality': LabelEncoder(), 'usertype': LabelEncoder()}, 'model_id': 'tree-foo', 'test_metadata': {'testset_fn': '/opt/ml/input/data/testing/201602-citibike-tripdata.csv'}, 'timestamp': '2018-12-04T210259ZUTC', 'train_metadata': {'hyperparameters': {u'max_depth': u'5', u'n_estimators': u'20'}, 'stations_df_fn': '/opt/ml/input/data/training/support/stations-2018-12-04-c.csv', 'trainset_fn': '/opt/ml/input/data/training/201601-citibike-tripdata.csv'}} I like how the AWS SageMaker setup allows for custom Docker image based models, only requiring a particular csv format as a data input for predictions and also requiring the Docker image implement a train command for running training jobs. I ended up adding a setup.py to my git repo to version what code a particular Dockerfile would use I made many iterations in trying to get the model on SageMaker up and running, so I liked the experience overall. Bad data strikes again After quickly updating docking station data and retraining, I ended up with a model which was returning only one value as an output I took a deeper dive into my docking station data and found yet again that I had a lot of blank geolocation neighborhood and postal code data. This is the same problem I had to deal with in the past as well. I ended up finding while debugging, that the empty data was essentially pinning the majority of the training data as neighborhood : 'nan', and so the predictions, per this confusion matrix, were basically all the same output. ipdb\u003e pp skm.confusion_matrix(y_validation, y_predictions, classes) array([[ 0, 0, 0, 0, 0, 0, 0, 174], [ 0, 0, 0, 0, 0, 0, 0, 116], [ 0, 0, 0, 0, 0, 0, 0, 130], [ 0, 0, 0, 0, 0, 0, 0, 357], [ 0, 0, 0, 0, 0, 0, 0, 364], [ 0, 0, 0, 0, 0, 0, 0, 255], [ 0, 0, 0, 0, 0, 0, 0, 862], [ 0, 0, 0, 0, 0, 0, 0, 97977]]) ipdb\u003e Of course one data problem always leads to another data problem. This time around, when I started updating my docking station geolocation data, I found that my payment information may have changed and so I was getting the following ipdb\u003e pp geocoding_result {u'error_message': u'You have exceeded your daily request quota for this API. If you did not set a custom daily request quota, verify your project has an active billing account: http://g.co/dev/maps-no-account', u'results': [], u'status': u'OVER_QUERY_LIMIT'} And of course I also ended up stepping on my own foot as well. I found I had created an unfortinate git commit where I accidentally undid the url-encoding and intersections with \u0026 are I think treated as query string parameters , , which ended up being a reason why some of my data was coming back as just the geolocation for a single street (route) and not an actual intersection. Future Improvements I am hoping to come back to this and continue to iterate the approach. In particular, I would like to continue to explore model degradation over time. And in discussing with a few colleagues, seasonality would also be a really good feature to consider. Time bucketing was explored to a limited extent, but the day of the week nor the month of the year was not explored. There may also be many other datasets which can be joined with this one to bolster the information available, including information about the weather or perhaps other demographic attributes available. A more thorough comparison of algorithms should also be considered. ","wordCount":"2893","inLanguage":"en","image":"https://my-blog-content.s3.amazonaws.com/2020/CartShare.jpeg","datePublished":"2016-12-18T18:33:28Z","dateModified":"2023-02-26T19:27:16-05:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/project/2016-12-18-citibike-project/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/project/>Side Projects</a></div><h1 class=post-title>Citibike Learn Project</h1><div class=post-meta><span title='2016-12-18 18:33:28 +0000 UTC'>December 18, 2016</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2893 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><figure class=entry-cover><img loading=lazy src=https://my-blog-content.s3.amazonaws.com/2020/CartShare.jpeg alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#citibike-project-can-your-destination-be-predicted>Citibike Project: Can your Destination be Predicted</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=citibike-project-can-your-destination-be-predicted>Citibike Project: Can your Destination be Predicted<a hidden class=anchor aria-hidden=true href=#citibike-project-can-your-destination-be-predicted>#</a></h3><p><em>( <a href=https://github.com/namoopsoo/learn-citibike>https://github.com/namoopsoo/learn-citibike</a> )</em></p><h4 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h4><p>I think sometimes the most interesting projects live behind ideas that sound impractical or even crazy. That&rsquo;s why I thought
it would be fun to use the Citibike bike share trip data to try and predict a person&rsquo;s destination based on what we know.</p><p><em>Roughly speaking trip data looks like this</em></p><pre tabindex=0><code>&#34;tripduration&#34;,&#34;starttime&#34;,&#34;stoptime&#34;,&#34;start station id&#34;,&#34;start station name&#34;,&#34;start station latitude&#34;,&#34;start station longitude&#34;,&#34;end station id&#34;,&#34;end station name&#34;,&#34;end station latitude&#34;,&#34;end station longitude&#34;,&#34;bikeid&#34;,&#34;usertype&#34;,&#34;birth year&#34;,&#34;gender&#34;
&#34;171&#34;,&#34;10/1/2015 00:00:02&#34;,&#34;10/1/2015 00:02:54&#34;,&#34;388&#34;,&#34;W 26 St &amp; 10 Ave&#34;,&#34;40.749717753&#34;,&#34;-74.002950346&#34;,&#34;494&#34;,&#34;W 26 St &amp; 8 Ave&#34;,&#34;40.74734825&#34;,&#34;-73.99723551&#34;,&#34;24302&#34;,&#34;Subscriber&#34;,&#34;1973&#34;,&#34;1&#34;
&#34;593&#34;,&#34;10/1/2015 00:00:02&#34;,&#34;10/1/2015 00:09:55&#34;,&#34;518&#34;,&#34;E 39 St &amp; 2 Ave&#34;,&#34;40.74780373&#34;,&#34;-73.9734419&#34;,&#34;438&#34;,&#34;St Marks Pl &amp; 1 Ave&#34;,&#34;40.72779126&#34;,&#34;-73.98564945&#34;,&#34;19904&#34;,&#34;Subscriber&#34;,&#34;1990&#34;,&#34;1&#34;
&#34;233&#34;,&#34;10/1/2015 00:00:11&#34;,&#34;10/1/2015 00:04:05&#34;,&#34;447&#34;,&#34;8 Ave &amp; W 52 St&#34;,&#34;40.76370739&#34;,&#34;-73.9851615&#34;,&#34;447&#34;,&#34;8 Ave &amp; W 52 St&#34;,&#34;40.76370739&#34;,&#34;-73.9851615&#34;,&#34;17797&#34;,&#34;Subscriber&#34;,&#34;1984&#34;,&#34;1&#34;
&#34;250&#34;,&#34;10/1/2015 00:00:15&#34;,&#34;10/1/2015 00:04:25&#34;,&#34;336&#34;,&#34;Sullivan St &amp; Washington Sq&#34;,&#34;40.73047747&#34;,&#34;-73.99906065&#34;,&#34;223&#34;,&#34;W 13 St &amp; 7 Ave&#34;,&#34;40.73781509&#34;,&#34;-73.99994661&#34;,&#34;23966&#34;,&#34;Subscriber&#34;,&#34;1984&#34;,&#34;1&#34;
&#34;528&#34;,&#34;10/1/2015 00:00:17&#34;,&#34;10/1/2015 00:09:05&#34;,&#34;3107&#34;,&#34;Bedford Ave &amp; Nassau Ave&#34;,&#34;40.72311651&#34;,&#34;-73.95212324&#34;,&#34;539&#34;,&#34;Metropolitan Ave &amp; Bedford Ave&#34;,&#34;40.71534825&#34;,&#34;-73.96024116&#34;,&#34;16246&#34;,&#34;Customer&#34;,&#34;&#34;,&#34;0&#34;
&#34;440&#34;,&#34;10/1/2015 00:00:17&#34;,&#34;10/1/2015 00:07:37&#34;,&#34;3107&#34;,&#34;Bedford Ave &amp; Nassau Ave&#34;,&#34;40.72311651&#34;,&#34;-73.95212324&#34;,&#34;539&#34;,&#34;Metropolitan Ave &amp; Bedford Ave&#34;,&#34;40.71534825&#34;,&#34;-73.96024116&#34;,&#34;23698&#34;,&#34;Customer&#34;,&#34;&#34;,&#34;0&#34;
</code></pre><p>The data if fairly clean and regular, so I thought this was a fun data set to sharpen my teeth on.</p><h4 id=quick-birds-eye--of-my-journey>Quick Bird&rsquo;s Eye of my Journey<a hidden class=anchor aria-hidden=true href=#quick-birds-eye--of-my-journey>#</a></h4><ul><li>First started just <a href=#more-on-this-data>looking</a> at this data.</li><li>Just out of curiosity, as a first mini starter project I decided to look at the <a href=#speed-and-age>relationship between rider age and speed</a></li><li>I realized pretty early that the bike station target was too small, so I started using the Google Geolocation API to get broader location data such
as <em>zip codes</em> and <em>neighborhoods</em> . <a href=#need-additional-location-data>geolocation</a></li><li>I also thought on a high level that knowing whether you got on your bike at <code>4:05</code> in the afternoon versus <code>4:06</code> shouldn&rsquo;t
influence my learning algorithm, so I added some more <a href=#time-bucketing>transformations</a>.</li><li>I compared the prediction accuracy of the new geolocation data as a <a href=#comparing-geolocation-granularities>first stab</a></li><li>I ran through a couple of modeling scenarios <a href=#deeper-into-the-weeds>here</a> and finding some impoartant inconsistencies in how I was running my testing</li><li>I ended up getting some good gains by <a href=#binarizing-the-inputs>binarizing my inputs</a></li><li>I ended up using a <a href=#changing-my-metric-one-more-time>different evaluation metric</a> again to get a different perspective on this problem.</li><li>Maybe a year or more later I came back to this problem from the point of view of experimenting with <a href=#sagemaker-approach>AWS approaches to training models</a> , including Docker and SageMaker.</li><li>Thoughts for <a href=#future-improvements>future improvements</a></li></ul><h4 id=more-on-this-data>More on this data<a hidden class=anchor aria-hidden=true href=#more-on-this-data>#</a></h4><ul><li>When I started looking at this data, there were 400+ stations for docking your citibike.</li><li>There is age, and some of the riders were actually born in the 1800s, which is kind of cool.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> load_data(<span style=color:#e6db74>&#39;data/201509_10-citibike-tripdata.csv.annotated.100000.06112016T1814.csv&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>6</span>]: df[<span style=color:#e6db74>&#39;birth year&#39;</span>]<span style=color:#f92672>.</span>describe()
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>6</span>]: 
</span></span><span style=display:flex><span>count    <span style=color:#ae81ff>83171.000000</span>
</span></span><span style=display:flex><span>mean      <span style=color:#ae81ff>1977.149680</span>
</span></span><span style=display:flex><span>std         <span style=color:#ae81ff>11.400096</span>
</span></span><span style=display:flex><span>min       <span style=color:#ae81ff>1885.000000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>25</span><span style=color:#f92672>%</span>       <span style=color:#ae81ff>1969.000000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>50</span><span style=color:#f92672>%</span>       <span style=color:#ae81ff>1980.000000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>75</span><span style=color:#f92672>%</span>       <span style=color:#ae81ff>1986.000000</span>
</span></span><span style=display:flex><span>max       <span style=color:#ae81ff>1999.000000</span>
</span></span><span style=display:flex><span>Name: birth year, dtype: float64
</span></span></code></pre></div><h4 id=speed-and-age>Speed and Age<a hidden class=anchor aria-hidden=true href=#speed-and-age>#</a></h4><p>Turns out that you need to know the miles per the longitude degree at a particular latitude on our planet. So for our particular location,
at lat around <code>40.723</code> and using the earth&rsquo;s radius of about <code>3958 miles</code> , we have about <code>52.3 miles/longitude degree</code>
here in NYC.</p><p>So from there, looking at some of the speed data just involved looking at the tripdata trip time and calculating the
cartesian distance.</p><p>(More on the code <a href=https://github.com/namoopsoo/learn-citibike/blob/master/bikelearn/utils.py#L86>here</a> )
(Also more detail on this analysis in the main <a href=https://github.com/namoopsoo/learn-citibike/blob/master/project%20report.ipynb>jupyter notebook</a>)</p><h4 id=need-additional-location-data>Need additional location data<a hidden class=anchor aria-hidden=true href=#need-additional-location-data>#</a></h4><ul><li>With the 400+ stations, trying to predict a multi-class problem of this sort with basic machine learning algorithms
would not be a way to get quick results to help keep the project going, so I decided to constrain the problem. I ended up
turning to the Google Geocoding API. Using this data became its own side project, because parsing through
the Google geolocation data can get pretty hairy!</li></ul><p><em>The meat of the output can look like this, for the docking station &ldquo;1st Avenue & E 15th St&rdquo;</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>{
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;raw_result&#39;</span>: [{<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;address_components&#39;</span>: [{<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;1st Avenue&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;1st Avenue&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;route&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;Midtown&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;Midtown&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;neighborhood&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;political&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;Manhattan&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;Manhattan&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;sublocality_level_1&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;sublocality&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;political&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;New York&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;New York&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;locality&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;political&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;New York County&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;New York County&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;administrative_area_level_2&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;political&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;New York&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;NY&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;administrative_area_level_1&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;political&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;United States&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;US&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;country&#39;</span>,
</span></span><span style=display:flex><span>                           <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;political&#39;</span>]},
</span></span><span style=display:flex><span>               {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;long_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;10003&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;short_name&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;10003&#39;</span>,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;types&#39;</span>: [<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;postal_code&#39;</span>]}],
</span></span><span style=display:flex><span>               <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;formatted_address&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;1st Avenue &amp; E 15th St, New York, NY 10003, USA&#39;</span>,
</span></span><span style=display:flex><span>               }
</span></span></code></pre></div><ul><li>Some of the challenges here were that the outputs from the API were not always consistent. The above output shows that the<br><code>'neighborhood'</code> is <code>'Midtown'</code>, but because there were 400+ stations, I did not initially notice that sometimes the <code>neighborhood</code>
was missing or that the <code>zip code</code> was missing. That ended up throwing off my code a couple of times.</li><li>It turned out that the <em>street intersection</em> was not an ideal clean data input to the API. For instance <code>E 3 St & 1 Ave, NY</code> was understood by the API as just a street or a <code>route</code> as it is called, ( <a href=assets/E%203%20St%20&%201%20Ave,%20NY.md>raw output</a> ). Later on I ended up refactoring this to
use the raw <em>latitude and longitude</em> .</li><li>However, I eventually noticed that often times the <em>docking stations</em> were on the edge of neighborhoods. So I literally had
edge cases! The Neighborhood would come back blank and I ended up having to fill in a lot of that data by hand anyhow!</li></ul><ul><li>Also the data calls were not free and I ended up building a small <em>caching layer</em> with <em>redis</em> .</li><li>The other reason I had done that was that I would often work out of cafes where the Wifi was spotty and I didn&rsquo;t want an internet
connection to hold me back.</li><li>I think in hind-sight, I could have avoided some of the automation here and just decoupled the data fetch so that I wouldn&rsquo;t have to
worry about that internet connection.</li><li>Of course every time I wanted to add additional data from Citibike, there would be new docking stations and I had to get back to making sure
my station location data was correct, so that bad data did not impact predictions.</li></ul><h4 id=time-bucketing>Time bucketing<a hidden class=anchor aria-hidden=true href=#time-bucketing>#</a></h4><p>In order to get better information from the source time, the source time was bucketted into 24 hour-buckets per day. That is since a ride starting at 1:04:23pm shouldn&rsquo;t be treated as being too different from a ride departing at 1:05:24pm . There is more value in intuitively clustering the rides.</p><h4 id=comparing-geolocation-granularities>Comparing Geolocation Granularities<a hidden class=anchor aria-hidden=true href=#comparing-geolocation-granularities>#</a></h4><p>There are about 463 stations found in the dataset, 28 neighborhoods, representing 49 postal codes and 3 out of 5 boroughs,</p><p>So using the <code>(start time bucket, start station id, age, gender)</code> as the inputs and with <code>RandomizedLogisticRegression</code> as a classifier ,
for about a months worth of trip data, I saw roughly the following comparison.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>{<span style=color:#e6db74>&#39;end station id&#39;</span>: OrderedDict([(<span style=color:#e6db74>&#39;training&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.041432771986099973</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.015138704086611844</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.041432771986099973</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.016942125433308568</span>)])),
</span></span><span style=display:flex><span>              (<span style=color:#e6db74>&#39;holdout&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.031533939070016032</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.0093952628045424723</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.031533939070016032</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.0067157290264759353</span>)]))]),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;end_neighborhood&#39;</span>: OrderedDict([(<span style=color:#e6db74>&#39;training&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.39047231270358307</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.28885663229134789</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.39047231270358307</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.26445041603375502</span>)])),
</span></span><span style=display:flex><span>              (<span style=color:#e6db74>&#39;holdout&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.39630836047774159</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.2935527390151364</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.39630836047774159</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.26579390443173939</span>)]))]),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;end_postal_code&#39;</span>: OrderedDict([(<span style=color:#e6db74>&#39;training&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.14129127122042506</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.068340173428106887</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.14129127122042506</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.068168259430770747</span>)])),
</span></span><span style=display:flex><span>              (<span style=color:#e6db74>&#39;holdout&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.13361838588989844</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.064738931917963718</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.13361838588989844</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.067139580345228156</span>)]))]),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;end_sublocality&#39;</span>: OrderedDict([(<span style=color:#e6db74>&#39;training&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.95354786589470852</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.95209028150037733</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.95354786589470852</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.95217920885515972</span>)])),
</span></span><span style=display:flex><span>              (<span style=color:#e6db74>&#39;holdout&#39;</span>,
</span></span><span style=display:flex><span>               OrderedDict([(<span style=color:#e6db74>&#39;accuracy_score&#39;</span>, <span style=color:#ae81ff>0.9493807215939688</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;f1_score&#39;</span>, <span style=color:#ae81ff>0.94990282092170586</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;recall_score&#39;</span>, <span style=color:#ae81ff>0.9493807215939688</span>),
</span></span><span style=display:flex><span>                            (<span style=color:#e6db74>&#39;precision_score&#39;</span>, <span style=color:#ae81ff>0.95132373575480056</span>)]))])}
</span></span></code></pre></div><ul><li>The rough accuracies for prediction, end station id (~3%), postal code (~13%), neighborhood (~40%), and borough (~95%), using the small dataset, shows the rough differences in what happens when you reduce the number of possible outputs.</li><li>Overall this gave me the motivation to focus on the <code>neighborhood</code> as a target to try to improve upon.</li><li>More in the <a href="https://render.githubusercontent.com/view/ipynb?commit=b2beb2af23f4f803a059161aeeb1a8e628a1bd4b&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e616d6f6f70736f6f2f6c6561726e2d6369746962696b652f623262656232616632336634663830336130353931363161656562316138653632386131626434622f70726f6a6563742532307265706f72742e6970796e62&amp;nwo=namoopsoo%2Flearn-citibike&amp;path=project+report.ipynb&amp;repository_id=60489657&amp;repository_type=Repository#A-basic-learning-strategy-is-used">jupyter notebook</a></li></ul><h4 id=deeper-into-the-weeds>Deeper into the weeds<a hidden class=anchor aria-hidden=true href=#deeper-into-the-weeds>#</a></h4><p>I compared the <code>SGDClassifier</code> with the <code>LogisticRegression</code> classifier (which I believe just uses Gradient Descent, while the <code>SGDClassifier</code> classifier is also a Logistic Regression classifier, but it uses Stochastic Gradient Descent).</p><p>I also tried applying <a href=https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>Standard Scaling</a> to my input data after <a href=http://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use>reading</a> that <code>scikit learn</code> &rsquo;s <code>SGDClassifier</code> implementation is sensitive unless the input data has a <code>mean=0</code> and <code>variance=1</code> . Indeed per the below this helped a little.</p><p>I also applied a GridSearch around the alpha parameter to the SGDClassifier, but this did not help at least the way I tried it,</p><p>I next started varying the training set size, given that a month worth of trip data had about a million rows, I went from <code>10k</code> to <code>1M</code>,</p><p>But this didn&rsquo;t look great. I realized a problem I had was that I was not randomly sampling my input data. Since a month-size dataset is around 1.2 Million rows, then a 10,000 large set just ends up barely dipping into the first day. So choosing the dataset sizes has to be done, by random sampling.</p><p>After making the sampling randomized, the output below, feels like it has a better upward trend, but it is still not visible enough.</p><p>I also realized I was being inconsistent in my assessment because I was not using a single holdout set to test. I was actually randomly generating a test set each time. That was really bad. So I created ten models on sizes 10,000 through 100,000 datasets, created from <code>09 and 10 2015</code>, and testing on a single holdout dataset, taken from <code>November 2015</code>. In this approach, the accuracy results are found using the same holdout set instead of using a differently derived test set each time.</p><p>Although the results were still pretty flat, at least I can trust the consistency of my test method more now.</p><p>More in the jupyter <a href="https://render.githubusercontent.com/view/ipynb?commit=b2beb2af23f4f803a059161aeeb1a8e628a1bd4b&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e616d6f6f70736f6f2f6c6561726e2d6369746962696b652f623262656232616632336634663830336130353931363161656562316138653632386131626434622f70726f6a6563742532307265706f72742e6970796e62&amp;nwo=namoopsoo%2Flearn-citibike&amp;path=project+report.ipynb&amp;repository_id=60489657&amp;repository_type=Repository#Also-comparing-with-additional-classifiers">notebook</a></p><h4 id=binarizing-the-inputs>Binarizing the inputs<a hidden class=anchor aria-hidden=true href=#binarizing-the-inputs>#</a></h4><ul><li>Another important modeling change to try was to do a better of job of preparing the input data to better expose the stratification across citibike trips across the sources. To do this, instead of using a source station column and source neighborhood columns, the source neighborhood column was binarized using the sklearn OneHotEncoder, to a column for each of the neighborhoods in the surface area of the city.</li><li>The same experiment as earlier was conducted, comparing results across the default SGDClassifier and LogisticRegression classifiers and also across 100,000 to 1,000,000 size datasets used for a train/test split along with a 100,000 large holdout set.</li><li>These were created from just the single 2015-09 dataset (201509-citibike-tripdata.csv).</li><li>I found this to be very helpful. Here&rsquo;s a summary graphic,</li></ul><p>More details <a href="https://render.githubusercontent.com/view/ipynb?commit=b2beb2af23f4f803a059161aeeb1a8e628a1bd4b&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e616d6f6f70736f6f2f6c6561726e2d6369746962696b652f623262656232616632336634663830336130353931363161656562316138653632386131626434622f70726f6a6563742532307265706f72742e6970796e62&amp;nwo=namoopsoo%2Flearn-citibike&amp;path=project+report.ipynb&amp;repository_id=60489657&amp;repository_type=Repository#Binarizing-geolocation-start-data">in the notebook</a></p><h4 id=changing-my-metric-one-more-time>Changing my metric one more time<a hidden class=anchor aria-hidden=true href=#changing-my-metric-one-more-time>#</a></h4><ul><li>Given that there are about <code>28</code> neighborhoods covered by Citibike (at least in the data end of <code>2015</code>), a high accuracy is difficult to achieve especially because there are many output classes to choose from.</li><li>Another idea that was explored was to create a Rank K Accuracy, such that a prediction is correct when the correct class is found in the top highest K probabilities.</li></ul><p>The overall reasoning I had here is two-fold. One, I think of the analogy of a search engine, where it is typically acceptable to show someone
<em>five results</em> as opposed to the so called <em>&ldquo;I am feeling lucky&rdquo;</em> result. Of course not every machine learning use case will have the tolerance to take five results as opposed to five, but I think my particular problem of choice it might be fine. Or at least asking people would help to answer that question.</p><p>But I think the main reason I wanted to do this was to just better understand whether my classification approach was doing anything at all. So since, out of these <code>28</code> or so neighborhoods, if going from the <code>top 1</code> result to the <code>top 2</code> results, yields an additional <code>20 points</code> of accuracy, then I feel a little better about the result making sense.</p><p>More detail in the <a href="https://render.githubusercontent.com/view/ipynb?commit=b2beb2af23f4f803a059161aeeb1a8e628a1bd4b&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e616d6f6f70736f6f2f6c6561726e2d6369746962696b652f623262656232616632336634663830336130353931363161656562316138653632386131626434622f70726f6a6563742532307265706f72742e6970796e62&amp;nwo=namoopsoo%2Flearn-citibike&amp;path=project+report.ipynb&amp;repository_id=60489657&amp;repository_type=Repository#Redefining-the-accuracy-score">notebook</a></p><h4 id=sagemaker-approach>SageMaker approach<a hidden class=anchor aria-hidden=true href=#sagemaker-approach>#</a></h4><p>I wanted to test drive AWS SageMaker ,</p><ul><li>To see if I could more easily train models without relying on my laptop</li><li>And make my training environment more reproducible</li><li>And the prospect of hyper parameter tuning jobs seemed pretty neat too</li><li>And I wanated to see just how simple serving models would be</li><li>And in general I wanted to do all of these things to see if I could end up using this at my job (which I did).</li></ul><h4 id=what-ended-up-happening>What ended up happening<a hidden class=anchor aria-hidden=true href=#what-ended-up-happening>#</a></h4><ul><li>I managed to recreate my basic model training and evaluation setup using Docker and Sagemaker.</li><li>With SageMaker, Docker, your model is more compartmentalized and I found this was helpful when iterating model code and boiler plate code.</li><li>I also ended up serving the model on an endpoint.</li><li>This is still a sort of a back burner project I would like to come back to at some point,</li><li>but below I show one update I made to have a slightly better time debugging <a href=#changes-to-model-iterations>changes my model iterations</a></li><li>And I go over <a href=#bad-data-strikes-again>several more data roadblocks</a> I ran into.</li></ul><h4 id=changes-to-model-iterations>Changes to Model Iterations<a hidden class=anchor aria-hidden=true href=#changes-to-model-iterations>#</a></h4><p>To make things slightly easier to understand, especially for debugging purposes, I now made models into json objects that are easier to display</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>localfn <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/Users/michal/Downloads/2018-12-07-update-model/2018-12-07-update-</span>
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>: model<span style=color:#f92672>/</span>tree<span style=color:#f92672>-</span>foo<span style=color:#f92672>-</span>bundle<span style=color:#f92672>-</span>pensive<span style=color:#f92672>-</span>swirles<span style=color:#ae81ff>.2018</span><span style=color:#f92672>-</span><span style=color:#ae81ff>12</span><span style=color:#f92672>-</span><span style=color:#ae81ff>04</span>T210259ZUTC<span style=color:#f92672>.</span>pkl<span style=color:#e6db74>&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>5</span>]: <span style=color:#66d9ef>with</span> open(localfn) <span style=color:#66d9ef>as</span> fd: bundle <span style=color:#f92672>=</span> cPickle<span style=color:#f92672>.</span>load(fd)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>6</span>]: <span style=color:#f92672>from</span> bikelearn <span style=color:#f92672>import</span> classify <span style=color:#66d9ef>as</span> blc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>12</span>]: blc<span style=color:#f92672>.</span>label_decode(bundle[<span style=color:#e6db74>&#39;label_encoders&#39;</span>][<span style=color:#e6db74>&#39;end_neighborhood&#39;</span>], range(<span style=color:#ae81ff>40</span>))
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>12</span>]: 
</span></span><span style=display:flex><span>array([<span style=color:#e6db74>&#39;-1&#39;</span>, <span style=color:#e6db74>&#39;Alphabet City&#39;</span>, <span style=color:#e6db74>&#39;Battery Park City&#39;</span>, <span style=color:#e6db74>&#39;Bedford-Stuyvesant&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Boerum Hill&#39;</span>, <span style=color:#e6db74>&#39;Bowery&#39;</span>, <span style=color:#e6db74>&#39;Broadway Triangle&#39;</span>, <span style=color:#e6db74>&#39;Brooklyn Heights&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Brooklyn Navy Yard&#39;</span>, <span style=color:#e6db74>&#39;Central Park&#39;</span>, <span style=color:#e6db74>&#39;Chelsea&#39;</span>, <span style=color:#e6db74>&#39;Chinatown&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Civic Center&#39;</span>, <span style=color:#e6db74>&#39;Clinton Hill&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Columbia Street Waterfront District&#39;</span>, <span style=color:#e6db74>&#39;Downtown Brooklyn&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Dumbo&#39;</span>, <span style=color:#e6db74>&#39;East Village&#39;</span>, <span style=color:#e6db74>&#39;Financial District&#39;</span>, <span style=color:#e6db74>&#39;Flatiron District&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Fort Greene&#39;</span>, <span style=color:#e6db74>&#39;Fulton Ferry District&#39;</span>, <span style=color:#e6db74>&#39;Garment District&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Gramercy Park&#39;</span>, <span style=color:#e6db74>&#39;Greenpoint&#39;</span>, <span style=color:#e6db74>&#39;Greenwich Village&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#34;Hell&#39;s Kitchen&#34;</span>, <span style=color:#e6db74>&#39;Hudson Square&#39;</span>, <span style=color:#e6db74>&#39;Hunters Point&#39;</span>, <span style=color:#e6db74>&#39;Kips Bay&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Korea Town&#39;</span>, <span style=color:#e6db74>&#39;Lenox Hill&#39;</span>, <span style=color:#e6db74>&#39;Lincoln Square&#39;</span>, <span style=color:#e6db74>&#39;Little Italy&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Long Island City&#39;</span>, <span style=color:#e6db74>&#39;Lower East Side&#39;</span>, <span style=color:#e6db74>&#39;Lower Manhattan&#39;</span>,
</span></span><span style=display:flex><span>       <span style=color:#e6db74>&#39;Meatpacking District&#39;</span>, <span style=color:#e6db74>&#39;Midtown&#39;</span>, <span style=color:#e6db74>&#39;Midtown East&#39;</span>], dtype<span style=color:#f92672>=</span>object)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>19</span>]: len(bundle[<span style=color:#e6db74>&#39;label_encoders&#39;</span>][<span style=color:#e6db74>&#39;end_neighborhood&#39;</span>]<span style=color:#f92672>.</span>classes_)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>19</span>]: <span style=color:#ae81ff>65</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>42</span>]: bu<span style=color:#f92672>.</span>print_bundle(bundle)
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>42</span>]: 
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;bundle_name&#39;</span>: <span style=color:#e6db74>&#39;tree-foo-bundle-pensive-swirles&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;clf&#39;</span>: RandomForestClassifier(bootstrap<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, class_weight<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, criterion<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gini&#39;</span>,
</span></span><span style=display:flex><span>             max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, max_features<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;auto&#39;</span>, max_leaf_nodes<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>             min_impurity_decrease<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>, min_impurity_split<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>             min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, min_samples_split<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>             min_weight_fraction_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>, n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, n_jobs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>             oob_score<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, warm_start<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;clf_info&#39;</span>: {<span style=color:#e6db74>&#39;feature_importances&#39;</span>: [(<span style=color:#e6db74>&#39;start_postal_code&#39;</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>0.4118465753431595</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;start_sublocality&#39;</span>, <span style=color:#ae81ff>0.2228924462201325</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;start_neighborhood&#39;</span>, <span style=color:#ae81ff>0.28008403752725497</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;start_day&#39;</span>, <span style=color:#ae81ff>0.006151427471547376</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;start_hour&#39;</span>, <span style=color:#ae81ff>0.03971509075090292</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;age&#39;</span>, <span style=color:#ae81ff>0.0180403831546044</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;gender&#39;</span>, <span style=color:#ae81ff>0.008144851450140815</span>),
</span></span><span style=display:flex><span>   (<span style=color:#e6db74>&#39;usertype&#39;</span>, <span style=color:#ae81ff>0.013125188082257624</span>)]},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;evaluation&#39;</span>: {<span style=color:#e6db74>&#39;test_metrics&#39;</span>: {<span style=color:#e6db74>&#39;confusion_matrix&#39;</span>: <span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;f1_scores&#39;</span>: {<span style=color:#e6db74>&#39;macro&#39;</span>: <span style=color:#ae81ff>0.041605116043552354</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;micro&#39;</span>: <span style=color:#ae81ff>0.1599860211928701</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;weighted&#39;</span>: <span style=color:#ae81ff>0.06355337311627869</span>},
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;rank_k_proba_scores&#39;</span>: {<span style=color:#ae81ff>1</span>: <span style=color:#ae81ff>0.1599860211928701</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>2</span>: <span style=color:#ae81ff>0.2536814721966162</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>3</span>: <span style=color:#ae81ff>0.3240640528912417</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>4</span>: <span style=color:#ae81ff>0.3873466833318772</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>5</span>: <span style=color:#ae81ff>0.443358194451626</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>10</span>: <span style=color:#ae81ff>0.629839760791229</span>}},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;validation_metrics&#39;</span>: {<span style=color:#e6db74>&#39;confusion_matrix&#39;</span>: <span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;f1_scores&#39;</span>: {<span style=color:#e6db74>&#39;macro&#39;</span>: <span style=color:#ae81ff>0.04327900735885162</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;micro&#39;</span>: <span style=color:#ae81ff>0.16284068269032595</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;weighted&#39;</span>: <span style=color:#ae81ff>0.06549596580599053</span>},
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;rank_k_proba_scores&#39;</span>: {<span style=color:#ae81ff>1</span>: <span style=color:#ae81ff>0.16284068269032595</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>2</span>: <span style=color:#ae81ff>0.2563720053782964</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>3</span>: <span style=color:#ae81ff>0.3247784397051751</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>4</span>: <span style=color:#ae81ff>0.3873943724175835</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>5</span>: <span style=color:#ae81ff>0.4436898254016547</span>,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>10</span>: <span style=color:#ae81ff>0.6304384096730821</span>}}},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;features&#39;</span>: {<span style=color:#e6db74>&#39;dtypes&#39;</span>: {<span style=color:#e6db74>&#39;age&#39;</span>: float,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;end_neighborhood&#39;</span>: str,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_neighborhood&#39;</span>: str,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_postal_code&#39;</span>: str,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_sublocality&#39;</span>: str,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;usertype&#39;</span>: str},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;input&#39;</span>: [<span style=color:#e6db74>&#39;start_postal_code&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_sublocality&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_neighborhood&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_day&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;start_hour&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;age&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;gender&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>&#39;usertype&#39;</span>],
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;output_label&#39;</span>: <span style=color:#e6db74>&#39;end_neighborhood&#39;</span>},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;label_encoders&#39;</span>: {<span style=color:#e6db74>&#39;age&#39;</span>: LabelEncoder(),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;end_neighborhood&#39;</span>: LabelEncoder(),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;start_neighborhood&#39;</span>: LabelEncoder(),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;start_postal_code&#39;</span>: LabelEncoder(),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;start_sublocality&#39;</span>: LabelEncoder(),
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;usertype&#39;</span>: LabelEncoder()},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;model_id&#39;</span>: <span style=color:#e6db74>&#39;tree-foo&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;test_metadata&#39;</span>: {<span style=color:#e6db74>&#39;testset_fn&#39;</span>: <span style=color:#e6db74>&#39;/opt/ml/input/data/testing/201602-citibike-tripdata.csv&#39;</span>},
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;timestamp&#39;</span>: <span style=color:#e6db74>&#39;2018-12-04T210259ZUTC&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_metadata&#39;</span>: {<span style=color:#e6db74>&#39;hyperparameters&#39;</span>: {<span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;5&#39;</span>,
</span></span><span style=display:flex><span>   <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;n_estimators&#39;</span>: <span style=color:#e6db74>u</span><span style=color:#e6db74>&#39;20&#39;</span>},
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;stations_df_fn&#39;</span>: <span style=color:#e6db74>&#39;/opt/ml/input/data/training/support/stations-2018-12-04-c.csv&#39;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#39;trainset_fn&#39;</span>: <span style=color:#e6db74>&#39;/opt/ml/input/data/training/201601-citibike-tripdata.csv&#39;</span>}}
</span></span></code></pre></div><ul><li>I like how the AWS SageMaker setup allows for custom Docker image based models, only requiring a particular <code>csv</code> format as a data input for predictions and also requiring the Docker image implement a <code>train</code> command for running <em>training</em> jobs.</li><li>I ended up adding a <code>setup.py</code> to my git repo to version what code a particular <code>Dockerfile</code> would use</li><li>I made many iterations in trying to get the model on SageMaker up and running, so I liked the experience overall.</li></ul><h4 id=bad-data-strikes-again>Bad data strikes again<a hidden class=anchor aria-hidden=true href=#bad-data-strikes-again>#</a></h4><ul><li>After quickly updating docking station data and retraining, I ended up with a model which was returning only one value as an output</li><li>I took a deeper dive into my docking station data and found yet again that I had a lot of blank geolocation neighborhood and postal code data.</li><li>This is the same problem I had to deal with in the past as well.</li><li>I ended up finding while debugging, that the empty data was essentially pinning the majority of the training data as <code>neighborhood : 'nan'</code>, and so the predictions, per this confusion matrix, were basically all the same output.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ipdb<span style=color:#f92672>&gt;</span> pp skm<span style=color:#f92672>.</span>confusion_matrix(y_validation, y_predictions, classes)
</span></span><span style=display:flex><span>array([[    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>174</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>116</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>130</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>357</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>364</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>255</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>862</span>],
</span></span><span style=display:flex><span>       [    <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>,     <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>97977</span>]])
</span></span><span style=display:flex><span>ipdb<span style=color:#f92672>&gt;</span> 
</span></span></code></pre></div><p><img loading=lazy src=https://user-images.githubusercontent.com/2048242/48679839-d5bc2600-eb62-11e8-8d99-b6bdbf82e6e0.png alt=image></p><ul><li>Of course one data problem always leads to another data problem. This time around, when I started updating my docking station geolocation data, I found that my payment information may have changed and so I was getting the following</li></ul><pre tabindex=0><code>ipdb&gt; pp geocoding_result
{u&#39;error_message&#39;: u&#39;You have exceeded your daily request quota for this API. If you did not set a custom daily request quota, verify your project has an active billing account: http://g.co/dev/maps-no-account&#39;,
 u&#39;results&#39;: [],
 u&#39;status&#39;: u&#39;OVER_QUERY_LIMIT&#39;}
</code></pre><ul><li>And of course I also ended up stepping on my own foot as well. I found I had created an unfortinate <a href=https://github.com/namoopsoo/learn-citibike/commit/438e425482db1c105ae6a22b8248696d0f91dfef>git commit</a> where I accidentally undid the url-encoding and intersections with <code>&</code> are I think treated as query string parameters , , which ended up being a reason why some of my data was coming back as just the geolocation for a single street (<code>route</code>) and not an actual intersection.</li></ul><h4 id=future-improvements>Future Improvements<a hidden class=anchor aria-hidden=true href=#future-improvements>#</a></h4><ul><li>I am hoping to come back to this and continue to iterate the approach.</li><li>In particular, I would like to continue to explore model degradation over time.</li><li>And in discussing with a few colleagues, seasonality would also be a really good feature to consider. Time bucketing was explored to a limited extent, but the day of the week nor the month of the year was not explored.</li><li>There may also be many other datasets which can be joined with this one to bolster the information available, including information about the weather or perhaps other demographic attributes available.</li><li>A more thorough comparison of algorithms should also be considered.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2017-12-01-spokes/><span class=title>« Prev</span><br><span>Spoke too soon</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2023-09-30-petrol-car-queue-practice-question/><span class=title>Next »</span><br><span>petrol car queue question</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>