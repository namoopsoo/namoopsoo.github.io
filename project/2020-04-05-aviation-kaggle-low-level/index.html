<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Physiological deep learnings | michal.piekarczyk.xyz</title>
<meta name=keywords content><meta name=description content="learning physiological state from time series data"><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Physiological deep learnings"><meta property="og:description" content="learning physiological state from time series data"><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/"><meta property="og:image" content="https://my-blog-content.s3.amazonaws.com/2019/2019-07-28+14.27.08-airplane.jpg"><meta property="article:section" content="project"><meta property="article:published_time" content="2020-04-05T10:00:00-04:00"><meta property="article:modified_time" content="2023-02-26T19:27:16-05:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://my-blog-content.s3.amazonaws.com/2019/2019-07-28+14.27.08-airplane.jpg"><meta name=twitter:title content="Physiological deep learnings"><meta name=twitter:description content="learning physiological state from time series data"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Side Projects","item":"https://michal.piekarczyk.xyz/project/"},{"@type":"ListItem","position":2,"name":"Physiological deep learnings","item":"https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Physiological deep learnings","name":"Physiological deep learnings","description":"learning physiological state from time series data","keywords":[],"articleBody":"Summary Below, I write a bit retrospectively about my notes from the “Reducing Commercial Aviation Fatalities” kaggle, trying to summarize some of the journey. I try to give some high lights from my various notebook entries. ( github )\nMy High Level Recap This physiology data classification challenge poses the question, given this time series voltage data of pilots’ respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain data) can something reliable be said about their physiological state? My response was to use this as an opportunity to learn about TensorFlow and LSTMs. I quickly discovered that data processing around time series data is 3 dimensional as opposed to typical 2 dimensional data. That means that the harmless 1.1 GiB of training data can quickly multiply to roughly 256 GiB if one is interested in using a 256 long sequence window. That means I learned a lot more about numpy for its simplicity around transforming 3 dimensional data. I had to adapt to using h5py chunks of data so as not to run out of memory quickly and not wait endless hours for training sessions to merely crash. As for TensorFlow and LSTMs, I did not realize right away but LSTMs (and likely neural nets in general) are quite sensitive to data that is not scaled and my logloss ended up reducing when I applied scaling techniques. Raw matplotlib became more intuitive for helping to visualize not just the time series data itself, but also for plotting logloss while training, across batches . . My Jupyter Notebook hygiene and workflow got better really quickly too, because I needed a reliable tool for distinguishing one day’s experiment from another’s without mixing the together the data or the models.\nThis dataset was highly skewed in terms of classes and so one of the really important preprocessing tasks was creating both balanced training “techniques” and balanced test data for more uniformly judging model performance. And I say “techniques”, because I tried both balanced weights and balanced training sets. I ended up preferring balanced training datasets, because that meant less preprocessing code.\nThe picture of Kaggle data I had in my head was clean datasets, but this dataset had one huge problem in that the time series data was not actually sorted by, you know, time. But in a way it is always fun to deal with messy data because it makes you more engaged with it and still more curious in the outcomes.\nIn general this project has given me a lot of fun memories.\nOne day after already getting deep into my LSTM approach, I decided to look through the Kaggle discussions for this project and I found that most people actually stuck to gradient boosting machines like lightGBM or XGBoost. But I decided to follow my personal motto of taking the path less traveled so I kept going with the LSTM .\nI have spent I think half a year of weekends on this problem. I have memories of learning about neural network architecture learning “capacity” at my niece’s birthday party. I came to understand that creating a larger network can cause it to memorize more as opposed to generalize .\nI remember tweaking my stochastic gradient descent batch size after reading this Yann LeCun tweet , “Training with large minibatches is bad for your health. More importantly, it’s bad for your test error. Friends dont let friends use minibatches larger than 32.” .\nI also have memories of starting modeling experiments before going on runs and before going to sleep, so that I could let TensorFlow spin its wheels while I took my mind into strategy mode or just let myself meditate.\nAt one point I was at the Boston Amtrak terminal waiting for my bus, getting deeper into why it is handy to look at raw logit data coming out of a model, especially in a multiclass problem because it can show how strongly a model classifies each class. But applying the logistic function or a softmax is of course good for sussing out probabilities. But then I realized I was waiting for a bus at an Amtrak terminal and I had to sprint several blocks to actually catch my bus!\nAt the end of the day I think of all of the amazing things I could one day do with this kind of technology, such as classifying music or building a chat bot (maybe even one that can tell jokes).\nTable of Contents Quick intro to the data The data is weirdly partially sorted Trickiness of the how the data is laid out (crews and seats?!) Some more visual inspection Building datasets Scaling Shuffling and adjusting dropout More epochs? Weight initialization Class balance Active Learning: changing the training approach Full training set error Shuffling train/test Reconsider that high dropout Prediction Speedup Quick intro to the data The physiological data includes several types (including respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain data)) across multiple \"crews\" . A crew includes two \"seats\" (0 and 1). We are provided with 256 measurements per second across three experiments (Channelized Attention (CA) , Diverted Attention (DA) and Startle/Surprise (SS) ). Across the three experiments, four target “states” (or classes) are labeled for all of the rows in the data.\nThe four classes of events in the training data, 'A', 'B', 'C', 'D' , correspond to three target physiological states of the three experiments, plus a neutral baseline state:\nlabel description A Baseline B Startle/Surprise C Channelized Attention D Diverted Attention In my early notebook I took a quick look at the proportion of experiment time had been spent in the different states. Per the below, looking at each person separately, I saw that for the first person as an example, (crew=1, seat=0) , ~98% of CA was labeled as “Channelized Attention” and 2% as baseline. But for the other two experiments, the target states appear to be much more brief, with only 13% of the “DA” and 9% of the “SS” experiments.\n(copying an output from that notebook…)\nstatsdf = gpdf.groupby(by=['crew', 'seat']).apply(extract_proportions).reset_index() In [78]: statsdf Out[78]: crew seat A/CA A/DA A/SS C/CA D/DA B/SS 0 1 0 0.018593 0.868871 0.903015 0.981407 0.131129 0.096985 1 1 1 0.018803 0.879030 0.902685 0.981197 0.120970 0.097315 2 2 0 0.001954 0.857941 0.916630 0.998046 0.142059 0.083370 3 2 1 0.001781 0.848619 0.916605 0.998219 0.151381 0.083395 4 3 0 0.001248 0.854135 0.916782 0.998752 0.145865 0.083218 5 3 1 0.000597 0.860974 0.916772 0.999403 0.139026 0.083228 6 4 0 0.001302 0.868853 0.916514 0.998698 0.131147 0.083486 7 4 1 0.001400 0.860968 0.916706 0.998600 0.139032 0.083294 8 5 0 0.001661 0.847193 0.916730 0.998339 0.152807 0.083270 9 5 1 0.001791 0.857766 0.916472 0.998209 0.142234 0.083528 10 6 0 0.002311 0.860514 0.916711 0.997689 0.139486 0.083289 11 6 1 0.001661 0.858872 0.916748 0.998339 0.141128 0.083252 12 7 0 0.001563 0.867075 0.916536 0.998437 0.132925 0.083464 13 7 1 0.001607 0.855907 0.916471 0.998393 0.144093 0.083529 14 8 0 0.000999 0.856505 0.915394 0.999001 0.143495 0.084606 15 8 1 0.001053 0.853877 0.915412 0.998947 0.146123 0.084588 16 13 0 0.001801 0.841341 0.916482 0.998199 0.158659 0.083518 17 13 1 0.001628 0.847312 0.916595 0.998372 0.152688 0.083405 The data is weirdly partially sorted As I mention in this notebook, when indexing on what I believe are the main uniqueness constraint columns (crew, seat, experiment), the time is not sorted. There are strange jumps, such as this one which I borrow from my notebook .. I show the first three rows (0, 1, 2) and also where I see the jump starting from around 6600, where row 6606 goes back in time.\npd.concat([ df[['crew', 'seat', 'time', 'r', 'experiment', 'event']].iloc[:3], df[['crew', 'seat', 'time', 'r', 'experiment', 'event']].iloc[6600:6610]]) crew\tseat\ttime\tr\texperiment\tevent 0\t1\t1\t0.011719\t817.705994\tCA\tA 1\t1\t1\t0.015625\t817.705994\tCA\tA 2\t1\t1\t0.019531\t817.705994\tCA\tA 6600\t1\t1\t109.988281\t817.437988\tCA\tC 6601\t1\t0\t109.988281\t664.265991\tCA\tC 6602\t1\t0\t109.992188\t664.265991\tCA\tC 6603\t1\t1\t109.992188\t817.442017\tCA\tC 6604\t1\t1\t109.996094\t817.442017\tCA\tC 6605\t1\t0\t109.996094\t664.265991\tCA\tC 6606\t1\t0\t11.000000\t664.331970\tCA\tC 6607\t1\t1\t11.000000\t817.898987\tCA\tC 6608\t1\t0\t11.003906\t664.331970\tCA\tC 6609\t1\t1\t11.003906\t817.898987\tCA\tC I sort this data myself, but it is confusing for sure.\nTrickiness of the how the data is laid out (crews and seats?!) In the process of visualizing data, I had been using matplot lib to visualize the four different classes of events, 'A', 'B', 'C', 'D' as red, green, blue and cyan. That way I could potentially try to get an intuition around the visual cues around state transitions. But at one point I had accidentally been combining the data of multiple people.\nAbove, extracting a plot from my 2019-10-26 notebook, is an example of where I plot combined multi seat data by accident. At one point this was weirding me out. But I realized finally that I had been combining the data of multiple people.\nFor diagram above (^^) , I had written a quick function produce_plots_for_col for plotting four features simultaneously, given a pandas dataframe, some features and an interval, but indeed the zig zag plot was a bit baffling for a bit.\nstart = 3400; produce_plots_for_col(df, ['r', 'ecg', 'gsr', 'eeg_fp1'], range(start,start+150)) When I look back at my notebook I wrote about how I was very dumbfounded when I realized I combined them by accident. The data is complicated however. It includes four indexing columns: id , time , crew and seat . And indeed being careful with splitting this was key in creating good datasets.\nSome more visual inspection in my 2019-06-08 notebook I used another nice quick visual inspection technique, looking at some time series data samples at the four classes, Building datasets I spent a lot of time next building data sets, here , and , and building basic quick and dirty LSTM tensor flow models. And also . I also tried different approaches for understanding the models I was building. Including looking at raw logits, as per the below graphic, from my 2019-07-13-Four notebook. I thought this was a cool method compared to a confusion matrix for instance is because it shows the raw logits of each of the four classes, before the argmax voting observed in a confusion matrix is done.\nScaling I took a deeper histogram look at my data, seeing quite a lot of ups and downs.\n(Given that there were some crazy jumps, I thought I needed to do something about that) And so on 2019-12-21 , I ended up trying out more scaling approaches, especially MinMaxScaler. I had 8 features I was focusing on at that point and I plotted how my minMaxScaler min and max parameters changed as I processed roughly 40 or so mini datasets I had in my h5 training file data/2019-12-21T215926Z/train.h5. Re-posting my image :\nLuckily I found I was able to use just a single sklearn MinMaxScaler object to capture all 8 features at once. I then applied the scalers to transform my train.h5 data to a train_scaled.h5 dataset. And I also ended up with a balanced dataset , train_balanced.h5, that I could use for training.\nI trained a model and plotted training and validation loss curves the next day .\nAnd wow the validation loss ( link ) looked intense ,\nAs a side note. although the validation loss here looks totally skewed towards class 1 , I want to step back and note I really appreciate the technique of actually creating the “balanced” test set I referred to above. That allows us to quickly knows the model is favoring one class over another in the first place. And also I really dig the technique of simply snapshotting the tensorflow models while training and then being able to know how the validation logloss looks across those training batches. I feel like combining these techniques was really helpful in digesting what is going on . I needed to enjoy little details like that amidst all of the trial and error that was happening here (Emphasis on the error part haha).\nShuffling and adjusting dropout At a later date , I adjusted my lstm dropout from 0.2 to 0.7 , seeing quite different behavior in the validation loss. I had also added some shuffling code taking my 'history/2019-12-22T174803Z/train_balanced.h5' dataset to produce 'history/2019-12-22T174803Z/train_scaled_balanced_shuffled.h5' , to possibly change some of the choppiness of the validation curve seen above ^^ . That produced a validation loss , reposting the image here,\nMore epochs? On 2018-12-28 I got curious about whether just throwing more data at this problem would help. So I extended my waiting time by two and let the training happen in two epochs . The validation loss from here , (reposting…) however showed that throwing more data is not always the answer. It always depends haha.\nWeight initialization Per my notebook entry I had read per this article that the default tensor flow weight initialization I had been using was GlorotUniform , ( which is aka Xavier Uniform apparently ) . I realized it was at least worth considering weight initialization as another hyper parameter so here I tried the Glorot or Xavier Normal instead . The validation loss did not necessarily convey the difference however:\nAt this point I think I was realizing that the order of ideas to try matters. And you do not know in advance what is the best order. Perhaps the weight initialization matters a good deal, but I had not yet found the critical next step yet at that point.\nClass balance In my next notebook I wanted to understand why my class 1 kept getting favored. I tried out forcing the weights of my training data to basically\n{0: 1., 1: 0., 2: 0., 3: 0.} to see what happens and sure enough, per the validation loss , the loss now went down only for class 0. So the effect was controlled.\nActive Learning: changing the training approach Somehow I came upon the idea of preferentially training on what your model is doing poorly on. So on 2020-01-19 I modified my training loop so that I dynamically adjusted my training weights according to which class was being misclassified. The effect on the training loss was really interesting. Everything was way smoother.\nLooking at a training loss plot from earlier ( such as from 2019-12-28 )\nhttps://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png\nThe validation loss was still favoring that one class, but I decided to hold on to this technique and keep trying other things.\nFull training set error Next in this notebook I wanted to better answer whether my particular test set perhaps had some very different size data compared to my training set, which was blowing up my test set error. I did not have enough data to better split apart my data at the moment actually, but instead I took a quick detour to compare my training mini batch loss curves to the full training set losses, during training. Naturally one would expect that if batch training losses improve that overall training set loss should also improve. Per the below diagram from my notebook, that was indeed the case.\nShuffling train/test After having consistently weird results with validation error, I decided to try re-building my train/test sets by doing a full random shuffle instead, in my 2020-02-01 notebook. Up until this point I had been using the fact that the data is divided into crew1, crew2, crew3, etc and I have used crew1 for train and crew2 for test. And I had built scalers from my crew1 training data, applying them to the the crew2 test data.\nSo this time around I instead built scalers from crew1 and then changed my function, build_many_scalers_from_h5 to take scalers as a parameter and I kept updating them with the test data. ( My scalers 'history/2020-02-02T044441Z/scalers.joblib' was the restulting artifact).\nIn validation , I think for the first time, I saw the validation error actually start going down ,\nI took that further in this 2020-02-08 notebook and ( showing my figure again ) going 3 epochs instead, I got ..\nSo the bright side I take from this is that the validation loss is actually doing better for three out of four of the classes.\nReconsider that high dropout Next in 2020-02-15 notebook, I decided to reduce my dropout slightly, after reading through this post about treating the dropout as yet another hyperparameter. After retraining , across 2 epochs, I saw a validation loss curve, which looked better still.\nThis indicates perhaps the context of hyperparameters being experimented with indeed matters . I think Andrew Ng’s characterization of “model babysitting”.\nhttps://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md\nPrediction Speedup At this point I had improved my validation results enough that I wanted to submit my predictions. But as I described in my notebook , the 17.9 million test examples would take potentially 25 hours per my back of the envelope calculation.\nBut luckily I discovered that just playing with my prediction batch size, changing it from 32 to 1024, I found I could cut my time from 56k examples/292 seconds to 56k examples/27s , taking the back of the envelope calculation from 25 hours to 2.5 hours !..\nI also ended up utilizing awk to actually build batches for predict, avoiding trying to squeeze everything into memory.\nAnd for one final optimization, I added multi-processing with joblib , to take advantage of all of my available cores.\nThe steps get more detailed in this notebook.\n","wordCount":"2922","inLanguage":"en","image":"https://my-blog-content.s3.amazonaws.com/2019/2019-07-28+14.27.08-airplane.jpg","datePublished":"2020-04-05T10:00:00-04:00","dateModified":"2023-02-26T19:27:16-05:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/project/>Side Projects</a></div><h1 class=post-title>Physiological deep learnings</h1><div class=post-meta>&lt;span title='2020-04-05 10:00:00 -0400 -0400'>April 5, 2020&lt;/span>&amp;nbsp;·&amp;nbsp;14 min&amp;nbsp;·&amp;nbsp;2922 words&amp;nbsp;·&amp;nbsp;Michal Piekarczyk</div></header><figure class=entry-cover><img loading=lazy src=https://my-blog-content.s3.amazonaws.com/2019/2019-07-28+14.27.08-airplane.jpg alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li></li><li><a href=#scaling>Scaling</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h4 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h4><p><em>Below, I write a bit retrospectively about my notes from the <a href=https://www.kaggle.com/c/reducing-commercial-aviation-fatalities/data>&ldquo;Reducing Commercial Aviation Fatalities&rdquo; kaggle</a>, trying to summarize some of the journey. I try to give some high lights
from my various notebook entries. ( <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm>github</a> )</em></p><h4 id=my-high-level-recap>My High Level Recap<a hidden class=anchor aria-hidden=true href=#my-high-level-recap>#</a></h4><p>This physiology data classification challenge poses the question, <em>given this time series voltage data of pilots&rsquo; respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-04-05-organzing-thoughts.md#as-a-quick-intro-to-the-data>data</a>) can something reliable be said about their physiological state?</em> My response was to use this as an opportunity to learn about TensorFlow and <strong>LSTMs</strong>. I quickly discovered that data processing around time series data is <code>3 dimensional</code> as opposed to typical <code>2 dimensional</code> data. That means that the harmless <code>1.1 GiB</code> of training data can quickly multiply to roughly <code>256 GiB</code> if one is interested in using a <code>256 long</code> sequence window. That means I learned a lot more about <code>numpy</code> for its simplicity around transforming <code>3 dimensional</code> data. I had to adapt to using <code>h5py</code> <em>chunks</em> of data so as not to run out of memory quickly and not wait endless hours for training sessions to merely crash. As for <em>TensorFlow</em> and <em>LSTMs</em>, I did not realize right away but <em>LSTMs</em> (and likely neural nets in general) are quite sensitive to data that is not scaled and my logloss ended up reducing when I <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-04-05-organzing-thoughts.md#scaling>applied scaling techniques</a>. Raw <code>matplotlib</code> became more intuitive for helping to visualize not just the time series data itself, but also for plotting <code>logloss</code> while training, across <em>batches</em> . . My <strong>Jupyter Notebook</strong> hygiene and workflow got better really quickly too, because I needed a reliable tool for distinguishing one day&rsquo;s experiment from another&rsquo;s without mixing the together the data or the models.</p><p>This dataset was highly skewed in terms of <em>classes</em> and so one of the really important preprocessing tasks was creating both <em>balanced</em> training &ldquo;techniques&rdquo; and <em>balanced</em> test data for more uniformly judging model performance. And I say &ldquo;techniques&rdquo;, because I tried both balanced weights and balanced training sets. I ended up preferring balanced training datasets, because that meant less preprocessing code.</p><p>The picture of <em>Kaggle</em> data I had in my head was clean datasets, but this dataset had one huge problem in that the time series data was not actually sorted by, you know, <code>time</code>. But in a way it is always fun to deal with messy data because it makes you more engaged with it and still more curious in the outcomes.</p><p><em>In general this project has given me a lot of fun memories.</em></p><p>One day after already getting deep into my <strong>LSTM</strong> approach, I decided to look through the <em>Kaggle</em> discussions for this project and I found that most people actually stuck to gradient boosting machines like lightGBM or XGBoost. But I decided to follow my personal motto of taking the path less traveled so I kept going with the <em>LSTM</em> .</p><p>I have spent I think half a year of weekends on this problem. I have memories of learning about neural network architecture learning &ldquo;capacity&rdquo; at my niece&rsquo;s birthday party. I came to understand that creating a larger network can cause it to <em>memorize</em> more as opposed to <em>generalize</em> .</p><p>I remember tweaking my <em>stochastic gradient descent</em> batch size after reading this Yann LeCun <a href=https://mobile.twitter.com/ylecun/status/989610208497360896>tweet</a> , <em>&ldquo;Training with large minibatches is bad for your health. More importantly, it&rsquo;s bad for your test error. Friends dont let friends use minibatches larger than 32.&rdquo;</em> .</p><p>I also have memories of starting modeling experiments before going on runs and before going to sleep, so that I could let <em>TensorFlow</em> spin its wheels while I took my mind into strategy mode or just let myself meditate.</p><p>At one point I was at the Boston Amtrak terminal waiting for my bus, getting deeper into why it is handy to look at raw <em>logit</em> data coming out of a model, especially in a multiclass problem because it can show how strongly a model classifies each class. But applying the logistic function or a <em>softmax</em> is of course good for sussing out probabilities. But then I realized I was waiting for a bus at an Amtrak terminal and I had to sprint several blocks to actually catch my bus!</p><p>At the end of the day I think of all of the amazing things I could one day do with this kind of technology, such as classifying music or building a chat bot (maybe even one that can tell jokes).</p><h4 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h4><ul><li><a href=#quick-intro-to-the-data>Quick intro to the data</a></li><li><a href=#the-data-is-weirdly-partially-sorted>The data is weirdly partially sorted</a></li><li><a href=#trickiness-of-the-how-the-data-is-laid-out-crews-and-seats>Trickiness of the how the data is laid out (crews and seats?!)</a></li><li><a href=#some-more-visual-inspection>Some more visual inspection</a></li><li><a href=#building-datasets>Building datasets</a></li><li><a href=#scaling>Scaling</a></li><li><a href=#shuffling-and-adjusting-dropout>Shuffling and adjusting dropout</a></li><li><a href=#more-epochs>More epochs?</a></li><li><a href=#weight-initialization>Weight initialization</a></li><li><a href=#class-balance>Class balance</a></li><li><a href=#active-learning-changing-the-training-approach>Active Learning: changing the training approach</a></li><li><a href=#full-training-set-error>Full training set error</a></li><li><a href=#shuffling-traintest>Shuffling train/test</a></li><li><a href=#reconsider-that-high-dropout>Reconsider that high dropout</a></li><li><a href=#prediction-speedup>Prediction Speedup</a></li></ul><h4 id=quick-intro-to-the-data>Quick intro to the data<a hidden class=anchor aria-hidden=true href=#quick-intro-to-the-data>#</a></h4><p>The physiological data includes several types <em>(including respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain data))</em> across multiple <code>"crews"</code> . A crew includes two <code>"seats"</code> (<code>0</code> and <code>1</code>). We are provided with <code>256 measurements per second</code> across three experiments (Channelized Attention (CA) , Diverted Attention (DA) and Startle/Surprise (SS) ). Across the three experiments, four target &ldquo;states&rdquo; (or classes) are labeled for all of the rows in the data.</p><p>The four classes of events in the training data, <code>'A', 'B', 'C', 'D'</code> , correspond to three target physiological states of the three experiments, plus a neutral baseline state:</p><table><thead><tr><th>label</th><th>description</th></tr></thead><tbody><tr><td>A</td><td>Baseline</td></tr><tr><td>B</td><td>Startle/Surprise</td></tr><tr><td>C</td><td>Channelized Attention</td></tr><tr><td>D</td><td>Diverted Attention</td></tr></tbody></table><p>In my early <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-10-initial-look.md>notebook</a> I took a quick look at the proportion of experiment time had been spent in the different states. Per the below, looking at each person separately, I saw that for the first person as an example, (<code>crew=1, seat=0</code>) , <code>~98%</code> of <code>CA</code> was labeled as &ldquo;Channelized Attention&rdquo; and <code>2%</code> as baseline. But for the other two experiments, the target states appear to be much more brief, with only <code>13%</code> of the &ldquo;DA&rdquo; and <code>9%</code> of the &ldquo;SS&rdquo; experiments.</p><p><em>(copying an output from that notebook&mldr;)</em></p><pre tabindex=0><code>statsdf = gpdf.groupby(by=[&#39;crew&#39;, &#39;seat&#39;]).apply(extract_proportions).reset_index()
In [78]: statsdf                                                                                                                         
Out[78]:
    crew  seat      A/CA      A/DA      A/SS      C/CA      D/DA      B/SS
0      1     0  0.018593  0.868871  0.903015  0.981407  0.131129  0.096985
1      1     1  0.018803  0.879030  0.902685  0.981197  0.120970  0.097315
2      2     0  0.001954  0.857941  0.916630  0.998046  0.142059  0.083370
3      2     1  0.001781  0.848619  0.916605  0.998219  0.151381  0.083395
4      3     0  0.001248  0.854135  0.916782  0.998752  0.145865  0.083218
5      3     1  0.000597  0.860974  0.916772  0.999403  0.139026  0.083228
6      4     0  0.001302  0.868853  0.916514  0.998698  0.131147  0.083486
7      4     1  0.001400  0.860968  0.916706  0.998600  0.139032  0.083294
8      5     0  0.001661  0.847193  0.916730  0.998339  0.152807  0.083270
9      5     1  0.001791  0.857766  0.916472  0.998209  0.142234  0.083528
10     6     0  0.002311  0.860514  0.916711  0.997689  0.139486  0.083289
11     6     1  0.001661  0.858872  0.916748  0.998339  0.141128  0.083252
12     7     0  0.001563  0.867075  0.916536  0.998437  0.132925  0.083464
13     7     1  0.001607  0.855907  0.916471  0.998393  0.144093  0.083529
14     8     0  0.000999  0.856505  0.915394  0.999001  0.143495  0.084606
15     8     1  0.001053  0.853877  0.915412  0.998947  0.146123  0.084588
16    13     0  0.001801  0.841341  0.916482  0.998199  0.158659  0.083518
17    13     1  0.001628  0.847312  0.916595  0.998372  0.152688  0.083405
</code></pre><h4 id=the-data-is-weirdly-partially-sorted>The data is weirdly partially sorted<a hidden class=anchor aria-hidden=true href=#the-data-is-weirdly-partially-sorted>#</a></h4><p>As I mention in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-14-wrangling-time-data.md>this notebook</a>, when indexing on what I believe are the main uniqueness constraint columns (<code>crew</code>, <code>seat</code>, <code>experiment</code>), the <code>time</code> is not sorted. There are strange jumps, such as this one which I borrow from my notebook .. I show the first three rows (<code>0, 1, 2</code>) and also where I see the jump starting from around <code>6600</code>, where row <code>6606</code> goes back in time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pd<span style=color:#f92672>.</span>concat([
</span></span><span style=display:flex><span>        df[[<span style=color:#e6db74>&#39;crew&#39;</span>, <span style=color:#e6db74>&#39;seat&#39;</span>, <span style=color:#e6db74>&#39;time&#39;</span>, <span style=color:#e6db74>&#39;r&#39;</span>, <span style=color:#e6db74>&#39;experiment&#39;</span>, <span style=color:#e6db74>&#39;event&#39;</span>]]<span style=color:#f92672>.</span>iloc[:<span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>        df[[<span style=color:#e6db74>&#39;crew&#39;</span>, <span style=color:#e6db74>&#39;seat&#39;</span>, <span style=color:#e6db74>&#39;time&#39;</span>, <span style=color:#e6db74>&#39;r&#39;</span>, <span style=color:#e6db74>&#39;experiment&#39;</span>, <span style=color:#e6db74>&#39;event&#39;</span>]]<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>6600</span>:<span style=color:#ae81ff>6610</span>]])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>	crew	seat	time	r	experiment	event
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0.011719</span>	<span style=color:#ae81ff>817.705994</span>	CA	A
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0.015625</span>	<span style=color:#ae81ff>817.705994</span>	CA	A
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0.019531</span>	<span style=color:#ae81ff>817.705994</span>	CA	A
</span></span><span style=display:flex><span><span style=color:#ae81ff>6600</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>109.988281</span>	<span style=color:#ae81ff>817.437988</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6601</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>109.988281</span>	<span style=color:#ae81ff>664.265991</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6602</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>109.992188</span>	<span style=color:#ae81ff>664.265991</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6603</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>109.992188</span>	<span style=color:#ae81ff>817.442017</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6604</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>109.996094</span>	<span style=color:#ae81ff>817.442017</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6605</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>109.996094</span>	<span style=color:#ae81ff>664.265991</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6606</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>11.000000</span>	<span style=color:#ae81ff>664.331970</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6607</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>11.000000</span>	<span style=color:#ae81ff>817.898987</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6608</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>11.003906</span>	<span style=color:#ae81ff>664.331970</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6609</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>11.003906</span>	<span style=color:#ae81ff>817.898987</span>	CA	C
</span></span></code></pre></div><p>I sort this data myself, but it is confusing for sure.</p><h4 id=trickiness-of-the-how-the-data-is-laid-out-crews-and-seats>Trickiness of the how the data is laid out (crews and seats?!)<a hidden class=anchor aria-hidden=true href=#trickiness-of-the-how-the-data-is-laid-out-crews-and-seats>#</a></h4><p>In the process of visualizing data, I had been using matplot lib to visualize the four different classes of events, <code>'A', 'B', 'C', 'D'</code> as red, green, blue and cyan. That way I could potentially try to get an intuition around the visual cues around state transitions. But at one point I had accidentally been combining the data of multiple people.</p><p>Above, extracting a plot from my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-10-26.md>2019-10-26 notebook</a>, is an example of where I plot combined multi seat data by accident. At one point this was weirding me out. But I realized finally that I had been combining the data of multiple people.</p><p>For diagram above (^^) , I had written a quick function <code>produce_plots_for_col</code> for plotting four features simultaneously, given a pandas dataframe, some features and an interval, but indeed the <em>zig zag</em> plot was a bit baffling for a bit.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>start <span style=color:#f92672>=</span> <span style=color:#ae81ff>3400</span>; produce_plots_for_col(df, [<span style=color:#e6db74>&#39;r&#39;</span>, <span style=color:#e6db74>&#39;ecg&#39;</span>, <span style=color:#e6db74>&#39;gsr&#39;</span>, <span style=color:#e6db74>&#39;eeg_fp1&#39;</span>],
</span></span><span style=display:flex><span>                                range(start,start<span style=color:#f92672>+</span><span style=color:#ae81ff>150</span>))
</span></span></code></pre></div><p>When I look back at my notebook I wrote about how I was very <em>dumbfounded</em> when I realized I combined them by accident. The data is complicated however. It includes four indexing columns: <code>id</code> , <code>time</code> , <code>crew</code> and <code>seat</code> . And indeed being careful with splitting this was key in creating good datasets.</p><h4 id=some-more-visual-inspection>Some more visual inspection<a hidden class=anchor aria-hidden=true href=#some-more-visual-inspection>#</a></h4><ul><li><a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-06-08-visually-inspect-generated-sequences.md#and-plot>in my 2019-06-08 notebook</a> I used another nice quick visual inspection technique, looking at some time series data samples at the four classes,</li></ul><p></p><h4 id=building-datasets>Building datasets<a hidden class=anchor aria-hidden=true href=#building-datasets>#</a></h4><ul><li>I spent a lot of time next building data sets, <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-06-23-today.md>here</a> , <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-07-06-today.md>and</a> , and building basic quick and dirty LSTM tensor flow models. <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-07-13-Five-more-data.md>And also</a> .</li></ul><p>I also tried different approaches for understanding the models I was building. Including looking at raw logits, as per the below graphic, from my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-07-13-Four.md#look-at-them-logits>2019-07-13-Four notebook</a>. I thought this was a cool method compared to a confusion matrix for instance is because it shows the raw logits of each of the four classes, before the argmax voting observed in a confusion matrix is done.</p><h3 id=scaling>Scaling<a hidden class=anchor aria-hidden=true href=#scaling>#</a></h3><p>I took a deeper <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-14--annotated.md>histogram</a> look at my data, seeing quite a lot of <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-14--annotated.md#another-time-series-look>ups and downs</a>.</p><p><em>(Given that there were some crazy jumps, I thought I needed to do something about that)</em></p><p>And so
<a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md>on 2019-12-21</a> , I ended up trying out more scaling approaches, especially <code>MinMaxScaler</code>. I had <code>8</code> features I was focusing on at that point and I plotted how my <code>minMaxScaler</code> <code>min</code> and <code>max</code> parameters changed as I processed roughly <code>40</code> or so mini datasets I had in my h5 training file <code>data/2019-12-21T215926Z/train.h5</code>. <em>Re-posting <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md#plot-the-scale-parameters>my image</a></em> :</p><p>Luckily I found I was able to use just a single sklearn MinMaxScaler object to capture all <code>8</code> features at once.
I then <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md#make-scaled-data>applied</a> the scalers to transform my <code>train.h5</code> data to a <code>train_scaled.h5</code> dataset. And I also ended up with a <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md#ok-now-make-balanced-data-again>balanced dataset</a> , <code>train_balanced.h5</code>, that I could use for training.</p><p>I trained a model and plotted training and validation loss curves the <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-22.md>next day</a> .</p><p>And wow the validation loss ( <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-22.md#plotting-validation-loss-at-model-snapshots>link</a> ) looked intense ,</p><p>As a side note. although the validation loss here looks totally skewed towards <code>class 1</code> , I want to step back and note I really appreciate the technique of actually creating the <em>&ldquo;balanced&rdquo;</em> test set I referred to above. That allows us to quickly knows the model is favoring one class over another in the first place. And also I really dig the technique of simply snapshotting the tensorflow models while training and then being able to know how the validation logloss looks across those training batches. I feel like combining these techniques was really helpful in digesting what is going on . I needed to enjoy little details like that amidst all of the trial and error that was happening here (Emphasis on the error part haha).</p><h4 id=shuffling-and-adjusting-dropout>Shuffling and adjusting dropout<a hidden class=anchor aria-hidden=true href=#shuffling-and-adjusting-dropout>#</a></h4><p>At a <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-25.md>later date</a> , I adjusted my lstm dropout from <code>0.2</code> to <code>0.7</code> , seeing quite different behavior in the validation loss. I had also added some <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-25.md#do-a-shuffle>shuffling code</a> taking my <code>'history/2019-12-22T174803Z/train_balanced.h5'</code> dataset to produce <code>'history/2019-12-22T174803Z/train_scaled_balanced_shuffled.h5'</code> , to possibly change some of the choppiness of the validation curve seen above ^^ . That produced a <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-25.md#validation-loss>validation loss</a> , reposting the image here,</p><h4 id=more-epochs>More epochs?<a hidden class=anchor aria-hidden=true href=#more-epochs>#</a></h4><p>On <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-28-two.md>2018-12-28</a> I got curious about whether just throwing more data at this problem would help. So I extended my waiting time by <code>two</code> and let the training happen in two <em>epochs</em> . The validation loss <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-28-two-plot.md#final-validation-logloss-plot>from here</a> , (reposting&mldr;) however showed that throwing more data is not always the answer. It always depends haha.</p><h4 id=weight-initialization>Weight initialization<a hidden class=anchor aria-hidden=true href=#weight-initialization>#</a></h4><p>Per my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-12.md>notebook entry</a> I had read per <a href=https://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/>this article</a> that the default tensor flow weight initialization I had <a href=https://www.tensorflow.org/api_docs/python/tf/keras/initializers>been using</a> was <em>GlorotUniform , ( which is aka Xavier Uniform apparently )</em> . I realized it was at least worth considering weight initialization as another hyper parameter so here I tried the <em>Glorot or Xavier Normal</em> instead . The <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-12.md#validation-loss>validation loss</a> did not necessarily convey the difference however:</p><p>At this point I think I was realizing that the order of ideas to try matters. And you do not know in advance what is the best order. Perhaps the weight initialization matters a good deal, but I had not yet found the critical next step yet at that point.</p><h4 id=class-balance>Class balance<a hidden class=anchor aria-hidden=true href=#class-balance>#</a></h4><p>In my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-18.md>next notebook</a> I wanted to understand why my <code>class 1</code> kept getting favored. I tried out <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-18.md#force-weights>forcing the weights</a> of my training data to basically</p><pre tabindex=0><code>{0: 1., 1: 0., 2: 0., 3: 0.}
</code></pre><p>to see what happens and sure enough, per the <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-18.md#validation-loss>validation loss</a> , the loss now went down only for class <code>0</code>. So the effect was controlled.</p><h4 id=active-learning-changing-the-training-approach>Active Learning: changing the training approach<a hidden class=anchor aria-hidden=true href=#active-learning-changing-the-training-approach>#</a></h4><p>Somehow I came upon the idea of preferentially training on what your model is doing poorly on. So on <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--update.md>2020-01-19</a> I modified my training loop so that I dynamically adjusted my training weights according to which class was being misclassified. The effect on the <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--update.md#plotting-train-loss-now-and-per-label-losses-too>training loss</a> was really interesting. Everything was way smoother.</p><p>Looking at a training loss plot from earlier ( such as from <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-28.md#crashed-last-batch-but-thats-okay>2019-12-28</a> )</p><p><a href=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png>https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png</a></p><p>The validation loss was still favoring that one class, but I decided to hold on to this technique and keep trying other things.</p><h4 id=full-training-set-error>Full training set error<a hidden class=anchor aria-hidden=true href=#full-training-set-error>#</a></h4><p>Next in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--more-test-evaluate-train.md>this notebook</a> I wanted to better answer whether my particular test set perhaps had some very different size data compared to my training set, which was blowing up my test set error. I did not have enough data to better split apart my data at the moment actually, but instead I took a quick detour to compare my training mini batch loss curves to the full training set losses, during training. Naturally one would expect that if batch training losses improve that overall training set loss should also improve. Per the below diagram from my notebook, that was indeed the case.</p><h4 id=shuffling-traintest>Shuffling train/test<a hidden class=anchor aria-hidden=true href=#shuffling-traintest>#</a></h4><p>After having consistently weird results with validation error, I decided to try re-building my train/test sets by doing a full random shuffle instead, in my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-01.md>2020-02-01 notebook</a>. Up until this point I had been using the fact that the data is divided into <code>crew1, crew2, crew3, etc</code> and I have used <code>crew1</code> for train and <code>crew2</code> for test. And I had built <code>scalers</code> from my <code>crew1</code> training data, applying them to the the <code>crew2</code> test data.</p><p>So this time around I instead built <code>scalers</code> from <code>crew1</code> and then changed my function, <code>build_many_scalers_from_h5</code> to take <code>scalers</code> as a parameter and I kept updating them with the test data. ( My scalers <code>'history/2020-02-02T044441Z/scalers.joblib'</code> was the restulting artifact).</p><p>In <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-01.md#final-chart>validation</a> , I think for the first time, I saw the validation error actually start going down ,</p><p>I took that further in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-08-take2--update--2.md>this 2020-02-08 notebook</a> and ( <em>showing my figure again</em> ) going 3 epochs instead, I got ..</p><p>So the bright side I take from this is that the validation loss is actually doing better for three out of four of the classes.</p><h4 id=reconsider-that-high-dropout>Reconsider that high dropout<a hidden class=anchor aria-hidden=true href=#reconsider-that-high-dropout>#</a></h4><p>Next in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md>2020-02-15 notebook</a>, I decided to reduce my dropout slightly, after reading through <a href=https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/>this post</a> about treating the dropout as yet another hyperparameter. After retraining , across 2 epochs, I saw a validation loss curve, which looked better still.</p><p>This indicates perhaps the context of hyperparameters being experimented with indeed matters . I think Andrew Ng&rsquo;s characterization of <em>&ldquo;model babysitting&rdquo;</em>.</p><p><a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md>https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md</a></p><h4 id=prediction-speedup>Prediction Speedup<a hidden class=anchor aria-hidden=true href=#prediction-speedup>#</a></h4><p>At this point I had improved my validation results enough that I wanted to submit my predictions. But as I described in my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-29-time.md>notebook</a> , the <code>17.9 million test examples</code> would take potentially <code>25 hours</code> per my back of the envelope calculation.</p><p>But luckily I discovered that just playing with my prediction batch size, changing it from <code>32</code> to <code>1024</code>, I found I could cut my time from <code>56k examples/292 seconds</code> to <code>56k examples/27s</code> , taking the back of the envelope calculation from <code>25 hours</code> to <code>2.5 hours</code> !..</p><p>I also ended up utilizing <code>awk</code> to actually build batches for predict, avoiding trying to squeeze everything into memory.</p><p>And for one final optimization, I added multi-processing with <code>joblib</code> , to take advantage of all of my available cores.</p><p>The steps get more detailed in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-03-07-run-test-set-snapshot6.md>this notebook</a>.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2020-05-30-trying-again/><span class=title>« Prev</span><br><span>Updates</span>
</a><a class=next href=https://michal.piekarczyk.xyz/post/2019-05-13--keras-hello-world-fashion/><span class=title>Next »</span><br><span>Keras Hello WOrld</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>