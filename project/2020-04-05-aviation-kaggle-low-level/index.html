<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Physiological deep learnings | My blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="learning physiological state from time series data"><meta name=generator content="Hugo 0.103.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/ananke/dist/main.css_5c99d70a7725bacd4c701e995b969fea.css><meta property="og:title" content="Physiological deep learnings"><meta property="og:description" content="learning physiological state from time series data"><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/"><meta property="article:section" content="project"><meta property="article:published_time" content="2020-04-05T10:00:00-04:00"><meta property="article:modified_time" content="2020-04-05T10:00:00-04:00"><meta property="og:site_name" content="My blog"><meta itemprop=name content="Physiological deep learnings"><meta itemprop=description content="learning physiological state from time series data"><meta itemprop=datePublished content="2020-04-05T10:00:00-04:00"><meta itemprop=dateModified content="2020-04-05T10:00:00-04:00"><meta itemprop=wordCount content="2987"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Physiological deep learnings"><meta name=twitter:description content="learning physiological state from time series data"></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://my-blog-content.s3.amazonaws.com/2019/2019-07-28+14.27.08-airplane.jpg)><div class="pb3-m pb6-l bg-black-60"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">My blog</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/handy/ title="Handy page">Handy</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/post/ title="Post page">Post</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/project/ title="Side Projects page">Side Projects</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/foo/ title="The Foos page">The Foos</a></li></ul></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Physiological deep learnings</h1></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">SIDE PROJECTS</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/&text=Physiological%20deep%20learnings" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://michal.piekarczyk.xyz/project/2020-04-05-aviation-kaggle-low-level/&title=Physiological%20deep%20learnings" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">Physiological deep learnings</h1><time class="f6 mv4 dib tracked" datetime=2020-04-05T10:00:00-04:00>April 5, 2020</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h4 id=summary>Summary</h4><p><em>Below, I write a bit retrospectively about my notes from the <a href=https://www.kaggle.com/c/reducing-commercial-aviation-fatalities/data>&ldquo;Reducing Commercial Aviation Fatalities&rdquo; kaggle</a>, trying to summarize some of the journey. I try to give some high lights
from my various notebook entries. ( <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm>github</a> )</em></p><h4 id=my-high-level-recap>My High Level Recap</h4><p>This physiology data classification challenge poses the question, <em>given this time series voltage data of pilots&rsquo; respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-04-05-organzing-thoughts.md#as-a-quick-intro-to-the-data>data</a>) can something reliable be said about their physiological state?</em> My response was to use this as an opportunity to learn about TensorFlow and <strong>LSTMs</strong>. I quickly discovered that data processing around time series data is <code>3 dimensional</code> as opposed to typical <code>2 dimensional</code> data. That means that the harmless <code>1.1 GiB</code> of training data can quickly multiply to roughly <code>256 GiB</code> if one is interested in using a <code>256 long</code> sequence window. That means I learned a lot more about <code>numpy</code> for its simplicity around transforming <code>3 dimensional</code> data. I had to adapt to using <code>h5py</code> <em>chunks</em> of data so as not to run out of memory quickly and not wait endless hours for training sessions to merely crash. As for <em>TensorFlow</em> and <em>LSTMs</em>, I did not realize right away but <em>LSTMs</em> (and likely neural nets in general) are quite sensitive to data that is not scaled and my logloss ended up reducing when I <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-04-05-organzing-thoughts.md#scaling>applied scaling techniques</a>. Raw <code>matplotlib</code> became more intuitive for helping to visualize not just the time series data itself, but also for plotting <code>logloss</code> while training, across <em>batches</em> . . My <strong>Jupyter Notebook</strong> hygiene and workflow got better really quickly too, because I needed a reliable tool for distinguishing one day&rsquo;s experiment from another&rsquo;s without mixing the together the data or the models.</p><p>This dataset was highly skewed in terms of <em>classes</em> and so one of the really important preprocessing tasks was creating both <em>balanced</em> training &ldquo;techniques&rdquo; and <em>balanced</em> test data for more uniformly judging model performance. And I say &ldquo;techniques&rdquo;, because I tried both balanced weights and balanced training sets. I ended up preferring balanced training datasets, because that meant less preprocessing code.</p><p>The picture of <em>Kaggle</em> data I had in my head was clean datasets, but this dataset had one huge problem in that the time series data was not actually sorted by, you know, <code>time</code>. But in a way it is always fun to deal with messy data because it makes you more engaged with it and still more curious in the outcomes.</p><p><em>In general this project has given me a lot of fun memories.</em></p><p>One day after already getting deep into my <strong>LSTM</strong> approach, I decided to look through the <em>Kaggle</em> discussions for this project and I found that most people actually stuck to gradient boosting machines like lightGBM or XGBoost. But I decided to follow my personal motto of taking the path less traveled so I kept going with the <em>LSTM</em> .</p><p>I have spent I think half a year of weekends on this problem. I have memories of learning about neural network architecture learning &ldquo;capacity&rdquo; at my niece&rsquo;s birthday party. I came to understand that creating a larger network can cause it to <em>memorize</em> more as opposed to <em>generalize</em> .</p><p>I remember tweaking my <em>stochastic gradient descent</em> batch size after reading this Yann LeCun <a href=https://mobile.twitter.com/ylecun/status/989610208497360896>tweet</a> , <em>&ldquo;Training with large minibatches is bad for your health. More importantly, it&rsquo;s bad for your test error. Friends dont let friends use minibatches larger than 32.&rdquo;</em> .</p><p>I also have memories of starting modeling experiments before going on runs and before going to sleep, so that I could let <em>TensorFlow</em> spin its wheels while I took my mind into strategy mode or just let myself meditate.</p><p>At one point I was at the Boston Amtrak terminal waiting for my bus, getting deeper into why it is handy to look at raw <em>logit</em> data coming out of a model, especially in a multiclass problem because it can show how strongly a model classifies each class. But applying the logistic function or a <em>softmax</em> is of course good for sussing out probabilities. But then I realized I was waiting for a bus at an Amtrak terminal and I had to sprint several blocks to actually catch my bus!</p><p>At the end of the day I think of all of the amazing things I could one day do with this kind of technology, such as classifying music or building a chat bot (maybe even one that can tell jokes).</p><h4 id=table-of-contents>Table of Contents</h4><ul><li><a href=#quick-intro-to-the-data>Quick intro to the data</a></li><li><a href=#the-data-is-weirdly-partially-sorted>The data is weirdly partially sorted</a></li><li><a href=#trickiness-of-the-how-the-data-is-laid-out-crews-and-seats>Trickiness of the how the data is laid out (crews and seats?!)</a></li><li><a href=#some-more-visual-inspection>Some more visual inspection</a></li><li><a href=#building-datasets>Building datasets</a></li><li><a href=#scaling>Scaling</a></li><li><a href=#shuffling-and-adjusting-dropout>Shuffling and adjusting dropout</a></li><li><a href=#more-epochs>More epochs?</a></li><li><a href=#weight-initialization>Weight initialization</a></li><li><a href=#class-balance>Class balance</a></li><li><a href=#active-learning-changing-the-training-approach>Active Learning: changing the training approach</a></li><li><a href=#full-training-set-error>Full training set error</a></li><li><a href=#shuffling-traintest>Shuffling train/test</a></li><li><a href=#reconsider-that-high-dropout>Reconsider that high dropout</a></li><li><a href=#prediction-speedup>Prediction Speedup</a></li></ul><h4 id=quick-intro-to-the-data>Quick intro to the data</h4><p>The physiological data includes several types <em>(including respiration, electrocardiograms (ecg heart data), galvanic skin response (gsr), electroencephalography (eeg brain brain data))</em> across multiple <code>"crews"</code> . A crew includes two <code>"seats"</code> (<code>0</code> and <code>1</code>). We are provided with <code>256 measurements per second</code> across three experiments (Channelized Attention (CA) , Diverted Attention (DA) and Startle/Surprise (SS) ). Across the three experiments, four target &ldquo;states&rdquo; (or classes) are labeled for all of the rows in the data.</p><p>The four classes of events in the training data, <code>'A', 'B', 'C', 'D'</code> , correspond to three target physiological states of the three experiments, plus a neutral baseline state:</p><table><thead><tr><th>label</th><th>description</th></tr></thead><tbody><tr><td>A</td><td>Baseline</td></tr><tr><td>B</td><td>Startle/Surprise</td></tr><tr><td>C</td><td>Channelized Attention</td></tr><tr><td>D</td><td>Diverted Attention</td></tr></tbody></table><p>In my early <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-10-initial-look.md>notebook</a> I took a quick look at the proportion of experiment time had been spent in the different states. Per the below, looking at each person separately, I saw that for the first person as an example, (<code>crew=1, seat=0</code>) , <code>~98%</code> of <code>CA</code> was labeled as &ldquo;Channelized Attention&rdquo; and <code>2%</code> as baseline. But for the other two experiments, the target states appear to be much more brief, with only <code>13%</code> of the &ldquo;DA&rdquo; and <code>9%</code> of the &ldquo;SS&rdquo; experiments.</p><p><em>(copying an output from that notebook&mldr;)</em></p><pre tabindex=0><code>statsdf = gpdf.groupby(by=[&#39;crew&#39;, &#39;seat&#39;]).apply(extract_proportions).reset_index()
In [78]: statsdf                                                                                                                         
Out[78]:
    crew  seat      A/CA      A/DA      A/SS      C/CA      D/DA      B/SS
0      1     0  0.018593  0.868871  0.903015  0.981407  0.131129  0.096985
1      1     1  0.018803  0.879030  0.902685  0.981197  0.120970  0.097315
2      2     0  0.001954  0.857941  0.916630  0.998046  0.142059  0.083370
3      2     1  0.001781  0.848619  0.916605  0.998219  0.151381  0.083395
4      3     0  0.001248  0.854135  0.916782  0.998752  0.145865  0.083218
5      3     1  0.000597  0.860974  0.916772  0.999403  0.139026  0.083228
6      4     0  0.001302  0.868853  0.916514  0.998698  0.131147  0.083486
7      4     1  0.001400  0.860968  0.916706  0.998600  0.139032  0.083294
8      5     0  0.001661  0.847193  0.916730  0.998339  0.152807  0.083270
9      5     1  0.001791  0.857766  0.916472  0.998209  0.142234  0.083528
10     6     0  0.002311  0.860514  0.916711  0.997689  0.139486  0.083289
11     6     1  0.001661  0.858872  0.916748  0.998339  0.141128  0.083252
12     7     0  0.001563  0.867075  0.916536  0.998437  0.132925  0.083464
13     7     1  0.001607  0.855907  0.916471  0.998393  0.144093  0.083529
14     8     0  0.000999  0.856505  0.915394  0.999001  0.143495  0.084606
15     8     1  0.001053  0.853877  0.915412  0.998947  0.146123  0.084588
16    13     0  0.001801  0.841341  0.916482  0.998199  0.158659  0.083518
17    13     1  0.001628  0.847312  0.916595  0.998372  0.152688  0.083405
</code></pre><h4 id=the-data-is-weirdly-partially-sorted>The data is weirdly partially sorted</h4><p>As I mention in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-05-14-wrangling-time-data.md>this notebook</a>, when indexing on what I believe are the main uniqueness constraint columns (<code>crew</code>, <code>seat</code>, <code>experiment</code>), the <code>time</code> is not sorted. There are strange jumps, such as this one which I borrow from my notebook .. I show the first three rows (<code>0, 1, 2</code>) and also where I see the jump starting from around <code>6600</code>, where row <code>6606</code> goes back in time.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pd<span style=color:#f92672>.</span>concat([
</span></span><span style=display:flex><span>        df[[<span style=color:#e6db74>&#39;crew&#39;</span>, <span style=color:#e6db74>&#39;seat&#39;</span>, <span style=color:#e6db74>&#39;time&#39;</span>, <span style=color:#e6db74>&#39;r&#39;</span>, <span style=color:#e6db74>&#39;experiment&#39;</span>, <span style=color:#e6db74>&#39;event&#39;</span>]]<span style=color:#f92672>.</span>iloc[:<span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>        df[[<span style=color:#e6db74>&#39;crew&#39;</span>, <span style=color:#e6db74>&#39;seat&#39;</span>, <span style=color:#e6db74>&#39;time&#39;</span>, <span style=color:#e6db74>&#39;r&#39;</span>, <span style=color:#e6db74>&#39;experiment&#39;</span>, <span style=color:#e6db74>&#39;event&#39;</span>]]<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>6600</span>:<span style=color:#ae81ff>6610</span>]])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>	crew	seat	time	r	experiment	event
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0.011719</span>	<span style=color:#ae81ff>817.705994</span>	CA	A
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0.015625</span>	<span style=color:#ae81ff>817.705994</span>	CA	A
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0.019531</span>	<span style=color:#ae81ff>817.705994</span>	CA	A
</span></span><span style=display:flex><span><span style=color:#ae81ff>6600</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>109.988281</span>	<span style=color:#ae81ff>817.437988</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6601</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>109.988281</span>	<span style=color:#ae81ff>664.265991</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6602</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>109.992188</span>	<span style=color:#ae81ff>664.265991</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6603</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>109.992188</span>	<span style=color:#ae81ff>817.442017</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6604</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>109.996094</span>	<span style=color:#ae81ff>817.442017</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6605</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>109.996094</span>	<span style=color:#ae81ff>664.265991</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6606</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>11.000000</span>	<span style=color:#ae81ff>664.331970</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6607</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>11.000000</span>	<span style=color:#ae81ff>817.898987</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6608</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>0</span>	<span style=color:#ae81ff>11.003906</span>	<span style=color:#ae81ff>664.331970</span>	CA	C
</span></span><span style=display:flex><span><span style=color:#ae81ff>6609</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>1</span>	<span style=color:#ae81ff>11.003906</span>	<span style=color:#ae81ff>817.898987</span>	CA	C
</span></span></code></pre></div><p>I sort this data myself, but it is confusing for sure.</p><h4 id=trickiness-of-the-how-the-data-is-laid-out-crews-and-seats>Trickiness of the how the data is laid out (crews and seats?!)</h4><p>In the process of visualizing data, I had been using matplot lib to visualize the four different classes of events, <code>'A', 'B', 'C', 'D'</code> as red, green, blue and cyan. That way I could potentially try to get an intuition around the visual cues around state transitions. But at one point I had accidentally been combining the data of multiple people.</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-10-26_files/2019-10-26_14_0.png><p>Above, extracting a plot from my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-10-26.md>2019-10-26 notebook</a>, is an example of where I plot combined multi seat data by accident. At one point this was weirding me out. But I realized finally that I had been combining the data of multiple people.</p><p>For diagram above (^^) , I had written a quick function <code>produce_plots_for_col</code> for plotting four features simultaneously, given a pandas dataframe, some features and an interval, but indeed the <em>zig zag</em> plot was a bit baffling for a bit.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>start <span style=color:#f92672>=</span> <span style=color:#ae81ff>3400</span>; produce_plots_for_col(df, [<span style=color:#e6db74>&#39;r&#39;</span>, <span style=color:#e6db74>&#39;ecg&#39;</span>, <span style=color:#e6db74>&#39;gsr&#39;</span>, <span style=color:#e6db74>&#39;eeg_fp1&#39;</span>],
</span></span><span style=display:flex><span>                                range(start,start<span style=color:#f92672>+</span><span style=color:#ae81ff>150</span>))
</span></span></code></pre></div><p>When I look back at my notebook I wrote about how I was very <em>dumbfounded</em> when I realized I combined them by accident. The data is complicated however. It includes four indexing columns: <code>id</code> , <code>time</code> , <code>crew</code> and <code>seat</code> . And indeed being careful with splitting this was key in creating good datasets.</p><h4 id=some-more-visual-inspection>Some more visual inspection</h4><ul><li><a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-06-08-visually-inspect-generated-sequences.md#and-plot>in my 2019-06-08 notebook</a> I used another nice quick visual inspection technique, looking at some time series data samples at the four classes,</li></ul><p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/assets/Screen%20Shot%202019-06-15%20at%2011.29.22%20AM.png width=607 height=383></p><h4 id=building-datasets>Building datasets</h4><ul><li>I spent a lot of time next building data sets, <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-06-23-today.md>here</a> , <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-07-06-today.md>and</a> , and building basic quick and dirty LSTM tensor flow models. <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-07-13-Five-more-data.md>And also</a> .</li></ul><p>I also tried different approaches for understanding the models I was building. Including looking at raw logits, as per the below graphic, from my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-07-13-Four.md#look-at-them-logits>2019-07-13-Four notebook</a>. I thought this was a cool method compared to a confusion matrix for instance is because it shows the raw logits of each of the four classes, before the argmax voting observed in a confusion matrix is done.</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-07-13-Four_files/2019-07-13-Four_6_0.png><h3 id=scaling>Scaling</h3><p>I took a deeper <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-14--annotated.md>histogram</a> look at my data, seeing quite a lot of <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-14--annotated.md#another-time-series-look>ups and downs</a>.</p><p><em>(Given that there were some crazy jumps, I thought I needed to do something about that)</em>
<img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-12-14--annotated_files/2019-12-14--annotated_16_0.png></p><p>And so
<a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md>on 2019-12-21</a> , I ended up trying out more scaling approaches, especially <code>MinMaxScaler</code>. I had <code>8</code> features I was focusing on at that point and I plotted how my <code>minMaxScaler</code> <code>min</code> and <code>max</code> parameters changed as I processed roughly <code>40</code> or so mini datasets I had in my h5 training file <code>data/2019-12-21T215926Z/train.h5</code>. <em>Re-posting <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md#plot-the-scale-parameters>my image</a></em> :</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-12-21--update_files/2019-12-21--update_17_0.png><p>Luckily I found I was able to use just a single sklearn MinMaxScaler object to capture all <code>8</code> features at once.
I then <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md#make-scaled-data>applied</a> the scalers to transform my <code>train.h5</code> data to a <code>train_scaled.h5</code> dataset. And I also ended up with a <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-21--update.md#ok-now-make-balanced-data-again>balanced dataset</a> , <code>train_balanced.h5</code>, that I could use for training.</p><p>I trained a model and plotted training and validation loss curves the <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-22.md>next day</a> .</p><p>And wow the validation loss ( <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-22.md#plotting-validation-loss-at-model-snapshots>link</a> ) looked intense ,</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-12-22_files/2019-12-22_11_1.png><p>As a side note. although the validation loss here looks totally skewed towards <code>class 1</code> , I want to step back and note I really appreciate the technique of actually creating the <em>&ldquo;balanced&rdquo;</em> test set I referred to above. That allows us to quickly knows the model is favoring one class over another in the first place. And also I really dig the technique of simply snapshotting the tensorflow models while training and then being able to know how the validation logloss looks across those training batches. I feel like combining these techniques was really helpful in digesting what is going on . I needed to enjoy little details like that amidst all of the trial and error that was happening here (Emphasis on the error part haha).</p><h4 id=shuffling-and-adjusting-dropout>Shuffling and adjusting dropout</h4><p>At a <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-25.md>later date</a> , I adjusted my lstm dropout from <code>0.2</code> to <code>0.7</code> , seeing quite different behavior in the validation loss. I had also added some <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-25.md#do-a-shuffle>shuffling code</a> taking my <code>'history/2019-12-22T174803Z/train_balanced.h5'</code> dataset to produce <code>'history/2019-12-22T174803Z/train_scaled_balanced_shuffled.h5'</code> , to possibly change some of the choppiness of the validation curve seen above ^^ . That produced a <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-25.md#validation-loss>validation loss</a> , reposting the image here,</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-12-25_files/2019-12-25_13_1.png><h4 id=more-epochs>More epochs?</h4><p>On <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-28-two.md>2018-12-28</a> I got curious about whether just throwing more data at this problem would help. So I extended my waiting time by <code>two</code> and let the training happen in two <em>epochs</em> . The validation loss <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-28-two-plot.md#final-validation-logloss-plot>from here</a> , (reposting&mldr;) however showed that throwing more data is not always the answer. It always depends haha.</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-12-28-two-plot_files/2019-12-28-two-plot_9_1.png><h4 id=weight-initialization>Weight initialization</h4><p>Per my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-12.md>notebook entry</a> I had read per <a href=https://adventuresinmachinelearning.com/weight-initialization-tutorial-tensorflow/>this article</a> that the default tensor flow weight initialization I had <a href=https://www.tensorflow.org/api_docs/python/tf/keras/initializers>been using</a> was <em>GlorotUniform , ( which is aka Xavier Uniform apparently )</em> . I realized it was at least worth considering weight initialization as another hyper parameter so here I tried the <em>Glorot or Xavier Normal</em> instead . The <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-12.md#validation-loss>validation loss</a> did not necessarily convey the difference however:</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-12_files/2020-01-12_16_48.png><p>At this point I think I was realizing that the order of ideas to try matters. And you do not know in advance what is the best order. Perhaps the weight initialization matters a good deal, but I had not yet found the critical next step yet at that point.</p><h4 id=class-balance>Class balance</h4><p>In my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-18.md>next notebook</a> I wanted to understand why my <code>class 1</code> kept getting favored. I tried out <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-18.md#force-weights>forcing the weights</a> of my training data to basically</p><pre tabindex=0><code>{0: 1., 1: 0., 2: 0., 3: 0.}
</code></pre><p>to see what happens and sure enough, per the <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-18.md#validation-loss>validation loss</a> , the loss now went down only for class <code>0</code>. So the effect was controlled.</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-18_files/2020-01-18_11_4.png><h4 id=active-learning-changing-the-training-approach>Active Learning: changing the training approach</h4><p>Somehow I came upon the idea of preferentially training on what your model is doing poorly on. So on <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--update.md>2020-01-19</a> I modified my training loop so that I dynamically adjusted my training weights according to which class was being misclassified. The effect on the <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--update.md#plotting-train-loss-now-and-per-label-losses-too>training loss</a> was really interesting. Everything was way smoother.</p><p>Looking at a training loss plot from earlier ( such as from <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2019-12-28.md#crashed-last-batch-but-thats-okay>2019-12-28</a> )</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2019-12-28_files/2019-12-28_12_1.png>
, it shows the batch loss is all over the place. That makes perferct sense perhaps, because each batch I have been using in stochastic gradient descent really is from all over the training data. And compared to the training loss plot for the two figures below (extracted from my [2020-01-19 notebook](https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--update.md) ) the combined and per-class training batch losses are way more stable looking.
<img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png>
<img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_2.png><p><a href=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png>https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--update_files/2020-01-19--update_10_0.png</a></p><p>The validation loss was still favoring that one class, but I decided to hold on to this technique and keep trying other things.</p><h4 id=full-training-set-error>Full training set error</h4><p>Next in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-01-19--more-test-evaluate-train.md>this notebook</a> I wanted to better answer whether my particular test set perhaps had some very different size data compared to my training set, which was blowing up my test set error. I did not have enough data to better split apart my data at the moment actually, but instead I took a quick detour to compare my training mini batch loss curves to the full training set losses, during training. Naturally one would expect that if batch training losses improve that overall training set loss should also improve. Per the below diagram from my notebook, that was indeed the case.</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-01-19--more-test-evaluate-train_files/2020-01-19--more-test-evaluate-train_12_4.png><h4 id=shuffling-traintest>Shuffling train/test</h4><p>After having consistently weird results with validation error, I decided to try re-building my train/test sets by doing a full random shuffle instead, in my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-01.md>2020-02-01 notebook</a>. Up until this point I had been using the fact that the data is divided into <code>crew1, crew2, crew3, etc</code> and I have used <code>crew1</code> for train and <code>crew2</code> for test. And I had built <code>scalers</code> from my <code>crew1</code> training data, applying them to the the <code>crew2</code> test data.</p><p>So this time around I instead built <code>scalers</code> from <code>crew1</code> and then changed my function, <code>build_many_scalers_from_h5</code> to take <code>scalers</code> as a parameter and I kept updating them with the test data. ( My scalers <code>'history/2020-02-02T044441Z/scalers.joblib'</code> was the restulting artifact).</p><p>In <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-01.md#final-chart>validation</a> , I think for the first time, I saw the validation error actually start going down ,</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-02-01_files/2020-02-01_46_14.png><p>I took that further in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-08-take2--update--2.md>this 2020-02-08 notebook</a> and ( <em>showing my figure again</em> ) going 3 epochs instead, I got ..</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-02-08-take2--update--2_files/2020-02-08-take2--update--2_11_1.png><p>So the bright side I take from this is that the validation loss is actually doing better for three out of four of the classes.</p><h4 id=reconsider-that-high-dropout>Reconsider that high dropout</h4><p>Next in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md>2020-02-15 notebook</a>, I decided to reduce my dropout slightly, after reading through <a href=https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/>this post</a> about treating the dropout as yet another hyperparameter. After retraining , across 2 epochs, I saw a validation loss curve, which looked better still.</p><img src=https://raw.githubusercontent.com/namoopsoo/aviation-pilot-physiology-hmm/master/notes/2020-02-15_files/2020-02-15_10_44.png><p>This indicates perhaps the context of hyperparameters being experimented with indeed matters . I think Andrew Ng&rsquo;s characterization of <em>&ldquo;model babysitting&rdquo;</em>.</p><p><a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md>https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-15.md</a></p><h4 id=prediction-speedup>Prediction Speedup</h4><p>At this point I had improved my validation results enough that I wanted to submit my predictions. But as I described in my <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-02-29-time.md>notebook</a> , the <code>17.9 million test examples</code> would take potentially <code>25 hours</code> per my back of the envelope calculation.</p><p>But luckily I discovered that just playing with my prediction batch size, changing it from <code>32</code> to <code>1024</code>, I found I could cut my time from <code>56k examples/292 seconds</code> to <code>56k examples/27s</code> , taking the back of the envelope calculation from <code>25 hours</code> to <code>2.5 hours</code> !..</p><p>I also ended up utilizing <code>awk</code> to actually build batches for predict, avoiding trying to squeeze everything into memory.</p><p>And for one final optimization, I added multi-processing with <code>joblib</code> , to take advantage of all of my available cores.</p><p>The steps get more detailed in <a href=https://github.com/namoopsoo/aviation-pilot-physiology-hmm/blob/master/notes/2020-03-07-run-test-set-snapshot6.md>this notebook</a>.</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://michal.piekarczyk.xyz/>&copy; My blog 2022</a><div></div></div></footer></body></html>