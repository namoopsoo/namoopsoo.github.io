<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Bike Share Learn Reboot | michal.piekarczyk.xyz</title><meta name=keywords content><meta name=description content="This project is a reboot of an earlier project predicting bicycle ride share riders destinations."><meta name=author content="Michal Piekarczyk"><link rel=canonical href=https://michal.piekarczyk.xyz/project/2020-10-20-bike-share-learn-reboot/><link crossorigin=anonymous href=/assets/css/stylesheet.dd867d536fb6202811c1ee15fa181ef8945826662b49d3016ac91365a2621d58.css integrity="sha256-3YZ9U2+2ICgRwe4V+hge+JRYJmYrSdMBaskTZaJiHVg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://michal.piekarczyk.xyz/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://michal.piekarczyk.xyz/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://michal.piekarczyk.xyz/favicon-32x32.png><link rel=apple-touch-icon href=https://michal.piekarczyk.xyz/apple-touch-icon.png><link rel=mask-icon href=https://michal.piekarczyk.xyz/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-74MK08REDT"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-74MK08REDT",{anonymize_ip:!1})}</script><meta property="og:title" content="Bike Share Learn Reboot"><meta property="og:description" content="This project is a reboot of an earlier project predicting bicycle ride share riders destinations."><meta property="og:type" content="article"><meta property="og:url" content="https://michal.piekarczyk.xyz/project/2020-10-20-bike-share-learn-reboot/"><meta property="og:image" content="https://s3.amazonaws.com/my-blog-content/2020/2020-10-20-bike-share-learn-reboot/IMG_8499.jpg"><meta property="article:section" content="project"><meta property="article:published_time" content="2020-10-20T10:00:05-04:00"><meta property="article:modified_time" content="2020-10-20T10:00:05-04:00"><meta property="og:site_name" content="michal.piekarczyk.xyz"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://s3.amazonaws.com/my-blog-content/2020/2020-10-20-bike-share-learn-reboot/IMG_8499.jpg"><meta name=twitter:title content="Bike Share Learn Reboot"><meta name=twitter:description content="This project is a reboot of an earlier project predicting bicycle ride share riders destinations."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Side Projects","item":"https://michal.piekarczyk.xyz/project/"},{"@type":"ListItem","position":2,"name":"Bike Share Learn Reboot","item":"https://michal.piekarczyk.xyz/project/2020-10-20-bike-share-learn-reboot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bike Share Learn Reboot","name":"Bike Share Learn Reboot","description":"This project is a reboot of an earlier project predicting bicycle ride share riders destinations.","keywords":[],"articleBody":"Summary This project is a reboot of my earlier project of predicting bicycle ride share riders destinations. https://bike-hop-predict.s3.amazonaws.com/index.html\nThis time around I used XGBoost, newer features, hyper parameter tuning and I have a Demo Site as well! I wanted very much to see if XGBoost has online learning like I used in an earlier TensorFlow project, but as I wrote here, I could not pick up where I left off at least the way I tried it. And here is a mini post on looking at hyper parameter tuning results. And here is a visual look at some of the new features I explored including.\nAgain, the data looks like this\n\"tripduration\",\"starttime\",\"stoptime\",\"start station id\",\"start station name\",\"start station latitude\",\"start station longitude\",\"end station id\",\"end station name\",\"end station latitude\",\"end station longitude\",\"bikeid\",\"usertype\",\"birth year\",\"gender\" \"171\",\"10/1/2015 00:00:02\",\"10/1/2015 00:02:54\",\"388\",\"W 26 St \u0026 10 Ave\",\"40.749717753\",\"-74.002950346\",\"494\",\"W 26 St \u0026 8 Ave\",\"40.74734825\",\"-73.99723551\",\"24302\",\"Subscriber\",\"1973\",\"1\" \"593\",\"10/1/2015 00:00:02\",\"10/1/2015 00:09:55\",\"518\",\"E 39 St \u0026 2 Ave\",\"40.74780373\",\"-73.9734419\",\"438\",\"St Marks Pl \u0026 1 Ave\",\"40.72779126\",\"-73.98564945\",\"19904\",\"Subscriber\",\"1990\",\"1\" \"233\",\"10/1/2015 00:00:11\",\"10/1/2015 00:04:05\",\"447\",\"8 Ave \u0026 W 52 St\",\"40.76370739\",\"-73.9851615\",\"447\",\"8 Ave \u0026 W 52 St\",\"40.76370739\",\"-73.9851615\",\"17797\",\"Subscriber\",\"1984\",\"1\" \"250\",\"10/1/2015 00:00:15\",\"10/1/2015 00:04:25\",\"336\",\"Sullivan St \u0026 Washington Sq\",\"40.73047747\",\"-73.99906065\",\"223\",\"W 13 St \u0026 7 Ave\",\"40.73781509\",\"-73.99994661\",\"23966\",\"Subscriber\",\"1984\",\"1\" \"528\",\"10/1/2015 00:00:17\",\"10/1/2015 00:09:05\",\"3107\",\"Bedford Ave \u0026 Nassau Ave\",\"40.72311651\",\"-73.95212324\",\"539\",\"Metropolitan Ave \u0026 Bedford Ave\",\"40.71534825\",\"-73.96024116\",\"16246\",\"Customer\",\"\",\"0\" \"440\",\"10/1/2015 00:00:17\",\"10/1/2015 00:07:37\",\"3107\",\"Bedford Ave \u0026 Nassau Ave\",\"40.72311651\",\"-73.95212324\",\"539\",\"Metropolitan Ave \u0026 Bedford Ave\",\"40.71534825\",\"-73.96024116\",\"23698\",\"Customer\",\"\",\"0\" TOC Prior probability baseline Xgboost detour Multi class classification notes How does this time compare with the previous attempt Model highlights Gluing everything together Looking at hyperparameter tuning results Follow on Prior probability baseline Here, I first wanted to get a metric baseline using a simple model which only uses the highest prior probability destination as the prediction for a source bike share station. Anything even slightly more sophisticated should perform better. I also used this opportunity to apply multi class logloss as an evaluation metric for this problem, which I had not tried last time. So for an output probability vector of 54 possible destination stations, log loss can more granularly assess the prediction probabilities against a vector of the correct station, [0, 1, 0, 0, 0,...] compared to just accuracy.\nFor example\nfrom sklearn.metrics import log_loss # and some noisy predictions noisy_pred = np.array([[.05, .05, .9], [.95, 0.05, 0], [.9, 0.1, 0], [0.05, .05, .9], [0, 1, 0]]) log_loss([3, 1, 1, 3, 2], noisy_pred) the output here is 0.07347496827220674, which is just slightly worse than the perfect 0., showing that log loss can be handy for comparing models.\nThe detail is in the notes, but basically the cross validation log loss using this method ended up being\narray([29.03426394, 25.61716199, 29.19083979, 28.312853 , 22.04601817]) Dockerization Next, for repeatability and portability, here I re-adapted some earlier Dockerization I had setup before to wrap xgboost, along with jupyter notebook for experimentation. This was crucial, because if you want to jump between some quick experiments on your laptop and a notebook in the cloud, you don’t want to deal with strange differences in library dependencies between MacOs and linux.\nXgboost detour To start, I wanted to better understand how to use Xgboost abilities with respect to training a model, putting it down, saving it to disk, loading it again and continuing to train on new data. I had used this capability in Tensorflow land earlier and I read it might be possible with Xgboost, but even after trial and error with both the main Xgboost API and its scikit learn API, I could not get this to work properly. My notes on this are here in an earlier post.\nOne cool thing I did learn however was that when repeating a model train and evaluation experiment with both the functional API and the scikit learn API, the functional API took advantage of multithreading, and produced a particular result in 4min 18s vs 49min 6s, with both models using the same seed=42 and ending up with the same accuracy and log loss on some held out data.\nAs I mentioned here , I experienced some early problems running out of memory and crashing, for instance computing log los son 843416 rows. And that is why I was seeking out approaches of online learning. But because of the limitations, my workout ended up being the use of at least carefully deleting objects in memory with del to free up space for, between preprocessing, training and validation. And I also played around with the approach of initializing a xgb.DMatrix using the xgb.DMatrix('/my/blah/path#dtrain.cache') syntax where you specify # a cache file to allow for file access to reduce the in-memory burden, also requiring to dump your pre-processed training data to file first. (And doing that is good anyway because it allows you to free up that precious memory).\nCompared to the initial baseline logloss from earlier of around 29, here I noted a result of 3.9934347 with the initial xgboost approach.\nOn 2020-06-14, I tried using the xgboost caching with the scikitlearn api approach. In the meantime I also ran into a fun issue where an xgboost model was trained on data without a particular output class , with only 53 classes in fact and would produce predict probability vectors of length 53 instead of 54, so I ended up having to make sure to better shuffle the data to make sure when trying to use less data (when using cross validation for instance) that all of the output classes are accounted for, without having a more direct way of telling Xgboost what the output classes should be.\nAlso another fun Tensorflow comparison was I got XGBoostError: need to call fit or load_model beforehand when trying to call predict on a bare model that had not undergone training. Whereas with Tensorflow, I experienced in a previous project that this is absolutely fine, because you simply have a fully formed neural network with some randomly (or otherwise) initialized weights. But with xgboost, or at least the particular implementation I was using, this is not possible, because there is no notion of a base model.\nHere, I tried cutting up the training data like\nclf = xgb.XGBClassifier() workdir = fu.make_work_dir(); print(workdir) fu.log(workdir, 'Starting') prev_model = None loss_vec = []; acc_vec = [] for i, part in enumerate(tqdm(parts)): clf.fit(X_transformed[part], y_enc[part], xgb_model=prev_model) fu.log(workdir, f'[{i}] Done fit') prev_model = f'{workdir}/model.xg' clf.save_model(prev_model) y_prob_vec = clf.predict_proba(X_test_transformed) fu.log(workdir, f'[{i}] Done predict_proba') loss = fu.big_logloss(y_test_enc, y_prob_vec, labels) fu.log(workdir, f'[{i}] Done big_logloss, loss={loss}.') loss_vec.append(loss) acc = accuracy_score(y_test_enc, np.argmax(y_prob_vec, axis=1)) acc_vec.append(acc) fu.log(workdir, f'[{i}] Done accuracy, acc={acc}.') to see if the scikit learn API can allow saving and restoring previously trained models and continuing, with the fit(X, xgb_model=prev_model) syntax, but the output performance data was basically random indicating to me that the fit was starting from scratch each time.\nHere, below, is a plot of accuracy after multiple epochs, just to visually show the lack of any progression. (This plot is from a similar experiment in my 2020-06-16 notebook ).\nSo basically I gave up on this approach for using xgboost.\nAlso in that notebook, I found I was oddly getting 0 logloss during some experimentation because I had like in this toy example below, been specifying labels to the log_loss func, not matching the actual y_true data (which is the first arg).\na, b = np.array([0, 0, 0]), np.array([[0, 0., 0], [0., 0, 0], [0., 0, 0]]) print(log_loss(a, b, labels=['a', 'b', 'c'])) # =\u003e 0.0 2020-06-19 Here , because a lot of the test set prediction for model evaluation takes time I ended up creating a mini parallelization func. I verified that it was producing roughly the same validation and the time to execute was less.\nI also wrote about how I had needed to use less data to avoid crashing, using the pandas sample() func like,\ntripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv' ).sample(frac=0.017, random_state=42) but that it would be better to build a more balanced dataset instead of just random sampling.\nA rapid fire list of some additional experiments Here, I had another training attempt using the so called “cache data” with functional api. (But finding especially here that the max_depth was not changing so likely no learning was happening). Here I aadded new features here for a ‘v2’ dataset, including weekday. and time_of_day. Added this in a new module, fresh/preproc/v2.py. Here , more memory struggles (especially since I added more data). Here, describing that after lots of crashing, starting to use numpy append mode, here , to allow for doing preprocessing in chunks. And starting to look at target class distribution. Here I’m discovering that it is quite possible that this caching is only allowed w/ the “libsvm” format! And here, I see hmm it is kind of weird, that with cache, without… producing different feature_names ? more kernel dying! Here , bigger box? Here , class distribution for size reduction and dataset rebalancing. Here ok I took the result from the balancing/shrinking concept from the “2020-06-29.ipynb” notebook and tried to use less data see if we can avoid a kernel crash that happened in “2020-06-28-take2.ipynb”. Here I wanted to do a quick recalc of yesterday’s model using the sklearn API Again. Here, more rebalancing. Here. Here, end result: compared with “2020-07-03-aws.md” , I am not really seeing much of a difference. the balanced test accuracy perhaps looks every so slightly better but probably not significantly. Here , change up 'subsample' and 'max_depth', measuring logloss, accuracy and balanced_accuracy , there are some noticable changes in logloss, but overall the changes are probably not significant. Here. Here. Here. Here. Multi class classification notes Notes on multi class classification Understanding tuning results hyper parameter tuning and train/test acc\nPreviously vs This time Last time around, I segmented the starting data into 24 hour-long segments. This time, I segmented time into only 5 bins to make the model slightly more generalizable. # time_of_day 0: 6-9, 1: 10-13, 2: 14-16, 3: 17-21, 4: 22-0, 0-5 Actually after picking these bins arbitrarily, I ended up also looking at the time of day histograms here and the peaks look close to what I had as a mental model in my mind. It might be interesting try some other bins at some point later.\nOne other new feature this time is the binary weekday feature, specifying weekday vs weekend.\nThe starting neighborhood one hot encoded was kept as an input.\nAlso last time around, the main model was a Random Forest classifier, but using XGBoost this time.\nModel Highlights The top model has these stats…\n(pandars3) $ docker run -p 8889:8889 -p 8080:8080 -i -t -v $(pwd):/opt/program \\ -v ${MY_LOCAL_DATA_DIRECTORY}:/opt/data \\ -v ~/Downloads:/opt/downloads \\ -v $(pwd)/artifacts/2020-08-19T144654Z:/opt/ml \\ citibike-learn:latest \\ import fresh.predict_utils as fpu bundle = fpu.load_bundle_in_docker() In [7]: bundle['model_bundle']['bundle']['validation_metrics'] Out[7]: {'accuracy': 0.12171455130090014, 'balanced_accuracy': 0.10451301995291779, 'confusion': array([[415, 64, 4, ..., 0, 103, 69], [ 56, 541, 4, ..., 0, 130, 27], [ 23, 10, 136, ..., 0, 16, 130], ..., [ 2, 0, 2, ..., 1, 3, 36], [151, 222, 3, ..., 0, 260, 35], [ 84, 25, 46, ..., 0, 29, 861]]), 'logloss': 3.4335361255637977, 'test': '/home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-08T143732Z/test.libsvm', 'karea': 0.760827309330065} More on the “k-area” metric is here Top Model’s Top Fscore features Extracting from this notebook ,\n.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } And it can be interesting to look at a random tree from xgboost too sometimes, again extracting from the above mentioned notebook.\nGluing everything together In this notebook, I face the challenges of taking the model from bundle to a demo site. There were a lot of challenges involved. My concept was to use the Google Static Map API to display the top neighborhood predictions. Hitting this API properly did take a little bit of time, but it was not that bad. And later on, I updated the whole AWS Lambda approach so the lambda function calls the API with the result from the dockerized SageMaker served model.\nAdmittedly, the most time consuming part was figuring out the API Gateway Cognito “Unauthenticated Authentication”. AWS has this Cognito service which manages user/password based authentication for you but it also lets you use Anonymous authentication. But there must be a lot of degrees of freedom in how this is used, because I could not find good documentation on how to set this up properly for my usecase at all.\nI had used API Gateway for authentication through CORS in the past and I recalled a bit of nuance that for example you may have setup CORS properly for 200 status codes, but if your program crashes with a 500 then your browser will scream about a CORS error, because the response is not returning the expected allow-origin-blah header. In the past this had taken me a while to figure out, but now I luckily had that knowledge in my back pocket. In any case, it is worth it for the serverless approach.\nAutomation made the process very convenient https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-local-docker-notes.md\nI also described my build process in the earlier mentioned glue notes too. With so many tweaks to the python side, the model and the javascript side, being able to build and deploy with quick make style commands made everything faster. I document some of these here too.\nquick pearson’s chi squared independence test quick pearson’s chi squared independence test https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-05.md\nLooking at hyperparameter tuning results ( EDIT: After writing the below section, I realized I had already here earlier, on 2020-07-24 , described some of these results already haha. Doing the work twice, forgetting what I had done. )\nI spent a bit of time on hyper parameter tuning, looking at the results, fixing some parameters two focus on two others at a time.\nSo per here , the num_round as expected improves logloss,\nkeep_fixed = { 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bylevel': 0.1, 'colsample_bynode': 1, 'colsample_bytree': 0.1, 'subsample': 0.1, 'num_round': 10, } col1, col2, metric_col = 'max_depth', 'num_round', 'logloss' fp.compare_tuning(df, feature_col_1=col1, feature_col_2=col2, metric_col=metric_col, keep_fixed=fvu.without( keep_fixed, keys=[col1, col2])) And maybe this is good as a sanity check, but more rounds take more time, ( per here )\nAnd from here it was interesting to see that walltime is stable mostly when it comes to learning rate except sometimes…\nAnd per here at least per the fixed parameters, the 0.1 learning rate is better than the 0.01 learning rate.\nAnd per here , subsample row sampling is just not appearing to be influencing accuracy.\nAnd per here , the random column sampling may have just removed the good columns\nTrain and test accuracy comparison Here , I took all of my 1000+ models from earlier, (which were on S3 so I had to copy them locally for convenience) and calculated accuracy, logloss and karea metrics for the training data, in order to be able to get learning curves to understand underfitting/overfitting. Just showing ihere an example run for one model… # As per https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-10-aws.md # the data dir was artifacts/2020-07-08T143732Z ... going to re-create that locally too # datadir = '/opt/program/artifacts/2020-07-08T143732Z' artifactsdir = '/opt/program/artifacts/2020-07-10T135910Z' train_results = [] train_loc = f'{datadir}/train.libsvm' dtrain = xgb.DMatrix(f'{train_loc}?format=libsvm') actuals = dtrain.get_label() print('evaluate using ', train_loc) train_data = load_svmlight_file(train_loc) X_train = train_data[0].toarray() y_train = train_data[1] %%time ######## # Try one i = 0 bundle = joblib.load(f'{artifactsdir}/{i}_bundle_with_metrics.joblib') model = bundle['xgb_model'] y_prob_vec = model.predict(dtrain) predictions = np.argmax(y_prob_vec, axis=1) logloss = fu.big_logloss(actuals, y_prob=y_prob_vec, labels= list(range(54))) acc = accuracy_score(actuals, predictions) balanced_acc = balanced_accuracy_score(actuals, predictions) correct_kth, karea = fm.kth_area(y_train, y_prob_vec, num_classes=54) CPU times: user 31.3 s, sys: 110 ms, total: 31.4 s Wall time: 21.4 s acc, balanced_acc, karea (0.05276320740101365, 0.03727538888502701, 0.6435250908504123) The whole thing, took about 10 hours as measured by the final line from tqdm,\n100%|█████████▉| 1052/1054 [10:00:57\u003c01:08, 34.27s/it] Putting that together, Here putting that together ..\ntraining and test accuracy are pretty consistently close, with training accuracy being slightly better as expected. So there is no evidence overall of overfitting. But perhaps some evidence of underfitting .\nThe first thing I just looked at the parameters fixed by just what happened to be the first model built, so pretty arbitrary and comparing over the number of rounds. The results not unexpected not showing much learning happening.\nThen I took the model with the best test accuracy results,\nbest_params = dict(alldf.sort_values(by='acc').iloc[-1]) best_params {'train_acc': 0.12693459297270465, 'train_balanced_acc': 0.11012147901980039, 'i': 755, 'train_logloss': 3.4301962566050057, 'train_karea': 0.76345208497788, 'max_depth': 4, 'learning_rate': 0.1, 'objective': 'multi:softprob', 'num_class': 54, 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bynode': 1, 'colsample_bytree': 1.0, 'gamma': 0, 'max_delta_step': 0, 'min_child_weight': 1, 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': 42, 'subsample': 0.4, 'verbosity': 0, 'acc': 0.12304248437307332, 'balanced_acc': 0.10551953202851949, 'logloss': 3.4480742986458592, 'walltime': 1918.593945, 'karea': 0.75845582462009, 'num_round': 100} And plotted all the train/test metrics across rounds, and this figure definitely shows learning happening . Very exciting!\nAlso looked for the biggest gap between train/test accuracy And per the below, interestingly, it’s seeming like the biggest train/test gap is very small.. alldf['train_test_acc_delta'] = alldf.apply(lambda x: abs(x['acc'] - x['train_acc']), axis=1) alldf.sort_values(by='train_test_acc_delta').iloc[-1] train_acc 0.128123 train_balanced_acc 0.111239 i 1241 train_logloss 3.40954 train_karea 0.767823 max_depth 5 learning_rate 0.1 objective multi:softprob num_class 54 base_score 0.5 booster gbtree colsample_bylevel 1 colsample_bynode 1 colsample_bytree 1 gamma 0 max_delta_step 0 min_child_weight 1 random_state 0 reg_alpha 0 reg_lambda 1 scale_pos_weight 1 seed 42 subsample 0.4 verbosity 0 acc 0.12253 balanced_acc 0.104698 logloss 3.43584 walltime 2327.88 karea 0.760578 num_round 100 train_test_acc_delta 0.00559313 Name: 1242, dtype: object Initial time of day look https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-05-woe.md\nsuper\ndiscuss https://github.com/namoopsoo/learn-citibike/blob/2020-oct/notes/2020-08-25-glue.md\nFeature importances notes\nFrom the many hyper parameter tuning jobs I had run, I used the xgboost feature importance functionality to dump the perceived feature importances for all of the models. And in the notes I plotted feature importances against accuracy for all of them.\nFor example, here are some of the more interesting plots,\nThe point here is that I had one hot encoded all of the starting neighborhoods. I am hoping of course that if a particular starting location looks important, then that should mean it is important in discriminating where you go next. Meaning it narrows down where you go. On the other hand, if your starting location is boring then that should mean it is more like a hub and there are too many destinations for the start along to be a helpful feature.\nIn the above plots, there is a wide range of models and they are showing that for some reason high importance does not necessarily mean high accuracy. If anything, I want to make a mental note that maybe these kinds of plots can be indicators of something wrong and some kind of under-fitting in particular. Or weak fitting at least. And one of the other scenarios is that fitting is weak, because there is not enough entropy in the data available to yield helpful discrimination with a model. No matter how well XGBoost can extract information, if the raw material does not have any diamonds, then we will be stuck.\nThe other thought is that there is an overfitting danger around not just an imbalance in the target variable (aka the destination neighborhood) but an imbalance in the starting locations too. This is why it would be really interesting to also look at the entropy of the multiclass outputs for signs of clear uncertainty for specific examples. Putting a pin on this in the follow-on section\nThe time of day features look like this, below, but again, this is not to say that these views represent the full story.\nThinking about this abit more in retrospect, these particular representations are probably not very meaningful to look at because if there are trends they need to be looked at “localizing” or “fixing” some of the parameters. Because these representations are all over the place but the relationship may still be hidden inside.\nI think one of the top follow ons has to be to find better time of day splits. I chose my time of day splits based on a model in my head, and so there is definitely some room for exploration here.\nFollow On Time of day more splitting exploration Find some more interesting techniques to try out different segmentation of the time of day. ( I’m thinking “adaptive binning \" as described here )\nBetter understanding of model uncertainty As discussed in the feature importances section, it would be really interesting to take the test dataset and for the output probability vectors of all of the examples, to calculate the multi-class entropy, to see if indeed high uncertainty is associated with worse correctness rank (kth accuracy and karea in other terminology I have been using). Of course this is really tricky from an Active Learning point of view, because I can see a scenario where adding more training examples around the cases which have a higher uncertainty may improve the accuracy for the related test examples , but that feels like there is a risk of overfitting to the test set. In any case, however, if the live data is not reflective of the training/test data distributions ( covariate shift ), then refreshing the model is important. Some lessons for the future Approach to training and artifacts Training and hyperparameter tuning takes a long time. Dumping artifacts along the way, including models and results (for example using json), is helpful to allow another notebook to actively monitor the results as they are running. And doing this is also helpful because notebooks that run long experiments can sometimes crash. So it is nice to save intermediary results.\nNotebooks I like the concept of keeping a daily notebook, because keeping several experiments in one notebook can risk running out of memory and sometimes it is difficult to load large notebooks on github, even if they are turned into markdown, if there are a lot of images.\nWrite sooner rather than later Although it is tempting to just keep trying more and more experiments and to keep iterating the frontier forward, I think a difficult lesson to learn is that putting together the results of the day or the week takes much more time when done weeks or months later. I think summarizing and discussing your results as you go along is way more useful. But if you do wait, another idae is to just create a notebook table of concents as I am doing below, as a way of having quick chronological reference about the work that was done. Notebooks TOC 2020-07-10 , like “2020-07-09-aws” , another hyperparameter tuning round here. max_depth , subsample , colsample_bytree . 2020-07-11 , here I plot a bunch of results (on my laptop) , from the 2020-07-10 notebook running on aws. 2020-07-16-local.md , recalculataing train metrics for the ~1250 or so models from the hyper parameter tuning session 2020-07-26-feature-importances.md , looking at feature importances , reverse engineering my proc_bundle , to get back my list of feature names, which I had not done originally. Initially trying model.get_score() , dumping from each model. This actually took 3.5 hours. I plotted features and accuracy in a few ways to try to gauge features being more oftan associated with high accuracy models. Plotting the correlation of feature importance and acuracy. I think this was not a super useful method. Ultimately, the fscore approach was better 2020-08-05-woe.md , EDA on the time_of_day feature, visual histogram comparisons. Not the most fruitful however. 2020-08-17-bundle-glue.md 2020-08-18-glue.md some reverse engineering to repurpose my preprocessor bundle for live etraffic. And combining preprocessor and model to make a joblib bundle with everything in it. And drafint a full_predict method. 2020-08-22-static-map-api.md getting setup with the Google Static Map API . Very nice. 2020-08-25-glue.md Docker entry code end to end live code. And building the code for the lambda that calls Docker. Unfortunately xgboost does not fit on the lambda. And oops lambda cannot write to the file system. And working through the new API Gateway authentication methods here. I wrote some support code for quick lambda deployment because I ended up using many iterations to get this right. Content type weirdness. Javascript plus cognito. This was not documented very well, so a lot of blundering here. Can’t believe I finally made all of this work. This was insane. 2020-10-20-karea-worst.md K Area worst case scenario. 2020-10-21-look-at-model-plot.md looking at Fscore and as well as plotting individual trees with graphviz . Also some interesting issues with versions of xgboost in docker and lack of backward compatibility. 2020-10-21-uncertainty-xgboost.md this is mainly just a footnote about the idea around measuring uncertainty in xgboost. But this is likely not super reliable. 2020-10-22-features-v3.md Take a quick look at time of day distribution 2020-10-23-quick-new-v3-proc-bundle.md one more model iteration using new features. 2020-10-25.md evaluate new v3. But although not yet done any tuning, so far this does not seem significantly better, with karea 0.761 versus earlier best 0.760 karea. ","wordCount":"4070","inLanguage":"en","image":"https://s3.amazonaws.com/my-blog-content/2020/2020-10-20-bike-share-learn-reboot/IMG_8499.jpg","datePublished":"2020-10-20T10:00:05-04:00","dateModified":"2020-10-20T10:00:05-04:00","author":{"@type":"Person","name":"Michal Piekarczyk"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://michal.piekarczyk.xyz/project/2020-10-20-bike-share-learn-reboot/"},"publisher":{"@type":"Organization","name":"michal.piekarczyk.xyz","logo":{"@type":"ImageObject","url":"https://michal.piekarczyk.xyz/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://michal.piekarczyk.xyz/ accesskey=h title="michal.piekarczyk.xyz (Alt + H)">michal.piekarczyk.xyz</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://michal.piekarczyk.xyz/post/ title=posts><span>posts</span></a></li><li><a href=https://michal.piekarczyk.xyz/project/ title=projects><span>projects</span></a></li><li><a href=https://michal.piekarczyk.xyz/handy/ title=handy><span>handy</span></a></li><li><a href=https://michal.piekarczyk.xyz/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://michal.piekarczyk.xyz/about/ title=about><span>about</span></a></li><li><a href=https://world.hey.com/michal.piekarczyk title=frivolity><span>frivolity</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://michal.piekarczyk.xyz/>Home</a>&nbsp;»&nbsp;<a href=https://michal.piekarczyk.xyz/project/>Side Projects</a></div><h1 class=post-title>Bike Share Learn Reboot</h1><div class=post-meta><span title='2020-10-20 10:00:05 -0400 -0400'>October 20, 2020</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4070 words&nbsp;·&nbsp;Michal Piekarczyk</div></header><figure class=entry-cover><img loading=lazy src=https://s3.amazonaws.com/my-blog-content/2020/2020-10-20-bike-share-learn-reboot/IMG_8499.jpg alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#summary>Summary</a></li><li><a href=#toc>TOC</a></li><li><a href=#prior-probability-baseline>Prior probability baseline</a></li><li><a href=#xgboost-detour>Xgboost detour</a></li><li><a href=#multi-class-classification-notes>Multi class classification notes</a></li><li><a href=#understanding-tuning-results>Understanding tuning results</a></li><li><a href=#previously-vs-this-time>Previously vs This time</a></li><li><a href=#model-highlights>Model Highlights</a></li><li><a href=#follow-on>Follow On</a></li><li><a href=#some-lessons-for-the-future>Some lessons for the future</a></li><li><a href=#notebooks-toc>Notebooks TOC</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>This project is a reboot of <a href=/project/2016-12-18-citibike-project/>my earlier project</a> of predicting bicycle ride share riders destinations.
<a href=https://bike-hop-predict.s3.amazonaws.com/index.html>https://bike-hop-predict.s3.amazonaws.com/index.html</a></p><p>This time around I used XGBoost, newer features, hyper parameter tuning and I have a Demo Site as well! I wanted very much to see if XGBoost has online learning like I used in <a href=/project/2020-04-05-aviation-kaggle-low-level/>an earlier TensorFlow project</a>, but as I <a href=/post/2020-06-21-notes-xgboost/>wrote here</a>, I could not pick up where I left off at least the way I tried it. And <a href=/post/2020-07-24-understanding-tuning-results/>here</a> is a mini post on looking at hyper parameter tuning results. And <a href=/post/2020-10-22-features-v3/>here</a> is a visual look at some of the new features I explored including.</p><p>Again, the data looks like this</p><pre tabindex=0><code>&#34;tripduration&#34;,&#34;starttime&#34;,&#34;stoptime&#34;,&#34;start station id&#34;,&#34;start station name&#34;,&#34;start station latitude&#34;,&#34;start station longitude&#34;,&#34;end station id&#34;,&#34;end station name&#34;,&#34;end station latitude&#34;,&#34;end station longitude&#34;,&#34;bikeid&#34;,&#34;usertype&#34;,&#34;birth year&#34;,&#34;gender&#34;
&#34;171&#34;,&#34;10/1/2015 00:00:02&#34;,&#34;10/1/2015 00:02:54&#34;,&#34;388&#34;,&#34;W 26 St &amp; 10 Ave&#34;,&#34;40.749717753&#34;,&#34;-74.002950346&#34;,&#34;494&#34;,&#34;W 26 St &amp; 8 Ave&#34;,&#34;40.74734825&#34;,&#34;-73.99723551&#34;,&#34;24302&#34;,&#34;Subscriber&#34;,&#34;1973&#34;,&#34;1&#34;
&#34;593&#34;,&#34;10/1/2015 00:00:02&#34;,&#34;10/1/2015 00:09:55&#34;,&#34;518&#34;,&#34;E 39 St &amp; 2 Ave&#34;,&#34;40.74780373&#34;,&#34;-73.9734419&#34;,&#34;438&#34;,&#34;St Marks Pl &amp; 1 Ave&#34;,&#34;40.72779126&#34;,&#34;-73.98564945&#34;,&#34;19904&#34;,&#34;Subscriber&#34;,&#34;1990&#34;,&#34;1&#34;
&#34;233&#34;,&#34;10/1/2015 00:00:11&#34;,&#34;10/1/2015 00:04:05&#34;,&#34;447&#34;,&#34;8 Ave &amp; W 52 St&#34;,&#34;40.76370739&#34;,&#34;-73.9851615&#34;,&#34;447&#34;,&#34;8 Ave &amp; W 52 St&#34;,&#34;40.76370739&#34;,&#34;-73.9851615&#34;,&#34;17797&#34;,&#34;Subscriber&#34;,&#34;1984&#34;,&#34;1&#34;
&#34;250&#34;,&#34;10/1/2015 00:00:15&#34;,&#34;10/1/2015 00:04:25&#34;,&#34;336&#34;,&#34;Sullivan St &amp; Washington Sq&#34;,&#34;40.73047747&#34;,&#34;-73.99906065&#34;,&#34;223&#34;,&#34;W 13 St &amp; 7 Ave&#34;,&#34;40.73781509&#34;,&#34;-73.99994661&#34;,&#34;23966&#34;,&#34;Subscriber&#34;,&#34;1984&#34;,&#34;1&#34;
&#34;528&#34;,&#34;10/1/2015 00:00:17&#34;,&#34;10/1/2015 00:09:05&#34;,&#34;3107&#34;,&#34;Bedford Ave &amp; Nassau Ave&#34;,&#34;40.72311651&#34;,&#34;-73.95212324&#34;,&#34;539&#34;,&#34;Metropolitan Ave &amp; Bedford Ave&#34;,&#34;40.71534825&#34;,&#34;-73.96024116&#34;,&#34;16246&#34;,&#34;Customer&#34;,&#34;&#34;,&#34;0&#34;
&#34;440&#34;,&#34;10/1/2015 00:00:17&#34;,&#34;10/1/2015 00:07:37&#34;,&#34;3107&#34;,&#34;Bedford Ave &amp; Nassau Ave&#34;,&#34;40.72311651&#34;,&#34;-73.95212324&#34;,&#34;539&#34;,&#34;Metropolitan Ave &amp; Bedford Ave&#34;,&#34;40.71534825&#34;,&#34;-73.96024116&#34;,&#34;23698&#34;,&#34;Customer&#34;,&#34;&#34;,&#34;0&#34;
</code></pre><h3 id=toc>TOC<a hidden class=anchor aria-hidden=true href=#toc>#</a></h3><ul><li><a href=#prior-probability-baseline>Prior probability baseline</a></li><li><a href=#xgboost-detour>Xgboost detour</a></li><li><a href=#multi-class-classification-notes>Multi class classification notes</a></li><li><a href=#previously-vs-this-time>How does this time compare with the previous attempt</a></li><li><a href=#model-highlights>Model highlights</a></li><li><a href=#gluing-everything-together>Gluing everything together</a></li><li><a href=#looking-at-hyperparameter-tuning-results>Looking at hyperparameter tuning results</a></li><li><a href=#follow-on>Follow on</a></li></ul><h3 id=prior-probability-baseline>Prior probability baseline<a hidden class=anchor aria-hidden=true href=#prior-probability-baseline>#</a></h3><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-04-pure-prior-probability-model.md>Here</a>, I first wanted to get a metric baseline using a simple model which only uses the highest prior probability destination as the prediction for a source bike share station. Anything even slightly more sophisticated should perform better. I also used this opportunity to apply multi class logloss as an evaluation metric for this problem, which I had not tried last time. So for an output probability vector of <code>54</code> possible destination stations, log loss can more granularly assess the prediction probabilities against a vector of the correct station, <code>[0, 1, 0, 0, 0,...]</code> compared to just <code>accuracy</code>.</p><p>For example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> log_loss
</span></span><span style=display:flex><span><span style=color:#75715e># and some noisy predictions</span>
</span></span><span style=display:flex><span>noisy_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>.05</span>, <span style=color:#ae81ff>.05</span>, <span style=color:#ae81ff>.9</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>.95</span>, <span style=color:#ae81ff>0.05</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>.9</span>, <span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0.05</span>, <span style=color:#ae81ff>.05</span>, <span style=color:#ae81ff>.9</span>],
</span></span><span style=display:flex><span>                   [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]])
</span></span><span style=display:flex><span>log_loss([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>],
</span></span><span style=display:flex><span>         noisy_pred)
</span></span></code></pre></div><p>the output here is <code>0.07347496827220674</code>, which is just slightly worse than the perfect <code>0.</code>, showing that log loss can be handy for comparing models.</p><p>The detail is in the <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-04-pure-prior-probability-model.md>notes</a>, but basically the cross validation log loss using this method ended up being</p><pre tabindex=0><code>array([29.03426394, 25.61716199, 29.19083979, 28.312853  , 22.04601817])
</code></pre><h4 id=dockerization>Dockerization<a hidden class=anchor aria-hidden=true href=#dockerization>#</a></h4><p>Next, for repeatability and portability, <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-local-docker-notes.md>here</a> I re-adapted some earlier Dockerization I had setup before to wrap xgboost, along with jupyter notebook for experimentation. This was crucial, because if you want to jump between some quick experiments on your laptop and a notebook in the cloud, you don&rsquo;t want to deal with strange differences in library dependencies between MacOs and linux.</p><h3 id=xgboost-detour>Xgboost detour<a hidden class=anchor aria-hidden=true href=#xgboost-detour>#</a></h3><p>To start, I wanted to better understand how to use Xgboost abilities with respect to training a model, putting it down, saving it to disk, loading it again and continuing to train on new data. I had used this capability in Tensorflow land earlier and I read it might be possible with Xgboost, but even after trial and error with both the main Xgboost API and its scikit learn API, I could not get this to work properly.
My notes on this are <a href=/post/2020-06-21-notes-xgboost/>here in an earlier post</a>.</p><p>One cool thing I did <a href=/post/2020-06-21-notes-xgboost/#parallelism>learn</a> however was that when repeating a model train and evaluation experiment with both the functional API and the scikit learn API, the functional API took advantage of multithreading, and produced a particular result in <code>4min 18s</code> vs <code>49min 6s</code>, with both models using the same <code>seed=42</code> and ending up with the same accuracy and log loss on some held out data.</p><p>As I mentioned <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-12--snapshot-2020-06-14T2258Z.md#2020-06-13>here</a> , I experienced some early problems running out of memory and crashing, for instance computing log los son <code>843416 rows</code>. And that is why I was seeking out approaches of online learning. But because of the limitations, my workout ended up being the use of at least carefully deleting objects in memory with <code>del</code> to free up space for, between preprocessing, training and validation. And I also played around with the approach of initializing a <code>xgb.DMatrix</code> using the <code>xgb.DMatrix('/my/blah/path#dtrain.cache')</code> syntax where you specify <code>#</code> a cache file to allow for file access to reduce the in-memory burden, also requiring to dump your pre-processed training data to file first. (And doing that is good anyway because it allows you to free up that precious memory).</p><p>Compared to the initial baseline logloss from earlier of around <code>29</code>, here I <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-12--snapshot-2020-06-14T2258Z.md#averaging-log-losses>noted</a> a result of <code>3.9934347</code> with the initial xgboost approach.</p><p>On <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-14.md>2020-06-14</a>, I tried using the xgboost caching with the scikitlearn api approach. In the meantime I also ran into a fun issue where an xgboost model was trained on data without a particular output class , with only <code>53</code> classes in fact and would produce predict probability vectors of length <code>53</code> instead of <code>54</code>, so I ended up having to make sure to better shuffle the data to make sure when trying to use less data (when using cross validation for instance) that all of the output classes are accounted for, without having a more direct way of telling Xgboost what the output classes should be.</p><p>Also another fun Tensorflow comparison was I got <code>XGBoostError: need to call fit or load_model beforehand</code> when trying to call predict on a bare model that had not undergone training. Whereas with Tensorflow, I experienced in a previous project that this is absolutely fine, because you simply have a fully formed neural network with some randomly (or otherwise) initialized weights. But with xgboost, or at least the particular implementation I was using, this is not possible, because there is no notion of a base model.</p><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-14.md#trying-out-that-model-save>Here</a>, I tried cutting up the training data like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>clf <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>XGBClassifier()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workdir <span style=color:#f92672>=</span> fu<span style=color:#f92672>.</span>make_work_dir(); print(workdir)
</span></span><span style=display:flex><span>fu<span style=color:#f92672>.</span>log(workdir, <span style=color:#e6db74>&#39;Starting&#39;</span>)
</span></span><span style=display:flex><span>prev_model <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>loss_vec <span style=color:#f92672>=</span> []; acc_vec <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, part <span style=color:#f92672>in</span> enumerate(tqdm(parts)):
</span></span><span style=display:flex><span>    clf<span style=color:#f92672>.</span>fit(X_transformed[part], y_enc[part], xgb_model<span style=color:#f92672>=</span>prev_model)
</span></span><span style=display:flex><span>    fu<span style=color:#f92672>.</span>log(workdir, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>] Done fit&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    prev_model <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>workdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/model.xg&#39;</span>
</span></span><span style=display:flex><span>    clf<span style=color:#f92672>.</span>save_model(prev_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    y_prob_vec <span style=color:#f92672>=</span> clf<span style=color:#f92672>.</span>predict_proba(X_test_transformed)
</span></span><span style=display:flex><span>    fu<span style=color:#f92672>.</span>log(workdir, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>] Done predict_proba&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> fu<span style=color:#f92672>.</span>big_logloss(y_test_enc, y_prob_vec, labels)
</span></span><span style=display:flex><span>    fu<span style=color:#f92672>.</span>log(workdir, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>] Done big_logloss, loss=</span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>}</span><span style=color:#e6db74>.&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss_vec<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    acc <span style=color:#f92672>=</span> accuracy_score(y_test_enc, np<span style=color:#f92672>.</span>argmax(y_prob_vec, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    acc_vec<span style=color:#f92672>.</span>append(acc)
</span></span><span style=display:flex><span>    fu<span style=color:#f92672>.</span>log(workdir, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;[</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>] Done accuracy, acc=</span><span style=color:#e6db74>{</span>acc<span style=color:#e6db74>}</span><span style=color:#e6db74>.&#39;</span>)
</span></span></code></pre></div><p>to see if the scikit learn API can allow saving and restoring previously trained models and continuing, with the <code>fit(X, xgb_model=prev_model)</code> syntax, but the output performance data was basically random indicating to me that the <code>fit</code> was starting from scratch each time.</p><p>Here, below, is a plot of accuracy after multiple epochs, just to visually show the lack of any progression. (This plot is from a similar experiment in my <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-16.md>2020-06-16 notebook</a> ).</p><p>So basically I gave up on this approach for using xgboost.</p><p>Also in that notebook, I found I was oddly getting <code>0</code> logloss during some experimentation because I had like in this toy example below, been specifying labels to the <code>log_loss</code> func, not matching the actual <code>y_true</code> data (which is the first arg).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a, b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]), np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                                      [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>                                      [<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]])
</span></span><span style=display:flex><span>print(log_loss(a, b, labels<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;a&#39;</span>, <span style=color:#e6db74>&#39;b&#39;</span>, <span style=color:#e6db74>&#39;c&#39;</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># =&gt; 0.0</span>
</span></span></code></pre></div><h4 id=2020-06-19>2020-06-19<a hidden class=anchor aria-hidden=true href=#2020-06-19>#</a></h4><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-19.md>Here</a> , because a lot of the test set prediction for model evaluation takes time I ended up creating a mini parallelization func. I verified that it was producing roughly the same validation and the time to execute was less.</p><p>I also wrote about how I had needed to use less data to avoid crashing, using the pandas <code>sample()</code> func like,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tripsdf <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>datadir<span style=color:#e6db74>}</span><span style=color:#e6db74>/2013-07 - Citi Bike trip data.csv&#39;</span>
</span></span><span style=display:flex><span>                     )<span style=color:#f92672>.</span>sample(frac<span style=color:#f92672>=</span><span style=color:#ae81ff>0.017</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span></code></pre></div><p>but that it would be better to build a more balanced dataset instead of just random sampling.</p><h4 id=a-rapid-fire-list-of-some-additional-experiments>A rapid fire list of some additional experiments<a hidden class=anchor aria-hidden=true href=#a-rapid-fire-list-of-some-additional-experiments>#</a></h4><ul><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-21.md>Here</a>, I had another training attempt using the so called &ldquo;cache data&rdquo; with functional api. (But finding especially <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-21.md#log-dump-w-num_round100>here</a> that the <code>max_depth</code> was not changing so likely no learning was happening).</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-23.md>Here</a> I aadded new features here for a &lsquo;v2&rsquo; dataset, including <code>weekday</code>. and <code>time_of_day</code>. Added this in a new module, <code>fresh/preproc/v2.py</code>.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-24.md>Here</a> , more memory struggles (especially since I added more data).</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-26.md>Here</a>, describing that after lots of crashing, starting to use numpy append mode, <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-26.md#2020-06-27>here</a> , to allow for doing preprocessing in chunks. And starting to look at target class distribution.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-27.md>Here</a> I&rsquo;m discovering that it is quite possible that this caching is only allowed w/ the &ldquo;libsvm&rdquo; format!</li><li>And <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-28.md>here</a>, I see hmm it is kind of weird, that with cache, without&mldr; producing different <code>feature_names</code> ? more kernel dying!</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-28-take2.md>Here</a> , bigger box?</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-29.md>Here</a> , class distribution for size reduction and <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-29.md#2020-07-01>dataset rebalancing</a>.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-03-aws.md>Here</a> ok I took the result from the balancing/shrinking concept from the &ldquo;2020-06-29.ipynb&rdquo; notebook and tried to use less data see if we can avoid a kernel crash that happened in &ldquo;2020-06-28-take2.ipynb&rdquo;.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-04-aws.md>Here</a> I wanted to do a quick recalc of yesterday&rsquo;s model using the sklearn API Again.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-05-aws-two.md>Here</a>, more rebalancing.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-05.md>Here</a>.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-08-aws.md>Here</a>, end result: compared with &ldquo;2020-07-03-aws.md&rdquo; , I am not really seeing much of a difference. the balanced test accuracy perhaps looks every so slightly better but probably not significantly.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-09-aws.md>Here</a> , change up <code>'subsample'</code> and <code>'max_depth'</code>, measuring logloss, accuracy and balanced_accuracy , there are some noticable changes in logloss, but overall the changes are probably not significant.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-10-aws.md>Here</a>.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md>Here</a>.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-16-local.md>Here</a>.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-26-feature-importances.md>Here</a>.</li></ul><h3 id=multi-class-classification-notes>Multi class classification notes<a hidden class=anchor aria-hidden=true href=#multi-class-classification-notes>#</a></h3><ul><li><a href=/post/2020-07-13-multi-multi-class/>Notes on multi class classification</a></li></ul><h3 id=understanding-tuning-results>Understanding tuning results<a hidden class=anchor aria-hidden=true href=#understanding-tuning-results>#</a></h3><p><a href=/post/2020-07-24-understanding-tuning-results/>hyper parameter tuning and train/test acc</a></p><h3 id=previously-vs-this-time>Previously vs This time<a hidden class=anchor aria-hidden=true href=#previously-vs-this-time>#</a></h3><ul><li>Last time around, I segmented the starting data into <code>24</code> hour-long segments. This time, I segmented time into only <code>5</code> bins to make the model slightly more generalizable.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># time_of_day</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>: <span style=color:#ae81ff>6</span><span style=color:#f92672>-</span><span style=color:#ae81ff>9</span>,
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>: <span style=color:#ae81ff>10</span><span style=color:#f92672>-</span><span style=color:#ae81ff>13</span>,
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>: <span style=color:#ae81ff>14</span><span style=color:#f92672>-</span><span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>: <span style=color:#ae81ff>17</span><span style=color:#f92672>-</span><span style=color:#ae81ff>21</span>,
</span></span><span style=display:flex><span><span style=color:#ae81ff>4</span>: <span style=color:#ae81ff>22</span><span style=color:#f92672>-</span><span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span><span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>
</span></span></code></pre></div><ul><li><p>Actually after picking these bins arbitrarily, I ended up also looking at the time of day histograms <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-22-features-v3.md>here</a> and the peaks look close to what I had as a mental model in my mind. It might be interesting try some other bins at some point later.</p></li><li><p>One other new feature this time is the binary <code>weekday</code> feature, specifying weekday vs weekend.</p></li><li><p>The starting neighborhood one hot encoded was kept as an input.</p></li><li><p>Also last time around, the main model was a Random Forest classifier, but using XGBoost this time.</p></li></ul><h3 id=model-highlights>Model Highlights<a hidden class=anchor aria-hidden=true href=#model-highlights>#</a></h3><p>The top model has these stats&mldr;</p><pre tabindex=0><code>(pandars3) $ docker run -p 8889:8889 -p 8080:8080 -i -t -v $(pwd):/opt/program \
             -v ${MY_LOCAL_DATA_DIRECTORY}:/opt/data \
             -v   ~/Downloads:/opt/downloads \
             -v  $(pwd)/artifacts/2020-08-19T144654Z:/opt/ml \
             citibike-learn:latest \
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> fresh.predict_utils <span style=color:#66d9ef>as</span> fpu
</span></span><span style=display:flex><span>bundle <span style=color:#f92672>=</span> fpu<span style=color:#f92672>.</span>load_bundle_in_docker()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>In [<span style=color:#ae81ff>7</span>]: bundle[<span style=color:#e6db74>&#39;model_bundle&#39;</span>][<span style=color:#e6db74>&#39;bundle&#39;</span>][<span style=color:#e6db74>&#39;validation_metrics&#39;</span>]                                                                 
</span></span><span style=display:flex><span>Out[<span style=color:#ae81ff>7</span>]:
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;accuracy&#39;</span>: <span style=color:#ae81ff>0.12171455130090014</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;balanced_accuracy&#39;</span>: <span style=color:#ae81ff>0.10451301995291779</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;confusion&#39;</span>: array([[<span style=color:#ae81ff>415</span>,  <span style=color:#ae81ff>64</span>,   <span style=color:#ae81ff>4</span>, <span style=color:#f92672>...</span>,   <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>103</span>,  <span style=color:#ae81ff>69</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>56</span>, <span style=color:#ae81ff>541</span>,   <span style=color:#ae81ff>4</span>, <span style=color:#f92672>...</span>,   <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>130</span>,  <span style=color:#ae81ff>27</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>23</span>,  <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>136</span>, <span style=color:#f92672>...</span>,   <span style=color:#ae81ff>0</span>,  <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>130</span>],
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>,
</span></span><span style=display:flex><span>        [  <span style=color:#ae81ff>2</span>,   <span style=color:#ae81ff>0</span>,   <span style=color:#ae81ff>2</span>, <span style=color:#f92672>...</span>,   <span style=color:#ae81ff>1</span>,   <span style=color:#ae81ff>3</span>,  <span style=color:#ae81ff>36</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>151</span>, <span style=color:#ae81ff>222</span>,   <span style=color:#ae81ff>3</span>, <span style=color:#f92672>...</span>,   <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>260</span>,  <span style=color:#ae81ff>35</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>84</span>,  <span style=color:#ae81ff>25</span>,  <span style=color:#ae81ff>46</span>, <span style=color:#f92672>...</span>,   <span style=color:#ae81ff>0</span>,  <span style=color:#ae81ff>29</span>, <span style=color:#ae81ff>861</span>]]),
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;logloss&#39;</span>: <span style=color:#ae81ff>3.4335361255637977</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;test&#39;</span>: <span style=color:#e6db74>&#39;/home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-08T143732Z/test.libsvm&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;karea&#39;</span>: <span style=color:#ae81ff>0.760827309330065</span>}
</span></span></code></pre></div><ul><li>More on the &ldquo;k-area&rdquo; metric is <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-20-karea-worst.md>here</a></li></ul><h4 id=top-models-top-fscore-features>Top Model&rsquo;s Top Fscore features<a hidden class=anchor aria-hidden=true href=#top-models-top-fscore-features>#</a></h4><p>Extracting from <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-21-look-at-model-plot.md>this notebook</a> ,</p><pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre><p></p><p>And it can be interesting to look at a random tree from xgboost too sometimes, again extracting from the above mentioned notebook.</p><h4 id=gluing-everything-together>Gluing everything together<a hidden class=anchor aria-hidden=true href=#gluing-everything-together>#</a></h4><p>In <a href=https://github.com/namoopsoo/learn-citibike/blob/2020-oct/notes/2020-08-25-glue.md>this notebook</a>, I face the challenges of taking the model from bundle to a demo site. There were a lot of challenges involved. My concept was to use the Google Static Map API to display the top neighborhood predictions. Hitting this API properly did take a little bit of time, but it was not that bad. And later on, I updated the whole AWS Lambda approach so the lambda function calls the API with the result from the dockerized SageMaker served model.</p><p>Admittedly, the most time consuming part was figuring out the API Gateway Cognito &ldquo;Unauthenticated Authentication&rdquo;. AWS has this Cognito service which manages user/password based authentication for you but it also lets you use Anonymous authentication. But there must be a lot of degrees of freedom in how this is used, because I could not find good documentation on how to set this up properly for my usecase at all.</p><p>I had used API Gateway for authentication through CORS in the past and I recalled a bit of nuance that for example you may have setup CORS properly for <code>200</code> status codes, but if your program crashes with a <code>500</code> then your browser will scream about a CORS error, because the response is not returning the expected <code>allow-origin-blah</code> header. In the past this had taken me a while to figure out, but now I luckily had that knowledge in my back pocket. In any case, it is worth it for the serverless approach.</p><h4 id=automation-made-the-process-very-convenient>Automation made the process very convenient<a hidden class=anchor aria-hidden=true href=#automation-made-the-process-very-convenient>#</a></h4><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-local-docker-notes.md>https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-06-07-local-docker-notes.md</a></p><p>I also described my build process in the earlier mentioned <a href=https://github.com/namoopsoo/learn-citibike/blob/2020-oct/notes/2020-08-25-glue.md>glue notes</a> too. With so many tweaks to the python side, the model and the javascript side, being able to build and deploy with quick <code>make</code> style commands made everything faster. I document some of these <a href=https://github.com/namoopsoo/learn-citibike/blob/master/docs/common_tasks.md>here</a> too.</p><h4 id=quick-pearsons-chi-squared-independence-test>quick pearson&rsquo;s chi squared independence test<a hidden class=anchor aria-hidden=true href=#quick-pearsons-chi-squared-independence-test>#</a></h4><p>quick pearson&rsquo;s chi squared independence test
<a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-05.md>https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-05.md</a></p><h4 id=looking-at-hyperparameter-tuning-results>Looking at hyperparameter tuning results<a hidden class=anchor aria-hidden=true href=#looking-at-hyperparameter-tuning-results>#</a></h4><ul><li><p>( EDIT: After writing the below section, I realized I had already <a href=/post/2020-07-24-understanding-tuning-results/>here earlier, on 2020-07-24</a> , described some of these results already haha. Doing the work twice, forgetting what I had done. )</p></li><li><p>I spent a bit of time on hyper parameter tuning, looking at the results, fixing some parameters two focus on two others at a time.</p></li><li><p>So per <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md#looking-at-num_round-fundamentally>here</a> ,
the <code>num_round</code> as expected improves logloss,</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>keep_fixed <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bylevel&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bynode&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bytree&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;subsample&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;num_round&#39;</span>: <span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>col1, col2, metric_col <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;max_depth&#39;</span>, <span style=color:#e6db74>&#39;num_round&#39;</span>, <span style=color:#e6db74>&#39;logloss&#39;</span>
</span></span><span style=display:flex><span>fp<span style=color:#f92672>.</span>compare_tuning(df, feature_col_1<span style=color:#f92672>=</span>col1,
</span></span><span style=display:flex><span>             feature_col_2<span style=color:#f92672>=</span>col2,
</span></span><span style=display:flex><span>             metric_col<span style=color:#f92672>=</span>metric_col,
</span></span><span style=display:flex><span>             keep_fixed<span style=color:#f92672>=</span>fvu<span style=color:#f92672>.</span>without(
</span></span><span style=display:flex><span>                 keep_fixed, keys<span style=color:#f92672>=</span>[col1, col2]))
</span></span></code></pre></div><p>And maybe this is good as a sanity check, but more rounds take more time, ( <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md#walltime>per here</a> )</p><p>And from <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md#look-at-walltime-and-learning-rate-again>here</a> it was interesting to see that walltime is stable mostly when it comes to learning rate except sometimes&mldr;</p><p>And per <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md#also-learning-rate-vs-acc>here</a> at least per the fixed parameters, the <code>0.1</code> learning rate is better than the <code>0.01</code> learning rate.</p><p>And per <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md#look-at-subsample-w-different-rounds>here</a> , <code>subsample</code> row sampling is just not appearing to be influencing accuracy.</p><p>And per <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md#hmm-colsample_bylevel>here</a> , the random column sampling may have just removed the good columns</p><h4 id=train-and-test-accuracy-comparison>Train and test accuracy comparison<a hidden class=anchor aria-hidden=true href=#train-and-test-accuracy-comparison>#</a></h4><ul><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-16-local.md>Here</a> , I took all of my <code>1000+</code> models from earlier, (which were on S3 so I had to copy them locally for convenience) and calculated accuracy, logloss and karea metrics for the training data, in order to be able to get learning curves to understand underfitting/overfitting.</li><li>Just showing ihere an example run for one model&mldr;</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># As per https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-10-aws.md</span>
</span></span><span style=display:flex><span><span style=color:#75715e># the data dir was artifacts/2020-07-08T143732Z  ... going to re-create that locally too</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span>datadir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/opt/program/artifacts/2020-07-08T143732Z&#39;</span>
</span></span><span style=display:flex><span>artifactsdir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/opt/program/artifacts/2020-07-10T135910Z&#39;</span>
</span></span><span style=display:flex><span>train_results <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_loc <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>datadir<span style=color:#e6db74>}</span><span style=color:#e6db74>/train.libsvm&#39;</span>
</span></span><span style=display:flex><span>dtrain <span style=color:#f92672>=</span> xgb<span style=color:#f92672>.</span>DMatrix(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>train_loc<span style=color:#e6db74>}</span><span style=color:#e6db74>?format=libsvm&#39;</span>)
</span></span><span style=display:flex><span>actuals <span style=color:#f92672>=</span> dtrain<span style=color:#f92672>.</span>get_label()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;evaluate using &#39;</span>, train_loc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_data <span style=color:#f92672>=</span> load_svmlight_file(train_loc)
</span></span><span style=display:flex><span>X_train <span style=color:#f92672>=</span> train_data[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>toarray()
</span></span><span style=display:flex><span>y_train <span style=color:#f92672>=</span> train_data[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span><span style=color:#75715e>########</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Try one</span>
</span></span><span style=display:flex><span>i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>bundle <span style=color:#f92672>=</span> joblib<span style=color:#f92672>.</span>load(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>artifactsdir<span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>_bundle_with_metrics.joblib&#39;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> bundle[<span style=color:#e6db74>&#39;xgb_model&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_prob_vec <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(dtrain)
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(y_prob_vec, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logloss <span style=color:#f92672>=</span> fu<span style=color:#f92672>.</span>big_logloss(actuals, y_prob<span style=color:#f92672>=</span>y_prob_vec,
</span></span><span style=display:flex><span>                         labels<span style=color:#f92672>=</span> list(range(<span style=color:#ae81ff>54</span>)))
</span></span><span style=display:flex><span>acc <span style=color:#f92672>=</span> accuracy_score(actuals, predictions)
</span></span><span style=display:flex><span>balanced_acc <span style=color:#f92672>=</span> balanced_accuracy_score(actuals, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>correct_kth, karea <span style=color:#f92672>=</span> fm<span style=color:#f92672>.</span>kth_area(y_train, y_prob_vec,
</span></span><span style=display:flex><span>        num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>54</span>)
</span></span></code></pre></div><pre tabindex=0><code>CPU times: user 31.3 s, sys: 110 ms, total: 31.4 s
Wall time: 21.4 s
</code></pre><pre tabindex=0><code>acc, balanced_acc, karea
</code></pre><pre tabindex=0><code>(0.05276320740101365,
 0.03727538888502701,
 0.6435250908504123)
</code></pre><p>The whole thing, took about <code>10 hours</code> as measured by the final line from <code>tqdm</code>,</p><pre tabindex=0><code>100%|█████████▉| 1052/1054 [10:00:57&lt;01:08, 34.27s/it]
</code></pre><h5 id=putting-that-together>Putting that together,<a hidden class=anchor aria-hidden=true href=#putting-that-together>#</a></h5><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-16-local.md#2020-07-23>Here</a> putting that together ..</p><p>training and test accuracy are pretty consistently close, with training accuracy being slightly better as expected. So there is no evidence overall of overfitting. But perhaps some evidence of underfitting .</p><p>The first thing I just looked at the parameters fixed by just what happened to be the first model built, so pretty arbitrary and comparing over the number of rounds. The results not unexpected not showing much learning happening.</p><p>Then I took the model with the best test accuracy results,</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>best_params <span style=color:#f92672>=</span> dict(alldf<span style=color:#f92672>.</span>sort_values(by<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;acc&#39;</span>)<span style=color:#f92672>.</span>iloc[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>best_params
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;train_acc&#39;</span>: <span style=color:#ae81ff>0.12693459297270465</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_balanced_acc&#39;</span>: <span style=color:#ae81ff>0.11012147901980039</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;i&#39;</span>: <span style=color:#ae81ff>755</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_logloss&#39;</span>: <span style=color:#ae81ff>3.4301962566050057</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;train_karea&#39;</span>: <span style=color:#ae81ff>0.76345208497788</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_depth&#39;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;objective&#39;</span>: <span style=color:#e6db74>&#39;multi:softprob&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;num_class&#39;</span>: <span style=color:#ae81ff>54</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;base_score&#39;</span>: <span style=color:#ae81ff>0.5</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;booster&#39;</span>: <span style=color:#e6db74>&#39;gbtree&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bylevel&#39;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bynode&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;colsample_bytree&#39;</span>: <span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;gamma&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;max_delta_step&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;min_child_weight&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;random_state&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;reg_alpha&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;reg_lambda&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;scale_pos_weight&#39;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;seed&#39;</span>: <span style=color:#ae81ff>42</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;subsample&#39;</span>: <span style=color:#ae81ff>0.4</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;verbosity&#39;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;acc&#39;</span>: <span style=color:#ae81ff>0.12304248437307332</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;balanced_acc&#39;</span>: <span style=color:#ae81ff>0.10551953202851949</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;logloss&#39;</span>: <span style=color:#ae81ff>3.4480742986458592</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;walltime&#39;</span>: <span style=color:#ae81ff>1918.593945</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;karea&#39;</span>: <span style=color:#ae81ff>0.75845582462009</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;num_round&#39;</span>: <span style=color:#ae81ff>100</span>}
</span></span></code></pre></div><p>And plotted all the train/test metrics across rounds, and this figure definitely shows learning happening . Very exciting!</p><h5 id=also-looked-for-the-biggest-gap-between-traintest-accuracy>Also looked for the biggest gap between train/test accuracy<a hidden class=anchor aria-hidden=true href=#also-looked-for-the-biggest-gap-between-traintest-accuracy>#</a></h5><ul><li>And per the below, interestingly, it&rsquo;s seeming like the biggest train/test gap is very small..</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>alldf[<span style=color:#e6db74>&#39;train_test_acc_delta&#39;</span>] <span style=color:#f92672>=</span> alldf<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> x: abs(x[<span style=color:#e6db74>&#39;acc&#39;</span>] <span style=color:#f92672>-</span> x[<span style=color:#e6db74>&#39;train_acc&#39;</span>]), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>alldf<span style=color:#f92672>.</span>sort_values(by<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;train_test_acc_delta&#39;</span>)<span style=color:#f92672>.</span>iloc[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_acc                     <span style=color:#ae81ff>0.128123</span>
</span></span><span style=display:flex><span>train_balanced_acc            <span style=color:#ae81ff>0.111239</span>
</span></span><span style=display:flex><span>i                                 <span style=color:#ae81ff>1241</span>
</span></span><span style=display:flex><span>train_logloss                  <span style=color:#ae81ff>3.40954</span>
</span></span><span style=display:flex><span>train_karea                   <span style=color:#ae81ff>0.767823</span>
</span></span><span style=display:flex><span>max_depth                            <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>learning_rate                      <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>objective               multi:softprob
</span></span><span style=display:flex><span>num_class                           <span style=color:#ae81ff>54</span>
</span></span><span style=display:flex><span>base_score                         <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>booster                         gbtree
</span></span><span style=display:flex><span>colsample_bylevel                    <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>colsample_bynode                     <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>colsample_bytree                     <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>gamma                                <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>max_delta_step                       <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>min_child_weight                     <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>random_state                         <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>reg_alpha                            <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>reg_lambda                           <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>scale_pos_weight                     <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>seed                                <span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>subsample                          <span style=color:#ae81ff>0.4</span>
</span></span><span style=display:flex><span>verbosity                            <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>acc                            <span style=color:#ae81ff>0.12253</span>
</span></span><span style=display:flex><span>balanced_acc                  <span style=color:#ae81ff>0.104698</span>
</span></span><span style=display:flex><span>logloss                        <span style=color:#ae81ff>3.43584</span>
</span></span><span style=display:flex><span>walltime                       <span style=color:#ae81ff>2327.88</span>
</span></span><span style=display:flex><span>karea                         <span style=color:#ae81ff>0.760578</span>
</span></span><span style=display:flex><span>num_round                          <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>train_test_acc_delta        <span style=color:#ae81ff>0.00559313</span>
</span></span><span style=display:flex><span>Name: <span style=color:#ae81ff>1242</span>, dtype: object
</span></span></code></pre></div><h4 id=initial-time-of-day-look>Initial time of day look<a hidden class=anchor aria-hidden=true href=#initial-time-of-day-look>#</a></h4><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-05-woe.md>https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-05-woe.md</a></p><p>super</p><h4 id=discuss>discuss<a hidden class=anchor aria-hidden=true href=#discuss>#</a></h4><p><a href=https://github.com/namoopsoo/learn-citibike/blob/2020-oct/notes/2020-08-25-glue.md>https://github.com/namoopsoo/learn-citibike/blob/2020-oct/notes/2020-08-25-glue.md</a></p><h4 id=feature-importances>Feature importances<a hidden class=anchor aria-hidden=true href=#feature-importances>#</a></h4><p><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-26-feature-importances.md>notes</a></p><p>From the many hyper parameter tuning jobs I had run, I used the xgboost feature importance functionality to dump the perceived feature importances for all of the models. And in the <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-26-feature-importances.md#2020-08-02>notes</a> I plotted feature importances against accuracy for all of them.</p><p>For example, here are some of the more interesting plots,</p><p>The point here is that I had one hot encoded all of the starting neighborhoods. I am hoping of course that if a particular starting location looks important, then that should mean it is important in discriminating where you go next. Meaning it narrows down where you go. On the other hand, if your starting location is boring then that should mean it is more like a hub and there are too many destinations for the start along to be a helpful feature.</p><p>In the above plots, there is a wide range of models and they are showing that for some reason high importance does not necessarily mean high accuracy. If anything, I want to make a mental note that maybe these kinds of plots can be indicators of something wrong and some kind of under-fitting in particular. Or weak fitting at least. And one of the other scenarios is that fitting is weak, because there is not enough entropy in the data available to yield helpful discrimination with a model. No matter how well XGBoost can extract information, if the raw material does not have any diamonds, then we will be stuck.</p><p>The other thought is that there is an overfitting danger around not just an imbalance in the target variable (aka the destination neighborhood) but an imbalance in the starting locations too. This is why it would be really interesting to also look at the entropy of the multiclass outputs for signs of clear uncertainty for specific examples. Putting a pin on this <a href=#follow-on>in the follow-on section</a></p><p>The time of day features look like this, below, but again, this is not to say that these views represent the full story.</p><p>Thinking about this abit more in retrospect, these particular representations are probably not very meaningful to look at because if there are trends they need to be looked at &ldquo;localizing&rdquo; or &ldquo;fixing&rdquo; some of the parameters. Because these representations are all over the place but the relationship may still be hidden inside.</p><p>I think one of the top <a href=#follow-on>follow ons</a> has to be to find better time of day splits. I chose my time of day splits based on a model in my head, and so there is definitely some room for exploration here.</p><h3 id=follow-on>Follow On<a hidden class=anchor aria-hidden=true href=#follow-on>#</a></h3><h4 id=time-of-day-more-splitting-exploration>Time of day more splitting exploration<a hidden class=anchor aria-hidden=true href=#time-of-day-more-splitting-exploration>#</a></h4><p>Find some more interesting techniques to try out different segmentation of the time of day. ( I&rsquo;m thinking &ldquo;adaptive binning " as described <a href=https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b>here</a> )</p><h4 id=better-understanding-of-model-uncertainty>Better understanding of model uncertainty<a hidden class=anchor aria-hidden=true href=#better-understanding-of-model-uncertainty>#</a></h4><ul><li>As discussed in the <a href=#feature-importances>feature importances section</a>, it would be really interesting to take the test dataset and for the output probability vectors of all of the examples, to calculate the multi-class entropy, to see if indeed high uncertainty is associated with worse correctness rank (<code>kth accuracy</code> and <code>karea</code> in other terminology I have been using).</li><li>Of course this is really tricky from an <em>Active Learning</em> point of view, because I can see a scenario where adding more training examples around the cases which have a higher uncertainty may improve the accuracy for the related test examples , but that feels like there is a risk of overfitting to the test set. In any case, however, if the live data is not reflective of the training/test data distributions ( covariate shift ), then refreshing the model is important.</li></ul><h3 id=some-lessons-for-the-future>Some lessons for the future<a hidden class=anchor aria-hidden=true href=#some-lessons-for-the-future>#</a></h3><h4 id=approach-to-training-and-artifacts>Approach to training and artifacts<a hidden class=anchor aria-hidden=true href=#approach-to-training-and-artifacts>#</a></h4><p>Training and hyperparameter tuning takes a long time. Dumping artifacts along the way, including models and results (for example using json), is helpful to allow another notebook to actively monitor the results as they are running. And doing this is also helpful because notebooks that run long experiments can sometimes crash. So it is nice to save intermediary results.</p><h4 id=notebooks>Notebooks<a hidden class=anchor aria-hidden=true href=#notebooks>#</a></h4><p>I like the concept of keeping a daily notebook, because keeping several experiments in one notebook can risk running out of memory and sometimes it is difficult to load large notebooks on github, even if they are turned into markdown, if there are a lot of images.</p><h4 id=write-sooner-rather-than-later>Write sooner rather than later<a hidden class=anchor aria-hidden=true href=#write-sooner-rather-than-later>#</a></h4><ul><li>Although it is tempting to just keep trying more and more experiments and to keep iterating the frontier forward, I think a difficult lesson to learn is that putting together the results of the day or the week takes much more time when done weeks or months later. I think summarizing and discussing your results as you go along is way more useful.</li><li>But if you do wait, another idae is to just create a notebook table of concents as I am doing below, as a way of having quick chronological reference about the work that was done.</li></ul><h3 id=notebooks-toc>Notebooks TOC<a hidden class=anchor aria-hidden=true href=#notebooks-toc>#</a></h3><ul><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-10-aws.md>2020-07-10</a> , like &ldquo;2020-07-09-aws&rdquo; , another hyperparameter tuning round here. <code>max_depth</code> , <code>subsample</code> , <code>colsample_bytree</code> .</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-11-local.md>2020-07-11</a> , here I plot a bunch of results (on my laptop) , from the <em>2020-07-10</em> notebook running on aws.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-16-local.md>2020-07-16-local.md</a> , recalculataing train metrics for the ~1250 or so models from the hyper parameter tuning session</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-07-26-feature-importances.md>2020-07-26-feature-importances.md</a> , looking at feature importances , reverse engineering my <code>proc_bundle</code> , to get back my list of feature names, which I had not done originally. Initially trying <code>model.get_score()</code> , dumping from each model. This actually took <code>3.5 hours</code>. I plotted features and accuracy in a few ways to try to gauge features being more oftan associated with high accuracy models. Plotting the correlation of feature importance and acuracy. I think this was not a super useful method. Ultimately, <a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-21-look-at-model-plot.md>the fscore approach was better</a></li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-05-woe.md>2020-08-05-woe.md</a> , EDA on the time_of_day feature, visual histogram comparisons. Not the most fruitful however.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-17-bundle-glue.md>2020-08-17-bundle-glue.md</a></li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-18-glue.md>2020-08-18-glue.md</a> some reverse engineering to repurpose my preprocessor bundle for live etraffic. And combining preprocessor and model to make a joblib bundle with everything in it. And drafint a <code>full_predict</code> method.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-22-static-map-api.md>2020-08-22-static-map-api.md</a> getting setup with the Google Static Map API . Very nice.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-08-25-glue.md>2020-08-25-glue.md</a> Docker entry code end to end live code. And building the code for the lambda that calls Docker. Unfortunately xgboost does not fit on the lambda. And oops lambda cannot write to the file system. And working through the new API Gateway authentication methods here. I wrote some support code for quick lambda deployment because I ended up using many iterations to get this right. Content type weirdness. Javascript plus cognito. This was not documented very well, so a lot of blundering here. Can&rsquo;t believe I finally made all of this work. This was insane.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-20-karea-worst.md>2020-10-20-karea-worst.md</a> K Area worst case scenario.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-21-look-at-model-plot.md>2020-10-21-look-at-model-plot.md</a> looking at Fscore and as well as plotting individual trees with graphviz . Also some interesting issues with versions of xgboost in docker and lack of backward compatibility.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-21-uncertainty-xgboost.md>2020-10-21-uncertainty-xgboost.md</a> this is mainly just a footnote about the idea around measuring uncertainty in xgboost. But this is likely not super reliable.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-22-features-v3.md>2020-10-22-features-v3.md</a> Take a quick look at time of day distribution</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-23-quick-new-v3-proc-bundle.md>2020-10-23-quick-new-v3-proc-bundle.md</a> one more model iteration using new features.</li><li><a href=https://github.com/namoopsoo/learn-citibike/blob/master/notes/2020-10-25.md>2020-10-25.md</a> evaluate new v3. But although not yet done any tuning, so far this does not seem significantly better, with karea <code>0.761</code> versus earlier best <code>0.760</code> karea.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://michal.piekarczyk.xyz/post/2020-10-22-features-v3/><span class=title>« Prev</span><br><span>Visualizing time of day and birth year</span></a>
<a class=next href=https://michal.piekarczyk.xyz/post/2020-07-24-understanding-tuning-results/><span class=title>Next »</span><br><span>Understanding Tuning Results</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://michal.piekarczyk.xyz/>michal.piekarczyk.xyz</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>